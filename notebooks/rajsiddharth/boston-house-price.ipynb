{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Regression Machine Learning Case Study Project"},{"metadata":{},"cell_type":"markdown","source":"# Problem Definition\n\nFor this project we will investigate the Boston House Price dataset. Each record in the database\ndescribes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan\nStatistical Area (SMSA) in 1970. The attributes are defined as follows (taken from the UCI\nMachine Learning Repository1):\n    \n1. CRIM: per capita crime rate by town\n    \n2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n    \n3. INDUS: proportion of non-retail business acres per town\n    \n4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n    \n5. NOX: nitric oxides concentration (parts per 10 million)\n    \n6. RM: average number of rooms per dwelling\n    \n7. AGE: proportion of owner-occupied units built prior to 1940\n    \n8. DIS: weighted distances to five Boston employment centers\n    \n9. RAD: index of accessibility to radial highways\n    \n10. TAX: full-value property-tax rate per $10,000\n    \n11. PTRATIO: pupil-teacher ratio by town\n    \n12. B: 1000(Bk 􀀀 0:63)2 where Bk is the proportion of blacks by town\n    \n13. LSTAT: % lower status of the population\n    \n14. MEDV: Median value of owner-occupied homes in $1000s\n    \n    "},{"metadata":{},"cell_type":"markdown","source":"# Load the Dataset\nLet's start off by loading the libraries required for this project."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load libraries\nimport numpy\nfrom numpy import arange\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# repository website.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load dataset\nfilename = '../input/boston-house-prices/housing.csv'\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n'B', 'LSTAT', 'MEDV']\ndataset = read_csv(filename, delim_whitespace=True, names=names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyze Data\nWe can now take a closer look at our loaded data."},{"metadata":{},"cell_type":"markdown","source":"# Descriptive Statistics\nLet's start off by confirming the dimensions of the dataset, e.g. the number of rows and columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape\nprint(dataset.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's also look at the data types of each attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"# types\nprint(dataset.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We can see that all of the attributes are numeric, mostly real values (float) and some have\n# been interpreted as integers (int).","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's now take a peek at the first 20 rows of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# head\nprint(dataset.head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We can confirm that the scales for the attributes are all over the place because of the differing\n# units. We may benefit from some transforms later on.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's summarize the distribution of each attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"# descriptions\nset_option('precision', 3)\n#print(dataset.describe(percentiles=[.1,.2,.9]))\nprint(dataset.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We now have a better feeling for how different the attributes are. The min and max values\n# as well are the means vary a lot. We are likely going to get better results by rescaling the data\n# in some way.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now, let's now take a look at the correlation between all of the numeric attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation\nset_option('precision', 3)\ndataset.corr(method='pearson')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# This is interesting. We can see that many of the attributes have a strong correlation (e.g.\n> 0:70 or < 􀀀0:70). For example:\n    \n NOX and INDUS with 0.77.\n\n DIS and INDUS with -0.71.\n\n TAX and INDUS with 0.72.\n\n AGE and NOX with 0.73.\n\n DIS and NOX with -0.78.\n\nIt also looks like LSTAT has a good negative correlation with the output variable MEDV with\na value of -0.74."},{"metadata":{},"cell_type":"markdown","source":"# Data Visualizations"},{"metadata":{},"cell_type":"markdown","source":"# Unimodal Data Visualizations\nLet's look at visualizations of individual attributes. It is often useful to look at your data\nusing multiple different visualizations in order to spark ideas. Let's look at histograms of each\nattribute to get a sense of the data distributions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms\ndataset.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We can see that some attributes may have an exponential distribution, such as CRIM, ZN,\n# AGE and B. We can see that others may have a bimodal distribution such as RAD and TAX.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's look at the same distributions using density plots that smooth them out a bit."},{"metadata":{"trusted":true},"cell_type":"code","source":"# density\npyplot.figure(figsize = (10,12))\ndataset.plot(kind='density', subplots=True, layout=(4,4), sharex=False, legend=True,\nfontsize=1)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# This perhaps adds more evidence to our suspicion about possible exponential and bimodal\n# distributions. It also looks like NOX, RM and LSTAT may be skewed Gaussian distributions, which\n# might be helpful later with transforms.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's look at the data with box and whisker plots of each attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"# box and whisker plots\n\ndataset.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False,\nfontsize=8)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"#  This helps point out the skew in many distributions so much so that data looks like outliers\n# (e.g. beyond the whisker of the plots).","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multimodal Data Visualizations\nLet's look at some visualizations of the interactions between variables. The best place to start\nis a scatter plot matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"#drawing scatter plot and ,, increasing the size for better view\n#fig_size = pyplot.rcParams[\"figure.figsize\"]\n#fig_size[0]= 12\n#fig_size[1]= 9\n#pyplot.rcParams[\"figure.figsize\"]= fig_size\npyplot.figure(figsize=(12,9))\n\nscatter_matrix(dataset)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# We can see that some of the higher correlated attributes do show good structure in their\n# relationship. Not linear, but nice predictable curved relationships.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's also visualize the correlations between the attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation matrix\nfig = pyplot.figure(figsize=(12,9))\nax = fig.add_subplot(111)\ncax = ax.matshow(dataset.corr(), vmin=-1, vmax=1, interpolation='none')\nfig.colorbar(cax)\nticks = numpy.arange(0,14,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names)\nax.set_yticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# The dark YELLOW color shows positive correlation whereas the dark blue color shows negative\n# correlation. We can also see some dark YELLOW and dark blue that suggest candidates for removal\n# to better improve accuracy of models later on.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary of Ideas\nThere is a lot of structure in this dataset. We need to think about transforms that we could use\nlater to better expose the structure which in turn may improve modeling accuracy. So far it\nwould be worth trying:\n    \n Feature selection and removing the most correlated attributes.\n\n Normalizing the dataset to reduce the effect of differing scales.\n\n Standardizing the dataset to reduce the effects of differing distributions.\n\nWith lots of additional time I would also explore the possibility of binning (discretization)\nof the data. This can often improve accuracy for decision tree algorithms."},{"metadata":{},"cell_type":"markdown","source":"# Validation Dataset\nIt is a good idea to use a validation hold-out set. This is a sample of the data that we hold\nback from our analysis and modeling. We use it right at the end of our project to confirm the\naccuracy of our final model. It is a smoke test that we can use to see if we messed up and to\ngive us confidence on our estimates of accuracy on unseen data. We will use 80% of the dataset\nfor modeling and hold back 20% for validation."},{"metadata":{},"cell_type":"markdown","source":"# we have two variables that are categories- to dummy variables we are making it object "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['CHAS'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['CHAS'] = dataset['CHAS'].map({1:'tract' , 0:'notract' })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# variabel RAD has 9 categories so we atre making it 3 categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['RAD'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['RAD'] =dataset['RAD'].map({4:'4town' , 3:'4town',2:'4town',1:'4town' , 5:'8town', 6:'8town' ,7:'8town',8:'8town',24:'24town'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['RAD'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split-out validation dataset\n#array = dataset.values\n#X = array[:,0:13]\n#Y = array[:,13]\nvalidation_size = 0.20\nseed = 7\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# input"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset.drop('MEDV' , axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = dataset.MEDV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y[1:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n#X = pd.get_dummies(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split-out validation dataset\n#array = dataset.values\n#X = array[:,0:13]\n#Y = array[:,13]\n#validation_size = 0.20\n#seed = 7\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_ix = X.select_dtypes(include=['object', 'bool']).columns\nnum_ix = X.select_dtypes(include=['int64', 'float64']).columns\n# one hot encode cat features only\nct = ColumnTransformer([('c',OneHotEncoder(),cat_ix), ('n',MinMaxScaler(),num_ix)])\nX = ct.fit_transform(X)\n# label encode the target variable to have the classes 0 and 1\nX[0:5 ,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_DataFrame = pd.DataFrame(X)\nX_DataFrame.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_validation, Y_train, Y_validation = train_test_split(X, y,\ntest_size=validation_size, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate Algorithms: Baseline\nWe have no idea what algorithms will do well on this problem. Gut feel suggests regression\nalgorithms like Linear Regression and ElasticNet may do well. It is also possible that decision\ntrees and even SVM may do well. I have no idea. Let's design our test harness. We will use\n10-fold cross-validation. The dataset is not too small and this is a good standard test harness\nconfiguration. We will evaluate algorithms using the Mean Squared Error (MSE) metric. MSE\nwill give a gross idea of how wrong all predictions are (0 is perfect)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test options and evaluation metric\nnum_folds = 10\nseed = 7\nscoring = 'neg_mean_squared_error'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's create a baseline of performance on this problem and spot-check a number of different\nalgorithms. We will select a suite of different algorithms capable of working on this regression\nproblem. The six algorithms selected include:\n    \n Linear Algorithms: Linear Regression (LR), Lasso Regression (LASSO) and ElasticNet\n(EN).\n\n Nonlinear Algorithms: Classi\fcation and Regression Trees (CART), Support Vector\nRegression (SVR) and k-Nearest Neighbors (KNN)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings \nwarnings.filterwarnings('ignore')\n# Spot-Check Algorithms\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('SGD', SGDRegressor()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The algorithms all use default tuning parameters. Let's compare the algorithms. We will\n# display the mean and standard deviation of MSE for each algorithm as we calculate it and\n# collect the results for use later.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n kfold = KFold(n_splits=num_folds, random_state=seed)\n cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n results.append(cv_results)\n names.append(name)\n msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It looks like LR has the lowest MSE, followed closely by CART.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's take a look at the distribution of scores across all cross-validation folds by algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see similar distributions for the regression algorithms and perhaps a tighter \n# distribution of scores for CART.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The differing scales of the data is probably hurting the skill of all of the algorithms and\n# perhaps more so for SVR and KNN. In the next section we will look at running the same\n# algorithms using a standardized copy of the data.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate Algorithms: Standardization & Feature Engg\nWe suspect that the differing scales of the raw data may be negatively impacting the skill of\nsome of the algorithms. Let's evaluate the same algorithms with a standardized copy of the\ndataset. This is where the data is transformed such that each attribute has a mean value of\nzero and a standard deviation of 1. We also need to avoid data leakage when we transform the\ndata. A good way to avoid leakage is to use pipelines that standardize the data and build the\nmodel for each fold in the cross-validation test harness. That way we can get a fair estimation\nof how each model with standardized data might perform on unseen data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import RFE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearRegression()\nfeatures = []\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=6)))\nfeatures.append(('rfe', RFE(model, 3)))\nfeature_union = FeatureUnion(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardize the dataset\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('feature_union', feature_union),('LR',\n LinearRegression())])))\npipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('feature_union', feature_union) ,('LASSO',\n Lasso())])))\npipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('feature_union', feature_union),('EN',\n ElasticNet())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('feature_union', feature_union),\n                                         ('KNN',KNeighborsRegressor())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('feature_union', feature_union)\n                                          ,('CART',\n DecisionTreeRegressor())])))\npipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('feature_union', feature_union),\n                                         ('SVR', SVR())])))\npipelines.append(('ScaledSGD', Pipeline([('Scaler', StandardScaler()),('feature_union', feature_union),\n                                         ('SGD', SGDRegressor())])))\n\n\n\n\nresults = []\n\nnames = []\n\nfor name, model in pipelines:\n kfold = KFold(n_splits=num_folds, random_state=seed)\n cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n results.append(cv_results)\n names.append(name)\n msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's take a look at the distribution of the scores across the cross-validation folds."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see that KNN has both a tight distribution of error and has the lowest score.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Improve Results With Tuning\nWe know from the results in the previous section that KNN achieves good results on a scaled\nversion of the dataset. But can it do better. The default value for the number of neighbors in\nKNN is 7. We can use a grid search to try a set of dierent numbers of neighbors and see if\nwe can improve the score. The below example tries odd k values from 1 to 21, an arbitrary\nrange covering a known good value of 7. Each k value (n neighbors) is evaluated using 10-fold\ncross-validation on a standardized copy of the training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_union.fit(X_train ,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN Algorithm tuning\n#scaler = StandardScaler().fit(X_train)\n#rescaledX = scaler.transform(X_train)\n\n#k_values = numpy.array([1,3,5,7,9,11,13,15,17,19,21])\n#param_grid = dict(n_neighbors=k_values)\n#model = KNeighborsRegressor()\n#kfold = KFold(n_splits=num_folds, random_state=seed)\n#grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n#grid_result = grid.fit(rescaledX, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nLR = LinearRegression()\nfeatures = []\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=6)))\nfeatures.append(('rfe', RFE(LR, 3)))\nfeature_union = FeatureUnion(features)\n\nScaler =StandardScaler()\nKNN = KNeighborsRegressor()\n\n#pipelines = Pipeline(steps =[('KNN',KNeighborsRegressor())])\nmodel = Pipeline(steps=[('Scaler', Scaler),(\"Feature_Union\",feature_union), ('KNN', KNN)])\n\nk_values = numpy.array([1,3,5,7,9,11,13,15,17,19,21])\n#param_grid = dict(n_neighbors=k_values)\nparam_grid = dict(KNN__n_neighbors=k_values)\n#model = KNeighborsRegressor()\n\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n\ngrid_result = grid.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can display the mean and standard deviation scores as well as the best performing value\n# for k below.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# You can see that the best for k (n neighbors) is 3 providing a mean squared error of\n# -18.172137, the best so far.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.predict(rescaledX)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finalize model & Test  "},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\n\nfrom sklearn.pipeline import FeatureUnion\nLR = LinearRegression()\nftu =FeatureUnion([('SKB' ,SelectKBest(k=7)) , ('pca' , PCA(n_components=3)),('rfe',RFE(LR ,5))])\nrescaled_T = ftu.fit_transform(rescaledX ,Y_train)\n\nmodel = KNeighborsRegressor(n_neighbors =3)\nmodel.fit(rescaled_T, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(rescaled_T)\nmean_squared_error( predictions,Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# model on Test Data - Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler().fit(X_validation)\nrescaledX_t = scaler.transform(X_validation)\n\n\nfrom sklearn.pipeline import FeatureUnion\nLR = LinearRegression()\nftu =FeatureUnion([('SKB' ,SelectKBest(k=7)) , ('pca' , PCA(n_components=3)),('rfe',RFE(LR ,5))])\nrescaled_test = ftu.fit_transform(rescaledX_t ,Y_validation)\n\npredictions = grid.predict(rescaled_test)\nprint(mean_squared_error(Y_validation, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble Methods\nAnother way that we can improve the performance of algorithms on this problem is by using\nensemble methods. In this section we will evaluate four different ensemble machine learning\nalgorithms, two boosting and two bagging methods:\n    \n Boosting Methods: AdaBoost (AB) and Gradient Boosting (GBM).\n    \n Bagging Methods: Random Forests (RF) and Extra Trees (ET).\n    \nWe will use the same test harness as before, 10-fold cross-validation and pipelines that\nstandardize the training data for each fold."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ensembles\nensembles = []\nensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('feature_union', feature_union),('AB',\n AdaBoostRegressor())])))\nensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('feature_union', feature_union),('GBM',\n GradientBoostingRegressor())])))\nensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('feature_union', feature_union),('RF',\n RandomForestRegressor())])))\nensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('feature_union', feature_union),('ET',\n ExtraTreesRegressor())])))\nresults = []\nnames = []\nfor name, model in ensembles:\n kfold = KFold(n_splits=num_folds, random_state=seed)\n cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n results.append(cv_results)\n names.append(name)\n msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the example calculates the mean squared error for each method using the default\n# parameters. We can see that we're generally getting better scores than our linear and nonlinear\n# algorithms in previous sections.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We can also plot the distribution of scores across the cross-validation folds."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Scaled Ensemble Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It looks like Gradient Boosting has a better mean score, it also looks like Extra Trees has a\n# similar distribution and perhaps a better median score.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can probably do better, given that the ensemble techniques used the default parameters.\n# In the next section we will look at tuning the Gradient Boosting to further lift the performance.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tune Ensemble Methods\nThe default number of boosting stages to perform (n estimators) is 100. This is a good\ncandidate parameter of Gradient Boosting to tune. Often, the larger the number of boosting\nstages, the better the performance but the longer the training time. In this section we will\nlook at tuning the number of stages for gradient boosting. Below we de\fne a parameter grid\nn estimators values from 50 to 400 in increments of 50. Each setting is evaluated using 10-fold\ncross-validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nLR = LinearRegression()\nfeatures = []\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=6)))\nfeatures.append(('rfe', RFE(LR, 3)))\nfeature_union = FeatureUnion(features)\n\nScaler =StandardScaler()\nGB = GradientBoostingRegressor(random_state=seed)\n\n#pipelines = Pipeline(steps =[('KNN',KNeighborsRegressor())])\nmodel = Pipeline(steps=[('Scaler', Scaler),(\"Feature_Union\",feature_union), ('GB', GB)])\n\nparam_grid = dict(GB__n_estimators=numpy.array([50,100,150,200,250,300,350,400]))\n\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n\ngrid_result = grid.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# As before, we can summarize the best configuration and get an idea of how performance\n# changed with each different configuration.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# We can see that the best configuration was n estimators=400 resulting in a mean squared\n# error of -9.356471, about 0.65 units better than the untuned method.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finalize Model\nIn this section we will finalize the gradient boosting model and evaluate it on our hold out\nvalidation dataset. First we need to prepare the model and train it on the entire training dataset.\nThis includes standardizing the training dataset before training."},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the model\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\n\n\nfrom sklearn.pipeline import FeatureUnion\nLR = LinearRegression()\nftu =FeatureUnion([('SKB' ,SelectKBest(k=7)) , ('pca' , PCA(n_components=3)),('rfe',RFE(LR ,5))])\nrescaled_T = ftu.fit_transform(rescaledX ,Y_train)\n\nmodel = GradientBoostingRegressor(random_state=seed, n_estimators=300)\nmodel.fit(rescaled_T, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test "},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler().fit(X_validation)\nrescaledX_t = scaler.transform(X_validation)\n\n\nfrom sklearn.pipeline import FeatureUnion\nLR = LinearRegression()\nftu =FeatureUnion([('SKB' ,SelectKBest(k=7)) , ('pca' , PCA(n_components=3)),('rfe',RFE(LR ,5))])\nrescaled_test = ftu.fit_transform(rescaledX_t ,Y_validation)\n\npredictions = model.predict(rescaled_test)\nprint(mean_squared_error(Y_validation, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = grid.predict(X_validation)\nprint(mean_squared_error(Y_validation, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Model that works better than withoiut Feature Engg"},{"metadata":{},"cell_type":"markdown","source":"# GBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune scaled GBM\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nparam_grid = dict(n_estimators=numpy.array([50,100,150,200,250,300,350,400]))\nmodel = GradientBoostingRegressor(random_state=seed)\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the model\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\n\nmodel = GradientBoostingRegressor(random_state=seed, n_estimators=300)\nmodel.fit(rescaledX, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We can then scale the inputs for the validation dataset and generate predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform the validation dataset\nrescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(rescaledValidationX)\nprint(mean_squared_error(Y_validation, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(predictions)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# We can see that the estimated mean squared error is 11.8, close to our estimate of -9.3.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Complete project at one plcae"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Regression Project: Boston House Prices\n\n# Load libraries\nimport numpy\nfrom numpy import arange\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Load dataset\nfilename = 'housing.csv'\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndataset = read_csv(filename, delim_whitespace=True, names=names)\n\n# Summarize Data\n\n# Descriptive statistics\n# shape\nprint(dataset.shape)\n# types\nprint(dataset.dtypes)\n# head\nprint(dataset.head(20))\n# descriptions, change precision to 2 places\nset_option('precision', 1)\nprint(dataset.describe())\n# correlation\nset_option('precision', 2)\nprint(dataset.corr(method='pearson'))\n\n\n# Data visualizations\n\n# histograms\ndataset.hist()\npyplot.show()\n# density\ndataset.plot(kind='density', subplots=True, layout=(4,4), sharex=False)\npyplot.show()\n# box and whisker plots\ndataset.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False)\npyplot.show()\n\n# scatter plot matrix\nscatter_matrix(dataset)\npyplot.show()\n# correlation matrix\nfig = pyplot.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(dataset.corr(), vmin=-1, vmax=1, interpolation='none')\nfig.colorbar(cax)\nticks = arange(0,14,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names)\nax.set_yticklabels(names)\npyplot.show()\n\n# Prepare Data\n\n# Split-out validation dataset\narray = dataset.values\nX = array[:,0:13]\nY = array[:,13]\nvalidation_size = 0.20\nseed = 7\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n\n# Evaluate Algorithms\n# Test options and evaluation metric\nnum_folds = 10\nseed = 7\nscoring = 'neg_mean_squared_error'\n\n# Spot Check Algorithms\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('SVR', SVR()))\n\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n\tkfold = KFold(n_splits=num_folds, random_state=seed)\n\tcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tprint(msg)\n\n# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()\n\n\n# Standardize the dataset\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))\npipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\npipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\npipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n\tkfold = KFold(n_splits=num_folds, random_state=seed)\n\tcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tprint(msg)\n\n# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()\n\n\n# KNN Algorithm tuning\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nk_values = numpy.array([1,3,5,7,9,11,13,15,17,19,21])\nparam_grid = dict(n_neighbors=k_values)\nmodel = KNeighborsRegressor()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n\n# ensembles\nensembles = []\nensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostRegressor())])))\nensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\nensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor())])))\nensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesRegressor())])))\nresults = []\nnames = []\nfor name, model in ensembles:\n\tkfold = KFold(n_splits=num_folds, random_state=seed)\n\tcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tprint(msg)\n\n# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Scaled Ensemble Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()\n\n\n# Tune scaled GBM\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nparam_grid = dict(n_estimators=numpy.array([50,100,150,200,250,300,350,400]))\nmodel = GradientBoostingRegressor(random_state=seed)\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n\n# Make predictions on validation dataset\n\n# prepare the model\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = GradientBoostingRegressor(random_state=seed, n_estimators=400)\nmodel.fit(rescaledX, Y_train)\n# transform the validation dataset\nrescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(rescaledValidationX)\nprint(mean_squared_error(Y_validation, predictions))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}