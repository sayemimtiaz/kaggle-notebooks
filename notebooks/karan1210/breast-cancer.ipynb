{"cells":[{"metadata":{},"cell_type":"markdown","source":"Machine Learning algorithms use to predict the probebality and class of the breast cancer.\nwe import some extra packages for better visual purpose\n1. matplotlib\n2. sklearn\n3. from sklearn LR and train_test_split models"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We got this from click on data.csv file and path shown in upper bar.Path for this csv is '../input/breast-cancer-wisconsin-data/data.csv' \n#read this file as name core_data\ncore_data=pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\n#for print file we  can show entier 32 columns and 500+ rows.\nprint(core_data)\n#disply and show the (500+,32) dataset is not easy so for that we have to use command .HEAD() from pandas\nprint(core_data.head())\n#we use print cmd several time because if we refuse it, it will exicute the last command","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x_parameter will drop the unnecessary columns.\n#I am using the .DROP  cmd for the eliminate and modify the data for which breast cancer really depands\nx_parameter = core_data.drop(['id','diagnosis','Unnamed: 32'],axis=1)\nx_parameter.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_parameter datafram in our case the core_data has to br modify in such way that only resulten columns will remain\n#you can either directly add the coloumns or drop the other columns \n#pd.drop(['column1','column2',..........],axis=1)\ny_parameter = core_data['diagnosis']\ny_parameter.shape\ny_parameter.head()\n#disply and show the (500+,32) dataset is not easy so for that we have to use command .HEAD() from pandas but \n#here we only get the 1D array so we can scroll but work easy with .HEAD() cmd\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the parameter and how it is react on the X,Y parameter\nimport matplotlib.pyplot as plt\n#from skleran we are importing the  several machine learing pack.\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression\n# 'ALL ARE IMPORTED' i used this here because i want to know when all the pack imported.\nprint('ALL ARE IMPORTED')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x_parameter.radius_mean,y_parameter,marker='+',color='red')\nplt.show() \n#Just to in sure that parameter which wee choose is effacting the Y parameter or not.\n#we are just plotting the one of the feature among the 30.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  train_test_split split our data into the train and test.\n# As we vary the train_size=0.8 so 80% data will be train and remain 20% are test\n\nx_train, x_test, y_train, y_test = train_test_split(x_parameter,y_parameter,train_size=0.8)\nprint('x test',x_test.head())\nprint('x train',x_train.head())\nprint('y test',y_test.head())\nprint('y train',y_train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# you can get more idea about LogisticRegression https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n# .FIT model is use for fitting the data to our model\nmodel = LogisticRegression()# this will create the empty the model.\nmodel.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# you can get the accuracy for the model by writing this cmd\n# model.score(x_test,y_test)\nprint('accuracy',model.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('probability :',model.predict_proba(x_test))\n# probability of if Benign and Malignant canacer. \n# prediction of M and B type cancer.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict the our data which is  taken from the data set.\n#58 num from data2.\nprint(model.predict([[14.71, 21.59, 95.55, 656.9, 0.1137, 0.1365, 0.1293, 0.08123, 0.2027, 0.06758, 0.4226, 1.15, 2.735, 40.09, 0.003659, 0.02855, 0.02572, 0.01272, 0.01817, 0.004108, 17.87, 30.7, 115.7, 985.5, 0.1368, 0.429, 0.3587, 0.1834, 0.3698, 0.1094]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#probabity of the our prediction.\nprint('probability :',model.predict_proba([[14.71, 21.59, 95.55, 656.9, 0.1137, 0.1365, 0.1293, 0.08123, 0.2027, 0.06758, 0.4226, 1.15, 2.735, 40.09, 0.003659, 0.02855, 0.02572, 0.01272, 0.01817, 0.004108, 17.87, 30.7, 115.7, 985.5, 0.1368, 0.429, 0.3587, 0.1834, 0.3698, 0.1094]]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}