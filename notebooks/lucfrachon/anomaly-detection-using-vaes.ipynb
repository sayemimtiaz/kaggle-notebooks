{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Anomaly Detection using a Variational Auto-Encoder\nIn this project, we are using a Variational Auto-Encoder (VAE) to detect anomalies in a timeseries."},{"metadata":{},"cell_type":"markdown","source":"## 1. Introduction to VAEs in Anomaly Detection\n### 1.1 What is a VAE?\nVAEs are a class of generative models that learn a compressed latent representation of the data. Intuitively, they learn the inherent structure of the data, so that it can be reconstructed just by providing a few values. As an analogy, imagine that you were asked to describe any object to someone just from basic shapes, so that they can draw it. It would take ages to explain everything in enough detail. Now, imaging you can tell them the object is in fact a dog. Immediately, it becomes much easier to explain, and within a few minutes, you should be able to give a pretty accurate description of the dog. This is because you and your friend share a common knowledge of what constitutes a dog and what distinguishes one dog from another. With this pre-learned knowledge, the description of the object becomes much more compact. \n\nTo learn this structure, the VAE has two parts: An encoder and a decoder. The compact representation (the \"latent vector\") is sandwiched between these two parts. During training, we present the encoder with data samples that it converts into latent vectors. Then the decoder takes the latent vectors and tries to reconstruct the data samples. By minimising the recontruction error (plus a regularisation loss, more on that later), the encoder gets better at compressing the data into a meaningful latent vector, and the decoder becomes better at converting this latent vector into plausible reconstructed samples.\n\nHowever, we also need to make sure that the VAE is able to reconstruct samples outside of the training set, i.e. to generalise. This is achieved by penalising (\"regularising\") the reconstruction loss with an additional term, the Kullback-Leibler Divergence. In broad terms, this ensures that the VAE learns the distribution of the data, rather than the datapoints themselves. There is much more to the theory of VAEs (e.g. the \"reparameterisation trick\") but this is outside the scope of this notebook. You can learn a lot more about VAEs [here](https://arxiv.org/abs/1606.05908) \\[1\\].\n\n### 1.2 Why use a VAE in Anomaly Detection?\n\nThe idea behind Anomaly Detection is to detect samples that are far from what is usually seen, in some sense or other. The definition of \"far\" is the difficult bit. In simple, one-dimensional cases, we could just look at the value tracked over time and decide that extreme values are anomalies. Lots of methods use this idea in more or less sophisticated ways.\n\nHowever, things become trickier in higher-dimensional spaces where variables interact with each other or are correlated in some non-obvious ways. Sure, a sheep is \"far\" from a dog, but how do you formalise this distance? Its size is clearly not enough: some dogs are as big or larger than sheeps. Colour doesn't work either -- there are black sheeps and white dogs. Ear shape -- let's not even go there. The decision will clearly require a combination of many variables.\n\nThis is where VAEs come in useful. As we saw, a VAE trained on dogs will have learned a latent, non-explicit representation of the structural features of a dog. What will happen if we pass it an instance of a sheep? It will try to reconstruct the sheep, but since the structure of a sheep is different to that of a dog, most likely the reconstruction will not be very good. As a result, the reconstruction loss will probably be quite poor on this particular sample, much worse than on dog instances -- at least if our VAE has benn properly trained. By detecting data samples that cause a large reconstruction loss, we can therefore hope to identify anomalies.\n\nThe process is as follows:  \n\n- Gather and preprocess data, including train/test split,\n- Build a VAE and train it on the training set,\n- Pass test samples to the VAE and record the reconstruction loss for each,\n- Identify test samples with a reconstruction loss higher than some criterion and flag them as anomalies."},{"metadata":{},"cell_type":"markdown","source":"### 1.3 Scope of this project\nIn this notebook, we will use one of the public datasets available on Kaggle: https://www.kaggle.com/boltzmannbrain/nab\nIt contains many one-dimensional timeseries, but to make things a bit more interesting, we will do some feature engineering and make the data multi-dimensional. This is also often useful in improving the accuracy of any model."},{"metadata":{},"cell_type":"markdown","source":"## 2. Data preprocessing\n\nMuch of the pre-processing in this section has been inspired by this great [notebook](https://www.kaggle.com/victorambonati/unsupervised-anomaly-detection) by Victor Ambonati.\n\nBefore we start, we need to install and import some useful libraries. For this project, I am using [Pytorch-Lightning](https://pytorch-lightning.readthedocs.io/en/latest/), a thin layer on top of Pytorch that handles a lot of the tedious tasks such as writing a training loop. The experiments are recorded using [Weights and Biases](https://app.wandb.ai/), but you can use whatever method you want, or none."},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install pytorch-lightning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install matplotlib==3.1.3  # had issues with package incompatibilies","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport sklearn\nfrom sklearn import preprocessing\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nimport wandb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Package versions:')\nprint('Pytorch:', torch.__version__)\nprint('Pytorch-Lightning:', pl.__version__)\nprint('Matplotlib:', matplotlib.__version__)\nprint('scikit-learn:', sklearn.__version__)\nprint('Weights&Biases:', wandb.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the available datasets:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We pick `machine_temperature_system_failure.csv`, but the pipeline below can apply to others too, with some minor adaptation. This data comes from a temperature sensor from an internal component of a large industrial machine."},{"metadata":{"trusted":true},"cell_type":"code","source":"datafile_path = Path('/kaggle/input/nab/realKnownCause/realKnownCause/machine_temperature_system_failure.csv')\ndatasets_root = Path('/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is monodimensional: It only has one \"predictor\" (`timestamp`) and the outcome (`value`):"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_dt = pd.read_csv(datafile_path)\nraw_dt.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_dt.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = raw_dt.copy()\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\ndata.plot(x='timestamp', y='value', figsize=(12, 10));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can already intuitively see a few of unusual values. We will see if our model will be able to pick them up... For reference, the paper that introduced this dataset can be found [here](https://reader.elsevier.com/reader/sd/pii/S0925231217309864?token=C53EC725CFFDDB81C89A60CA5CA9B4DDD0E1DBFC0F8975119E6CB3555C8B2D6AC0418BF5AE7AE721DA3BB77DDD638190) \\[1\\] and its figure 1 shows this timeseries with hand-labeled anomalies. The first anomaly, around Dec 16th, is a planned shutdown. The third anomaly, around Feb 8th, is a catastrophic failure. Around Jan 30th, a harder-to-detect anomaly indicated the onset of the problem that led to anomaly 3."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['timestamp'].min(), data['timestamp'].max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do not have much information about what kind of machine or industry we are dealing with, which is a bit of an issue when trying to use this data, especially when it comes to feature engineering. We will therefore have to make assumptions. \n\nThe timestamps cover the Christmas and New Year holidays. Since we are dealing with an industrial machine, it stands to reason that its workload might be affected by holidays, and maybe even by the proximity (in time) of a holiday. In the absence of additional information, we are going to assume that the applicable holidays are those typical in Europe and the Americas, i.e. Christmas and New Year's Day. By the same reasoning, we might need to know the day of the week (possibly lower workload on weekends?) or the hour of the day. Again, we will assume that weekends are Satuday and Sunday. \n\nWe can easily extract all this information from the timestamp."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['day'] = data['timestamp'].dt.day\ndata['month'] = data['timestamp'].dt.month\ndata['hour_min'] = data['timestamp'].dt.hour + data['timestamp'].dt.minute / 60\n\ndata['day_of_week'] = data['timestamp'].dt.dayofweek\ndata['holiday'] = 0\ndata.loc[(data['day'] == 25) & (data['month'] == 12),'holiday'] = 1  # Christmas\ndata.loc[(data['day'] == 1) & (data['month'] == 1),'holiday'] = 1  # New Year's Day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"holidays = data.loc[data['holiday'] == 1, 'timestamp'].dt.date.unique()\nholidays","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We add two temporary columns to compute the distance in days to or from each holiday."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, hd in enumerate(holidays):\n    data['hol_' + str(i)] = data['timestamp'].dt.date - hd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are interested by the proximity to or from any holiday, so we want only the shortest gap:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(data.shape[0]):\n    if np.abs(data.loc[data.index[i], 'hol_0']) <= np.abs(data.loc[data.index[i], 'hol_1']):\n        data.loc[data.index[i], 'gap_holiday'] = data.loc[data.index[i], 'hol_0']\n    else:\n        data.loc[data.index[i], 'gap_holiday'] = data.loc[data.index[i], 'hol_1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['gap_holiday'] = data['gap_holiday'].astype('timedelta64[D]')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We no longer need the temporary columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['hol_0', 'hol_1'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we convert the timestamp into something easier to plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['t'] = (data['timestamp'].astype(np.int64)/1e11).astype(np.int64)\ndata.drop('timestamp', axis=1, inplace=True)\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_vars = ['value', 'hour_min', 'gap_holiday', 't']\ncat_vars = ['day', 'month', 'day_of_week', 'holiday']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now apply a Label Encoder to encode the categorical data from 0 to n, with n being the number of classes for the variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoders = [LabelEncoder() for _ in cat_vars] \nfor col, enc in zip(cat_vars, label_encoders):\n    data[col] = enc.fit_transform(data[col])\n    \ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that some of our data are continuous (or 'close to' continuous): `value`, `hour_min`, `gap_holiday` (even though this one only has a resolution of 1 day), `t`. Other variables are categorical: `day`, `month`, `day_week`, `holiday`. In a neural network model, these two types of data need to be handled differently, as we will see later."},{"metadata":{},"cell_type":"markdown","source":"It is time to split our data into a train set and a test set. Let's set 30% of the data aside."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ratio = 0.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_idx = np.random.choice(np.arange(data.shape[0]), size=round(data.shape[0] * (1 - test_ratio)), replace=False)\ntst_idx = list(set(range(data.shape[0])).difference(set(tr_idx)))\nlen(tr_idx), len(tst_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_data = data.iloc[tr_idx]\ntst_data = data.iloc[tst_idx]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we are using a neural network model, the continuous variables need to be normalised. This is important because the weights of a neural network are initialised at random from a common distribution, so all tend to have similar scales initially. Variables whose values spread are across different orders of magnitude would therefore cause serious difficulties when it comes to learning the optimal weights."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = preprocessing.StandardScaler().fit(tr_data[cont_vars])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_data_scaled = tr_data.copy()\ntr_data_scaled[cont_vars] = scaler.transform(tr_data[cont_vars])\ntst_data_scaled = tst_data.copy()\ntst_data_scaled[cont_vars] = scaler.transform(tst_data[cont_vars])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that the test data is normalised using parameters observed on the train set. Otherwise, we would be cheating!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# tr_data_scaled = pd.DataFrame(tr_data_scaled, columns=tr_data.columns)\n# tst_data_scaled = pd.DataFrame(tst_data_scaled, columns=tst_data.columns)\n\ntr_data, tr_data_scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the continuous variables are now standardised, while the categorical variables are unchanged. We can now save our data as CSV files and we are ready for the next phase."},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_data_scaled.to_csv(datasets_root/'train.csv', index=False)\ntst_data_scaled.to_csv(datasets_root/'test.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. VAE Model\nWe first define a `Dataset` class that takes data from either the train set or the test set. It is also able to filter out some of the columns.\n\n**Note**: For VAE training, we want to use the column `value` as a feature rather than a label, because we are doing unsupervised learning. In that case, we set the argument `lbl_as_feat` to True. If we were training a supervised model, then we would be able to use the same `Dataset` class but we would need to set `lbl_as_feat` to False, and the dataset would return `value` as a label."},{"metadata":{"trusted":true},"cell_type":"code","source":"class TSDataset(Dataset):\n    def __init__(self, split, cont_vars=None, cat_vars=None, lbl_as_feat=True):\n        \"\"\"\n        split: 'train' if we want to get data from the training examples, 'test' for\n        test examples, or 'both' to merge the training and test sets and return samples\n        from either.\n        cont_vars: List of continuous variables to return as features. If None, returns\n        all continuous variables available.\n        cat_vars: Same as above, but for categorical variables.\n        lbl_as_feat: Set to True when training a VAE -- the labels (temperature values)\n        will be included as another dimension of the data. Set to False when training\n        a model to predict temperatures.\n        \"\"\"\n        super().__init__()\n        assert split in ['train', 'test', 'both']\n        self.lbl_as_feat = lbl_as_feat\n        if split == 'train':\n            self.df = pd.read_csv(datasets_root/'train.csv')\n        elif split == 'test':\n            self.df = pd.read_csv(datasets_root/'test.csv')\n        else:\n            df1 = pd.read_csv(datasets_root/'train.csv')\n            df2 = pd.read_csv(datasets_root/'test.csv')\n            self.df = pd.concat((df1, df2), ignore_index=True)\n        \n        # Select continuous variables to use\n        if cont_vars:\n            self.cont_vars = cont_vars\n            # If we want to use 'value' as a feature, ensure it is returned\n            if self.lbl_as_feat:\n                try:\n                    assert 'value' in self.cont_vars\n                except AssertionError:\n                    self.cont_vars.insert(0, 'value')\n            # If not, ensure it not returned as a feature\n            else:\n                try:\n                    assert 'value' not in self.cont_vars\n                except AssertionError:\n                    self.cont_vars.remove('value')\n                    \n        else:  # if no list provided, use all available\n            self.cont_vars = ['value', 'hour_min', 'gap_holiday', 't']\n        \n        # Select categorical variables to use\n        if cat_vars:\n            self.cat_vars = cat_vars\n        else:  # if no list provided, use all available\n            self.cat_vars = ['day', 'month', 'day_of_week', 'holiday']\n        \n        # Finally, make two Numpy arrays for continuous and categorical\n        # variables, respectively:\n        if self.lbl_as_feat:\n            self.cont = self.df[self.cont_vars].copy().to_numpy(dtype=np.float32)\n        else:\n            self.cont = self.df[self.cont_vars].copy().to_numpy(dtype=np.float32)\n            self.lbl = self.df['value'].copy().to_numpy(dtype=np.float32)\n        self.cat = self.df[self.cat_vars].copy().to_numpy(dtype=np.int64)\n            \n    def __getitem__(self, idx):\n        if self.lbl_as_feat:  # for VAE training\n            return torch.tensor(self.cont[idx]), torch.tensor(self.cat[idx])\n        else:  # for supervised prediction\n            return torch.tensor(self.cont[idx]), torch.tensor(self.cat[idx]), torch.tensor(self.lbl[idx])\n    \n    def __len__(self):\n        return self.df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try to call from our dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = TSDataset(split='both', cont_vars=['value', 't'], cat_vars=['day_of_week', 'holiday'], lbl_as_feat=True)\nprint(len(ds))\nit = iter(ds)\nfor _ in range(10):\n    print(next(it))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, with `lbl_as_feat==True`, we get two arrays for each sample: An array of continuous values represented by floats, and an array of categorical values represented by integers."},{"metadata":{},"cell_type":"markdown","source":"Now we move on to the fun part: The model itself. First, define individual modules. The core of our neural network will be a sequence of fully connected layers, whose number and sizes are passed through the hyperparameter `layer_dims` (a list of integers). In our experiment, we used `64,128,64`, i.e. 3 layers of dimensions 64, 128 and 64. Each layer can be batch-normalised. The input into the first layer is an aggregation of the continuous variables and embedding vectors encoding the categorical variables. \n\nEmbedding vectors are a notion that is heavily used in Natural Language Processing. They are learnable vectors (of dimension 8 in our experiments) that express some non-explicit features of the variables. For instance, the vector for `day_of_week` can take 7 different sets of values (one for each day), each 8-dimensional. Depending on the value of this variable for each sample, the corresponding vector is retrieved in a lookup table, passed to the network, and updated during backpropagation. The vector for `day_of_week==0` could eventually capture the fact that the machine's activity is lower on such days, for instance. The important thing is that this learning is done without our intervention, and there is no easy way to interpret the learned embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Layer(nn.Module):\n    '''\n    A single fully connected layer with optional batch normalisation and activation.\n    '''\n    def __init__(self, in_dim, out_dim, bn = True):\n        super().__init__()\n        layers = [nn.Linear(in_dim, out_dim)]\n        if bn: layers.append(nn.BatchNorm1d(out_dim))\n        layers.append(nn.LeakyReLU(0.1, inplace=True))\n        self.block = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.block(x)\n\n    \nclass Encoder(nn.Module):\n    '''\n    The encoder part of our VAE. Takes a data sample and returns the mean and the log-variance of the \n    latent vector's distribution.\n    '''\n    def __init__(self, hparams):\n        super().__init__()\n\n        self.embeds = nn.ModuleList([\n            nn.Embedding(n_cats, emb_size) for (n_cats, emb_size) in hparams.embedding_sizes\n        ])\n        # The input to the first layer is the concatenation of all embedding vectors and continuous\n        # values\n        in_dim = sum(emb.embedding_dim for emb in self.embeds) + len(hparams.cont_vars)\n        layer_dims = [in_dim] + [int(s) for s in hparams.layer_sizes.split(',')]\n        bn = hparams.batch_norm\n        self.layers = nn.Sequential(\n            *[Layer(layer_dims[i], layer_dims[i + 1], bn) for i in range(len(layer_dims) - 1)],\n        )\n        self.mu = nn.Linear(layer_dims[-1], hparams.latent_dim)\n        self.logvar = nn.Linear(layer_dims[-1], hparams.latent_dim)\n    \n    def forward(self, x_cont, x_cat):\n        x_embed = [e(x_cat[:, i]) for i, e in enumerate(self.embeds)]        \n        x_embed = torch.cat(x_embed, dim=1)\n        x = torch.cat((x_embed, x_cont), dim=1)\n        h = self.layers(x)\n        mu_ = self.mu(h)\n        logvar_ = self.logvar(h)\n        return mu_, logvar_, x  # we return the concatenated input vector for use in loss fn\n    \n\nclass Decoder(nn.Module):\n    '''\n    The decoder part of our VAE. Takes a latent vector (sampled from the distribution learned by the \n    encoder) and converts it back to a reconstructed data sample.\n    '''\n    def __init__(self, hparams):\n        super().__init__()\n#         self.final_activ = hparams.final_activ\n        hidden_dims = [hparams.latent_dim] + [int(s) for s in reversed(hparams.layer_sizes.split(','))]\n        out_dim = sum(emb_size for _, emb_size in hparams.embedding_sizes) + len(hparams.cont_vars)\n        bn = hparams.batch_norm\n        self.layers = nn.Sequential(\n            *[Layer(hidden_dims[i], hidden_dims[i + 1], bn) for i in range(len(hidden_dims) - 1)],\n        )\n        self.reconstructed = nn.Linear(hidden_dims[-1], out_dim)\n        \n    def forward(self, z):\n        h = self.layers(z)\n        recon = self.reconstructed(h)\n        return recon","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now for the full VAE, defined as a LightningModule:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class VAE(pl.LightningModule):\n    def __init__(self, hparams):\n        super().__init__()\n        if isinstance(hparams, dict):\n            hparams = Namespace(**hparams)\n        self.hparams = hparams\n        self.encoder = Encoder(hparams)\n        self.decoder = Decoder(hparams)\n        self.stdev = hparams.stdev\n        self.kld_beta = hparams.kld_beta\n        self.lr = hparams.lr\n        self.wd = hparams.weight_decay\n        \n    def reparameterize(self, mu, logvar):\n        '''\n        The reparameterisation trick allows us to backpropagate through the encoder.\n        '''\n        if self.training:\n            std = torch.exp(0.5 * logvar)\n            eps = torch.randn_like(std) * self.stdev\n            return eps * std + mu\n        else:\n            return mu\n        \n    def forward(self, batch):\n        x_cont, x_cat = batch\n        assert x_cat.dtype == torch.int64\n        mu, logvar, x = self.encoder(x_cont, x_cat)\n        z = self.reparameterize(mu, logvar)\n        recon = self.decoder(z)\n        return recon, mu, logvar, x\n        \n    def loss_function(self, obs, recon, mu, logvar):\n        recon_loss = F.smooth_l1_loss(recon, obs, reduction='sum')\n        kld = -0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp())\n        return recon_loss, kld\n                               \n    def training_step(self, batch, batch_idx):\n        recon, mu, logvar, x = self.forward(batch)\n        # The loss function compares the concatenated input vector including\n        # embeddings to the reconstructed vector\n        recon_loss, kld = self.loss_function(x, recon, mu, logvar)\n        loss = recon_loss + self.kld_beta * kld\n\n        self.log('total_tr_loss', loss.mean(dim=0), on_step=True, prog_bar=True, \n                 logger=True)\n        self.log('recon_loss', recon_loss.mean(dim=0), on_step=True, prog_bar=True, \n                 logger=True)\n        self.log('kld', kld.mean(dim=0), on_step=True, prog_bar=True, logger=True)\n        return loss\n    \n    def test_step(self, batch, batch_idx):\n        recon, mu, logvar, x = self.forward(batch)\n        recon_loss, kld = self.loss_function(x, recon, mu, logvar)\n        loss = recon_loss + self.kld_beta * kld\n        self.log('test_loss', loss)\n        return loss\n        \n    def configure_optimizers(self):\n        opt = torch.optim.AdamW(self.parameters(), lr=self.lr, \n                                weight_decay=self.hparams.weight_decay, \n                                eps=1e-4)\n        sch = torch.optim.lr_scheduler.MultiplicativeLR(opt, lr_lambda=lambda epoch: 0.95)\n        return opt\n    \n    def train_dataloader(self):\n        dataset = TSDataset('train', cont_vars=self.hparams.cont_vars, \n                            cat_vars = self.hparams.cat_vars, lbl_as_feat=True)\n        return DataLoader(dataset, batch_size=self.hparams.batch_size, num_workers=8)\n    \n    def test_dataloader(self):\n        dataset = TSDataset('test', cont_vars=self.hparams.cont_vars,\n                            cat_vars=self.hparams.cat_vars, lbl_as_feat=True)\n        return DataLoader(dataset, batch_size=self.hparams.batch_size, num_workers=8)        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that different reconstruction loss functions are possible, the most obvious being the Mean Square Error (MSE). However, it is quite sensitive to outliers. In our case, almost by definition, we expect outliers to be present so it might be useful to use a loss function that is less sensitive to them. We use the Huber loss with $\\delta=1$ (called `smooth_l1_loss` in Pytorch)."},{"metadata":{},"cell_type":"markdown","source":"Let's define the model and data hyperparameters. Note that `stdev` (the standard deviation of the normal distribution from which we sample $\\epsilon$ in the `reparameterize()` method) would typically be 1. However, several papers found improvements with smaller values, such as 1e-1. This is what we are going to use, as it gives us lower loss values.\n\nAnother hyperparameter worth mentioning is `kld_beta`, the coefficient applied to the KLD loss term in the total loss computation (this hyperparameter means that our VAE is technically a $\\beta$-VAE). This helps because the reconstruction loss is typically harder to improve than the KLD loss, therefore if both were weighted equally, the model would start by optimising the KLD loss before improving the reconstruction loss substantially. By setting this coefficient to a value < 1, we try to make sure that both loss values improve at a similar rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_features = ['value', 'hour_min', 'gap_holiday', 't'] \ncat_features = ['day', 'month', 'day_of_week', 'holiday'] \n\nembed_cats = [len(tr_data_scaled[c].unique()) for c in cat_features]\n\nhparams = OrderedDict(\n    run='all_vars_embsz8_latsz32_bsz128_lay64-128-64_ep100',\n    cont_vars = cont_features,\n    cat_vars = cat_features,\n    embedding_sizes = [(embed_cats[i], 8) for i in range(len(embed_cats))],\n    latent_dim = 32,\n    layer_sizes = '64,128,64',\n    batch_norm = True,\n    stdev = 0.1,\n    kld_beta = 0.2,\n    lr = 0.0005,\n    weight_decay = 1e-5,\n    batch_size = 128,\n    epochs = 200,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hparams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from argparse import Namespace\n# Simulate a Namespace with the defined hyperparameters\nhparams = Namespace(**hparams)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, let's choose our learning rate. Instead of searching ourselves, we can leverage a functionality provided by Pytorch-Lightning that automates the search for this hyperparameter.\n\nEDIT: Following an update to Pytorch-Lightning, this functionality return a value that is too high and causes the loss to diverge. Therefore, we set the loss manually to 0.001. Note that we also use gradient clipping to further enforce convergence."},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm /kaggle/working/vae_weights*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()\nmodel = VAE(hparams)\nlogger = WandbLogger(name=hparams.run, project='VAE_Anomaly', version=hparams.run)\nckpt_callback = pl.callbacks.ModelCheckpoint(dirpath='.', filename='vae_weights')\n# Replace argument logger by None if you don't have a WandB account (and don't want to create one)\ntrainer = pl.Trainer(gpus=-1, logger=logger, max_epochs=hparams.epochs, \n                     auto_lr_find=False, benchmark=True, callbacks=[ckpt_callback],\n                     gradient_clip_val=0.5\n                     )\n# trainer.tune(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"trainer.fit(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training takes only a few minutes on GPU. Let's see how this model performs on the test set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.test()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hopefully, we should find a test loss that is fairly close to the training loss, indicating that we are not overfitting. A good way to regularise training would be creating a separate validation set and stopping the training early when the loss computed on that set starts to increase (or stops decreasing). Pytorch-Lightning makes early stopping extremely easy to implement: once you have created the validation set, it is just a case of passing an additional argument to the Trainer class. Note also that we could probably improve on these results by further optimising hyperparameters (WandB actually offers an easy framework for running hyperparameter optimisation by random, grid or Bayesian search). For now, let's go with these weights and see if the model can find outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"trained_model = VAE.load_from_checkpoint('./vae_weights-v1.ckpt')\ntrained_model.freeze()\ndataset = TSDataset('test', cont_vars=hparams.cont_vars, \n                    cat_vars=hparams.cat_vars,\n                    lbl_as_feat=True) \nlosses = []\n# run predictions for the training set examples\nfor i in range(len(dataset)):\n    x_cont, x_cat = dataset[i]\n    x_cont.unsqueeze_(0)\n    x_cat.unsqueeze_(0)\n    recon, mu, logvar, x = trained_model.forward((x_cont, x_cat))\n    recon_loss, kld = trained_model.loss_function(x, recon, mu, logvar)\n    losses.append(recon_loss + trained_model.hparams.kld_beta * kld)\n    \ndata_with_losses = dataset.df\ndata_with_losses['loss'] = np.asarray(losses)\ndata_with_losses.sort_values('t', inplace=True)\ndata_with_losses.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How are loss values distributed?"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean, sigma = data_with_losses['loss'].mean(), data_with_losses['loss'].std()\nmean, sigma","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's define a threshold value (in multiples of the standard deviation) beyond which instances are classified as anomalies:"},{"metadata":{"trusted":true},"cell_type":"code","source":"thresh = 6 # threshold for anomaly, in multiples of sigma.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now flag as anomalies any instance where the loss is very far from the centre of the distribution:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_with_losses['anomaly'] = data_with_losses['loss'] > (mean + sigma * thresh)\nprint(data_with_losses.head())\ncolors = ['red' if anomaly else 'blue' for anomaly in data_with_losses['anomaly']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see this in a histogram. Red colour denotes abnormal samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"anomalies_loss = data_with_losses.loc[data_with_losses['anomaly'], 'loss']\nnormals_loss   = data_with_losses.loc[~data_with_losses['anomaly'], 'loss']\nplt.hist([normals_loss, anomalies_loss], bins=100, stacked=True, color=['blue', 'red'], label=['normal', 'abnormal'])\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What does it mean in terms of the distribution of the temperature values? We first \"unscale\" the data to put each feature back into the value ranges of the raw dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_with_losses_unscaled = data_with_losses.copy()\ndata_with_losses_unscaled[cont_vars] = scaler.inverse_transform(data_with_losses[cont_vars])\nfor enc, var in zip(label_encoders, cat_vars):\n    data_with_losses_unscaled[var] = enc.inverse_transform(data_with_losses[var])\ndata_with_losses_unscaled = pd.DataFrame(data_with_losses_unscaled, columns=data_with_losses.columns)\nprint(data_with_losses_unscaled.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_with_losses_unscaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we build the histogram of `values`, coloured depending on whether or not the loss value for the corresponding observations are considered abnormal or not. "},{"metadata":{"trusted":true},"cell_type":"code","source":"anomalies_value = data_with_losses_unscaled.loc[data_with_losses_unscaled['anomaly'], ['loss','value']]\nnormals_value = data_with_losses_unscaled.loc[~data_with_losses_unscaled['anomaly'], ['loss','value']]\n\nplt.hist([normals_value['value'], anomalies_value['value']], bins=100, stacked=True, color=['blue', 'red'], label=['normal', 'abnormal'])\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)It's a little hard to tell on this figure, but some of the instances flagged as abnormal are not at the extreme end of the temperature spectrum, as they would be using a more naive approach based only on the temperature. Let see these flagged examples on the original timeseries:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"anomalies_ts = data_with_losses_unscaled.loc[data_with_losses_unscaled['anomaly'], ('t', 'value')]\nholidays_ts = data_with_losses_unscaled.loc[data_with_losses_unscaled['holiday'] == 1, ('t', 'value')]\nprint(anomalies_ts.head())\nfig, ax = plt.subplots()\nax.plot(data_with_losses_unscaled['t'], data_with_losses_unscaled['value'], color='blue')\nax.scatter(anomalies_ts['t'], anomalies_ts['value'], color='red', label='anomaly')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[](http://)With a threshold at 6 sigmas, our model detects two of the three anomalies (it misses the second one), and does not generate too many false positives. The threshold that we defined can be adjusted to favour precision or recall and the optimal value could be determined by the area under the precision-recall curve. It would also be interesting to perform an ablation study to understand which of the engineered features are the most helpful.\n\nI hope this notebook was informative and gave you some insight into VAEs and their usefulness!"},{"metadata":{},"cell_type":"markdown","source":"## References\n\\[1\\] Doersch, C. (2016). Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908.  \n\\[2\\] Ahmad, S., Lavin, A., Purdy, S., & Agha, Z. (2017). Unsupervised real-time anomaly detection for streaming data. Neurocomputing, 262, 134-147."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}