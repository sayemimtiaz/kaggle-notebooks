{"nbformat_minor":1,"nbformat":4,"cells":[{"source":"# Introduction\nThe purpose of this notebook is to provide a clean and organized medium for performing an exploration into a [bitcoin dataset](https://www.kaggle.com/mczielinski/bitcoin-historical-data).  The goal of the exploration is to try and create a system that can correctly classify price change predictions of the cryptocurrency.  \n\nAs a template, I will be recreating the work of Amjad and Shah in their paper [Trading Bitcoin and Online Time Series Prediction](http://proceedings.mlr.press/v55/amjad16.pdf).","cell_type":"markdown","metadata":{"_uuid":"b4c99b9a0a772a07f1446284c61802eafc1ab6c8","_cell_guid":"53146f13-4a86-4e89-b41a-bc55d4df99be"}},{"source":"# Read In and Format Data\n\nThe daily closing price is read in from a CSV file.  Lines with NaN or unusual characters are removed.","cell_type":"markdown","metadata":{"_uuid":"67465f3c502badb74387572ddc54cdd6ba2b671b","_cell_guid":"d506f722-8ff6-4080-b201-2931601b2dbe"}},{"source":"import pandas as pd \nimport datetime\n\ndef dateparse (time_in_secs):    \n    return datetime.datetime.fromtimestamp(float(time_in_secs))\n\ndataSetSize = 5000\ndata = pd.read_csv('../input/btceUSD_1-min_data_2012-01-01_to_2017-05-31.csv', parse_dates=True, date_parser=dateparse, index_col=[0])\ndata = data[['Close']].apply(pd.to_numeric)\ndata = data.dropna()\ndata = data.head(dataSetSize)\n\nprint(data.head())\nprint('\\nDataset Size:\\t' + str(len(data.index.tolist())))","outputs":[],"cell_type":"code","metadata":{"_uuid":"7eb6dff98fd07894d851fc5545395769a8e8abeb","_cell_guid":"f3ff45f7-4a49-4dd1-abd8-54ab96206df6"},"execution_count":null},{"source":"# Plot the Time Series\nWe plot the time series to provide an initial visualization of the data.","cell_type":"markdown","metadata":{"_uuid":"ec10697ee5e8c07767d967722a39713e186f2c6e","_cell_guid":"08ec2866-e751-4ad0-9db1-58980accfa29"}},{"source":"from plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\nlayout = go.Layout(\n    title='Original Time Series',\n    xaxis=dict(title='Time (Minutes)'),\n    yaxis=dict(title='USD')\n)\n\nfigData = [{'x': [i for i in range(len(data['Close'].tolist()))], 'y': data['Close'].tolist()}]\n\nfig = go.Figure(data=figData, layout=layout)\n\niplot(fig, show_link=False)","outputs":[],"cell_type":"code","metadata":{"_uuid":"10e10ecc4da1df6ed4b65f9cb65cd6918dc3a05d","_cell_guid":"02e1db11-6757-443e-8cd9-2f96b3a40c9b"},"execution_count":null},{"source":"# Test if Time Series is Stationary\nRun Augmented Dickey-Fuller and KPSS tests to determine whether the time series is stationary.  ADF assumes non-stationarity as the null hypothesis, and KPSS assumes stationarity.  We will use an alpha value of 0.05.","cell_type":"markdown","metadata":{"_uuid":"ac624894d316dda1c777141cbc9205134bd72cd9","_cell_guid":"3fec03d8-257e-4cfe-baa4-3bdb74bcdecb"}},{"source":"from statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import kpss\n\ndef testStationarity(inputData, alphs):\n    \n    # Augmented Dickey-Fuller Test\n    # H0 is non-stationary\n    results = adfuller(inputData)\n    pValue = results[1]\n    if pValue < alpha:\n        print('ADF Result: \\t Stationary' + '\\t P-Value: \\t' + str(pValue))\n    else:\n        print('ADF Result: \\t Non-Stationary' + '\\t P-Value: \\t' + str(pValue))\n\n    # Kwiatkowski-Phillips-Schmidt-Shin Test\n    # H0 is stationary\n    results = kpss(inputData)\n    pValue = results[1]\n    if pValue >= alpha:\n        print('KPSS Result: \\t Stationary' + '\\t P-Value: \\t' + str(pValue))\n    else:\n        print('KPSS Result: \\t Non-Stationary' + '\\t P-Value: \\t' + str(pValue))\n\n# Define alpha value for hypothesis testing\nalpha = 0.05\ntestStationarity(data['Close'].values, alpha)","outputs":[],"cell_type":"code","metadata":{"_uuid":"080766f8316f54f494a1c517ab034a9f5f1972a5","_cell_guid":"c5337be8-e0bb-47ae-b047-0323e170346d"},"execution_count":null},{"source":"# Apply First-Differences to the Closing Price\nApplying first-differences is when you transform the time series doing `data[t] = data[t] - data[t-1]`.  Differencing the time series in this way is a common method for achieving stationarity.","cell_type":"markdown","metadata":{"_uuid":"a2e7bb556f23bcbd33141b02f469be8d1050f547","_cell_guid":"b2178718-e277-46ef-b339-c76eb8152cdf"}},{"source":"# Calculate different orders of difference in closing price\ndef calculateDifferences(data, numDifs):\n    if numDifs == 0:\n        keyColumn = 'Close'\n    else:\n        keyColumn = str(numDifs) + '_Dif'\n    for i in range(1, numDifs+1):\n        if i == 1:\n            data[str(i) + '_Dif'] = data['Close'] - data['Close'].shift(1)\n        else:\n            data[str(i) + '_Dif'] = data[str(i-1) + '_Dif'] - data[str(i-1) + '_Dif'].shift(1)\n    data = data.dropna()\n    return data, keyColumn\n\nnumDifs = 1\ndata, keyColumn = calculateDifferences(data, numDifs)\n\nprint(data.head())","outputs":[],"cell_type":"code","metadata":{"_uuid":"a8cad4bf8f2cafd27c4d2e08044fbc7d8b342a18","_cell_guid":"fc7a8f8d-2cd0-4c03-89e4-9afbd08100c6"},"execution_count":null},{"source":"# Test if Time Series (w/ First-Differences) is Stationary\nRun Augmented Dickey-Fuller and KPSS tests to determine whether the time series is stationary.  ADF assumes non-stationarity as the null hypothesis, and KPSS assumes stationarity.  We will use an alpha value of 0.05.","cell_type":"markdown","metadata":{"_uuid":"383bc828b45dc14457c3074007172c683cf745f5","_cell_guid":"e579983a-81c5-4ea8-acc7-074c40ac9ed2"}},{"source":"testStationarity(data[keyColumn].values, alpha)","outputs":[],"cell_type":"code","metadata":{"_uuid":"fc73b4272a3c49bbd3294cb339544f22155dcf60","_cell_guid":"d19f4f38-d3ab-4e4f-8023-7e05f9c97d85"},"execution_count":null},{"source":"# Plot the Differenced Time Series\nWe will now plot the differenced time series, so we have a visualization of the transformation.","cell_type":"markdown","metadata":{"_uuid":"37643545f5f95c0b5a611bcb8bff51aaa7bbe569","_cell_guid":"df9449f4-2c0e-44f0-b2ec-d614187a3a71"}},{"source":"layout = go.Layout(\n    title='Differenced Time Series (' + str(keyColumn) + ')',\n    xaxis=dict(title='Time (Minutes)'),\n    yaxis=dict(title='USD')\n)\n\nfigData = [{'x': [i for i in range(len(data[keyColumn].tolist()))], 'y': data[keyColumn].tolist()}]\n\nfig = go.Figure(data=figData, layout=layout)\n\niplot(fig, show_link=False)","outputs":[],"cell_type":"code","metadata":{"_uuid":"a09c4da87e3ad342ce51390ddb426ff4df337586","_cell_guid":"d8db1c50-973d-4e10-b608-c211826ae91e"},"execution_count":null},{"source":"# Results So Far (1)\nSo far, we have read in the data for the daily closing price of bitcoin.  We found that the baseline dataset was not stationary, but that the differenced dataset was.  Now, we will try to visualize the differenced dataset using a histogram.","cell_type":"markdown","metadata":{"_uuid":"4542a4bb7a91e998f355db1680f86b8c70c64f9a","_cell_guid":"2ac93da8-b297-418a-bedb-4dd0813ea606"}},{"source":"# Histogram of Time Series (w/ First-Differences)\nWe are plotting a histogram of the differenced time series to visualize whether the dataset appears to be Gaussian.","cell_type":"markdown","metadata":{"_uuid":"58226e2c800221131e952495194380b2c3bc6b9d","_cell_guid":"18076517-7bf7-47a5-a1bd-dd3503521624"}},{"source":"%matplotlib inline\nhist = data[keyColumn].hist(bins=50)\nhist.set_title(\"Histogram of Changes in Closing Price\")\nhist.set_xlabel(\"Difference in Closing Price (USD)\")\nhist.set_ylabel(\"Number of Minutes\")","outputs":[],"cell_type":"code","metadata":{"_uuid":"cff82098acf238cf63799732492f0ef15dfce5a9","_cell_guid":"a4c3cc8e-8c21-4f76-99cc-651c2d841f41"},"execution_count":null},{"source":"# QQ Plot of Time Series (w/ First-Differences)\nWe will make a QQ plot to get a better idea if the distribution is indeed Gaussian.  If the points fall (mostly) along a 45 degree red line, that will be a strong indicator that the distribution is Gaussian.","cell_type":"markdown","metadata":{"_uuid":"b333880827c2919e4e3fa83ada03c6f6ed4859ab","_cell_guid":"9f4417c6-ab88-435e-830f-dffcbc301a80"}},{"source":"from scipy import stats\nimport pylab \nstats.probplot(data[keyColumn].values, dist='norm', plot=pylab)\npylab.show()","outputs":[],"cell_type":"code","metadata":{"_uuid":"87dadde7d8f792b250c851ea3ce3636c033f19f6","_cell_guid":"52be605d-4c54-4c07-9bae-70fec03fbc34"},"execution_count":null},{"source":"# Do KS Test to Determine if Time Series (w/ First-Differences) is Gaussian\nThe histogram and QQ plot seem to show that the time series not Guassian.  To be sure, we will run the Kolmogorov-Smirnov test to answer this question decisively, using an alpha value of 0.05.","cell_type":"markdown","metadata":{"_uuid":"cba21dd701b003d672d4a6a379d6b4832e29c1b4","_cell_guid":"cd3b8fe9-5556-444c-9317-25611c13a74c"}},{"source":"# Kolmogorov-Smirnov Test\n# H0 is that both distributions are identical\nresults = stats.kstest(data[keyColumn].values, 'norm')\npValue = results[1]\nif pValue >= alpha:\n    print('KS Result: \\t Gaussian' + '\\t P-Value: \\t' + str(pValue))\nelse:\n    print('KS Result: \\t Non-Gaussian' + '\\t P-Value: \\t' + str(pValue))","outputs":[],"cell_type":"code","metadata":{"_uuid":"22ee268a520482c44a5c205a590e46de90787582","_cell_guid":"f7ef6846-0402-43f4-a830-265642d0af28"},"execution_count":null},{"source":"# Results So Far (2)\nWe have determined that the differenced time series is not a Gaussian distribution.  The QQ plot suggests that the differenced time series is a \"thinner\" distribution than the Gaussian.","cell_type":"markdown","metadata":{"_uuid":"14968c8097f70ce6a511f8bf229adcccf3721ea5","_cell_guid":"77777de9-9e78-43d4-8b09-114cfaa2bb71"}},{"source":"# Plot the Autocorrelation / Partial Autocorrelation Functions\nThese plots will visualize the the dependence of the time series on the past.  We are hoping for two things:\n\n1.  That the time series only depends on the recent past, as this implies mixing.\n2.  That the time series show exponential decay, as this implies mixing.\n\nA quote form the [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/visualization.html#autocorrelation-plot):\n\n\"If time series is random, such autocorrelations should be near zero for any and all time-lag separations. If time series is non-random then one or more of the autocorrelations will be significantly non-zero.\" \n\nThe blue hilight region displayed in the plot corresponds to the 95% confidence bands.","cell_type":"markdown","metadata":{"_uuid":"238fb4fa96ea899783421ac85b3ad67395339546","_cell_guid":"984e8d52-e86b-4468-863d-ffb3033472bc"}},{"source":"from statsmodels.graphics.tsaplots import plot_acf\nimport matplotlib.pyplot as pyplot\nplot_acf(data[keyColumn])\npylab.xlim([0, 50])\npylab.ylim([-0.2, 0.2])\npyplot.show()","outputs":[],"cell_type":"code","metadata":{"_uuid":"791f9489b6ae040cfcda33757fa4df9eba04b1f1","_cell_guid":"c765520d-21a0-4dca-92d0-c39003b0edca"},"execution_count":null},{"source":"## Partial Autocorrelaiton\nBelow is the code to run partial autocorrelation.  Do not do this unless you have time to spare, as it takes forever.  To improve runtime, I have taken the PACF of a subset (via `head()`) of the data.","cell_type":"markdown","metadata":{"_uuid":"d77597cd56e608c8a344f6601ba199c19e5d836f","_cell_guid":"5f6f5cfe-1f08-4a4f-be9b-d54e5106e93f"}},{"source":"from statsmodels.graphics.tsaplots import plot_pacf\nplot_pacf(data[keyColumn].head(100))\npylab.xlim([0, 100])\npylab.ylim([-0.8, 0.8])\npyplot.show()","outputs":[],"cell_type":"code","metadata":{"_uuid":"582ab9fb5b9511faae1b6b4f3c85cf86029d4692","_cell_guid":"a669855c-76d6-4da3-9eb0-ca019ba354ad"},"execution_count":null},{"source":"# Results So Far (3)\nWe have plotted the autocorrelation and partial autocorrelation.  Both plots seem to imply that the very earliest lags are statistically significant.  Both plots seem to show that esponential-esque decay is occuring and that widely-separated times are asymptotically independent.  This implies mixing.\n\nNext, we will add in the custom features specified by Amjad and Shah on page 6, and will form our feature vectors.","cell_type":"markdown","metadata":{"_uuid":"6747a9fa4c5ec11d8c200ace5b4be6a1665bc87e","_cell_guid":"95388cfc-10a5-4437-a4f5-27beaeb40561"}},{"source":"# Extracting Features & Forming Feature Vectors\nHere, we will extract the three features described on pg. 6 of Amjad and Shah's paper.  These are:\n\n1. The class of at data[t-1]\n2. The tally count for each class in the past d time steps\n3. The maximum consecutive run-length for each class in the past d time steps","cell_type":"markdown","metadata":{"_uuid":"3a4e134aeff7cccf6bc5e391e5abd3ab4fcef3ec","_cell_guid":"65dbaf1a-d5ad-4d30-b551-5b38b38d26b3"}},{"source":"from itertools import groupby\n\n# Define the transaction fee (theta), and number of previous time steps to use (d)\ntheta = 0.000\nd = 4\n\n# Calculate the class at each timestamp\ndata['Class'] = data[keyColumn].apply(lambda x: 1 if x>theta else -1 if x<(-1*theta) else 0)\n\n# Extract the class at data[t-1]\ndata['Previous_Class'] = data['Class'].shift(1)\n\n# Extract the tally count for each class in the past d time steps\ndata['Class_-1_Tally'] = data['Class'].shift(1).rolling(window=d).apply(lambda x: sum([1 for i in x if i == -1]))\ndata['Class_0_Tally'] = data['Class'].shift(1).rolling(window=d).apply(lambda x: sum([1 for i in x if i == 0]))\ndata['Class_1_Tally'] = data['Class'].shift(1).rolling(window=d).apply(lambda x: sum([1 for i in x if i == 1]))\n\n# Extract the maximum consecutive run-length for each classin the past d time steps\ndef getLongestRun(x, val):\n    groupedX = [sum(1 for i in g) for k, g in groupby(x) if k == val]\n    if groupedX == []:\n        longestRun = 0\n    else:\n        longestRun = max(groupedX)\n    return longestRun\ndata['Class_-1_Consec'] = data['Class'].shift(1).rolling(window=d).apply(lambda x: getLongestRun(x, -1))\ndata['Class_0_Consec'] = data['Class'].shift(1).rolling(window=d).apply(lambda x: getLongestRun(x, 0))\ndata['Class_1_Consec'] = data['Class'].shift(1).rolling(window=d).apply(lambda x: getLongestRun(x, 1))\n\n# Clean the data\ndata = data[['Class', 'Previous_Class', 'Class_-1_Tally', 'Class_0_Tally', 'Class_1_Tally', 'Class_-1_Consec', 'Class_0_Consec', 'Class_1_Consec']]\ndata = data.dropna()\n\nprint(data.head())","outputs":[],"cell_type":"code","metadata":{"_uuid":"653104a064ab55ceb83ec34e00890a27b533c640","_cell_guid":"5e4139f9-9a2e-409b-aac9-3d945dc8d341"},"execution_count":null},{"source":"# Create Training, Validation, and Test Set\nIt is now time to divide the data set into three pieces: a training, validation, and test set.  The data will be split into 60/20/20 proportions, as is common when using a validation set.","cell_type":"markdown","metadata":{"_uuid":"9c991055b787be7987343bed525fedec186f03e7","_cell_guid":"36b70171-accf-4893-9087-a0a149ebfef2"}},{"source":"timestamps = data.index.tolist()\n\ntrainValSplit = timestamps[round(len(timestamps) * 0.6)]\nvalTestSplit = timestamps[round(len(timestamps) * 0.8)]\n\ntrain = data.loc[:trainValSplit]\nvalidation = data.loc[trainValSplit:valTestSplit]\ntest = data.loc[valTestSplit:]\n\nprint('Training Set Size: \\t' + str(len(train)))\nprint('Validation Set Size: \\t' + str(len(validation)))\nprint('Test Size: \\t\\t' + str(len(test)))","outputs":[],"cell_type":"code","metadata":{"_uuid":"64e9e89267f87162dda6c7332213b17e9d5f1dff","_cell_guid":"d15cb58c-8b21-46d6-8d2f-adf408dc553d"},"execution_count":null},{"source":"# Train the Classifiers\nNow, we will train Logistic Regression and Random Forest classifiers.","cell_type":"markdown","metadata":{"_uuid":"bf60852c6b4ace6e0b867d73e9233daaa9ce30a5","_cell_guid":"c79d25d8-2844-483f-9644-f4d4deefb0bf"}},{"source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\ny_train = train['Class'].values\nX_train = train.drop(['Class'], axis=1).values\n\nlr = LogisticRegression(class_weight='balanced')\nlr.fit(X_train, y_train)\n\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)","outputs":[],"cell_type":"code","metadata":{"_uuid":"4554edd34b9a4b78fa844b1ac15ab08a3f3dbe70","_cell_guid":"78e652ce-5963-458e-a2c8-bd03992e4ff5"},"execution_count":null},{"source":"# Determine Threshold Quality Estimator\nWe now tune the parameter gamma, which represents the threshold confidence a classifier must have for its prediction to be considered.  Amjad and Shah note, quite brilliantly, that there is an inverse relationship between the accuracy of the classifiers, and the number of trades the classifier recommends.  Gamma should therefore be chosen to maximize the product of these two values.","cell_type":"markdown","metadata":{"_uuid":"45fb526d77dc752b74a508f438cbc54a1e44650e","_cell_guid":"5675b282-7ac8-4bc8-af32-e829f9f86392"}},{"source":"import numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\n\ndef getGammaByAccuracy(clf, clfName, X_val, y_val):\n    y_pp = clf.predict_proba(X_val)\n    results = []\n    gammaRange = np.linspace(0, 1, num=100)\n    for gamma in gammaRange:\n        acceptedPredictionIndicies = [i for i in range(len(y_pp)) if (max(y_pp[i]) > gamma)]\n            \n        if acceptedPredictionIndicies != []:\n            subX_val = [X_val[i] for i in acceptedPredictionIndicies]\n            subY_val = [y_val[i] for i in acceptedPredictionIndicies]\n                        \n            # Parse out the instances where 0 was predicted\n            subY_pred = clf.predict(subX_val)\n            keepIndicies = [i for i in range(len(subY_pred)) if subY_pred[i] != 0]\n            subY_pred_parsed = [subY_pred[i] for i in keepIndicies]\n            subY_val_parsed = [subY_val[i] for i in keepIndicies]\n            \n            # Calculate Metrics\n            accuracy = accuracy_score(subY_val_parsed, subY_pred_parsed)\n            precision, recall, fscore, support = precision_recall_fscore_support(subY_val_parsed, subY_pred_parsed, labels=[-1, 0, 1])\n            product = accuracy * len(acceptedPredictionIndicies)\n            results.append((product, gamma, accuracy, len(acceptedPredictionIndicies), precision, recall, fscore, support))\n            \n        else:\n            break\n    product, gamma, accuracy, numPredictions, precision, recall, fscore, support = max(results)\n\n    print('\\n' + clfName + ' Results')\n    print('Product: \\t\\t' + str(product))\n    print('Gamma: \\t\\t\\t' + str(gamma))\n    print('Accuracy: \\t\\t' + str(accuracy)) \n    print('Num Predictions: \\t' + str(numPredictions))\n    print('Precision: \\t\\t' + str(precision)) \n    print('Recall: \\t\\t' + str(recall)) \n    print('F-Score: \\t\\t' + str(fscore)) \n    print('support: \\t\\t' + str(support)) \n    \n    return gamma\n    \ny_val = validation['Class'].values\nX_val = validation.drop(['Class'], axis=1).values\n\nlrGamma = getGammaByAccuracy(lr, 'Logistic Regression', X_val, y_val)\nrfGamma = getGammaByAccuracy(rf, 'Random Forest', X_val, y_val)","outputs":[],"cell_type":"code","metadata":{"_uuid":"80563ff535ab50a131148e9e9ce43163d19d2e48","_cell_guid":"a4a51a61-98fd-4afa-9f57-0e28b687e0a8"},"execution_count":null},{"source":"# Determine Threshold Quality Estimator (My Way)\nAmjad and Shah sought to maximize the product of accuracy and the number of trades.  Accuracy is a risky metric to use for the goodness of a classifier, as it is not always reliable when classes are imbalanced.  For that reason, I believe a better metric to use would be the product of the precisions for class -1 and +1.  Using this figure rather than accuracy will guarentee that the gamma chosen will maximize the precision of our classifier on the relevant classes.","cell_type":"markdown","metadata":{"_uuid":"bb2e46cbebdca8c6c7296823fcbadc60386c2060","_cell_guid":"9d0c6404-8daf-4eda-af26-31c31fc7b475"}},{"source":"def getGammaByPrecision(clf, clfName, X_val, y_val):\n    y_pp = clf.predict_proba(X_val)\n    results = []\n    gammaRange = np.linspace(0, 1, num=100)\n    for gamma in gammaRange:\n        acceptedPredictionIndicies = [i for i in range(len(y_pp)) if (max(y_pp[i]) > gamma)]\n        if acceptedPredictionIndicies != []:\n            subX_val = [X_val[i] for i in acceptedPredictionIndicies]\n            subY_val = [y_val[i] for i in acceptedPredictionIndicies]\n                        \n            # Parse out the instances where 0 was predicted\n            subY_pred = clf.predict(subX_val)\n            keepIndicies = [i for i in range(len(subY_pred)) if subY_pred[i] != 0]\n            subY_pred_parsed = [subY_pred[i] for i in keepIndicies]\n            subY_val_parsed = [subY_val[i] for i in keepIndicies]\n            \n            # Calculate Metrics\n            accuracy = accuracy_score(subY_val_parsed, subY_pred_parsed)\n            precision, recall, fscore, support = precision_recall_fscore_support(subY_val_parsed, subY_pred_parsed, labels=[-1, 0, 1])\n            product = precision[0] * precision[2] * len(acceptedPredictionIndicies)\n            results.append((product, gamma, accuracy, len(acceptedPredictionIndicies), precision, recall, fscore, support))\n            \n        else:\n            break\n    product, gamma, accuracy, numPredictions, precision, recall, fscore, support = max(results)\n    \n    print('\\n' + clfName + ' Results')\n    print('Product: \\t\\t' + str(product))\n    print('Gamma: \\t\\t\\t' + str(gamma))\n    print('Accuracy: \\t\\t' + str(accuracy)) \n    print('Num Predictions: \\t' + str(numPredictions))\n    print('Precision: \\t\\t' + str(precision)) \n    print('Recall: \\t\\t' + str(recall)) \n    print('F-Score: \\t\\t' + str(fscore)) \n    print('support: \\t\\t' + str(support)) \n    \n    return gamma\n    \ny_val = validation['Class'].values\nX_val = validation.drop(['Class'], axis=1).values\n\nlrGamma = getGammaByPrecision(lr, 'Logistic Regression', X_val, y_val)\nrfGamma = getGammaByPrecision(rf, 'Random Forest', X_val, y_val)","outputs":[],"cell_type":"code","metadata":{"_uuid":"37405bc3294a738e09dac35a9b1bb4da6fed7f03","_cell_guid":"20ddb3bd-d18e-4464-a644-ff4a07e656a8"},"execution_count":null},{"source":"# Results So Far (4)\nThe results at this point are not looking good.  Both methods of selecting gamma appear to have yielded the same results.  My thoughts on these results are as follows:\n\n- Accuracy: Neither Logistic Regression nor Random Forest was able to achieve an accuracy over 50%.\n- Logistic Regression: This classifier is has reasonable recall for each class, but poor precision.\n- Random Forest: This classifier has poor precision, and a recall that clearly favors the +1 class.","cell_type":"markdown","metadata":{"_uuid":"71469178d88498b2ed246ae2e8f65edd2552ce5f","_cell_guid":"52bec23c-8472-400b-a641-c551178502d3"}},{"source":"# Results on Test Set\nNow, we will run our trained classifiers on the test set.  In evaluating the classifiers, we will calculate the classification accuracy as Amjad and Shah did.  Additionally, we will calculate the precision, recall, and F-score, as these metrics provide a more complete picture of the classifier's performance.","cell_type":"markdown","metadata":{"_uuid":"d00fa14d4417cbac5634a0ece9c60741f19c51c5","_cell_guid":"3c6f0263-8ef5-43b3-a915-fae1d4ac76a7"}},{"source":"def testPerformance(clf, gamma, clfName, X_test, y_test, timestamps):\n    \n    # Parse out predicitons that don't meet gamma value\n    y_pp = clf.predict_proba(X_test)\n    acceptedPredictionIndicies = [i for i in range(len(y_pp)) if (max(y_pp[i]) > gamma)]\n    subX_test = [X_test[i] for i in acceptedPredictionIndicies]\n    subY_test = [y_test[i] for i in acceptedPredictionIndicies]\n    subTimestamps = [timestamps[i] for i in acceptedPredictionIndicies]\n    \n    # Parse out the instances where 0 was predicted\n    subY_pred = clf.predict(subX_test)\n    keepIndicies = [i for i in range(len(subY_pred)) if subY_pred[i] != 0]\n    subY_pred_parsed = [subY_pred[i] for i in keepIndicies]\n    subY_test_parsed = [subY_test[i] for i in keepIndicies]\n    subTimestamps_parsed = [subTimestamps[i] for i in keepIndicies]\n    \n    # Calculate Metrics\n    accuracy = accuracy_score(subY_test_parsed, subY_pred_parsed)\n    precision, recall, fscore, support = precision_recall_fscore_support(subY_test_parsed, subY_pred_parsed)\n\n    # Print Metrics\n    print('\\n' + clfName + ' Results')\n    print('Accuracy: \\t\\t' + str(accuracy))\n    print('Precision: \\t\\t' + str(precision))\n    print('Recall: \\t\\t' + str(recall)) \n    print('F-Score: \\t\\t' + str(recall))\n    print('Support: \\t\\t' + str(support))\n    \n    # Return predictions, true classes, and timestamps\n    return (subY_pred_parsed, subY_test_parsed, subTimestamps_parsed)\n\ntimestamps = test.index.tolist()\ny_test = test['Class'].values\nX_test = test.drop(['Class'], axis=1).values\n\nlrResults = testPerformance(lr, lrGamma, 'Logistic Regression', X_test, y_test, timestamps)\nrfResults = testPerformance(rf, rfGamma, 'Random Forest', X_test, y_test, timestamps)","outputs":[],"cell_type":"code","metadata":{"_uuid":"9b0ac98c5944b3102762529490d95a2696b9b31a","_cell_guid":"3afa500d-e6a1-4b76-a897-bc8ee167ab25"},"execution_count":null},{"source":"# Conclusion\nWhen it came to the test set, the results were unimpressive.  Both classifiers have poor precision and accuracy.  The best attribute present is the recall of Logistic Regression, which sadly dipped below 50% for the -1 class.  All things considered, I believe this shows that the [results](http://proceedings.mlr.press/v55/amjad16.pdf) of Amjad and Shah were not reproducable.","cell_type":"markdown","metadata":{"_uuid":"1ca05bc5ef67ebbaca467695c8796539617a8d38","_cell_guid":"3132ee2e-5796-4095-be11-f4321d3b53ab"}}],"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1","file_extension":".py","mimetype":"text/x-python","name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}}