{"cells":[{"metadata":{},"cell_type":"markdown","source":"# EDA of Sales of summer clothes in E-commerce Wish dataset\n\nFirst of all we will build a function to analyse the sales data for us and provide important information such as:\n* Data type of each field\n* Which columns has missing data and number of missing records in each column\n* What is the correlation of all the other numeric columns with the target column \n* Now for the numeric features: the mean, median and mode\n* For categorical columns: mode\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"product_sales_df = pd.read_csv('/kaggle/input/summer-products-and-sales-in-ecommerce-wish/summer-products-with-rating-and-performance_2020-08.csv')\ncat_count_df = pd.read_csv('/kaggle/input/summer-products-and-sales-in-ecommerce-wish/unique-categories.sorted-by-count.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_col = 'units_sold'\n\nprint(f\"Shape of dataframe {product_sales_df.shape}\")\n\nrows = []\nfor col in product_sales_df.columns:\n    if product_sales_df[col].isin([0, 1, np.nan]).all():\n        row_dict = {'ColumnName': col, 'DataType': 'binary', 'HasMissing':product_sales_df.isnull().any().loc[col],\n                    'NumberOfMissingCells': product_sales_df.isnull().sum().loc[col], 'CorrelationWithTarget': product_sales_df.corr()[target_col].loc[col],\n                    'Mean': np.nan, 'Median': np.nan,'Mode': product_sales_df.mode()[col].loc[0], 'MinValue': np.nan, 'MaxValue': np.nan }\n    \n    elif product_sales_df.dtypes.loc[col] == 'int64' or product_sales_df.dtypes.loc[col] == 'float64':\n        \n        row_dict = {'ColumnName': col, 'DataType': product_sales_df.dtypes.loc[col], 'HasMissing':product_sales_df.isnull().any().loc[col],\n                    'NumberOfMissingCells': product_sales_df.isnull().sum().loc[col], 'CorrelationWithTarget': product_sales_df.corr()[target_col].loc[col],\n                    'Mean': product_sales_df.mean().loc[col], 'Median':product_sales_df.median().loc[col], 'Mode': product_sales_df.mode()[col].loc[0],\n                    'MinValue': product_sales_df.min().loc[col], 'MaxValue': product_sales_df.max().loc[col] }\n        \n    else:\n        row_dict = {'ColumnName': col, 'DataType': product_sales_df.dtypes.loc[col], 'HasMissing':product_sales_df.isnull().any().loc[col],\n                    'NumberOfMissingCells': product_sales_df.isnull().sum().loc[col], 'CorrelationWithTarget': np.nan, 'Mean': np.nan, 'Median': np.nan,\n                    'Mode': product_sales_df.mode()[col].loc[0], 'MinValue': np.nan, 'MaxValue': np.nan }\n        \n    rows.append(row_dict)\n    \n        \ninfo_df = pd.DataFrame(rows, columns=['ColumnName', 'DataType', 'HasMissing', 'NumberOfMissingCells', 'CorrelationWithTarget', 'Mean', 'Median', 'Mode', 'MinValue', 'MaxValue'])\n\ninfo_df.set_index('ColumnName', inplace=True)\n    \ninfo_df = info_df.sort_values('CorrelationWithTarget', ascending=False, na_position='last')\n\nprint(\"FOR NUMERICAL COLUMNS\")\ninfo_df[info_df['DataType']!='object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nFor categorical/non-numeric columns\")\ninfo_df[info_df['DataType']=='object'].drop(['CorrelationWithTarget', 'Mean', 'Median', 'MinValue', 'MaxValue'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(product_sales_df.corr()[[target_col]].sort_values(by=target_col, ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title(f'Features Correlating with {target_col}', fontdict={'fontsize':18}, pad=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's first start with categorical features and process them\n\n### 1. currency_buyer\nIt has no missing values and the only value is 'EUR'"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['currency_buyer'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. product_color\nHas 41 missing values and most common value is black"},{"metadata":{"trusted":true},"cell_type":"code","source":"count = product_sales_df['product_color'].value_counts()\ncount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sort(product_sales_df['product_color'].dropna().unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are so many colours, let's see if we can combine different shades of a colour into one colour like : example navy blue, blue and light blue into just blue"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['product_color'] = product_sales_df['product_color'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df[product_sales_df['product_color'].str.contains('&', na=False)]['product_color'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shade_to_colour = {\n    'navyblue': 'blue', 'lightblue': 'blue', 'skyblue': 'blue', 'lakeblue': 'blue', 'darkblue': 'blue', 'denimblue': 'blue', 'navy blue': 'blue', 'prussianblue': 'blue',\n    'navy': 'blue',\n    'armygreen': 'green', 'army green': 'green', 'fluorescentgreen': 'green', 'mintgreen': 'green', 'light green': 'green', 'lightgreen': 'green',\n    'applegreen': 'green', 'darkgreen': 'green', 'army': 'green', 'khaki': 'green', 'lightkhaki': 'green',\n    'lightyellow': 'yellow', \n    'winered': 'red', 'wine red': 'red', 'lightred': 'red', 'coralred': 'red', 'rose red': 'red', 'watermelonred': 'red', 'orange-red': 'red', 'rosered': 'red',\n    'claret': 'red', 'burgundy': 'red', \n    'gray': 'grey', 'silver': 'grey','lightgray': 'grey', 'lightgrey': 'grey', 'greysnakeskinprint': 'grey',\n    'coffee': 'brown', 'camel': 'brown', 'tan': 'brown', \n    'offwhite': 'white', 'ivory': 'white', 'nude': 'white',\n    'lightpink': 'pink', 'dustypink':'pink', 'rosegold': 'pink',\n    'lightpurple': 'purple', 'coolblack': 'black', 'apricot': 'orange', 'offblack': 'black'\n}\n\ndef update_color(col):\n    if shade_to_colour.get(col, False):\n        return shade_to_colour.get(col)\n    elif '&' in col:\n        return 'dual'\n    elif col in shade_to_colour.values():\n        return col\n    else:\n        return 'other'\n\nproduct_sales_df['product_color'].replace(np.nan, 'others', inplace=True)\n\nproduct_sales_df['product_color'] = product_sales_df.product_color.apply(update_color)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = product_sales_df['product_color'].value_counts()\ncount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_df = product_sales_df.groupby('product_color').agg('sum')['units_sold'].to_frame()\ncol_df.reset_index(level=0, inplace=True)\ncol_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.barplot(x=\"product_color\", y=\"units_sold\", data=col_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After applying the necesssary transformation of the colour column we can see that black has sold most units followe by white."},{"metadata":{},"cell_type":"markdown","source":"## Tags\nNo missing values\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['tags_count'] = product_sales_df['tags'].str.split(',').str.len()\n\nfig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.lineplot(data=product_sales_df, x=\"tags_count\", y=\"units_sold\", ci=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Products with tags more than 35 are more discoverable and are thus bought more often.There is a sudden spike at just below 10 tags so let's investigate if that's an outlier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df[product_sales_df['tags_count']<=10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there are only 27 products with tags count less than 10 and only 2 with sales of 20000 and rest have sales like 50, 100, 1000, 5000\nSo these two are outlires and thus reson for spike"},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the most common tags with the help of a wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nustr = \" \".join(product_sales_df['tags'].str.lower().str.split(',').sum())\n\nfig = plt.gcf()\nfig.set_size_inches( 16, 10)\nwordcloud = WordCloud(background_color='white').generate(ustr) \nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. product_variation_size_id\n14 missing values\nmost common value is 'S'"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['product_variation_size_id'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['product_variation_size_id'].value_counts().head(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets try to reduce the number of sizes here. "},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].str.lower().str.replace('.', '').str.replace('size--', '').str.replace('size -', '').str.replace('size/', '').str.replace('size ', '').str.replace('size-', '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['product_variation_size_id'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can still few are left."},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('2xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('3xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('4xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('5xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('6xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('x   l', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('sizel', 'l')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('size4xl', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('x   l', 'xl')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace('1 pc - xl', 'xl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def change_size(cl):\n    if cl in 'xl,l,s,xs,m,xxl,xxxs,xxxxxl,xxxxl'.split(','):\n        return cl\n    else:\n        return 'other'\n\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].replace(np.nan, 'OTHER')\nproduct_sales_df['product_variation_size_id'] = product_sales_df['product_variation_size_id'].apply(change_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.barplot(x=\"product_variation_size_id\", y=\"units_sold\", data=product_sales_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.countplot('product_variation_size_id',\n              order = product_sales_df['product_variation_size_id'].value_counts().index,\n              data = product_sales_df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that the majority of the products are of size 'S' but size 'M' has the most units_sold"},{"metadata":{},"cell_type":"markdown","source":"shipping_option_name"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df.groupby('shipping_option_name').agg(['count', 'sum'])['units_sold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches( 25, 16)\nsns.barplot(x=\"shipping_option_name\", y=\"units_sold\", data=product_sales_df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## urgency_text"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df.urgency_text.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's drop  urgency_text,\ntitle, title_orig, currency_buyer, urgency_tex, merchant_title,merchant_name, \nmerchant_id,merchant_profile_picture,product_url\tobject, product_picture, \nproduct_id, theme and crawl_month"},{"metadata":{},"cell_type":"markdown","source":"## origin_country"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df.groupby('origin_country').agg(['count', 'sum'])['units_sold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_na_merchants = product_sales_df[product_sales_df['origin_country'].isna()]['merchant_id'].values\n\nfor m in list_of_na_merchants:\n    print(\"merchant title \" + m)\n    print(product_sales_df[product_sales_df['merchant_id']==m]['origin_country'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So merchants which have missing origin country cannot be replaced by looking at the origin_country of the same merchant for another product.\nThis is based on the assumnption that a country of origin for a merchant should have the same value across product.\n\nWe will replace this with CN as its the most frequent value.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['origin_country'].fillna('CN', inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are done with processing the categorical columns.\nNow lets start  with the numerical data\n\nThe first column is 'units_sold' which is our target variable. But we don't want to predict the number of sales for a product but rather if the product has been successful on the Wish.com platform.\nSo we will start by converting the numerical data to a binary one. (successful or not)\nunits_sold has no missing values and has a median of 1000. So we will consider products to be successful if they have sales greater than 1000.\nNow we are using median because because it not affected by very high outliers\n\n## units_sold"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['success'] = product_sales_df['units_sold'].apply(lambda x: 1 if x>1000 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## rating_count\nno missing values, highest correlation with target"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.scatterplot(data=product_sales_df, x=\"rating_count\", y=\"units_sold\", hue='success', size='units_sold', sizes=(10, 200))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus we can see that the number of rating affects the success of a product greatly.\n\n## 'rating_five_count', 'rating_four_count', 'rating_three_count','rating_two_count', 'rating_one_count'\n\nThey all have very high correlation with the target variable and have 45 missing values each.\nIt turns out that these products with missing values for these 5 columns have no ratings (rating_count=0) \nSo let's replace these values with zero. Also we find that the value of 'rating' column for these products is 5 which is unlikely given the products have zero rating\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df.update(product_sales_df[['rating_five_count', 'rating_four_count', 'rating_three_count','rating_two_count', 'rating_one_count']].fillna(0))\nproduct_sales_df.loc[product_sales_df['rating_count']==0, 'rating'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our next step is to normalise these columns as rating count for 'five' has a similar correlation with rating count for 'one' \nThis is unlikely as product with low ratings is less likely to be successful"},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df[['rating_five_count', 'rating_four_count', 'rating_three_count','rating_two_count', 'rating_one_count', 'rating_count', 'rating']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='success', y='rating_five_count', data=product_sales_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='success', y='rating_one_count', data=product_sales_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['rating_three_count_prop'] = product_sales_df['rating_three_count']/product_sales_df['rating_count']\nproduct_sales_df['rating_four_count_prop'] = product_sales_df['rating_four_count']/product_sales_df['rating_count']\nproduct_sales_df['rating_five_count_prop'] = product_sales_df['rating_five_count']/product_sales_df['rating_count']\nproduct_sales_df['rating_two_count_prop'] = product_sales_df['rating_two_count']/product_sales_df['rating_count']\nproduct_sales_df['rating_one_count_prop'] = product_sales_df['rating_one_count']/product_sales_df['rating_count']\n# to remove nan due to zero division\nproduct_sales_df.update(product_sales_df[['rating_five_count_prop', 'rating_four_count_prop', 'rating_three_count_prop','rating_two_count_prop', 'rating_one_count_prop']].fillna(0))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='success', y='rating_one_count_prop', data=product_sales_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='success', y='rating_five_count_prop', data=product_sales_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='success', y='rating_four_count_prop', data=product_sales_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus normalising the data really solves the problem\n\n## merchant_rating_count\n\nhigh correlation and no missing values\n\n## merchant_has_profile_picture\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.countplot(data=product_sales_df, x='merchant_has_profile_picture', hue='success')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that a merchant is more likely to be successful if he has a profile picture\nThus the correlation with units_sold is high too\n\n## merchant_rating"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.gcf()\nfig.set_size_inches( 16, 10)\nsns.violinplot(data=product_sales_df, y='merchant_rating', x='success')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df['has_urgency_banner'] = product_sales_df['has_urgency_banner'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## dropping unnecessary columns\nproduct_sales_df = product_sales_df.drop(['crawl_month','product_id','product_picture', 'product_url', 'merchant_profile_picture', 'merchant_id',\n                       'currency_buyer', 'theme','urgency_text', 'merchant_title', 'merchant_name', 'merchant_info_subtitle',\n                      'title','title_orig','tags', 'shipping_option_name', \"inventory_total\" , \"badge_fast_shipping\" ,\n                       \"badge_local_product\" , \"shipping_is_express\", \"units_sold\"], axis = 1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df = product_sales_df.drop([\"rating_five_count\",\"rating_four_count\",\"rating_three_count\", \"rating_two_count\", \"rating_one_count\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df = pd.get_dummies(product_sales_df, \n                           columns = ['product_color'],\n                           prefix = 'color_',\n                           drop_first = True)\nproduct_sales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df = pd.get_dummies(product_sales_df, \n                           columns = ['product_variation_size_id'],\n                           prefix = 'size_',\n                           drop_first = True)\nproduct_sales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df = pd.get_dummies(product_sales_df, \n                           columns = ['origin_country'],\n                           prefix = 'country_',\n                           drop_first = True)\nproduct_sales_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"product_sales_df.isna().any().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(product_sales_df.drop(['success'], axis=1), product_sales_df['success'],\ntest_size=0.2, random_state=1, stratify=product_sales_df['success'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=1)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\n\naccuracy_score(y_pred, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forests Classifier gives us a really good accuracy of 96% on the test set. But let's check this with k fold cross validation and see if there is overfitting.\nWe can also checkout the important features (columns) in this dataset using the RandomForestClassifier as decision trees are one of the great ways to do so."},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.DataFrame()\nfeatures['feature'] = X_train.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\nfeatures.plot(kind='barh', figsize=(25, 25))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like rating count has too much effect on our target variable.\nLets try to remove this see the graph again"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.DataFrame()\nfeatures['feature'] = X_train.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures = features[features['feature']!='rating_count']\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\nfeatures.plot(kind='barh', figsize=(25, 25))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets try removing the unnecessary features and see how it affects our accuracy. We will be using Sklearn's SelectFromModel"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\n\nmodel = SelectFromModel(clf, prefit=True)\ntrain_reduced = model.transform(X_train)\nprint(train_reduced.shape)\n\ntest_reduced = model.transform(X_test)\nprint(test_reduced.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result are not that different, lets try PCA and see if we can improve the accuracy "},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n# from sklearn.decomposition import PCA\n\n# sc = StandardScaler()\n# train_scaled = sc.fit_transform(X_train)\n# test_scaled = sc.transform(X_test)\n\n# pca = PCA(n_components=20)\n# train_reduced = pca.fit_transform(train_scaled)\n# test_reduced = pca.transform(test_scaled)\n# print(train_reduced.shape)\n# print(test_reduced.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now fit our data into diferent classfiers and choose the best model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_score(clf, X, y, scoring='accuracy'):\n    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring)\n    return np.mean(xval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nlogreg = LogisticRegression()\nlogreg_cv = LogisticRegressionCV()\nrf = RandomForestClassifier()\ngboost = GradientBoostingClassifier()\ngnb = GaussianNB()\nsvm = SVC()\nknn = KNeighborsClassifier()\nxgboost = XGBClassifier()\ndc = DecisionTreeClassifier()\nadc = AdaBoostClassifier()\nmodels = [logreg, logreg_cv, rf, gboost, gnb, svm, knn, xgboost, dc, adc]\n\nfor model in models:\n    print('Cross-validation of : {0}'.format(model.__class__))\n    score = compute_score(clf=model, X=train_reduced, y=y_train, scoring='accuracy')\n    print('CV score = {0}'.format(score))\n    print('****')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = XGBClassifier()\nclf.fit(train_reduced, y_train)\n\ny_pred = clf.predict(test_reduced)\n\naccuracy_score(y_pred, y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}