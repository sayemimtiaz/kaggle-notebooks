{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Feature Selection:** is one of the core concept in machine learning which highly impact on the performance of model. Feature selection is a process when we automatically select the most important features which contributes the most in predicting the output.\n\nHaving irrelavent features in your data can decrease the accuracy of many models, especially linear algorithms like linear regression or logistic regression\n\n**There are 3 Benifits of using feature selection before modelling**\n- Reduce Overfitting\n- Improves accuracy\n- Reduce Training Time\n\nlet's get started with Feature selection methods. we will be looking at atlest 5 best feature selection methods which are mostly used.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load the data\ndata = pd.read_csv('/kaggle/input/mobile-price-classification/train.csv')\ndf = data.copy()  #create a copy\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* In this particular problem we have to determine the price_range of mobile phones using various features. Now by using various feature selection methods we will be selecting the top 10 features.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1) Univariate Selection\nStatistical tests can be used to select those features that have the strongest relationship with the output variable.\n\nThe scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n\nThe example below uses the chi-squared (chiÂ²) statistical test for non-negative features to select 10 of the best features from the Mobile Price Range Prediction Dataset","metadata":{}},{"cell_type":"code","source":"x = data.iloc[:, :-1]\ny = data.iloc[:, -1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#apply selectKBest to select top 10 features\nbest_features = SelectKBest(score_func=chi2, k=10)\nfit = best_features.fit(x, y)\n\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(x.columns)\n\n#concat 2 dataFrame for better visualization\nfeature_score = pd.concat([dfcolumns, dfscores], axis=1)\nfeature_score.columns = ['Features', 'Score']\nprint(feature_score.nlargest(10, 'Score'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Many different statistical test can be used with this selection method. For example the ANOVA F-value method is appropriate for numerical inputs and categorical data. This can be used via the f_classif() function.","metadata":{}},{"cell_type":"markdown","source":"## 2) Feature Importance\nwe can get the importance of each feature by using the feature importance property. The technique gives us a score for each feature in a data, The higher the score is, more relevant feature is in predicting the output.\n\nBagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.\n\nwe will be using Extra Tree Classifier for extracting the top 10 features for the dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\nmodel.fit(x, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importance = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importance.nlargest(10).plot(kind='barh')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3) Correlation Matrix using HeatMap\n\nCorrelation stats that how features are related to each-other or the target variable. Correlation can be positive(strongly positive relationship) as well as negative(strongly negative relationship)\n\nheatmap makes it easy to identify which features are highly corelated to each other.","metadata":{}},{"cell_type":"code","source":"corr_mat = df.corr()\ncorr_mat_features = corr_mat.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,14))\nsns.heatmap(df[corr_mat_features].corr(), annot=True, cmap='RdYlGn')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### How to remove the corelated features?\nwe can remove the highly corelated features using the threshold.   ","metadata":{}},{"cell_type":"code","source":"threshold = 0.8\n\n# find and remove correlated features\ndef correlation(dataset, threshold):\n    col_corr = set()             # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation(df.iloc[:, :-1], threshold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4) Information Gain\nwe can select the most important features using the impormation gain of each feature.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mutual_info = mutual_info_classif(x,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mutual_data = pd.Series(mutual_info,index=x.columns)\nmutual_data.sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5) Recursive Feature Elimination\nThe Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\n\nIt uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n\nThe example below uses RFE with the logistic regression algorithm to select the top 3 features. The choice of algorithm does not matter too much as long as it is skillful and consistent.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LogisticRegression(solver='lbfgs')\nrfe = RFE(model, 10)\nfit = rfe.fit(x,y)\n\nprint(\"Num Features: %d\", fit.n_features_)\nprint(\"selected features: %s\", fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6) Principal Component Analysis\nPrincipal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form.\n\nGenerally this is called a data reduction technique. A property of PCA is that you can choose the number of dimensions or principal component in the transformed result.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components = 10)\npca_fit = pca.fit(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize components\nprint(\"Explained Variance: %s\" % pca_fit.explained_variance_ratio_)\nprint(pca_fit.components_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}