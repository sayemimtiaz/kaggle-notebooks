{"cells":[{"metadata":{},"cell_type":"markdown","source":"# PMSM Temperature Estimation\n\n**Short Introduction:** Temperature estimation in electric motors using the classic non-ML approach (i.e. lumped parameter equivalent circuits) is a cumbersome task. Both modelling and testing is time consuming and requires expert domain knowledge. Therefore, machine learning seems like an appealing alternative, especially with modern and sophisticated deep neural nets for time series prediction. The dataset used in this notebook contains a series of reduced and anonymized test bench recordings of electric motors provided by the german \"Paderborn University\". \n\nPaderborn University has published several research paper on this topic. The main statements are:\n* State-of-the-art temperature estimation can be done with neural networks.\n* Both LSTMs and TCNs with residual connections do well on the task.\n* Even small LSTMs (around 50k parameters) can get reasonable accuracy.\n* Among other things, they used TBPTT, Chrono-Initialization, Dropout, Learning Rate Decay as well as Bayesian Hyperparameter Search to find the best model. \n\nFor more details on the data source and links to the papers, see the information that comes with the dataset.\n\nParts of the research findings are used in this notebook. The goal is to show the basic function principle. Therefore, the models are being kept relatively simple to reduce training time. In addition, there is no large hyperparameter search process. Instead, they are chosen close to the optimal hyperparameters found in the original research work. "},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# ML\nimport tensorflow as tf\nprint('TensorFlow version: ', tf.__version__)\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Other\nimport zipfile\n\n# Clear Session\ntf.keras.backend.clear_session()\nprint('TensorFlow session cleared.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Loading & Visualization\n* Load data into pandas dataframe.\n* Get basic info about dataframe and check if any data cleaning is necessary.\n* Plot some of the data (random load cycle).\n* Save all load cycles to disk.\n* Plot load cycle lengths."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions to be used in this section.\n\n# plot_ID plots most important features of a profile ID (= load cycle)\ndef plot_ID(df, PID, path = None):\n    \n    vis_df = df.where(data_df['profile_id'] == PID).dropna()\n\n    # Change index to time vector (sample rate is 2 Hz)\n    vis_df.index = np.arange(0, 2*len(vis_df['profile_id']), 2)\n    vis_df.index.name = 'time'\n    \n    fig, axes = plt.subplots(5, 1, sharex = True, sharey = True, figsize=(15, 10))\n\n    # Motor speed\n    axes[0].plot(vis_df.index, vis_df['motor_speed'])\n    axes[0].set_title('speed')\n    \n    # Torque\n    axes[1].plot(vis_df.index, vis_df['torque'])\n    axes[1].set_title('torque')\n\n    # Current (d/q)\n    axes[2].plot(vis_df.index, vis_df['i_d'])\n    axes[2].plot(vis_df.index, vis_df['i_q'])\n    axes[2].legend(['i_d', 'i_q'], loc='right')\n    axes[2].set_title('current d/q')\n    \n    # Voltage (d/q)\n    axes[3].plot(vis_df.index, vis_df['u_d'])\n    axes[3].plot(vis_df.index, vis_df['u_q'])\n    axes[3].set_title('voltage d/q');\n    axes[3].legend(['u_d', 'u_q'], loc='right')\n    \n    # Relevant Temperatures (Ambient, Coolant, PM, Winding)\n    axes[4].plot(vis_df.index, vis_df['ambient'])\n    axes[4].plot(vis_df.index, vis_df['coolant'])\n    axes[4].plot(vis_df.index, vis_df['pm'])\n    axes[4].plot(vis_df.index, vis_df['stator_winding'])\n    axes[4].legend(['amb', 'cool', 'pm', 'wdg'], loc='right')\n    axes[4].set_title('temperatures')\n    axes[4].set_xlabel('time')\n\n    plt.subplots_adjust(hspace=0.5);\n    fig.suptitle('Profile_ID ' + str(PID), fontsize=\"x-large\", fontweight='bold')\n    \n    # if path is given, figure is saved to directory instead of printed\n    if path:\n        fig.savefig(path + '/Profile_' + str(PID) + '.png'), \n                    #dpi=f.dpi, bbox_inches='tight', bbox_extra_artists=[ttl])\n        plt.close(fig)\n\n        \n# plot_ID_len plots the length of the unique load cycles.\ndef plot_ID_len(df):\n    \n    vis_df = df.groupby(['profile_id'])\n    vis_df = vis_df.size().sort_values().rename('length').reset_index()\n    ordered_ids = vis_df.profile_id.values.tolist()\n    fig = plt.figure(figsize=(17, 5))\n    sns.barplot(y='length', x='profile_id', data=vis_df, order=ordered_ids)\n    tcks = plt.yticks(2*3600*np.arange(1, 8), [f'{a} hrs' for a in range(1, 8)]) # 2Hz sample rate\n    print('Max load profile length: {:.2f} h'.format(max(vis_df['length'])/(2*3600)))\n    print('Min load profile length: {:.2f} h'.format(min(vis_df['length'])/(2*3600)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import the dataset and print basic info\n\ndata_df = pd.read_csv('/kaggle/input/electric-motor-temperature/pmsm_temperature_data.csv')\n\nprint('---- Dataset Info ----')\ndata_df.info()\nprint('----------------')\nprint('NaNs existing in dataset is: ', data_df.isnull().values.any())\nprint('----------------')\ndata_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize load cycles (= profile_id)\n\n# Profile_ID is a unique ID for each measurement.\nIDs = np.array(data_df['profile_id'].unique())\nprint('Profile ID count: ', len(IDs))\nprint('Unique Profile IDs (= load cycles):\\n', IDs)\n\n# Let's visualize a random load cycle\nPID = np.random.choice(IDs)\nprint('Plotting Profile ID: ', PID, '....')\nplot_ID(data_df, PID)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Manually examine all load cycles by saving figures to disk.\n    \nSaveAllCycles = False\n    \nif SaveAllCycles:    \n    path = os.getcwd()\n\n    if not os.path.exists(path + '/cycles'):\n        os.mkdir('/kaggle/working/cycles')\n\n    #for PID in data_df['profile_id'].unique():\n    #    plot_ID(data_df, PID, path = '/kaggle/working/cycles')\n\n    myzip = zipfile.ZipFile('/kaggle/working/cycles.zip', 'w')\n    for _, _, files in os.walk('/kaggle/working/cycles'):\n        [myzip.write('cycles/' + fname) for fname in files]\n\n    print('Saved all figures and created archive.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize load cycle lengths. Depending on the NN architecture, the profiles need to be zero-padded.\n\nplot_ID_len(data_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering and Preprocessing\n\n* Derive additional features.\n* Prepare data for training (zero-padding, downsampling, mini-batches).\n* Train/Dev/Test-Split.\n* Normalization / Scaling."},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameters and shared utils for Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters (downsampling, train/dev/test-split, feature selection, ...)\ndownsample_rate = 4\nn_dev = 2 \nn_test = 1\nwindow_len = 64\nfeatures = ['ambient', 'coolant', 'u_d', 'u_q', 'motor_speed', 'i_d', 'i_q', 'u_s', 'i_s', 's_el']\nfeature_len = len(features)\ntarget = ['stator_winding']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates single sample array of shape (1, T, n)\ndef get_single_sample(df, n_feat, sample_len, downsample_rate=1):\n    \n    # Get new length for downsampling\n    new_len = int(np.floor((max_len + downsample_rate - 1) / downsample_rate) * downsample_rate)\n    \n    # Convert df to numpy array of shape (1, T, n)\n    arr = df.to_numpy()\n    arr = np.expand_dims(arr, axis=0)\n    \n    # Zero-pad to sample_len at the end of the array\n    _arr = np.zeros((1, new_len - np.size(arr, 1), n_feat))\n    sample = np.concatenate((arr, _arr), axis=1)\n    \n    # Get sample_weights (zero-padded elements should have zero weight)\n    weights = np.concatenate((np.ones(arr.shape), np.zeros(_arr.shape)), axis=1)\n    weights = weights[:,:,0]\n    \n    # Perform Downsampling\n    dwn_sample = []\n    dwn_weights = []\n    for d in np.arange(1,downsample_rate+1):\n        dwn_sample.append(sample[:,(-1+d)::downsample_rate,:])\n        dwn_weights.append(weights[:,(-1+d)::downsample_rate])\n    \n    sample = np.concatenate(dwn_sample, axis=0)\n    weights = np.concatenate(dwn_weights, axis=0)\n        \n    return sample, weights\n\n\n# Creates windowed mini-batches of shape (m, T_windowed, n) with consistent order of batches. \n# This is necessary for TCNs where window_len should match the receptive field of the TCN.\n# It could also be used with stateful LSTMs to implement truncated backprop through time (TBPTT).\ndef get_windowed_batches(X, weights, Y, window_len):\n    \n    if window_len >= X.shape[1]:\n        raise ValueError('Window length must be less than total batch length.')\n    \n    # get number of splits and clip data to integer splits \n    # (the \"loss\" of data is affordable, mostly zero-padded data is cut away)\n    T = X.shape[1]\n    remainder = np.remainder(X.shape[1],window_len)\n    X = X[:,:-remainder,:]\n    weights = weights[:,:-remainder]\n    Y = Y[:,:-remainder,:]\n    n_splits = int(X.shape[1]/window_len)\n    \n    # split input data accordingly\n    X_win = np.split(X, n_splits, axis=1)\n    weights_win = np.split(weights, n_splits, axis=1)\n    Y_win = np.split(Y, n_splits, axis=1)\n\n    # reshape dimensions\n    X_win = np.vstack(X_win)\n    weights_win = np.vstack(weights_win)\n    Y_win = np.vstack(Y_win)\n    \n    return X_win, weights_win, Y_win","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Derive additional features, create batches and train/dev/test-split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Derive additional features: Current and voltage magnitude & electrical apparent power\n\ndata_df['u_s'] = np.sqrt(data_df['u_d']**2 + data_df['u_q']**2)\ndata_df['i_s'] = np.sqrt(data_df['i_d']**2 + data_df['i_q']**2)\ndata_df['s_el'] = 1.5 * data_df['u_s'] * data_df['i_s']\n\ndata_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare Data for use with LSTMs: Data needs to be in shape (m, T, n)\n\n# get maximum length, select features and target\nmax_len = data_df.groupby(['profile_id']).size().max()\n\n# Prepare index for faster iteration\niter_df = data_df.copy() # copy increases memory use, but avoids errors when executed twice. Better solution?\niter_df['idx'] = data_df.index\niter_df.set_index(['profile_id', 'idx'], inplace = True)\n\n# create (m, T, n) array for X_values, sample_weights and Y_values\nbatch_samples_X = []\nbatch_weights_X = []\nbatch_samples_Y = []\n\nfor pid in IDs:\n    # select profile\n    df = iter_df.loc[pid]\n    # get X samples and weights\n    sample, weights = get_single_sample(df[features], 10, max_len, downsample_rate)\n    batch_samples_X.append(sample)\n    batch_weights_X.append(weights)    \n    # get Y samples\n    sample, _ = get_single_sample(df[target], 1, max_len, downsample_rate)\n    batch_samples_Y.append(sample)\n    \nX_vals = np.concatenate(batch_samples_X, axis=0)\nX_weights = np.concatenate(batch_weights_X, axis=0)\nY_vals = np.concatenate(batch_samples_Y, axis=0)\n\nprint('Shape of batches')\nprint('X_vals:    ', X_vals.shape)\nprint('X_weights: ', X_weights.shape)\nprint('Y_vals:    ', Y_vals.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create train-dev-test-split for LSTMs\n# (when cycles are downsampled, all downsampled parts should belong to the same set)\nX_train = X_vals[:-(n_dev+n_test)*downsample_rate,:,:]\nX_train_weights = X_weights[:-(n_dev+n_test)*downsample_rate,:]\nY_train = Y_vals[:-(n_dev+n_test)*downsample_rate,:,:]\n\nX_dev = X_vals[-((n_dev+n_test)*downsample_rate):-(n_test)*downsample_rate,:,:]\nX_dev_weights = X_weights[-((n_dev+n_test)*downsample_rate):-(n_test)*downsample_rate,:]\nY_dev = Y_vals[-((n_dev+n_test)*downsample_rate):-(n_test)*downsample_rate,:,:]\n\nX_test = X_vals[-((n_test)*downsample_rate):,:,:]\nX_test_weights = X_weights[-((n_test)*downsample_rate):,:]\nY_test = Y_vals[-((n_test)*downsample_rate):,:,:]\n\nprint('Shape of train-test-split')\nprint('train (X, weights, Y): ', X_train.shape, X_train_weights.shape, Y_train.shape)\nprint('dev (X, weights, Y):   ', X_dev.shape, X_dev_weights.shape, Y_dev.shape)\nprint('test (X, weights, Y):  ', X_test.shape, X_test_weights.shape, Y_test.shape)\n\n\n# Normalization / Scaling \n# >> optional, values are already at a similar scale\n\n# EWMA filtering \n# >> optional, this de-noises the data which helps with learning and prediction\n# >> however, real sensor data is also noisy, so the task is harder and more realistic with noise.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data preparation for TCNs is a little different comparted to LSTMs.\n# TCNs will use windowed mini-batches that should match the receptive field of the TCN.\n\nX_train_tcn, X_train_weights_tcn, Y_train_tcn = get_windowed_batches(\n    X_train, X_train_weights, Y_train, window_len)\nX_dev_tcn, X_dev_weights_tcn, Y_dev_tcn = get_windowed_batches(\n    X_dev, X_dev_weights, Y_dev, window_len)\nX_test_tcn, X_test_weights_tcn, Y_test_tcn = get_windowed_batches(\n    X_test, X_test_weights, Y_test, window_len)  \n\nprint('Shape of windowed mini-batch train-test-split')\nprint('train (X, weights, Y): ', X_train_tcn.shape, X_train_weights_tcn.shape, Y_train_tcn.shape)\nprint('dev (X, weights, Y):   ', X_dev_tcn.shape, X_dev_weights_tcn.shape, Y_dev_tcn.shape)\nprint('test (X, weights, Y):  ', X_test_tcn.shape, X_test_weights_tcn.shape, Y_test_tcn.shape)\n\n# Get batch_size and verify mini-batches by plotting re-stacked mini-batch\nbatch_size = X_train.shape[0]\nsample_cycle = 49\nplt.plot(Y_train[sample_cycle,:,0])\nplt.plot(np.concatenate(Y_train_tcn[sample_cycle::batch_size,:,0], axis=0))\nplt.legend(['full batch', 'mini batch'])\nplt.title('Verification of mini-batch creation');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network Training\n* LSTM - A simple (sequential) LSTM architecture.\n* ResLSTM - LSTM with residual connections.\n* TCN - Temporal Convolutional Network with residual connections."},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameters and shared utils for model training\nTo keep things simple the same hyperparameters are used for all models that are being compared (if applicable). The number of units in the LSTM units is equal to the number of filters used for the TCN. Also, the model architecture is kept similar: All models share a final dense network consisting of three layers with descending number of units. For residual networks, two blocks are used for both LSTM and TCN. The same learning and dropout rate is used, however spatial dropout is only applied to TCNs."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 100\nlr = 0.01\nlr_decay = 1e-2\ndropout_rate = 0.1\nspatial_dropout = 0.7\nn_units = 64\nn_dense_in = 32\nn_dense_mid = 8\nn_dense_out = 1\nlen_kernel = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plots the loss over all epochs and a zoom on the last 20 epochs.\ndef plot_learning_curves(history, descr=' '):\n\n    # get results\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(len(loss)) \n    \n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n    # loss\n    axes[0].plot(epochs, loss, 'r')\n    axes[0].plot(epochs, val_loss, 'b')\n    axes[0].set_title('Loss - Train vs. Validation')\n    axes[0].legend(['Train', 'Validation'])\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('loss')\n    \n    # mse\n    axes[1].plot(epochs[n_epochs-20:], loss[n_epochs-20:], 'r')\n    axes[1].plot(epochs[n_epochs-20:], val_loss[n_epochs-20:], 'b')\n    axes[1].set_title('Loss - Zoom to last 20 epochs')\n    axes[1].legend(['Train', 'Validation'])\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('loss')\n    axes[1].set_xticks(np.arange(n_epochs-20, n_epochs, step=2))\n    \n    fig.suptitle(descr, fontsize=\"x-large\", fontweight='bold')\n    \n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SimpleLSTM\nFirst, let's start with a simple LSTM. There should be about 50k trained parameters. This number is close to the simplest model used in the research paper (though this one here uses no residual connections). According to the research findings it should already reach a descent performance in predicting the electric motor temperature."},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_lstm = tf.keras.models.Sequential([\n  tf.keras.layers.LSTM(n_units, return_sequences=True, input_shape=[None, feature_len]),\n  tf.keras.layers.LSTM(n_units, return_sequences=True),\n  tf.keras.layers.Dense(n_dense_in, activation=\"relu\"),\n  tf.keras.layers.Dropout(dropout_rate),\n  tf.keras.layers.Dense(n_dense_mid, activation=\"relu\"),\n  tf.keras.layers.Dropout(dropout_rate),\n  tf.keras.layers.Dense(n_dense_out),\n])\n\nsimple_lstm.summary()\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr, \n                                     decay=lr_decay,\n                                     beta_1=0.9, beta_2=0.999, amsgrad=False)\n\nsimple_lstm.compile(loss='mean_squared_error',\n              optimizer=optimizer,\n              metrics=['mse'],\n              sample_weight_mode='temporal')\n\nprint('---- training in progress ----')\n\nhistory = simple_lstm.fit(x=X_train, y=Y_train, \n                          validation_data=(X_dev, Y_dev, X_dev_weights), \n                          sample_weight=X_train_weights, \n                          epochs=n_epochs,\n                          verbose=0)\n\nprint('--- done ---')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curves(history, descr='simple_LSTM')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ResLSTM\nThe ResLSTM adds residual connections to the architecture using keras functional API. The Dense layers and number of nodes of the LSTMs will be the same as the simple_LSTM model. To reduce computational cost, the number of parameters is about 120k where in the original research paper >850k parameters were trained."},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = tf.keras.layers.Input(shape=[None, feature_len])\n\n# First residual LSTM block\nout_1 = tf.keras.layers.LSTM(n_units, return_sequences=True)(inputs)\nout_2 = tf.keras.layers.LSTM(n_units, return_sequences=True)(out_1)\nadd_1 = tf.keras.layers.Add()([out_1, out_2])\n\n# Second residual LSTM block\nout_3 = tf.keras.layers.LSTM(n_units, return_sequences=True)(add_1)\nout_4 = tf.keras.layers.LSTM(n_units, return_sequences=True)(out_3)\nadd_2 = tf.keras.layers.Add()([out_3, out_4])\n\n# Dense Layer\nx = tf.keras.layers.Dense(n_dense_in, activation=\"relu\")(add_2)\nx = tf.keras.layers.Dropout(dropout_rate)(x)\nx = tf.keras.layers.Dense(n_dense_mid, activation=\"relu\")(x)\nx = tf.keras.layers.Dropout(dropout_rate)(x)\ny = tf.keras.layers.Dense(n_dense_out)(x)\n\nres_lstm = tf.keras.models.Model(inputs=[inputs], outputs=y)\n\nres_lstm.summary()\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr, \n                                     decay=lr_decay,\n                                     beta_1=0.9, beta_2=0.999, amsgrad=False)\n\nres_lstm.compile(loss='mean_squared_error',\n              optimizer=optimizer,\n              metrics=['mse'],\n              sample_weight_mode='temporal')\n\nprint('---- training in progress ----')\n\nhistory = res_lstm.fit(x=X_train, y=Y_train, \n                          validation_data=(X_dev, Y_dev, X_dev_weights), \n                          sample_weight=X_train_weights, \n                          epochs=n_epochs,\n                          verbose=0)\n\nprint('--- done ---')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curves(history, descr='res_LSTM')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Temporal Convolutional Network\nTCNs use causual and dilated 1D convolutions for time series prediction. Latest studies show that they have longer memory capabilites than LSTMs and can outperform them in various tasks on both artifical and real datasets - including this temperature estimation task. Here, the TCN has some issues with the dev set, which could be because the dev set contains fairly noisy data and the windowing makes it difficult for the TCN to handle it."},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = tf.keras.layers.Input(shape=[None, feature_len])\n\n# First residual TCN block\nx_1 = tf.keras.layers.Conv1D(filters=n_units, kernel_size=len_kernel, \n           dilation_rate=1, activation='relu', padding='causal')(inputs)\nx_1 = tf.keras.layers.Conv1D(filters=n_units, kernel_size=len_kernel, \n           dilation_rate=2, activation='relu', padding='causal')(x_1)\nx_1 = tf.keras.layers.Conv1D(filters=n_units, kernel_size=len_kernel, \n           dilation_rate=4, activation='relu', padding='causal')(x_1)\nx_1 = tf.keras.layers.Conv1D(filters=n_units, kernel_size=len_kernel, \n           dilation_rate=8, activation='relu', padding='causal')(x_1)\nx_1 = tf.keras.layers.SpatialDropout1D(rate=spatial_dropout)(x_1)\nx_1_res = tf.keras.layers.Conv1D(filters=n_units, kernel_size=1, dilation_rate=2, padding='causal')(inputs)\nx_1 = tf.keras.layers.Add()([x_1_res, x_1])\n\n# Second residual TCN block\nx_2 = tf.keras.layers.Conv1D(filters=n_units, kernel_size=len_kernel, \n           dilation_rate=1, activation='relu', padding='causal')(x_1)\nx_2 = tf.keras.layers.Conv1D(filters=n_units, kernel_size=len_kernel, \n           dilation_rate=2, activation='relu', padding='causal')(x_2)\nx_2 = tf.keras.layers.Conv1D(filters=n_units, kernel_size=len_kernel, \n           dilation_rate=4, activation='relu', padding='causal')(x_2)\nx_2 = tf.keras.layers.Conv1D(filters=n_units, kernel_size=len_kernel, \n           dilation_rate=8, activation='relu', padding='causal')(x_2)\nx_2 = tf.keras.layers.SpatialDropout1D(rate=spatial_dropout)(x_2)\nx_2_res = tf.keras.layers.Conv1D(filters=n_units, kernel_size=1, dilation_rate=2, padding='causal')(x_1)\nx_2 = tf.keras.layers.Add()([x_2_res, x_2])\n\n# Dense Layer\nx_3 = tf.keras.layers.Dense(n_dense_in, activation=\"relu\")(x_2)\nx_3 = tf.keras.layers.Dropout(dropout_rate)(x_3)\nx_3 = tf.keras.layers.Dense(n_dense_mid, activation=\"relu\")(x_3)\nx_3 = tf.keras.layers.Dropout(dropout_rate)(x_3)\ny = tf.keras.layers.Dense(n_dense_out)(x_3)\n\nTCN = tf.keras.models.Model(inputs=[inputs], outputs=y)\n\nTCN.summary()\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr, \n                                     decay=lr_decay,\n                                     beta_1=0.9, beta_2=0.999, amsgrad=False)\n\nTCN.compile(loss='mean_squared_error',\n              optimizer=optimizer,\n              metrics=['mse'],\n              sample_weight_mode='temporal')\n\nprint('---- training in progress ----')\n\nhistory = TCN.fit(x=X_train_tcn, y=Y_train_tcn, \n                          validation_data=(X_dev_tcn, Y_dev_tcn, X_dev_weights_tcn), \n                          sample_weight=X_train_weights_tcn, \n                          epochs=n_epochs,\n                          batch_size=batch_size,\n                          verbose=0)\n\nprint('--- done ---')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curves(history, descr='TCN')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation\n* Optimizing metric is MSE (mean squared error).\n* An arbitrary load cycle from the training set is plotted to show and validate training results.\n* The worst load cycle from the dev set is plotted for hyperparameter tuning."},{"metadata":{},"cell_type":"markdown","source":"## Shared utils for evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function makes a prediction on a given X of shape (1,Tx,1).\n# The first skip_values are truncated, because initial temperature states may have large error.\n# Also, only \"not-zero-padded\" part of the sequence is taken into account (-> sample_weight = 1).\ndef eval_model(model, X, Y, weights, skip_values=10, scaler=None):\n    # prepare data for prediction\n    end_sequence = np.where(weights==0)[1][0] # get \"real\" (= not-zero-padded) end of sequence\n    X_pred = X[:,:end_sequence,:]\n    Y_truth = Y[0,:end_sequence,0]\n\n    # predict (and rescale if necessary)\n    Y_pred = model.predict(X_pred)\n    if scaler:\n        Y_pred = scaler.inverse_transform(Y_pred)     \n    Y_pred = Y_pred[0,:,0]\n    \n    # skip the first few values (large errors due to initialization phase)\n    Y_pred = Y_pred[skip_values:]\n    Y_truth = Y_truth[skip_values:]\n        \n    # calculate errors\n    abs_error = np.abs(Y_pred-Y_truth)\n    mse_error = np.mean(abs_error**2)    \n    \n    return Y_pred, Y_truth, abs_error, mse_error\n\n# This function outputs a plot showing the prediction vs. ground truth and the corresponding error.\ndef plot_prediction(Y_pred, Y_truth, abs_error, mse_error, descr=' '):\n\n    fig, axes = plt.subplots(1, 2, sharex = True, figsize=(15, 5))\n\n    # Temperature values\n    axes[0].plot(Y_truth, 'r')\n    axes[0].plot(Y_pred, 'b')\n    axes[0].set_title('Prediction vs. ground truth')\n    axes[0].legend(['Truth', 'Prediction'])\n    axes[0].set_xlabel('sample')\n    axes[0].set_ylabel('Temperature')\n    \n    # Error\n    axes[1].plot(abs_error, 'r')\n    axes[1].set_title('Error (total MSE: {:.5f})'.format(mse_error))\n    axes[1].set_xlabel('sample')\n    axes[1].set_ylabel('Error')\n    \n    fig.suptitle(descr, fontsize=\"x-large\", fontweight='bold')\n\n    return\n\n# This function finds the load cycle with the highest mse.\ndef get_worst_cycle(model, X, Y, weights):\n    \n    highest_mse = 0\n    worst_pid = 0\n\n    for pid in np.arange(0,X.shape[0]):\n        X_pred = X[pid:pid+1,:,:]\n        Y_truth = Y[pid:pid+1,:,:]\n        X_weights = weights[pid:pid+1,:]\n        Y_pred, Y_truth, abs_error, mse_error = eval_model(model, X_pred, Y_truth, X_weights)\n        if mse_error > highest_mse:\n            highest_mse = mse_error\n            worst_pid = pid\n    \n    return worst_pid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SimpleLSTM\nIn accordance to the research paper, a simple model with relatively few parameters can already reach descent accuracy of the temperature estimation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot prediction vs. ground truth for one sample of training set\n\n# training set\npid = 49 # basically a random load cycle\nX_pred_train = X_train[pid:pid+1,:,:]\nY_truth_train = Y_train[pid:pid+1,:,:]\nweights_train = X_train_weights[pid:pid+1,:]\nY_pred, Y_truth, abs_error, mse_error = eval_model(simple_lstm, X_pred_train, Y_truth_train, weights_train)\nplot_prediction(Y_pred, Y_truth, abs_error, mse_error, descr='simple_LSTM - TRAIN SET EXAMPLE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot worst load cycle from dev set (highest mse)\n\nworst_pid = get_worst_cycle(simple_lstm, X_dev, Y_dev, X_dev_weights)\n\nX_pred_dev = X_dev[worst_pid:worst_pid+1,:,:]\nY_truth_dev = Y_dev[worst_pid:worst_pid+1,:,:]\nweights_dev = X_dev_weights[worst_pid:worst_pid+1,:]\nY_pred, Y_truth, abs_error, mse_error = eval_model(simple_lstm, X_pred_dev, Y_truth_dev, weights_dev)        \nplot_prediction(Y_pred, Y_truth, abs_error, mse_error, descr='simple_LSTM - DEV SET - WORST MSE SAMPLE (Nr.: ' + str(pid) +')')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ResLSTM\nAdding residual connections and increasing the model complexity also leads to a good temperature estimation. Here, the difference between the simple LSTM and the residual LSTM is not too big."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot prediction vs. ground truth for one sample of training set\n\n# training set\npid = 49 # basically a random load cycle\nX_pred_train = X_train[pid:pid+1,:,:]\nY_truth_train = Y_train[pid:pid+1,:,:]\nweights_train = X_train_weights[pid:pid+1,:]\nY_pred, Y_truth, abs_error, mse_error = eval_model(res_lstm, X_pred_train, Y_truth_train, weights_train)\nplot_prediction(Y_pred, Y_truth, abs_error, mse_error, descr='res_LSTM - TRAIN SET EXAMPLE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot worst load cycle from dev set (highest mse)\n\nworst_pid = get_worst_cycle(simple_lstm, X_dev, Y_dev, X_dev_weights)\n\nX_pred_dev = X_dev[worst_pid:worst_pid+1,:,:]\nY_truth_dev = Y_dev[worst_pid:worst_pid+1,:,:]\nweights_dev = X_dev_weights[worst_pid:worst_pid+1,:]\nY_pred, Y_truth, abs_error, mse_error = eval_model(res_lstm, X_pred_dev, Y_truth_dev, weights_dev)        \nplot_prediction(Y_pred, Y_truth, abs_error, mse_error, descr='res_LSTM - DEV SET - WORST MSE SAMPLE (Nr.: ' + str(pid) +')')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TCN\nWith fairly simple hyperparameters, the TCN can only approximate the correct temperature of the elctric motor. So in this notebook the LSTMs outperform the TCN. This is not a general assumption, as the best model of the research paper was a residual TCN. However it might indicate that TCNs need some more attention regarding model architecture and hyperparameter tuning than LSTMs."},{"metadata":{"trusted":true},"cell_type":"code","source":"# training set\npid = 49 # basically a random load cycle\nX_pred_train = X_train[pid:pid+1,:,:]\nY_truth_train = Y_train[pid:pid+1,:,:]\nweights_train = X_train_weights[pid:pid+1,:]\nY_pred, Y_truth, abs_error, mse_error = eval_model(TCN, X_pred_train, Y_truth_train, weights_train)\nplot_prediction(Y_pred, Y_truth, abs_error, mse_error, descr='TCN - TRAIN SET EXAMPLE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot worst load cycle from dev set (highest mse)\n\nworst_pid = get_worst_cycle(TCN, X_dev, Y_dev, X_dev_weights)\n\nX_pred_dev = X_dev[worst_pid:worst_pid+1,:,:]\nY_truth_dev = Y_dev[worst_pid:worst_pid+1,:,:]\nweights_dev = X_dev_weights[worst_pid:worst_pid+1,:]\nY_pred, Y_truth, abs_error, mse_error = eval_model(TCN, X_pred_dev, Y_truth_dev, weights_dev)        \nplot_prediction(Y_pred, Y_truth, abs_error, mse_error, descr='TCN - DEV SET - WORST MSE SAMPLE (Nr.: ' + str(pid) +')')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\nThe goal of this notebook was to proof the concept of temperature estimation of electric motors with neural nets. It can be seen that low errors can be reached even without too complicated models or an extensive hyperparameter search. Even the temperature behaviour of highly dynamic load cycles can be mapped appropriately. There are some aspects left out that has been covered in the research paper, so there is room for improvement. For example, training could be even more efficient using \"chrono-initialization\" of the biases or TBPTT when training LSTMs. Also, the TCN model of this notebook still requires more detailed tuning. Some optimizations might be done in future versions of the notebook.  \n\nThe data has been anonymized, so the \"real\" accuracy can only be estimated. The values of the stator winding temperature ranges from about -2 to 2.5 in the dataset. Assuming that the real temperatures of the test bench recordings range from 20째C room temperature to 180째C maximum operation temperature, 1째C is equal to 0,028. Neural nets can reach state-of-the-art accuracy which varies from 3-5째C accuracy depending on the load cycle dynamics. The temperature estimation in the initial phase of a load cycle has been ignored, because typically the motor does not reach a critical temperature within the first few moments of operation. Of course, for a real application this would have to be considered."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}