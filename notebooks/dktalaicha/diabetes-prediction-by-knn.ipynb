{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"![](https://i.imgur.com/nagCao6.jpg)\n\nPhoto by Matt Chesin on Unsplash","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Index\n\nWe’ll be using Machine Learning to predict whether a Pima Indian Women has diabetes or not, based on information about the patient such as blood pressure, body mass index (BMI), age, etc. \n\nThis notebook walks through the various stages of the data science workflow. In particular, the notebook has the following sections:\n\n\n<ol>\n  <li>Introduction</li>\n  <li>Exploratory Data Analysis (EDA) and Statistical Analysis</li>\n  <li>Prediction</li>\n  <li>Model Performance Analysis</li>\n  <li>Conclusion</li>\n</ol>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nActually it was a question for many scientists why so many Pima Indian Women suffer from diabetes in relation to other ethnicities.\n\nThe study was carried out by scientists to investigate the significance of health-related predictors of diabetes in **Pima Indians Women**. The study population was the females 21 years and above of Pima Indian heritage patients of diabetes and digestive and kidney diseases. \n\nThe research question was: what are the health predictions that associated with the presence of diabetes in Pima Indians? \n\nThe study aimed at testing the significance of health-related predictors of diabetes in Pima Indians women. That was according to World Health Organization criteria (i.e. if the 2-hour post-load plasma glucose was at least 200 mg/dl at any survey examination or if found during routine medical care). A total of 768 women were registered in the database.\n\nSo, here we have to find out why so many Pima Indian Women suffer from diabetes in relation to other ethnicities?\n\nTo find the reason behind this, we have to find whether there is a relationship between the numbers of times a women was pregnant and the BMIs of Pima Indian Women older than 21 years old, or whether the women have diabetes and their diabetes pedigree function (a function that represents how likely they are to get the disease by extrapolating from their ancestor’s history).\n\nSo lets start the analysis, but before that, I think we should understand our dataset and good to have basic knowledge of diabetes also.\n\n\n### What is Diabetes?\n\nDiabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat. Insulin, a hormone made by the pancreas, helps glucose from food get into your cells to be used for energy. Sometimes your body doesn’t make enough—or any—insulin or doesn’t use insulin well. Glucose then stays in your blood and doesn’t reach your cells.\n\n### What is Gestational Diabetes?\nGestational diabetes develops in some women when they are pregnant. Most of the time, this type of diabetes goes away after the baby is born. However, if you’ve had gestational diabetes, you have a greater chance of developing type 2 diabetes later in life. Sometimes diabetes diagnosed during pregnancy is actually type 2 diabetes. Gestational diabetes causes high blood sugar that can affect the pregnancy and the baby’s health. \n\nResults indicate that the highest prevalence of diabetes is found in high rates in Indian Pima.\n\nSo, lets start and try to find out why Pima Indians Women have higher rate of diabetes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA) and Statistical Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#from mlxtend.plotting import plot_decision_regions\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.set_style('whitegrid')\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View top 5 rows of our dataset\ndiabetes_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Identification of variables and data types","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset comprises of 768 observations and 9 fields.\n\nThe following features have been provided to help us predict whether a person is diabetic or not:\n\n* **Pregnancies:** Number of times pregnant\n* **Glucose:** Plasma glucose concentration over 2 hours in an oral glucose tolerance test. Less than 140 mg/dL is considered normal level of glucose.\n* **BloodPressure:** Diastolic blood pressure (mm Hg). 120/80 is normal BP level for female above 18 yr old.\n* **SkinThickness:** Triceps skin fold thickness (mm)\n* **Insulin:** 2-Hour serum insulin (mu U/ml). 16-166 mIU/L is considered the normal level of insulin.\n* **BMI:** Body mass index (weight in kg/(height in m)2)\n* **DiabetesPedigreeFunction:** Diabetes pedigree function (a function which scores likelihood of diabetes based on family history)\n* **Age:** Age (years)\n* **Outcome:** Class variable (0 if non-diabetic, 1 if diabetic)\n\n\nLet’s also make sure that our data is clean (has no null values, etc).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lets check data types,columns names, null value counts, memory usage etc\ndiabetes_data.info(verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the details of each column\ndiabetes_data.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Value Treatment\n\nMissing data in the data set can reduce the power / fit of a model or can lead to a biased model because we have not analysed the behavior and relationship with other variables correctly. It can lead to wrong prediction or classification.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**By analysing the above details of the dataset found that few features have zero values and pregnancy variable has maximum = 17 which seems to be impossible.**\n\nThese column values of zero do not make sense as there is some range for a normal healthy human being which is certainly not the zero and thus indicates a missing value.\n\nBelow variables have an invalid zero value:\n* Glucose\n* BloodPressure\n* SkinThickness\n* Insulin\n* BMI\n\n**Initially we will replace these zeros with NaN so that it will easy to count the missing values. Then, later on, we will replace them with appropriate values.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data['Pregnancies'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see distribution and also boxplot for outliers of feature \"Pregnancies\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=2,figsize = (8,6))\n\nplot00=sns.distplot(diabetes_data['Pregnancies'],ax=axes[0],color='b')\naxes[0].set_title('Distribution of Pregnancy',fontdict={'fontsize':8})\naxes[0].set_xlabel('No of Pregnancies')\naxes[0].set_ylabel('Frequency')\nplt.tight_layout()\n\n\nplot01=sns.boxplot('Pregnancies',data=diabetes_data,ax=axes[1],orient = 'v', color='r')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace zeros with NaN\ndiabetes_data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing values counts","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total = diabetes_data.isnull().sum().sort_values(ascending=False)\npercent = ((diabetes_data.isnull().sum()/diabetes_data.isnull().count())*100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 6))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-weight: bold;color:#FF4500\">Highlights</p>\n* Insulin has 374 missing values which is about 48.7% of total missing values.\n* SkinThickness has 227 missing values which is only 29.6% of total missing values.\n* BloodPressure has 35 missing values which is only 4.6% of total missing values.\n* BMI has only 11 missing values which is only 1.4% of total missing values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Understand Data Distribution\n\nThe proportion of Insulin missing data are likely very high and this field seems to be very important to know wheather a person is diabetic or not, so we will replace the missing values with some form of imputation. So lets find the relationship between the field for better imputation of missing values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Pearson's Correlation Coefficient or simply Correlation\n\nPearson's Correlation Coefficient helps you find out the relationship between two quantities. It gives you the measure of the strength of association between two variables. The value of Pearson's Correlation Coefficient can be between -1 to +1. 1 means that they are highly correlated and 0 means no correlation.\n\nA heat map is a two-dimensional representation of information with the help of colours. Here Heat Map can help us to visualize the correlation information between colums.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Heatmap to find correlation ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))  \n# sns.heatmap(diabetes_data.corr(), annot=True,cmap ='RdYlGn')\nsns.heatmap(diabetes_data.corr(),annot=True, cmap='viridis',linewidths=.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-weight: bold;color:#FF4500\">Highlights</p>\n* It seems that Insulin is highly correlated with Glucose (about 0.58), BMI (about 0.23) and Age (about 0.22). It means that as the values of glucose, BMI and Age increase, the insuline is also increasing. It seems logical also that fat and aged people might have high level of insuline in their bodies.\n* In the same way SkinThickness is highly correlated with BMI (about 0.65).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Impute NaN values of columns according to their distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the distribution of each column, so that we can find wich is best central tendency (mean, medium or mode) to replace missing values:\ndiabetes_data.hist(figsize = (20,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-weight: bold;color:#FF4500\">Highlights</p>\nDistribution graph show that glucose & BloodPressure columns are almost normally distributed where as SkinThickness, Insulin and BMI are positive skewned.\n\nSo we will replace the missing values according to their distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data['Glucose'].fillna(diabetes_data['Glucose'].mean(), inplace = True)\ndiabetes_data['BloodPressure'].fillna(diabetes_data['BloodPressure'].mean(), inplace = True)\ndiabetes_data['SkinThickness'].fillna(diabetes_data['SkinThickness'].median(), inplace = True)\ndiabetes_data['Insulin'].fillna(diabetes_data['Insulin'].median(), inplace = True)\ndiabetes_data['BMI'].fillna(diabetes_data['BMI'].median(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting after NaN removal ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data.hist(figsize = (20,20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking  balance of data\n\nWe can produce a seaborn count plot to see how the output is dominated by one of the classes or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(x='Outcome',data=diabetes_data, palette='bright')\nplt.title(\"Emergency call category\")\n\nprint(diabetes_data['Outcome'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-weight: bold;color:#FF4500\">Highlights</p>\nA total of 768 women were registered in the database. 268 womens about 35% were having diabetes, while 500 women about 65% were not. \n\nThe above graph shows that the dataset is biased towards non-diabetic patient. The number of non-diabetics is almost twice the number of diabetic patients.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Scatter matrix of data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(diabetes_data,hue='Outcome')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-weight: bold;color:#FF4500\">Highlights</p>\nThe pairs plot builds on two basic figures, the histogram and the scatter plot. The histogram on the diagonal allows us to see the distribution of a single variable while the scatter plots on the upper and lower triangles show the relationship (or lack thereof) between two variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## BMI vs Pregnancy vs Diabetes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.boxplot(x='Pregnancies', y='BMI',data=diabetes_data, hue='Outcome')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.boxplot(x='Outcome', y='BMI',data=diabetes_data, hue='Outcome')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nsns.countplot(x='Pregnancies',data=diabetes_data,hue = 'Outcome', palette='bright')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-weight: bold;color:#FF4500\">Highlights</p>\n\nIt is surprising that the median BMI does not immensely change as the number of pregnancies increases. I expected there to be a strong positive relationship between the number of pregnancies and the BMI. Those who tested positive for diabetes had higher BMIs than those who does not; yet, not a larger difference between the medians.\n\nBMI will generally be higher for women who have had more numbers of pregnancy as well as for those who test positive for diabetes and that the relationship between the pedigree function and the test results will show that those who had a higher pedigree function tested positive and those who had a lower pedigree function tested negative.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Pedigree function vs Diabetes ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.boxplot(x='Outcome', y='DiabetesPedigreeFunction',data=diabetes_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-weight: bold;color:#FF4500\">Highlights</p>\nThis graph more clearly shows the relationship between the pedigree function and the test results that the women got for diabetes. Since those who tested positive have a higher median and more high outliers, it is clear that the pedigree function does in fact, accurately help estimate the test results for diabetes. It shows that diabetes does follow genetics so those whose ancestors suffered from it have a higher risk of getting the disease themselves as well. Both test results show many outliers yet the outliers for those who tested negative seem to be lower pedigree functions than those who tested positive. This concluded that the genetic component is likely to contribute more to the emergence of diabetes in the Pima Indians and their offspring.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Pregnancy vs Diabetes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.boxplot(x='Outcome', y='Pregnancies',data=diabetes_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-weight: bold;color:#FF4500\">Highlights</p>\nThe average number of pregnancies is higher (4.9) in diabetic in comparing to (3.3) in non-diabetic women with a significant difference between them.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## BMI vs Diabetes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.boxplot(x='Outcome', y='BMI',data=diabetes_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Diabetic in Normal BMI\n\nLet try to find out how is the probabiliy of having diabetic in a women having normal BMI. Please note that the range of noraml BMI is 18.5 to 25.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"normalBMIData = diabetes_data[(diabetes_data['BMI'] >= 18.5) & (diabetes_data['BMI'] <= 25)]\nnormalBMIData['Outcome'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"notNormalBMIData = diabetes_data[(diabetes_data['BMI'] < 18.5) | (diabetes_data['BMI'] > 25)]\nnotNormalBMIData['Outcome'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.boxplot(x='Outcome', y='BMI',data=notNormalBMIData)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-weight: bold;color:#FF4500\">Highlights</p>\n\nThe Body Mass Index (BMI) showed a significant association with the occurrence of diabetes and that even the normal weighted women were at almost 9 times risk of being diabetic in comparison to the overweight.\n\nIn addition, the interquartile range for the women who tested positive reaches a higher BMI than the IQR for those who tested negative. Therefore, women could have higher BMIs and not be outliers if they tested positive as opposed to negative, showing that more women who tested positive did, in fact, have higher BMIs than those who tested negative. \n\nPlease find BMI range on [google](https://www.vertex42.com/ExcelTemplates/bmi-chart.html).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Age vs Diabetes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.boxplot(x='Outcome', y='Age',data=diabetes_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_data['Age'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nsns.countplot(x='Age',data=diabetes_data,hue = 'Outcome', palette='bright')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-weight: bold;color:#FF4500\">Highlights</p>\nSignificant relation can be seen between the age distribution and diabetic occurrence. Women at age group > 31 years were at higher risk to contract diabetes in comparison to the younger age group.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Prediction using KNN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"KNN-classifier can be used when your data set is small enough.\n\nhttps://scikit-learn.org/stable/tutorial/machine_learning_map/","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Standardize the Variables\nStandardization (also called z-score normalization) is the process of putting different variables on the same scale. Standardization transforms your data such that the resulting distribution has a mean of 0 and a standard deviation of 1. \n\n$$ Z = {X - \\mu \\over \\sigma}$$ \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler.fit(diabetes_data.drop('Outcome',axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_features = scaler.transform(diabetes_data.drop('Outcome',axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feat = pd.DataFrame(scaled_features,columns=diabetes_data.columns[:-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feat.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test Split\n\n**Use train_test_split to split your data into a training set and a testing set.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(scaled_features,diabetes_data['Outcome'],\n                                                    test_size=0.30,random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create a KNN model instance with n_neighbors=1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fit this KNN model to the training data.**`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions and Evaluations\nLet's evaluate our KNN model!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Use the predict method to predict values using your KNN model and X_test.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a confusion matrix and classification report.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Choosing a K Value\nLet's go ahead and use the elbow method to pick a good K Value!\n\n** Create a for loop that trains various KNN models with different k values, then keep track of the error_rate for each of these models with a list. Refer to the lecture if you are confused on this step.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"error_rate = []\ntest_scores = []\ntrain_scores = []\n\n# Will take some time\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    \n    error_rate.append(np.mean(pred_i != y_test))\n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now create the following plot using the information from your for loop.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## score that comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Retrain with new K Value\n\n**Retrain your model with the best K value and re-do the classification report and the confusion matrix.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOW WITH K=20\nknn = KNeighborsClassifier(n_neighbors=20)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\nprint('WITH K=20')\nprint('\\n')\nprint(confusion_matrix(y_test,pred))\nprint('\\n')\nprint(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Result Visualisation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.lineplot(range(1,40),train_scores,marker='*',label='Train Score')\nsns.lineplot(range(1,40),test_scores,marker='o',label='Test Score')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The best result is captured at k = 20 hence 20 is used for the final model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setup a knn classifier with k neighbors\nknn = KNeighborsClassifier(20)\n\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Performance Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix\n\nThe confusion matrix is a technique used for summarizing the performance of a classification algorithm i.e. it has binary outputs.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import confusion_matrix\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let us get the predictions using the classifier we had fit above\ny_pred = knn.predict(X_test)\nconfusion_matrix(y_test,y_pred)\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"viridis\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Report\n\nReport which includes Precision, Recall and F1-Score.\n\n### Precision Score\n**Precision** – Accuracy of positive predictions.\n$$ Precision = {TP \\over (TP + FP)}$$ \n        \n### Recall Score\n**Recall(sensitivity or true positive rate)**: Fraction of positives that were correctly identified.\n$$ Recall = {TP \\over (TP + FN)}$$ \n        \n### F1 Score\n**F1 Score** – A helpful metric for comparing two classifiers. F1 Score takes into account precision and the recall. It is created by finding the the harmonic mean of precision and recall.\n$$ F1 = 2 \\times {(precision \\times recall) \\over (precision + recall)}$$ ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import classification_report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Printing the Overall Accuracy of the model\nprint(\"Accuracy of the model : {0:0.3f}\".format(metrics.accuracy_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ROC - AUC\n\nIn a Receiver Operating Characteristic (ROC) curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points. Each point on the ROC curve represents a sensitivity/specificity pair corresponding to a particular decision threshold. A test with perfect discrimination (no overlap in the two distributions) has a ROC curve that passes through the upper left corner (100% sensitivity, 100% specificity). Therefore the closer the ROC curve is to the upper left corner, the higher the overall accuracy of the test.\n\nAUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s.\n\n* The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. \n* It is a comparison of two operating characteristics (TPR and FPR) as the criterion changes.\n* The TPR defines how many correct positive results occur among all positive samples available during the test. \n* The FPR defines how many incorrect positive results occur among all negative samples available during the test.\n* What is a good ROC value? A rough guide for classifying the accuracy of a diagnostic test is the traditional academic point system\": .90-1 = excellent (A) .80-.90 = good (B) .70-.80 = fair (C) .60-.70 = poor (D)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\ny_pred_proba = knn.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='Knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('Knn(n_neighbors=11) ROC curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Area under ROC curve\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,y_pred_proba)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nOverall, it seems that there is some form of an association between BMI, number of pregnancies, pedigree function, and the test results for diabetes. It is surprising that the median BMI does not immensely change as the number of pregnancies increases. I expected there to be a strong positive relationship between the number of pregnancies and the BMI. Those who tested positive for diabetes had higher BMIs than those who did not; yet, I predicted a larger difference between the medians.\n\nTo find the relationship between the pedigree function and the test results, it would be interesting to also have males and those under 21 as well as 21 in the sample. That way, possible confounding variables such as a hormone that only females have that may cause diabetes, can be eliminated.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## I welcome comments, suggestions, corrections and of course votes also.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}