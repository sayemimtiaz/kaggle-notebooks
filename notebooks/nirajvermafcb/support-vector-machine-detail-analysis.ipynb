{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"8ed0a71d-81a2-9918-5943-c1703a0f27ff"},"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"ce53be83-4475-8d60-f4a5-b7baa884bd11"},"source":"Here I am going to run Support Vector machine on the datasets and do cross validation and then use accuracy score as a parameter to judge the best combination of kernel and values of C which are the hyperparameters in SVM . Here I am taking 1 kernel at a time although we could have avoided it using GridSearchCV. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a627464e-bd59-249c-ff6e-1483fcfcf5c8"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"markdown","metadata":{"_cell_guid":"cd01fe18-db57-3da4-2916-f99e096988ac"},"source":"# Importing all the necessary libraries"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0247ac12-6357-659f-0182-36d245e9b551"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline"},{"cell_type":"markdown","metadata":{"_cell_guid":"0200d0e4-320f-2822-88a9-f6c0225658e6"},"source":"# Reading the comma separated values file into the dataframe"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e847afc-36c5-b7bb-78c6-4288f847868e"},"outputs":[],"source":"df = pd.read_csv('../input/voice.csv')\ndf.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"2d654f09-1018-5498-e4ea-428752a19d4b"},"source":"# Checking the correlation between each feature"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c9f6f78-3e24-bb65-8270-f9a795441d5e"},"outputs":[],"source":"df.corr()"},{"cell_type":"markdown","metadata":{"_cell_guid":"51c112a4-8b80-2a6a-5688-825581b9c1dd"},"source":"# Checking whether there is any null values "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af637554-1c8d-cdf4-c76e-6429511dbcf7"},"outputs":[],"source":"df.isnull().sum()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da3a570c-0527-1f21-a980-edebea590239"},"outputs":[],"source":"df.shape[0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe354f60-1504-a06e-9675-c80c755bf66f"},"outputs":[],"source":"print(\"Total number of labels: {}\".format(df.shape[0]))\nprint(\"Number of male: {}\".format(df[df.label == 'male'].shape[0]))\nprint(\"Number of female: {}\".format(df[df.label == 'female'].shape[0]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"af99e2cb-9132-7c97-3dce-6a81c6b85bb9"},"source":"Thus we can see there are equal number of male and female labels"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78f518d5-a32d-376f-d132-1250bd0b7544"},"outputs":[],"source":"df.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"85fbbf85-f199-9528-8147-7583154c4c6e"},"source":"There are 21 features and 3168 instances."},{"cell_type":"markdown","metadata":{"_cell_guid":"35926698-93b2-6af7-7647-20e876461359"},"source":"# Separating features and labels"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36d5feea-f653-99c9-aa5d-a60c9f0d20f8"},"outputs":[],"source":"X=df.iloc[:, :-1]\nX.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"eabb15db-3594-8ad5-2a5b-10527ac27b88"},"source":"# Converting string value to int type for labels"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4681c81-0d46-deac-48a8-8238f7fe330c"},"outputs":[],"source":"from sklearn.preprocessing import LabelEncoder\ny=df.iloc[:,-1]\n\n# Encode label category\n# male -> 1\n# female -> 0\n\ngender_encoder = LabelEncoder()\ny = gender_encoder.fit_transform(y)\ny"},{"cell_type":"markdown","metadata":{"_cell_guid":"c4badc10-5597-845a-85e2-c8cbf4c3e0d4"},"source":"# Data Standardisation\nStandardization refers to shifting the distribution of each attribute to have a mean of zero and a standard deviation of one (unit variance). It is useful to standardize attributes for a model. Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c999a7a-cded-223a-3b4f-55ee8fbbdb7e"},"outputs":[],"source":"# Scale the data to be between -1 and 1\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)"},{"cell_type":"markdown","metadata":{"_cell_guid":"31ce4128-0a4a-ae10-3a25-1c362398295c"},"source":"# Splitting dataset into training set and testing set for better generalisation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ea416e41-d033-34de-6e28-5725be182435"},"outputs":[],"source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"416eb826-73fc-bbd2-0676-2a803ae9327b"},"source":"# Running SVM with default hyperparameter."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05ccf988-5612-03ac-5d2f-122baa32ab19"},"outputs":[],"source":"from sklearn.svm import SVC\nfrom sklearn import metrics\nsvc=SVC() #Default hyperparameters\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score:')\nprint(metrics.accuracy_score(y_test,y_pred))"},{"cell_type":"markdown","metadata":{"_cell_guid":"7bdca3cb-606e-b49b-eedf-9f4e128c05d8"},"source":"We are getting a good accuracy score.But data are split into training and testing data randomly.Thus a lot depends on how the data got split. When we are not using Random state as a hyperparameter everytime data is splitted differently into training and testing testsa and we get different accuracy score. This is when K-fold Cross validation is a good option"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fd19993f-2f0d-c9da-e944-3206e6a292c0"},"outputs":[],"source":"svc=SVC(kernel='linear',C=1)\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score:')\nprint(metrics.accuracy_score(y_test,y_pred))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61a30229-7028-815d-631f-1b24343da1b3"},"outputs":[],"source":"svc=SVC(kernel='rbf',C=1)\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score:')\nprint(metrics.accuracy_score(y_test,y_pred))"},{"cell_type":"markdown","metadata":{"_cell_guid":"3930931d-baeb-c86d-73a8-ec2b72cc67ba"},"source":"Again with kernel as rbf we are getting a marginal less accuracy score s compared to linear kernel"},{"cell_type":"markdown","metadata":{"_cell_guid":"c353406f-4374-0301-8d96-5767e038551a"},"source":"Thus with K-fold cross validation we are splitting data in K equal parts(in our case K=10).For every value of K we got different training and testing data picking 1/10th of the data a time and train all of them thus covering all the data.In the end we take mean of all the sets. Generally K=10 is taken"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"25f97e10-5a1c-103a-4be1-f9d766ade49d"},"outputs":[],"source":"from sklearn.cross_validation import cross_val_score\nsvc=SVC(kernel='linear',C=1)\nscores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\nprint(scores)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1e11e9e-0d61-70cb-8d80-3c31fcce6cef"},"outputs":[],"source":"We can see above how the accuracy score is different everytime.This shows that accuracy score depends upon how the datasets got split."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2e7ab72-5fa9-3e54-d59c-b2b4c24ed7dc"},"outputs":[],"source":"print(scores.mean())"},{"cell_type":"markdown","metadata":{"_cell_guid":"6d61c1df-0285-0a0e-64ad-c2bef642e15d"},"source":"In K-fold cross validation we generally take the mean of all the scores."},{"cell_type":"markdown","metadata":{"_cell_guid":"0912611e-a132-cd7e-edcf-8f63603a7f38"},"source":"### Taking all the values of C and checking out the accuracy score and kernel as linear."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"75bfb3a4-06d5-c828-23bb-a03ffea9b6c7"},"outputs":[],"source":"C_range=list(range(1,26))\nacc_score=[]\nfor c in C_range:\n    svc = SVC(kernel='linear', C=c)\n    svc.fit(X_train,y_train)\n    y_pred=svc.predict(X_test)\n    acc_score.append(metrics.accuracy_score(y_test,y_pred))\nprint(acc_score)    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d7d004c8-458f-ae86-d777-ab92b253df34"},"outputs":[],"source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nC_values=list(range(1,26))\n\n# plot the value of C for SVM (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(C_values,acc_score )\nplt.xlabel('Value of C for SVC')\nplt.ylabel('Cross-Validated Accuracy')"},{"cell_type":"markdown","metadata":{"_cell_guid":"5f902ce7-ae22-b338-1da6-885f607ca48c"},"source":"From the above plot we can see that accuracy has been close to 97.8% for C=1 and then it drops below 97.65% and remains constant.Thus we can conclude that C=1 is the best hyperparameter for linear kernel."},{"cell_type":"markdown","metadata":{"_cell_guid":"a8d63c32-a177-b287-08c4-f85f144c6324"},"source":"### Taking kernel as **rbc** and and taking different values of C"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"282b0fe3-0b34-d692-7412-8acab5711326"},"outputs":[],"source":"C_range=list(range(1,41))\nacc_score=[]\nfor c in C_range:\n    svc = SVC(kernel='rbf', C=c)\n    svc.fit(X_train,y_train)\n    y_pred=svc.predict(X_test)\n    acc_score.append(metrics.accuracy_score(y_test,y_pred))  \nprint(acc_score)        "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22bc0e0e-fdfc-6c6d-c9c5-8863f818254f"},"outputs":[],"source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nC_values=list(range(1,41))\n\n# plot the value of C for SVM (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(C_values,acc_score )\nplt.xlabel('Value of C for SVC with kernel rbf')\nplt.ylabel('Cross-Validated Accuracy')"},{"cell_type":"markdown","metadata":{"_cell_guid":"ce48d3d2-322b-9d01-9649-b05ae6414735"},"source":"We can see from the plot that the accuracy score is highest for C=3, and then drops at a bit and then again it is highest for C-=8,9,10,11,12,13.Also the accuracy score is slightly more than linear kernel.In this case it is around 98% particular values of C."},{"cell_type":"markdown","metadata":{"_cell_guid":"10129743-3044-9dbc-b6cb-30c7d84d31a9"},"source":"### Thus from the above two plots we can conclude that **rbf** kernel is performing better than the **linear** kernel."},{"cell_type":"markdown","metadata":{"_cell_guid":"2143d3d8-4abf-983d-ee4c-cc74f4fde0b7"},"source":"# Now performing SVM by taking hyperparameter C=1 and kernel as linear \n\n\n----------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"499ae7bd-c349-3c50-0f6e-3ca8633d0b28"},"outputs":[],"source":"from sklearn.svm import SVC\nsvc= SVC(kernel='linear',C=1)\nsvc.fit(X_train,y_train)\ny_predict=svc.predict(X_test)\naccuracy_score= metrics.accuracy_score(y_test,y_predict)\nprint(accuracy_score)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b4cf3a4a-9f64-5567-3037-bc2719ae098c"},"source":"# With K-fold cross validation(where K=10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b9e8e3f-2861-ed79-d48f-ca71c9264165"},"outputs":[],"source":"from sklearn.cross_validation import cross_val_score\nsvc=SVC(kernel='linear',C=1)\nscores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\nprint(scores)"},{"cell_type":"markdown","metadata":{"_cell_guid":"daad2a8a-c37f-86b7-9120-8fb3a67cdc38"},"source":"Taking the mean of all the scores"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"57b5946d-b6b8-b758-9570-b855ebaa6bd2"},"outputs":[],"source":"print(scores.mean())"},{"cell_type":"markdown","metadata":{"_cell_guid":"76ec14cd-f216-6179-063e-8ec211daa46c"},"source":"The accuracy is slightly good without K-fold cross validation but it may fail to generalise the unseen data.Hence it is advisable to perform K-fold cross validation where all the data is covered so it may predict unseen data well."},{"cell_type":"markdown","metadata":{"_cell_guid":"56374d8a-d033-e6bf-eac9-0bc508aaa171"},"source":"# Now performing SVM by taking hyperparameter C=8,9,10,11,12,13 and kernel as rbf"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d2233d5d-cbbb-037e-1b2f-91a1e76a3d5d"},"outputs":[],"source":"C_range=[8,9,10,11,12,13]\nacc_score_rbf=[]\nfor c in C_range:\n    svc= SVC(kernel='rbf',C=c)\n    svc.fit(X_train,y_train)\n    y_predict=svc.predict(X_test)\n    acc_score_rbf= metrics.accuracy_score(y_test,y_predict)\n    print(acc_score_rbf)\n    "},{"cell_type":"markdown","metadata":{"_cell_guid":"a5530b31-17ed-8c82-6e20-bf7683b0f23d"},"source":"Thus we can see that it is giving the same score"},{"cell_type":"markdown","metadata":{"_cell_guid":"e0b485e2-4ea3-ed6d-ece1-d29cda102038"},"source":"# With K-fold cross validation(where K=10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e3baf72f-df6a-f4a0-029e-5c5dcd0aa726"},"outputs":[],"source":"C_range=[8,9,10,11,12,13]\nacc_score_rbf=[]\nfor c in C_range:\n    svc=SVC(kernel='linear',C=c)\n    scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    print(scores)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dde5471d-ff08-7762-d799-00e15fa3110e"},"outputs":[],"source":"print(scores.mean())"},{"cell_type":"markdown","metadata":{"_cell_guid":"78cc1818-eded-9df6-fbdb-90db00eff6c0"},"source":"Thus we can conclude that kernel **rbf** and C in the range of 8 to 13 is the good choice since it is performing slightly better."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}