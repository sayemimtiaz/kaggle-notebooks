{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **About This Kernel**\n* What is the purpose of the study?\n\nI am working on Deep Learning and Computer Vision in Flying Automobile Project. The project I am working on are Semantic segmentation (Aerial images) during the flight of the vehicle to find suitable areas where the vehicle can land. To make volumetric control of the vehicle to these areas. \n\nWith this kernel, I have completed working on the **Semantic segmentation**\n    \n <a class=\"anchor\" id=\"0.\"></a>\n# **Content**\n\n1. [What is semantic segmentation](#1.)\n1. [Implementation of Segnet, FCN, UNet , PSPNet and other models in Keras](#2.)\n1. [I extracted Github codes](#3.)"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"1.\"></a> \n# 1.What is semantic segmentation\n\nSource: https://divamgupta.com/image-segmentation/2019/06/06/deep-learning-semantic-segmentation-keras.html\n\nSemantic image segmentation is the task of classifying each pixel in an image from a predefined set of classes. In the following example, different entities are classified.\n\n![Semantic segmentation of a bedroom image](https://divamgupta.com/assets/images/posts/imgseg/image15.png?style=centerme)\n\nIn the above example, the pixels belonging to the bed are classified in the class “bed”, the pixels corresponding to the walls are labeled as “wall”, etc.\n\nIn particular, our goal is to take an image of size W x H x 3 and generate a W x H matrix containing the predicted class ID’s corresponding to all the pixels.\n\n![Image source: jeremyjordan.me](https://divamgupta.com/assets/images/posts/imgseg/image14.png?style=centerme)\n\nUsually, in an image with various entities, we want to know which pixel belongs to which entity, For example in an outdoor image, we can segment the sky, ground, trees, people, etc.\n\nSemantic segmentation is different from object detection as it does not predict any bounding boxes around the objects. We do not distinguish between different instances of the same object. For example, there could be multiple cars in the scene and all of them would have the same label.\n\n![An example where there are multiple instances of the same object class](https://divamgupta.com/assets/images/posts/imgseg/image7.png?style=centerme)\n\nIn order to perform semantic segmentation, a higher level understanding of the image is required. The algorithm should figure out the objects present and also the pixels which correspond to the object. Semantic segmentation is one of the essential tasks for complete scene understanding.\n\n\n## Dataset\n\nThe first step in training our segmentation model is to prepare the dataset. We would need the input RGB images and the corresponding segmentation images. If you want to make your own dataset, a tool like labelme or GIMP can be used to manually generate the ground truth segmentation masks.\n\nAssign each class a unique ID. In the segmentation images, the pixel value should denote the class ID of the corresponding pixel. This is a common format used by most of the datasets and keras_segmentation. For the segmentation maps, do not use the jpg format as jpg is lossy and the pixel values might change. Use bmp or png format instead. And of course, the size of the input image and the segmentation image should be the same.\n\nIn the following example, pixel (0,0) is labeled as class 2, pixel (3,4) is labeled as class 1 and rest of the pixels are labeled as class 0.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nimport numpy as np\n\nann_img = np.zeros((30,30,3)).astype('uint8')\nann_img[ 3 , 4 ] = 1 # this would set the label of pixel 3,4 as 1\nann_img[ 0 , 0 ] = 2 # this would set the label of pixel 0,0 as 2\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After generating the segmentation images, place them in the training/testing folder. Make separate folders for input images and the segmentation images. The file name of the input image and the corresponding segmentation image should be the same. For this tutorial we would be using a data-set which is already prepared. You can download it from here ([Aerial Semantic Segmentation Drone Dataset](https://www.kaggle.com/bulentsiyah/semantic-drone-dataset))."},{"metadata":{},"cell_type":"markdown","source":"## [Aerial Semantic Segmentation Drone Dataset](https://www.kaggle.com/bulentsiyah/semantic-drone-dataset)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\noriginal_image = \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/original_images/001.jpg\"\nlabel_image_semantic = \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/label_images_semantic/001.png\"\n\nfig, axs = plt.subplots(1, 2, figsize=(16, 8), constrained_layout=True)\n\naxs[0].imshow( Image.open(original_image))\naxs[0].grid(False)\n\nlabel_image_semantic = Image.open(label_image_semantic)\nlabel_image_semantic = np.asarray(label_image_semantic)\naxs[1].imshow(label_image_semantic)\naxs[1].grid(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"2.\"></a> \n# 2.Implementation of Segnet, FCN, UNet , PSPNet and other models in Keras\n\nSource Github Link: https://github.com/divamgupta/image-segmentation-keras\n\nModels\nFollowing models are supported:\n\n| model_name       | Base Model     | Segmentation Model     |\n| :------------- | :----------: | -----------: |\n|  fcn_8 | Vanilla CNN  | FCN8   |\n|  fcn_32  | Vanilla CNN | FCN8 |\n|  fcn_8_vgg | VGG 16  | FCN8   |\n|  fcn_32_vgg  | VGG 16 | FCN32 |\n|  fcn_8_resnet50 | Resnet-50  | FCN32  |\n|  fcn_32_resnet50  | Resnet-50 | FCN32 |\n| fcn_8_mobilenet  | MobileNet  | FCN32   |\n| fcn_32_mobilenet   | MobileNet | FCN32 |\n| pspnet  | Vanilla CNN  | PSPNet   |\n| vgg_pspnet   | VGG 16 | PSPNet |\n|  resnet50_pspnet  | Resnet-50  |  PSPNet  |\n| unet_mini   | Vanilla Mini CNN  | U-Net   |\n| unet   | Vanilla CNN  | U-Net   |\n| vgg_unet   | VGG 16  | U-Net   |\n| resnet50_unet   | Resnet-50  |  U-Net  |\n|  mobilenet_unet  | MobileNet  | U-Net   |\n| segnet   | Vanilla CNN  | Segnet   |\n| vgg_segnet   | VGG 16  |  Segnet  |\n|  resnet50_segnet  | Resnet-50  | Segnet   |\n|  mobilenet_segnet  | MobileNet  | Segnet   |\n\t\t\n        \n\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install keras-segmentation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"kaggle_commit = True\n\nepochs = 20\nif kaggle_commit:\n    epochs = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_segmentation.models.unet import vgg_unet\n\nn_classes = 23 # Aerial Semantic Segmentation Drone Dataset tree, gras, other vegetation, dirt, gravel, rocks, water, paved area, pool, person, dog, car, bicycle, roof, wall, fence, fence-pole, window, door, obstacle\nmodel = vgg_unet(n_classes=n_classes ,  input_height=416, input_width=608  )\n\nmodel.train( \n    train_images =  \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/original_images/\",\n    train_annotations = \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/label_images_semantic/\",\n    checkpoints_path = \"vgg_unet\" , epochs=epochs\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nstart = time.time()\n\ninput_image = \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/original_images/001.jpg\"\nout = model.predict_segmentation(\n    inp=input_image,\n    out_fname=\"out.png\"\n)\n\nfig, axs = plt.subplots(1, 3, figsize=(20, 20), constrained_layout=True)\n\nimg_orig = Image.open(input_image)\naxs[0].imshow(img_orig)\naxs[0].set_title('original image-001.jpg')\naxs[0].grid(False)\n\naxs[1].imshow(out)\naxs[1].set_title('prediction image-out.png')\naxs[1].grid(False)\n\nvalidation_image = \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/label_images_semantic/001.png\"\naxs[2].imshow( Image.open(validation_image))\naxs[2].set_title('true label image-001.png')\naxs[2].grid(False)\n\ndone = time.time()\nelapsed = done - start","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(elapsed)\nprint(out)\nprint(out.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"3.\"></a> \n# 3. I extracted Github codes\n\n\n[Implementation of Segnet, FCN, UNet , PSPNet and other models in Keras](#2.) the codes in this section do everything for you. You have no chance to interfere with the codes. I extracted these codes and wrote them open and open. We will have the chance to trade on the model as we wish.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport keras\nfrom keras.models import *\nfrom keras.layers import *\n\nfrom types import MethodType\nimport random\nimport six\nimport json\nfrom tqdm import tqdm\nimport cv2\nimport numpy as np\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nprint(sys.version)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_ORDERING_CHANNELS_FIRST = \"channels_first\"\nIMAGE_ORDERING_CHANNELS_LAST = \"channels_last\"\n# Default IMAGE_ORDERING = channels_last\nIMAGE_ORDERING = IMAGE_ORDERING_CHANNELS_LAST\n\nif IMAGE_ORDERING == 'channels_first':\n    MERGE_AXIS = 1\nelif IMAGE_ORDERING == 'channels_last':\n    MERGE_AXIS = -1\n    \nif IMAGE_ORDERING == 'channels_first':\n    pretrained_url = \"https://github.com/fchollet/deep-learning-models/\" \\\n                     \"releases/download/v0.1/\" \\\n                     \"vgg16_weights_th_dim_ordering_th_kernels_notop.h5\"\nelif IMAGE_ORDERING == 'channels_last':\n    pretrained_url = \"https://github.com/fchollet/deep-learning-models/\" \\\n                     \"releases/download/v0.1/\" \\\n                     \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n    \nclass_colors = [(random.randint(0, 255), random.randint(\n    0, 255), random.randint(0, 255)) for _ in range(5000)]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_colored_segmentation_image( seg_arr  , n_classes , colors=class_colors ):\n    output_height = seg_arr.shape[0]\n    output_width = seg_arr.shape[1]\n\n    seg_img = np.zeros((output_height, output_width, 3))\n\n    for c in range(n_classes):\n        seg_img[:, :, 0] += ((seg_arr[:, :] == c)*(colors[c][0])).astype('uint8')\n        seg_img[:, :, 1] += ((seg_arr[:, :] == c)*(colors[c][1])).astype('uint8')\n        seg_img[:, :, 2] += ((seg_arr[:, :] == c)*(colors[c][2])).astype('uint8')\n\n    return seg_img \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_segmentation( seg_arr , inp_img=None  , n_classes=None , \n    colors=class_colors , class_names=None , overlay_img=False , show_legends=False , \n    prediction_width=None , prediction_height=None  ):\n    \n\n    if n_classes is None:\n        n_classes = np.max(seg_arr)\n\n    seg_img = get_colored_segmentation_image( seg_arr  , n_classes , colors=colors )\n\n    if not inp_img is None:\n        orininal_h = inp_img.shape[0]\n        orininal_w = inp_img.shape[1]\n        seg_img = cv2.resize(seg_img, (orininal_w, orininal_h))\n\n\n    if (not prediction_height is None) and  (not prediction_width is None):\n        seg_img = cv2.resize(seg_img, (prediction_width, prediction_height ))\n        if not inp_img is None:\n            inp_img = cv2.resize(inp_img, (prediction_width, prediction_height ))\n\n\n    if overlay_img:\n        assert not inp_img is None\n        seg_img = overlay_seg_image( inp_img , seg_img  )\n\n\n    if show_legends:\n        assert not class_names is None\n        legend_img = get_legends(class_names , colors=colors )\n\n        seg_img = concat_lenends( seg_img , legend_img )\n\n\n    return seg_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_image_array(image_input, width, height, imgNorm=\"sub_mean\",\n                  ordering='channels_first'):\n    \"\"\" Load image array from input \"\"\"\n\n    if type(image_input) is np.ndarray:\n        # It is already an array, use it as it is\n        img = image_input\n    elif  isinstance(image_input, six.string_types)  :\n        if not os.path.isfile(image_input):\n            raise DataLoaderError(\"get_image_array: path {0} doesn't exist\".format(image_input))\n        img = cv2.imread(image_input, 1)\n    else:\n        raise DataLoaderError(\"get_image_array: Can't process input type {0}\".format(str(type(image_input))))\n\n    if imgNorm == \"sub_and_divide\":\n        img = np.float32(cv2.resize(img, (width, height))) / 127.5 - 1\n    elif imgNorm == \"sub_mean\":\n        img = cv2.resize(img, (width, height))\n        img = img.astype(np.float32)\n        img[:, :, 0] -= 103.939\n        img[:, :, 1] -= 116.779\n        img[:, :, 2] -= 123.68\n        img = img[:, :, ::-1]\n    elif imgNorm == \"divide\":\n        img = cv2.resize(img, (width, height))\n        img = img.astype(np.float32)\n        img = img/255.0\n\n    if ordering == 'channels_first':\n        img = np.rollaxis(img, 2, 0)\n    return img\n\ndef get_image_arr( path , width , height , imgNorm=\"sub_mean\" , odering='channels_first' ):\n\n\tif type( path ) is np.ndarray:\n\t\timg = path\n\telse:\n\t\timg = cv2.imread(path, 1)\n\n\tif imgNorm == \"sub_and_divide\":\n\t\timg = np.float32(cv2.resize(img, ( width , height ))) / 127.5 - 1\n\telif imgNorm == \"sub_mean\":\n\t\timg = cv2.resize(img, ( width , height ))\n\t\timg = img.astype(np.float32)\n\t\timg[:,:,0] -= 103.939\n\t\timg[:,:,1] -= 116.779\n\t\timg[:,:,2] -= 123.68\n\t\timg = img[ : , : , ::-1 ]\n\telif imgNorm == \"divide\":\n\t\timg = cv2.resize(img, ( width , height ))\n\t\timg = img.astype(np.float32)\n\t\timg = img/255.0\n\n\tif odering == 'channels_first':\n\t\timg = np.rollaxis(img, 2, 0)\n\treturn img\n\n\ndef get_segmentation_array(image_input, nClasses, width, height, no_reshape=False):\n    \"\"\" Load segmentation array from input \"\"\"\n\n    seg_labels = np.zeros((height, width, nClasses))\n\n    if type(image_input) is np.ndarray:\n        # It is already an array, use it as it is\n        img = image_input\n    elif isinstance(image_input, six.string_types) :\n        if not os.path.isfile(image_input):\n            raise DataLoaderError(\"get_segmentation_array: path {0} doesn't exist\".format(image_input))\n        img = cv2.imread(image_input, 1)\n    else:\n        raise DataLoaderError(\"get_segmentation_array: Can't process input type {0}\".format(str(type(image_input))))\n\n    img = cv2.resize(img, (width, height), interpolation=cv2.INTER_NEAREST)\n    img = img[:, :, 0]\n\n    for c in range(nClasses):\n        seg_labels[:, :, c] = (img == c).astype(int)\n\n    if not no_reshape:\n        seg_labels = np.reshape(seg_labels, (width*height, nClasses))\n\n    return seg_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def image_segmentation_generator(images_path, segs_path, batch_size,\n                                 n_classes, input_height, input_width,\n                                 output_height, output_width,\n                                 do_augment=False ,augmentation_name=\"aug_all\" ):\n\n    img_seg_pairs = get_pairs_from_paths(images_path, segs_path)\n    random.shuffle(img_seg_pairs)\n    zipped = itertools.cycle(img_seg_pairs)\n\n    while True:\n        X = []\n        Y = []\n        for _ in range(batch_size):\n            im, seg = next(zipped)\n\n            im = cv2.imread(im, 1)\n            seg = cv2.imread(seg, 1)\n\n            if do_augment:\n                im, seg[:, :, 0] = augment_seg(im, seg[:, :, 0] , augmentation_name=augmentation_name )\n\n            X.append(get_image_array(im, input_width,\n                                   input_height, ordering=IMAGE_ORDERING))\n            Y.append(get_segmentation_array(\n                seg, n_classes, output_width, output_height))\n\n        yield np.array(X), np.array(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pairs_from_paths(images_path, segs_path, ignore_non_matching=False):\n    \"\"\" Find all the images from the images_path directory and\n        the segmentation images from the segs_path directory\n        while checking integrity of data \"\"\"\n\n    ACCEPTABLE_IMAGE_FORMATS = [\".jpg\", \".jpeg\", \".png\" , \".bmp\"]\n    ACCEPTABLE_SEGMENTATION_FORMATS = [\".png\", \".bmp\"]\n\n    image_files = []\n    segmentation_files = {}\n\n    for dir_entry in os.listdir(images_path):\n        if os.path.isfile(os.path.join(images_path, dir_entry)) and \\\n                os.path.splitext(dir_entry)[1] in ACCEPTABLE_IMAGE_FORMATS:\n            file_name, file_extension = os.path.splitext(dir_entry)\n            image_files.append((file_name, file_extension, os.path.join(images_path, dir_entry)))\n\n    for dir_entry in os.listdir(segs_path):\n        if os.path.isfile(os.path.join(segs_path, dir_entry)) and \\\n                os.path.splitext(dir_entry)[1] in ACCEPTABLE_SEGMENTATION_FORMATS:\n            file_name, file_extension = os.path.splitext(dir_entry)\n            if file_name in segmentation_files:\n                raise DataLoaderError(\"Segmentation file with filename {0} already exists and is ambiguous to resolve with path {1}. Please remove or rename the latter.\".format(file_name, os.path.join(segs_path, dir_entry)))\n            segmentation_files[file_name] = (file_extension, os.path.join(segs_path, dir_entry))\n\n    return_value = []\n    # Match the images and segmentations\n    for image_file, _, image_full_path in image_files:\n        if image_file in segmentation_files:\n            return_value.append((image_full_path, segmentation_files[image_file][1]))\n        elif ignore_non_matching:\n            continue\n        else:\n            # Error out\n            raise DataLoaderError(\"No corresponding segmentation found for image {0}.\".format(image_full_path))\n\n    return return_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def verify_segmentation_dataset(images_path, segs_path, n_classes, show_all_errors=False):\n    try:\n        img_seg_pairs = get_pairs_from_paths(images_path, segs_path)\n        if not len(img_seg_pairs):\n            print(\"Couldn't load any data from images_path: {0} and segmentations path: {1}\".format(images_path, segs_path))\n            return False\n\n        return_value = True\n        for im_fn, seg_fn in tqdm(img_seg_pairs):\n            img = cv2.imread(im_fn)\n            seg = cv2.imread(seg_fn)\n            # Check dimensions match\n            if not img.shape == seg.shape:\n                return_value = False\n                print(\"The size of image {0} and its segmentation {1} doesn't match (possibly the files are corrupt).\".format(im_fn, seg_fn))\n                if not show_all_errors:\n                    break\n            else:\n                max_pixel_value = np.max(seg[:, :, 0])\n                if max_pixel_value >= n_classes:\n                    return_value = False\n                    print(\"The pixel values of the segmentation image {0} violating range [0, {1}]. Found maximum pixel value {2}\".format(seg_fn, str(n_classes - 1), max_pixel_value))\n                    if not show_all_errors:\n                        break\n        if return_value:\n            print(\"Dataset verified! \")\n        else:\n            print(\"Dataset not verified!\")\n        return return_value\n    except Exception as e:\n        print(\"Found error during data loading\\n{0}\".format(str(e)))\n        return False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate( model=None , inp_images=None , annotations=None,inp_images_dir=None ,annotations_dir=None , checkpoints_path=None ):\n    \n    if model is None:\n        assert (checkpoints_path is not None) , \"Please provide the model or the checkpoints_path\"\n        model = model_from_checkpoint_path(checkpoints_path)\n        \n    if inp_images is None:\n        assert (inp_images_dir is not None) , \"Please privide inp_images or inp_images_dir\"\n        assert (annotations_dir is not None) , \"Please privide inp_images or inp_images_dir\"\n        \n        paths = get_pairs_from_paths(inp_images_dir , annotations_dir )\n        paths = list(zip(*paths))\n        inp_images = list(paths[0])\n        annotations = list(paths[1])\n        \n    assert type(inp_images) is list\n    assert type(annotations) is list\n        \n    tp = np.zeros( model.n_classes  )\n    fp = np.zeros( model.n_classes  )\n    fn = np.zeros( model.n_classes  )\n    n_pixels = np.zeros( model.n_classes  )\n    \n    for inp , ann   in tqdm( zip( inp_images , annotations )):\n        pr = predict(model , inp )\n        gt = get_segmentation_array( ann , model.n_classes ,  model.output_width , model.output_height , no_reshape=True  )\n        gt = gt.argmax(-1)\n        pr = pr.flatten()\n        gt = gt.flatten()\n                \n        for cl_i in range(model.n_classes ):\n            \n            tp[ cl_i ] += np.sum( (pr == cl_i) * (gt == cl_i) )\n            fp[ cl_i ] += np.sum( (pr == cl_i) * ((gt != cl_i)) )\n            fn[ cl_i ] += np.sum( (pr != cl_i) * ((gt == cl_i)) )\n            n_pixels[ cl_i ] += np.sum( gt == cl_i  )\n            \n    cl_wise_score = tp / ( tp + fp + fn + 0.000000000001 )\n    n_pixels_norm = n_pixels /  np.sum(n_pixels)\n    frequency_weighted_IU = np.sum(cl_wise_score*n_pixels_norm)\n    mean_IU = np.mean(cl_wise_score)\n    return {\"frequency_weighted_IU\":frequency_weighted_IU , \"mean_IU\":mean_IU , \"class_wise_IU\":cl_wise_score }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_multiple(model=None, inps=None, inp_dir=None, out_dir=None,\n                     checkpoints_path=None ,overlay_img=False ,\n    class_names=None , show_legends=False , colors=class_colors , prediction_width=None , prediction_height=None  ):\n\n    if model is None and (checkpoints_path is not None):\n        model = model_from_checkpoint_path(checkpoints_path)\n\n    if inps is None and (inp_dir is not None):\n        inps = glob.glob(os.path.join(inp_dir, \"*.jpg\")) + glob.glob(\n            os.path.join(inp_dir, \"*.png\")) + \\\n            glob.glob(os.path.join(inp_dir, \"*.jpeg\"))\n\n    assert type(inps) is list\n\n    all_prs = []\n\n    for i, inp in enumerate(tqdm(inps)):\n        if out_dir is None:\n            out_fname = None\n        else:\n            if isinstance(inp, six.string_types):\n                out_fname = os.path.join(out_dir, os.path.basename(inp))\n            else:\n                out_fname = os.path.join(out_dir, str(i) + \".jpg\")\n\n        pr = predict( model, inp, out_fname ,\n            overlay_img=overlay_img,class_names=class_names ,show_legends=show_legends , \n            colors=colors , prediction_width=prediction_width , prediction_height=prediction_height  )\n\n        all_prs.append(pr)\n\n    return all_prs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model=None, inp=None, out_fname=None, checkpoints_path=None,overlay_img=False ,\n    class_names=None , show_legends=False , colors=class_colors , prediction_width=None , prediction_height=None  ):\n\n    if model is None and (checkpoints_path is not None):\n        model = model_from_checkpoint_path(checkpoints_path)\n\n    assert (inp is not None)\n    assert((type(inp) is np.ndarray) or isinstance(inp, six.string_types)\n           ), \"Inupt should be the CV image or the input file name\"\n\n    if isinstance(inp, six.string_types):\n        inp = cv2.imread(inp)\n\n    assert len(inp.shape) == 3, \"Image should be h,w,3 \"\n    orininal_h = inp.shape[0]\n    orininal_w = inp.shape[1]\n\n    output_width = model.output_width\n    output_height = model.output_height\n    input_width = model.input_width\n    input_height = model.input_height\n    n_classes = model.n_classes\n\n    x = get_image_array(inp, input_width, input_height, ordering=IMAGE_ORDERING)\n    pr = model.predict(np.array([x]))[0]\n    pr = pr.reshape((output_height,  output_width, n_classes)).argmax(axis=2)\n\n    seg_img = visualize_segmentation( pr , inp ,n_classes=n_classes , colors=colors\n        , overlay_img=overlay_img ,show_legends=show_legends ,class_names=class_names ,prediction_width=prediction_width , prediction_height=prediction_height   )\n\n    if out_fname is not None:\n        cv2.imwrite(out_fname, seg_img)\n\n    return pr\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model,\n          train_images,\n          train_annotations,\n          input_height=None,\n          input_width=None,\n          n_classes=None,\n          verify_dataset=True,\n          checkpoints_path=None,\n          epochs=5,\n          batch_size=2,\n          validate=False,\n          val_images=None,\n          val_annotations=None,\n          val_batch_size=2,\n          auto_resume_checkpoint=False,\n          load_weights=None,\n          steps_per_epoch=512,\n          val_steps_per_epoch=512,\n          gen_use_multiprocessing=False,\n          ignore_zero_class=False , \n          optimizer_name='adadelta' , do_augment=False , augmentation_name=\"aug_all\"\n          ):\n\n    \n    # check if user gives model name instead of the model object\n    if isinstance(model, six.string_types):\n        # create the model from the name\n        assert (n_classes is not None), \"Please provide the n_classes\"\n        if (input_height is not None) and (input_width is not None):\n            model = model_from_name[model](\n                n_classes, input_height=input_height, input_width=input_width)\n        else:\n            model = model_from_name[model](n_classes)\n\n    n_classes = model.n_classes\n    input_height = model.input_height\n    input_width = model.input_width\n    output_height = model.output_height\n    output_width = model.output_width\n\n    if validate:\n        assert val_images is not None\n        assert val_annotations is not None\n\n    if optimizer_name is not None:\n\n        if ignore_zero_class:\n            loss_k = masked_categorical_crossentropy\n        else:\n            loss_k = 'categorical_crossentropy'\n\n        model.compile(loss= loss_k ,\n                      optimizer=optimizer_name,\n                      metrics=['accuracy'])\n\n    if checkpoints_path is not None:\n        with open(checkpoints_path+\"_config.json\", \"w\") as f:\n            json.dump({\n                \"model_class\": model.model_name,\n                \"n_classes\": n_classes,\n                \"input_height\": input_height,\n                \"input_width\": input_width,\n                \"output_height\": output_height,\n                \"output_width\": output_width\n            }, f)\n\n    if load_weights is not None and len(load_weights) > 0:\n        print(\"Loading weights from \", load_weights)\n        model.load_weights(load_weights)\n\n    if auto_resume_checkpoint and (checkpoints_path is not None):\n        latest_checkpoint = find_latest_checkpoint(checkpoints_path)\n        if latest_checkpoint is not None:\n            print(\"Loading the weights from latest checkpoint \",\n                  latest_checkpoint)\n            model.load_weights(latest_checkpoint)\n\n    if verify_dataset:\n        print(\"Verifying training dataset\")\n        verified = verify_segmentation_dataset(train_images, train_annotations, n_classes)\n        assert verified\n        if validate:\n            print(\"Verifying validation dataset\")\n            verified = verify_segmentation_dataset(val_images, val_annotations, n_classes)\n            assert verified\n\n    train_gen = image_segmentation_generator(\n        train_images, train_annotations,  batch_size,  n_classes,\n        input_height, input_width, output_height, output_width , do_augment=do_augment ,augmentation_name=augmentation_name )\n\n    if validate:\n        val_gen = image_segmentation_generator(\n            val_images, val_annotations,  val_batch_size,\n            n_classes, input_height, input_width, output_height, output_width)\n\n    if not validate:\n        for ep in range(epochs):\n            print(\"Starting Epoch \", ep)\n            model.fit_generator(train_gen, steps_per_epoch, epochs=1, use_multiprocessing=True)\n            if checkpoints_path is not None:\n                model.save_weights(checkpoints_path + \".\" + str(ep))\n                print(\"saved \", checkpoints_path + \".model.\" + str(ep))\n            print(\"Finished Epoch\", ep)\n    else:\n        for ep in range(epochs):\n            print(\"Starting Epoch \", ep)\n            model.fit_generator(train_gen, steps_per_epoch,\n                                validation_data=val_gen,\n                                validation_steps=val_steps_per_epoch,  epochs=1 , use_multiprocessing=gen_use_multiprocessing)\n            if checkpoints_path is not None:\n                model.save_weights(checkpoints_path + \".\" + str(ep))\n                print(\"saved \", checkpoints_path + \".model.\" + str(ep))\n            print(\"Finished Epoch\", ep)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_segmentation_model(input, output):\n\n    img_input = input\n    o = output\n\n    o_shape = Model(img_input, o).output_shape\n    i_shape = Model(img_input, o).input_shape\n\n    if IMAGE_ORDERING == 'channels_first':\n        output_height = o_shape[2]\n        output_width = o_shape[3]\n        input_height = i_shape[2]\n        input_width = i_shape[3]\n        n_classes = o_shape[1]\n        o = (Reshape((-1, output_height*output_width)))(o)\n        o = (Permute((2, 1)))(o)\n    elif IMAGE_ORDERING == 'channels_last':\n        output_height = o_shape[1]\n        output_width = o_shape[2]\n        input_height = i_shape[1]\n        input_width = i_shape[2]\n        n_classes = o_shape[3]\n        o = (Reshape((output_height*output_width, -1)))(o)\n\n    o = (Activation('softmax'))(o)\n    model = Model(img_input, o)\n    model.output_width = output_width\n    model.output_height = output_height\n    model.n_classes = n_classes\n    model.input_height = input_height\n    model.input_width = input_width\n    model.model_name = \"\"\n\n    model.train = MethodType(train, model)\n    model.predict_segmentation = MethodType(predict, model)\n    model.predict_multiple = MethodType(predict_multiple, model)\n    model.evaluate_segmentation = MethodType(evaluate, model)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_vgg_encoder(input_height=224,  input_width=224, pretrained='imagenet'):\n\n    assert input_height % 32 == 0\n    assert input_width % 32 == 0\n\n    if IMAGE_ORDERING == 'channels_first':\n        img_input = Input(shape=(3, input_height, input_width))\n    elif IMAGE_ORDERING == 'channels_last':\n        img_input = Input(shape=(input_height, input_width, 3))\n\n    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n               name='block1_conv1', data_format=IMAGE_ORDERING)(img_input)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n               name='block1_conv2', data_format=IMAGE_ORDERING)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool',\n                     data_format=IMAGE_ORDERING)(x)\n    f1 = x\n    # Block 2\n    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n               name='block2_conv1', data_format=IMAGE_ORDERING)(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n               name='block2_conv2', data_format=IMAGE_ORDERING)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool',\n                     data_format=IMAGE_ORDERING)(x)\n    f2 = x\n\n    # Block 3\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='block3_conv1', data_format=IMAGE_ORDERING)(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='block3_conv2', data_format=IMAGE_ORDERING)(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='block3_conv3', data_format=IMAGE_ORDERING)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool',\n                     data_format=IMAGE_ORDERING)(x)\n    f3 = x\n\n    # Block 4\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block4_conv1', data_format=IMAGE_ORDERING)(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block4_conv2', data_format=IMAGE_ORDERING)(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block4_conv3', data_format=IMAGE_ORDERING)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool',\n                     data_format=IMAGE_ORDERING)(x)\n    f4 = x\n\n    # Block 5\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block5_conv1', data_format=IMAGE_ORDERING)(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block5_conv2', data_format=IMAGE_ORDERING)(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block5_conv3', data_format=IMAGE_ORDERING)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool',\n                     data_format=IMAGE_ORDERING)(x)\n    f5 = x\n\n    if pretrained == 'imagenet':\n        VGG_Weights_path = keras.utils.get_file(pretrained_url.split(\"/\")[-1], pretrained_url)\n        Model(img_input, x).load_weights(VGG_Weights_path)\n\n    return img_input, [f1, f2, f3, f4, f5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _unet(n_classes, encoder, l1_skip_conn=True, input_height=416,\n          input_width=608):\n\n    img_input, levels = encoder(\n        input_height=input_height, input_width=input_width)\n    [f1, f2, f3, f4, f5] = levels\n\n    o = f4\n\n    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n    o = (Conv2D(512, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n    o = (BatchNormalization())(o)\n\n    o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n    o = (concatenate([o, f3], axis=MERGE_AXIS))\n    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n    o = (Conv2D(256, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n    o = (BatchNormalization())(o)\n\n    o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n    o = (concatenate([o, f2], axis=MERGE_AXIS))\n    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n    o = (Conv2D(128, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n    o = (BatchNormalization())(o)\n\n    o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n\n    if l1_skip_conn:\n        o = (concatenate([o, f1], axis=MERGE_AXIS))\n\n    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n    o = (Conv2D(64, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n    o = (BatchNormalization())(o)\n\n    o = Conv2D(n_classes, (3, 3), padding='same',data_format=IMAGE_ORDERING)(o)\n\n    model = get_segmentation_model(img_input, o)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def vgg_unet(n_classes, input_height=416, input_width=608, encoder_level=3):\n\n    model = _unet(n_classes, get_vgg_encoder,input_height=input_height, input_width=input_width)\n    model.model_name = \"vgg_unet\"\n    return model\n\nn_classes = 23 # Aerial Semantic Segmentation Drone Dataset tree, gras, other vegetation, dirt, gravel, rocks, water, paved area, pool, person, dog, car, bicycle, roof, wall, fence, fence-pole, window, door, obstacle\n\nmodel = vgg_unet(n_classes=n_classes,  input_height=416, input_width=608)\nmodel_from_name = {}\nmodel_from_name[\"vgg_unet\"] = vgg_unet\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"kaggle_commit = True\n\nepochs = 20\nif kaggle_commit:\n    epochs = 5\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train( \n    train_images =  \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/original_images/\",\n    train_annotations = \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/label_images_semantic/\",\n    checkpoints_path = \"vgg_unet\" , epochs=epochs\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nstart = time.time()\n\ninput_image = \"/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/original_images/002.jpg\"\nout = model.predict_segmentation(\n    inp=input_image,\n    out_fname=\"out.png\"\n)\n\nfig, axs = plt.subplots(1, 3, figsize=(20, 20), constrained_layout=True)\n\nimg_orig = Image.open(input_image)\naxs[0].imshow(img_orig)\naxs[0].set_title('original image-002.jpg')\naxs[0].grid(False)\n\naxs[1].imshow(out)\naxs[1].set_title('prediction image-out.png')\naxs[1].grid(False)\n\nvalidation_image = \"/kaggle/input/semantic-drone-dataset/dataset/dataset/semantic_drone_dataset/label_images_semantic/002.png\"\naxs[2].imshow( Image.open(validation_image))\naxs[2].set_title('true label image-002.png')\naxs[2].grid(False)\n\n\ndone = time.time()\nelapsed = done - start","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(elapsed)\nprint(out)\nprint(out.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}