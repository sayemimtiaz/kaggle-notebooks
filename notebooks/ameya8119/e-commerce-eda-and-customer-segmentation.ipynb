{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook aims at analyzing the content of an E-commerce database that lists purchases made by  ∼ 4000 customers over a period of one year (from 2010/12/01 to 2021/12/01). Based on this analysis, I develop a model that allows to anticipate the purchases that will be made by a new customer, during the following year and this, from its first purchase."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport time\nimport datetime as dt\nfrom sklearn import preprocessing, model_selection, metrics, feature_selection\nfrom sklearn.model_selection import GridSearchCV, learning_curve\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import neighbors, linear_model, svm, tree, ensemble\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import silhouette_samples,silhouette_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_roc_curve\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Insight\n\nGetting the basic data insights\nGiven some basic informations on the content of the dataframe: the type of the various variables, the number of null values and their percentage with respect to the total number of entries:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/ecommerce-data/data.csv',encoding=\"ISO-8859-1\")\nseparator = '\\n*******************************\\n'\nprint(data.info())\nprint(separator)\nprint(data.describe())\nprint(separator)\nprint(data.head(5))\nprint(separator)\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting the total number of the Null Values and calculating the perecentage of data that needs to remvoed in order to get a more accurate dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])\nprint(data.info())\nprint(separator)\nprint(data.isnull().sum().sort_values(ascending = False))\nprint(separator)\nprint((data.isnull().sum().sort_values(ascending = False))/len(data)*100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna(axis = 0, inplace = True)\nprint(data.shape)   #removing the null values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the number of null values in the dataframe, it is interesting to note that  ∼ 25% of the entries are not assigned to a particular customer. With the data available, it is impossible to impute values for the user and these entries are thus useless for the current exercise. So I delete them from the dataframe:\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_values = data['Quantity']\n\nsns.kdeplot(x_values, color='b', shade=True, Label='Quantity') \n  \n# Setting the X and Y Label \nplt.xlabel('Quantity') \nplt.ylabel('Probablity Density')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kernel Density Estimate (KDE) plot shows the distribution for the unit price "},{"metadata":{"trusted":true},"cell_type":"code","source":"x_values = data['UnitPrice']\n\nsns.kdeplot(x_values, color='r', shade=True, Label='UnitPrice') \n  \n# Setting the X and Y Label \nplt.xlabel('UnitPrice') \nplt.ylabel('Probablity Density')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data['Quantity']>=0]\ndata = data[data['UnitPrice']>=0]\n\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Basic info and describe analysis over the dataset'''\ndata.info()\nprint(separator)\nprint(data.describe())  #data description \ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This dataframe contains 8 variables that correspond to:\n\n**InvoiceNo:** Invoice number. Nominal, a 6-digit integral number uniquely assigned to each \ntransaction. If this code starts with letter 'c', it indicates a cancellation.\n\n**StockCode:** Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n\n**Description:** Product (item) name. Nominal.\n\n**Quantity:** The quantities of each product (item) per transaction. Numeric.\n\n**InvoiceDate:** Invice Date and time. Numeric, the day and time when each transaction was generated.\n\n**UnitPrice:** Unit price. Numeric, Product price per unit in sterling.\n\n**CustomerID:** Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n\n**Country:** Country name. Nominal, the name of the country where each customer resides."},{"metadata":{},"cell_type":"markdown","source":"Now adding a total quantity for price\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['TotalQuantity'] = data['Quantity']*data['UnitPrice']\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will look at the details of the countries through which most of the orders were placed\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[['InvoiceNo','Country']].groupby('Country').count().sort_values(\"InvoiceNo\",ascending = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a Pie Chart to Visualize the the data better"},{"metadata":{"trusted":true},"cell_type":"code","source":"  \n# Creating dataset \ncountry = ['Netherlands','United Kingdom', 'Belgium','Germany'   , 'France' , 'EIRE'  , 'Spain'    , 'Portugal'  , 'Others','Switzerland']\ninvoice = [2363,354345,2031,9042,8342,7238,2485 ,1462,8774,1842]\n# Creating plot \nfig = plt.figure(figsize =(10, 7)) \nplt.pie(invoice, labels = country) \n  \n# show plot \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the dataset is largely dominated by orders made from the UK."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Top 5 countries sales count wise in the cleaned up data.'''\ndata['Country'].value_counts().head(10).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gross Amount of countries\n\nComparing each companies gross amount  and using a bar plot to compare the data \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Top 5 countries Total Gross Amount sales wise.'''\ndata_temp = data.groupby(['Country'])['TotalQuantity'].sum().reset_index().sort_values('TotalQuantity',ascending=False).head(7)\nprint(data_temp)\nprint(data_temp.plot(x='Country', y='TotalQuantity',kind='bar'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find the largest amount order."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['TotalQuantity']==data['TotalQuantity'].max()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:** Since the given website is a United Kindom originated website. All the variables such as No. of Customers and the Gross total sales is dominated by the United Kingdom. The remaning portion is occupied by the neighbouring Europian Countries"},{"metadata":{},"cell_type":"markdown","source":"**Which description was used the most**"},{"metadata":{"trusted":true},"cell_type":"code","source":"items = data['Description'].value_counts().head()\nprint(items)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_counts = data['Description'].value_counts().sort_values(ascending=False).head(20)\nsns.barplot(item_counts.index, item_counts.values,palette = \"Blues_r\")\nplt.ylabel(\"Counts\")\nplt.title(\"Which items were bought more often?\");\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This analysis gave us a general idea of which product was most demanded by the customers and the bar plot is used for the same visulaization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[['InvoiceNo','Country','CustomerID','TotalQuantity']].sort_values('TotalQuantity',ascending = False).head(15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **RFM Analysis**\n\n**RFM (Recency, Frequency, Monetary)** analysis is a customer segmentation technique that uses past purchase behaviour to divide customers into groups.\nRFM helps divide customers into various categories or clusters to identify customers who are more likely to respond to promotions and also for future personalization services.\n\n**RECENCY (R)**: Days since last purchase\n\n**FREQUENCY (F)**: Total number of purchases\n\n**MONETARY VALUE (M)**: Total money this customer spent.\nWe will create those 3 customer attributes for each customer."},{"metadata":{},"cell_type":"markdown","source":"# **Recency**\n\nTo calculate recency, we need to choose a date point from which we evaluate how many days ago was the customer's last purchase."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Date'] = data['InvoiceDate'].apply(lambda x: x.date())\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#recency dataframe\nrecency_df = data.groupby(by='CustomerID', as_index=False)['Date'].max()\nrecency_df.columns = ['CustomerID','LastPurchaseDate']\nrecency_df.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"now = dt.date(2021,12,1)\nprint(now)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recency_df['Recency'] = recency_df['LastPurchaseDate'].apply(lambda x: (now - x).days)\nrecency_df.drop('LastPurchaseDate',axis = 1,inplace=True)\nrecency_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Frequency\nFrequency helps us to know how many times a customer purchased from us. To do that we need to check how many invoices are registered by the same customer."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = data.copy()\ntemp.drop_duplicates(['InvoiceNo','CustomerID'],keep='first',inplace=True)\nfrequency_df = temp.groupby(by=['CustomerID'], as_index=False)['InvoiceNo'].count()\nfrequency_df.columns = ['CustomerID','Frequency']\nfrequency_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Monetary\nMonetary attribute answers the question: How much money did the customer spent over time?\n\nTo do that, first, we will create a new column total cost to have the total price per invoice."},{"metadata":{"trusted":true},"cell_type":"code","source":"monetary_df = data.groupby(by = 'CustomerID',as_index=False).agg({'TotalQuantity':'sum'})\nmonetary_df.columns = ['CustomerID','TotalQuanity']\nmonetary_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create RFM Table\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfm_df = recency_df.merge(frequency_df,on='CustomerID').merge(monetary_df,on='CustomerID')\nrfm_df.set_index('CustomerID',inplace=True)\nrfm_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = rfm_df.columns\nrfm_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RFM Table Visualisation\n\nNow we will look at the correlation between the the Recency, Frequency and Monetary part of the RFM table which will be an integral part of customer segmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rfm_df.corr())\nsns.heatmap(rfm_df.corr(),cmap=\"YlGnBu\",annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(rfm_df, diag_kind=\"hist\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\nrfm_df = pd.DataFrame(pt.fit_transform(rfm_df))\nrfm_df.columns = features\nrfm_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nTo gain even further insight into customer behavior, we can dig deeper in the relationship between RFM variables.\n\n**RFM model** can be used in conjunction with certain predictive models like **K-means clustering, Logistic Regression and Recommendation Engines** to produce better informative results on customer behavior.\n\nWe will go for **K-means since it has been widely used for Market Segmentation** and it offers the advantage of being simple to implement."},{"metadata":{},"cell_type":"markdown","source":"# PCA\n\nApplying PCA to reduce the the dimensions and the correlation between Frequency and Monetary features."},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nrfm_scaled = sc.fit_transform(rfm_df)\nrfm_scaled[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA()\npca_tranformed_data = pca.fit_transform(rfm_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_exp = pca.explained_variance_ratio_\nvar_exp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.cumsum(var_exp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.mean_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.n_features_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,4))\nplt.bar(range(3), var_exp, alpha=0.5, align='center', label='Individual explained variance')\nplt.step(range(3), np.cumsum(var_exp), where='mid', label='Cumulative explained variance')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = rfm_df.copy()\npca = PCA(n_components = 2)\ndf_pca = pca.fit_transform(X)\n\ndf_pca = pd.DataFrame(df_pca)\ndf_pca.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Means Clustering \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_pca.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans \n\ncluster_range = range(1, 15)\ncluster_errors = []\ncluster_sil_scores = []\n\nfor num in cluster_range: \n    clusters = KMeans(num, n_init = 100,init='k-means++',random_state=0)\n    clusters.fit(X)\n    labels = clusters.labels_                     # capture the cluster lables\n    centroids = clusters.cluster_centers_         # capture the centroids\n    cluster_errors.append( clusters.inertia_ )    # capture the intertia\nclusters_df = pd.DataFrame({ \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors} )\nclusters_df[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.plot(clusters_df[\"num_clusters\"],clusters_df[\"cluster_errors\"],marker = 'o')\nplt.xlabel('count of clusters')\nplt.ylabel('error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for num in range(2,16):\n    clusters = KMeans(n_clusters=num,random_state=0)\n    labels = clusters.fit_predict(df_pca)\n    \n    sil_avg = silhouette_score(df_pca, labels)\n    print('For',num,'The Silhouette Score is =',sil_avg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inferences:\n\nWe observe from the elbow plot a sharp bend after the number of clusters increase by 2.\nSilhoutte Score is also the highest for 2 clusters.\n\nBut, there is also a significant reduce in cluster error as number of clusters increase from 2 to 4 and after 4, the reduction is not much.\n\nSo, we will choose n_clusters = 4 to properly segment our customers."},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 4)\nkmeans = kmeans.fit(df_pca)\nlabels = kmeans.predict(df_pca)\ncentroids = kmeans.cluster_centers_\n\nprint(labels)\nprint()\nprint('Cluster Centers')\nprint(centroids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca['Clusters'] = labels\ndf_pca.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca['Clusters'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca.boxplot(by = 'Clusters',figsize=(15,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.pairplot(df_pca,diag_kind='hist',hue='Clusters')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will use the models to find the number of"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_pca[[0,1]]\nY = df_pca['Clusters']\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)\nlr = LogisticRegression(max_iter=1000,random_state=0)\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\nprint('Test accuracy = ', accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 4) \n  \nknn.fit(X_train, y_train) \npred = knn.predict(X_test) \n  \n# Predictions and Evaluations \n# Let's evaluate our KNN model !  \nfrom sklearn.metrics import classification_report, confusion_matrix \nprint(confusion_matrix(y_test, pred)) \n  \nprint(classification_report(y_test, pred)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)\nregressor = DecisionTreeRegressor(random_state = 213)\nregressor.fit(X_train,y_train)\n\npred = regressor.predict(X_test) \nprint(confusion_matrix(y_test, pred)) \n  \nprint(classification_report(y_test, pred)) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nWe saw that using classification models like Logisitc Regression, KNeighborsClassifier ,DecisionTree we predicted the clusters for customers using RFM dataset as independent variables and Cluster as the target variable. The clusters predicted by the classification models perfectly aligns with K-Means clustering. So, we can conclude that our clusters are correct."},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\nThe work described in this notebook is based on a database providing details on purchases made on an E-commerce platform over a period of one year. Each entry in the dataset describes the purchase of a product, by a particular customer and at a given date. In total, approximately  **∼ 4000** clients appear in the database. Given the available information, I decided to develop a classifier that allows to anticipate the type of purchase that a customer will make, as well as the number of visits that he will make during a year, and this from its first visit to the E-commerce site.\n\nThe next part of the analysis consisted of some basic **data visualization**. This was done in order to get insights regarding the country which was using the E-commerce website the most. I used basic plots in order to show the results of my analysis. I also tried to analyse other important factors such as the Gross Purcahse by a country as well as which following description was used the most. \n\nThe final part of the analysis was the customer segmentation part. The main way to go around with this procces is to use the **RFM (Recency, Frequency, Monetory) table** to sort the customer in the groups. After creating the RFM table I used **K-Means clustering (Elbow curve and Silhoutte scores)** in order to create 4 clusters in which the customers should be Segmented. After each of the customers were segmented into their respective groups. I used models such as **Logisitc Regression, KNeighborsClassifier ,DecisionTree** in order the cross the accuracy of the clustering which resulted in an accuracy score **0.98**. Hence, I conclude the customer segmentation was done which effective methods and high accuracy. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}