{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime\nimport time\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nIn this notebook i will be analysing a news article dataset containing 4 million articles covering 6000 stocks. First, lets see the data we have in this dataset. I will only go in depth on data that has not been covered in the other analysis.\n\nIn this notebook, i will also add the data for the labels. I have made a dataset with the 10 year data of the most occuring stocks."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/massive-stock-news-analysis-db-for-nlpbacktests/analyst_ratings_processed.csv\",error_bad_lines=False)\ndata.sample(5).head(5)\n\n#data = data.sample(1000)\nhistdata = pd.read_csv(\"../input/10y-historical-stock-data/stockhistory.csv\")\n\ndata = data.dropna()\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getUniqueness(dataset):\n    for (columnName, columnData) in dataset.iteritems():\n        print(f\"unique values in [{columnName}]: {columnData.nunique()}\")\n\n    print(f\"total rows: {len(dataset.index)}\")\ngetUniqueness(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Date\nLet's split the date again. We can see that this dataset covers a much larger timeframe than the financial tweets dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.rename(columns={'date':'datetime'},inplace = True)\ndata[['date','time']] = data.datetime.str.split(expand=True)\ndata[['year','month','day']]= data.date.str.split('-',expand =True)\ndata['time'] = data.time.str[:-6]\ndata.sample(10).head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getUnixTime(row):\n    dt = datetime.datetime(int(row[\"year\"]), int(row[\"month\"]),int(row[\"day\"]))\n    return time.mktime(dt.timetuple())\n   \ndata[\"unix\"] = data.apply(lambda row: getUnixTime(row),axis=1 )\ndata.sample(10).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getUniqueness(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.title = data.title.replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nimport ssl\n \n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nnltk.download('wordnet') \nfrom nltk.stem.wordnet import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = pd.Series(' '.join(data['title']).lower().split()).value_counts()[:20]\nfreq\nstop_words = set(stopwords.words(\"english\"))\nstop_words = stop_words.union(freq.index.tolist())\nextra_words = ['amp']\nstop_words = stop_words.union(extra_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = []\n\ndef editText(textColumn):\n    #Remove punctuations\n   \n    text = re.sub('[^a-zA-Z]', ' ', textColumn)\n    \n    #Convert to lowercase\n    text = text.lower()\n    \n    #remove tags\n    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n    \n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    text = text.replace(\"\\n\",\"\")\n    ##Convert to list from string\n    text = text.split()\n    \n    ##Stemming\n    ps=PorterStemmer()    #Lemmatisation\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if not word in  \n            stop_words] \n    text = \" \".join(text)\n    corpus.append(text)\n    string = \"\".join(word for word in text)\n    return string\n\ndata[\"keywords\"] = data.apply(lambda row: editText(row[\"title\"]),axis=1 )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport re\ncv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=100000, ngram_range=(1,3))\nX=cv.fit_transform(corpus)\n\nwith open(\"countVector.pkl\", 'wb') as fout:\n    pickle.dump(X, fout)\n#list(cv.vocabulary_.keys())[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keywordData = data\nstockseries =histdata[\"stock\"].value_counts().index.tolist()\n#print(stockseries)\ndata = data[data[\"stock\"].isin(stockseries)]\ngetUniqueness(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef getDataForDay(stock,timestamp,days):\n    #t = time.process_time_ns() \n    timestamp = timestamp + (days* 86400)\n    lower = timestamp - 43200\n    higher = timestamp + 43200\n    match = histdata.loc[(histdata['stock']==stock) & (histdata['timestamps']>lower)&(histdata['timestamps']<higher) ]\n    #d = time.process_time_ns() - t\n    #print(f'getDataforday time [{stock}]: {d}')\n    return match\n\ndef getDelta(row):\n    global matches\n    \n    current = getDataForDay(row[\"stock\"],row[\"unix\"],0)\n    \n    \n    if not current.empty:\n        \n        future = getDataForDay(row[\"stock\"],row[\"unix\"],2)\n        if not future.empty:\n           \n            diff =  future[\"close\"].iloc[0]-current[\"close\"].iloc[0]\n            if diff >=0:\n                return 1\n            else:\n                return 0\n    return None\n\ndata = data.sample(120000)\ndata.set_index(\"unix\")\ndata.dropna()\nhistdata.set_index(\"timestamps\")\nt = time.process_time() \ndata[\"increased\"]=data.apply(lambda row: getDelta(row),axis=1 )\nd = time.process_time() - t\nprint(f'lambda time : {(d)/60/60}')\ndata.sample(5).head(5)\ndata['increased'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_words(corpus, n=None):\n    bag_of_words = cv.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = []\n    for word, idx in cv.vocabulary_.items():\n        if  len(word.split()) ==1:\n            words_freq.append((word, sum_words[0, idx]))\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                       reverse=True)\n    return words_freq[:n]#Convert most freq words to dataframe for plotting bar plot\ntop_words = get_top_n_words(corpus, n=20)\ntop_df = pd.DataFrame(top_words)\ntop_df.columns=[\"Word\", \"Freq\"]#Barplot of most freq words\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\ng = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\ng.set_xticklabels(g.get_xticklabels(), rotation=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n2_words(corpus, n=None):\n    \n    bag_of_words = cv.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq =[]\n    for word, idx in cv.vocabulary_.items():\n        if  len(word.split()) ==2:\n            words_freq.append((word, sum_words[0, idx]))\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\n\ntop2_words = get_top_n2_words(corpus, n=20)\ntop2_df = pd.DataFrame(top2_words)\ntop2_df.columns=[\"Bi-gram\", \"Freq\"]\nprint(top2_df)#Barplot of most freq Bi-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nh=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\nh.set_xticklabels(h.get_xticklabels(), rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n3_words(corpus, n=None):\n    \n    bag_of_words = cv.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = []\n    for word, idx in cv.vocabulary_.items():\n        if  len(word.split()) ==3:\n            words_freq.append((word, sum_words[0, idx]))\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\n\ntop3_words = get_top_n3_words(corpus, n=20)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"Tri-gram\", \"Freq\"]\nprint(top3_df)#Barplot of most freq Tri-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nj=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\nj.set_xticklabels(j.get_xticklabels(), rotation=45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding historical stock data"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data[\"delta1day\"] = np.random.randint(0,2, size=len(data))\n#data[\"delta7day\"] = np.random.randint(0,2, size=len(data))\n#data[\"delta30day\"] = np.random.randint(0,2, size=len(data))\nprint(len(np.where(data.applymap(lambda x: x == ''))))\n#data['delta1day'].replace('', np.nan, inplace=True)\n#data.dropna(subset=['delta1day'], inplace=True)\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = data[data[\"time\"].isna()]\ndf1.head()\nlen(df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.to_csv(\"processed_news.csv\",index = False)\nkeywordData.to_csv(\"keyword_data.csv\",index = False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}