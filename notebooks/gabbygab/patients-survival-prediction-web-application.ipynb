{"cells":[{"metadata":{},"cell_type":"markdown","source":"# [Can you feel my heartbeat?](http://)\n\n\n> The web app is available at :https://heartfailure-predictor.herokuapp.com/\n\n> Source code: https://github.com/gabbygab1233/Heart-Failure-Predictor-Application/\n![](https://i.imgur.com/Yrn231v.png)\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import pandas as pd\nimport pandas_profiling as pp\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport os\nimport plotly.graph_objects as go\nimport plotly.io as pio\nimport pickle\nfrom sklearn.utils import resample\n# Metrics\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, auc, roc_curve\n\n# Validation\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\n# Tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# Feature Extraction\nfrom sklearn.feature_selection import RFE\n\n# Preprocessing\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer, Binarizer, LabelEncoder\n\n# Models\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Ensembles\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\npio.templates.default = \"plotly_white\"\n\n\ndf = pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Analyze Data\ndef explore_data(df):\n    print(\"Number of Instances and Attributes:\", df.shape)\n    print('\\n')\n    print('Dataset columns:',df.columns)\n    print('\\n')\n    print('Data types of each columns: ', df.info())\n    \n# Checking for duplicates\ndef checking_removing_duplicates(df):\n    count_dups = df.duplicated().sum()\n    print(\"Number of Duplicates: \", count_dups)\n    if count_dups >= 1:\n        df.drop_duplicates(inplace=True)\n        print('Duplicate values removed!')\n    else:\n        print('No Duplicate values')\n    \n\n# Split training and validation set\ndef read_in_and_split_data(data, target):\n    X = data.drop(target, axis=1)\n    y = data[target]\n    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)\n    return X_train, X_test, y_train, y_test\n    \n# Spot-Check Algorithms\ndef GetModel():\n    Models = []\n    Models.append(('LR'   , LogisticRegression()))\n    Models.append(('LDA'  , LinearDiscriminantAnalysis()))\n    Models.append(('KNN'  , KNeighborsClassifier()))\n    Models.append(('CART' , DecisionTreeClassifier()))\n    Models.append(('NB'   , GaussianNB()))\n    Models.append(('SVM'  , SVC(probability=True)))\n    return Models\n\ndef ensemblemodels():\n    ensembles = []\n    ensembles.append(('AB'   , AdaBoostClassifier()))\n    ensembles.append(('GBM'  , GradientBoostingClassifier()))\n    ensembles.append(('RF'   , RandomForestClassifier()))\n    ensembles.append(( 'Bagging' , BaggingClassifier()))\n    ensembles.append(('ET', ExtraTreesClassifier()))\n    return ensembles\n\n# Spot-Check Normalized Models\ndef NormalizedModel(nameOfScaler):\n    \n    if nameOfScaler == 'standard':\n        scaler = StandardScaler()\n    elif nameOfScaler =='minmax':\n        scaler = MinMaxScaler()\n    elif nameOfScaler == 'normalizer':\n        scaler = Normalizer()\n    elif nameOfScaler == 'binarizer':\n        scaler = Binarizer()\n\n    pipelines = []\n    pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression())])))\n    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier())])))\n    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n    pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC())])))\n    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))\n    pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))\n    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier())])  ))\n    pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier())])  ))\n\n    return pipelines\n\n\n# Train model\ndef fit_model(X_train, y_train,models):\n    # Test options and evaluation metric\n    num_folds = 10\n    scoring = 'accuracy'\n\n    results = []\n    names = []\n    for name, model in models:\n        kfold = KFold(n_splits=num_folds, random_state=0)\n        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n    return names, results\n\n# Save trained model\ndef save_model(model,filename):\n    pickle.dump(model, open(filename, 'wb'))\n\n# Performance Measure\ndef classification_metrics(model, conf_matrix):\n    print(f\"Training Accuracy Score: {model.score(X_train, y_train) * 100:.1f}%\")\n    print(f\"Validation Accuracy Score: {model.score(X_test, y_test) * 100:.1f}%\")\n    fig,ax = plt.subplots(figsize=(8,6))\n    sns.heatmap(pd.DataFrame(conf_matrix), annot = True, cmap = 'YlGnBu',fmt = 'g')\n    ax.xaxis.set_label_position('top')\n    plt.tight_layout()\n    plt.title('Confusion matrix for Logisitic Regression Model', fontsize=20, y=1.1)\n    plt.ylabel('Actual label', fontsize=15)\n    plt.xlabel('Predicted label', fontsize=15)\n    plt.show()\n    print(classification_report(y_test, y_pred))\n    \n# ROC_AUC\ndef roc_auc(y_test, y_pred):\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n    plt.figure(figsize=(8,6))\n    print(f\"roc_auc score: {auc(fpr, tpr)*100:.1f}%\")\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate',fontsize=12)\n    plt.ylabel('True Positive Rate', fontsize=12)\n    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=20)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Data Analysis](http://)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"explore_data(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/niqOo77.png)"},{"metadata":{},"cell_type":"markdown","source":"# [ Checking for Nan Values and duplicates](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checking_removing_duplicates(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Checking and removing outliers](http://)\n\n* To check outliers of each columns use boxplot\n\n### These 5 columns have outliers\n\n* creatinine_phosphokinase\n* ejection_fraction\n* platelets\n* serum_creatinine\n* serum_sodium\n\n\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"checking_removing_duplicates(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_out = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\ndf_out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Imbalanced Classes](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(x=df['DEATH_EVENT'], palette='Set1');\nplt.title('Class Distribution',fontweight=\"bold\",alpha=0.8);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Oversample minority class](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"live = df_out[df_out.DEATH_EVENT == 0]\ndie = df_out[df_out.DEATH_EVENT==1]\n\ndie_upsampled = resample(die,\n                        replace=True,\n                        n_samples=len(live),\n                        random_state=0)\n\nupsampled = pd.concat([live, die_upsampled])\nupsampled.DEATH_EVENT.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Train model](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'DEATH_EVENT'\nX_train, X_test, y_train, y_test = read_in_and_split_data(upsampled, target)\n\nmodels = GetModel()\nnames,results = fit_model(X_train, y_train,models)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Improve accuracy using ensemble models](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemodel = ensemblemodels()\nnames,results = fit_model(X_train, y_train,ensemodel)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Experiment Different preprocessing techniques to improve accuracy](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"ScaledModel = NormalizedModel('standard')\nname,results = fit_model(X_train, y_train, ScaledModel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ScaledModel = NormalizedModel('normalizer')\nname,results = fit_model(X_train, y_train, ScaledModel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ScaledModel = NormalizedModel('minmax')\nname,results = fit_model(X_train, y_train, ScaledModel)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Fine tuning gradient boosting classifier model](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GradientBoostingClassifier()\nn_estimators = [10, 100, 1000]\nlearning_rate = [0.001, 0.01, 0.1]\nsubsample = [0.5, 0.7, 1.0]\nmax_depth = [3, 7, 9]\n#define grid search\n\ngrid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\nkfold = KFold(n_splits=10, random_state=0)\ncv_results = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=10, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = make_pipeline(MinMaxScaler(),  GradientBoostingClassifier(learning_rate=0.1, max_depth=9, n_estimators=1000, subsample=0.7))\nmodel = pipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\nconf_matrix = confusion_matrix(y_test,y_pred)\nclassification_metrics(pipeline, conf_matrix)\nroc_auc(y_test, y_pred)\n\nsave_model(model, 'model.pkl')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}