{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP Project Team 4"},{"metadata":{"_uuid":"77282ca6f97a7740db9f87619452f0a87c03f595","_cell_guid":"a11e8844-b449-4bef-aed7-46687dac4452","trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_colwidth', 500)\nfrom sklearn.model_selection import train_test_split\nfrom ktext.preprocess import processor","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52eee747b23516cb801b5922cd9cc81ebc3731f4","_cell_guid":"617564a3-b52b-4fff-a067-67736855eaa9"},"cell_type":"markdown","source":"# Read Data And Preview\n## To spilt into train and test sets"},{"metadata":{"_uuid":"99be74c506654cbd98fcef466e8c458b1f1026d3","_cell_guid":"6274092b-29bd-4c75-9dea-7206f4fe1fed","trusted":true},"cell_type":"code","source":"traindf, testdf = train_test_split(pd.read_csv('../input/github_issues.csv').sample(n=40000), \n                                   test_size=.10)\ntrain_body_raw = traindf.body.tolist()\ntrain_title_raw = traindf.issue_title.tolist()\ntraindf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"404a096f706e71370c8a610635b7c0a5f93dc138","_cell_guid":"0c57efcc-1db4-45c0-93ef-026877e8b611"},"cell_type":"markdown","source":"## Isssue Body and Title are stored in seperate lists using tolist.  The following code shows us the first issue title entry in the list:"},{"metadata":{"_uuid":"5f6b13cd6759e37ce6a83f0f943a8c93cf906a64","_cell_guid":"4959d954-2952-4033-85d9-5daf389c89ed","trusted":true},"cell_type":"code","source":"# Preview what is in this list\ntrain_title_raw[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccd61e51938f17e9681859976f71a12b0fca0e50","_cell_guid":"f8e524d0-ed7b-46bb-ba38-8b33f5cedb75"},"cell_type":"markdown","source":"## Use `ktext` to pre-process data"},{"metadata":{"_uuid":"d773fec2ced563f35c42b54644e2b01a84baf58c","_cell_guid":"1967292a-1611-46bf-a171-89bf80b94e03","trusted":true},"cell_type":"code","source":"num_encoder_tokens = 10000\nbody_pp = processor(keep_n=num_encoder_tokens, padding_maxlen=50)\ntrain_body_vecs = body_pp.fit_transform(train_body_raw)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42cb7e0db7a9501e5cae6055db7c7ec01d8136b1","_cell_guid":"2c75a0fc-b262-45e6-b3a9-d65182b551f4"},"cell_type":"markdown","source":"## An example of processed issue bodies"},{"metadata":{"_uuid":"49a378a782f6ee4aabbfe5518f4c942d006c1c15","_cell_guid":"9e56021c-fed6-42b4-b768-afbe5aab5f27","trusted":true},"cell_type":"code","source":"print('\\noriginal string:\\n', train_body_raw[0], '\\n')\nprint('after pre-processing:\\n', train_body_vecs[0], '\\n')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42fbc4f0c32651f0baefd87d2e7050526d4bb4b3","_cell_guid":"0513217d-3ce1-412a-8f76-afe0a98604d0","trusted":true},"cell_type":"code","source":"# Instantiate a text processor for the titles, with some different parameters\n# append_indicators = True appends the tokens '_start_' and '_end_' to each document\n# padding = 'post' means that zero padding is appended to the end of the of the document (default is 'pre')\n\nnum_decoder_tokens=9000\ntitle_pp = processor(append_indicators=True, keep_n=num_decoder_tokens, \n                     padding_maxlen=12, padding ='post')\n\n# process the title data\ntrain_title_vecs = title_pp.fit_transform(train_title_raw)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00414a474ad8c0d3e283c704d30e653b5549e5da","_cell_guid":"4ec20e49-d1e6-40f6-acfa-00e5780dd8ed","trusted":true},"cell_type":"code","source":"max(title_pp.id2token.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create the encoder decoder model"},{"metadata":{"_uuid":"0105ab4ca68e285d66507862fe4a6274b59b0591","_cell_guid":"9c4d8488-ce1a-48a0-a595-3d78dfc07a85","trusted":true},"cell_type":"code","source":"def load_encoder_inputs(vectorized_body):\n    encoder_input_data = vectorized_body\n    doc_length = encoder_input_data.shape[1]\n    print(f'Shape of encoder input: {encoder_input_data.shape}')\n    return encoder_input_data, doc_length\n\n\ndef load_decoder_inputs(vectorized_title):\n    # For Decoder Input, you don't need the last word as that is only for predictionwhen we are training using Teacher Forcing.\n    decoder_input_data = vectorized_title[:, :-1]\n\n    # Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing)\n    decoder_target_data = vectorized_title[:, 1:]\n\n    print(f'Shape of decoder input: {decoder_input_data.shape}')\n    print(f'Shape of decoder target: {decoder_target_data.shape}')\n    return decoder_input_data, decoder_target_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36379ebefdbccefe0e32d6a4e08e66209016ed92","_cell_guid":"a254d40a-0b1b-475b-89d1-85d955e6806f","trusted":true},"cell_type":"code","source":"import numpy as np\nencoder_input_data, doc_length = load_encoder_inputs(train_body_vecs)\ndecoder_input_data, decoder_target_data = load_decoder_inputs(train_title_vecs)\nnum_encoder_tokens = max(body_pp.id2token.keys()) + 1\nnum_decoder_tokens = max(title_pp.id2token.keys()) + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a6560ad29713d4f396763c84205ab0ba77195c3","_cell_guid":"aad7fdea-b331-4f28-b472-62127b838324","trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\nfrom keras import optimizers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e01a2f2122c42f232c60391452c11c26d00f20b3","_cell_guid":"e3c2fd45-6325-484c-87e3-c97146911a62","trusted":true},"cell_type":"code","source":"#setting latent dimensions arbitarily for embedding and hidden units\nlatent_dim = 80\n\n##### Define Model Architecture ######\n\n########################\n#### Encoder Model ####\nencoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n\n# Word embeding for encoder (ex: Issue Body)\nx = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\nx = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n\n# We do not need the `encoder_output` just the hidden state.\n_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n\n# Encapsulate the encoder as a separate entity so we can just encode without decoding if we want to.\nencoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n\nseq2seq_encoder_out = encoder_model(encoder_inputs)\n\n########################\n#### Decoder Model ####\ndecoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n\n# Word Embedding For Decoder (ex: Issue Titles)\ndec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\ndec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n\n# Set up the decoder, using `decoder_state_input` as initial state.\ndecoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\ndecoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\nx = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n\n# Dense layer for prediction\ndecoder_dense = Dense(num_decoder_tokens+2, activation='softmax', name='Final-Output-Dense')\n#softmax is a mathematical exponential function to calculate the probability distribution\ndecoder_outputs = decoder_dense(x)\n\n########################\n#### Seq2Seq Model ####\n\nseq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\nseq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy') \n#sparse_categorical_crossentropy is used to calculate probabilistic loss between label and predictions given word embeddimgs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model"},{"metadata":{"_uuid":"6ede1ab46b1a0dded0b98baf03a8a6acf0476378","_cell_guid":"c39cee27-88b4-42db-8c95-a679ae0bb024","trusted":true},"cell_type":"code","source":"from keras.callbacks import CSVLogger, ModelCheckpoint\n\nscript_name_base = 'tutorial_seq2seq'\nmodel_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n                                   save_best_only=True)\n\nbatch_size = 100\nepochs = 4\nhistory = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_split=0.10, callbacks=[model_checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e6acb8c0f8266334a4c1f00256b887787e3fa57","collapsed":true,"_cell_guid":"df1e6ee6-e4da-49fc-bd44-c1bdb1031499"},"cell_type":"markdown","source":"# Inference model"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install annoy\nfrom annoy import AnnoyIndex\nfrom tqdm import tqdm\nimport logging\nimport nltk\nfrom nltk.translate.bleu_score import corpus_bleu","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1e9dcd5b249f390fe6aab470e55986fcbcf18db","_cell_guid":"287d755e-d8bc-47a2-8b53-d0674750a0ab","trusted":true},"cell_type":"code","source":"def extract_decoder_model(model):\n    \"\"\"\n    Here we extract the decoder from the original model.\n    Inputs: keras model object\n    Outputs: A Keras model object with the following inputs and outputs:\n    Inputs of Keras Model That Is Returned:\n    1: the embedding index for the last predicted word or the <Start> indicator\n    2: the last hidden state\n    Outputs of Keras Model That Is Returned:\n    1.  Prediction (class probabilities) for the next word\n    2.  The hidden state of the decoder, to be fed back into the decoder at the next time step\n    \n    \"\"\"\n    # the latent dimension is the same so we copy it from the decoder output\n    latent_dim = model.get_layer('Decoder-Word-Embedding').output_shape[-1]\n\n    # Reconstruct the input into the decoder\n    decoder_inputs = model.get_layer('Decoder-Input').input\n    dec_emb = model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n    dec_bn = model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n\n    # Creating a layer for the feedback loop from predictions back into the GRU\n    gru_inference_state_input = Input(shape=(latent_dim,), name='hidden_state_input')\n\n    # Crete a layer to reuse the weights\n    # There are two outputs, 1- is the embedding layer output for the teacher forcing\n    #                        2- is the hidden state\n    gru_out, gru_state_out = model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n\n    # Reconstruct dense layers\n    dec_bn2 = model.get_layer('Decoder-Batchnorm-2')(gru_out)\n    dense_out = model.get_layer('Final-Output-Dense')(dec_bn2)\n    decoder_model = Model([decoder_inputs, gru_inference_state_input],\n                          [dense_out, gru_state_out])\n    return decoder_model\n\ndef extract_encoder_model(model):\n    \"\"\"\n    Here we extract the encoder from the original Sequence to Sequence Model.\n    Input:keras model object with body of issue as input\n    Returns: keras model object which is encoding of the issue with the last hidden state\n    \"\"\"\n    encoder_model = model.get_layer('Encoder-Model')\n    return encoder_model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d3714fd03fd908c101868a99b820125641f325b","_cell_guid":"86caa810-c55c-46bb-9737-13dd5e175c9e","trusted":true},"cell_type":"code","source":"class Seq2Seq_Inference(object):\n    def __init__(self,\n                 encoder_preprocessor,\n                 decoder_preprocessor,\n                 seq2seq_model):\n\n        self.pp_body = encoder_preprocessor\n        self.pp_title = decoder_preprocessor\n        self.seq2seq_model = seq2seq_model\n        self.encoder_model = extract_encoder_model(seq2seq_model)\n        self.decoder_model = extract_decoder_model(seq2seq_model)\n        self.default_max_len_title = self.pp_title.padding_maxlen\n        self.nn = None\n        self.rec_df = None\n\n    def generate_issue_title(self,\n                             raw_input_text,\n                             max_len_title=None):\n        \"\"\"\n        To generate a title given the body of an issue usin the seq2seq model .\n        Inputs: The body of the issue text as an input string\n        max_len_title: The maximum length of the title the model will generate\n        \"\"\"\n        if max_len_title is None:\n            max_len_title = self.default_max_len_title\n        # get the encoder's features for the decoder\n        raw_tokenized = self.pp_body.transform([raw_input_text])\n        body_encoding = self.encoder_model.predict(raw_tokenized)\n        # we want to save the encoder's embedding before its updated by decoder to use as an embedding for other tasks.\n        original_body_encoding = body_encoding\n        state_value = np.array(self.pp_title.token2id['_start_']).reshape(1, 1)\n\n        decoded_sentence = []\n        stop_condition = False\n        while not stop_condition:\n            preds, st = self.decoder_model.predict([state_value, body_encoding])\n\n            # We are going to ignore indices 0 (padding) and indices 1 (unknown)\n            # Argmax will return the integer index corresponding to the prediction + 2 since we chopped off first two\n            pred_idx = np.argmax(preds[:, :, 2:]) + 2\n\n            # retrieve word from index prediction\n            pred_word_str = self.pp_title.id2token[pred_idx]\n\n            if pred_word_str == '_end_' or len(decoded_sentence) >= max_len_title:\n                stop_condition = True\n                break\n            decoded_sentence.append(pred_word_str)\n\n            # update the decoder for the next word\n            body_encoding = st\n            state_value = np.array(pred_idx).reshape(1, 1)\n\n        return original_body_encoding, ' '.join(decoded_sentence)\n\n\n    def print_example(self,\n                      i,\n                      body_text,\n                      title_text,\n                      url,\n                      threshold):\n        \"\"\"\n        Prints examples\n        \"\"\"\n        if i:\n            print('\\n\\n==============================================')\n            print(f'============== Example # {i} =================\\n')\n\n        if url:\n            print(url)\n\n        print(f\"Issue Body:\\n {body_text} \\n\")\n\n        if title_text:\n            print(f\"Original Title:\\n {title_text}\")\n\n        emb, gen_title = self.generate_issue_title(body_text)\n        print(f\"\\n****** Machine Generated Title (Prediction) ******:\\n {gen_title}\")\n        \n        if self.nn:\n            # return neighbors and distances\n            n, d = self.nn.get_nns_by_vector(emb.flatten(), n=4,\n                                             include_distances=True)\n            neighbors = n[1:]\n            dist = d[1:]\n\n            if min(dist) <= threshold:\n                cols = ['issue_url', 'issue_title', 'body']\n                dfcopy = self.rec_df.iloc[neighbors][cols].copy(deep=True)\n                dfcopy['dist'] = dist\n                similar_issues_df = dfcopy.query(f'dist <= {threshold}')\n\n                print(\"\\n** Similar Issues (using encoder embedding) **:\\n\")\n                display(similar_issues_df)\n\n\n    def demo_model_predictions(self,\n                               n,\n                               issue_df,\n                               threshold=1):\n        \"\"\"\n        Pick n random Issues and display predictions.\n        Input: n- Number of issues to display from issue_df\n               issue_df- pandas DataFrame that contains two columns: `body` and `issue_title`.\n               threshold- float distance threshold for recommendation of similar issues.\n        Output: Prints the original issue body and the model's prediction.\n        \"\"\"\n        # Extract body and title from DF\n        body_text = issue_df.body.tolist()\n        title_text = issue_df.issue_title.tolist()\n        url = issue_df.issue_url.tolist()\n\n        if (len(body_text)==1):\n            demo_list=[0]\n        else:\n            demo_list = np.random.randint(low=1, high=len(body_text), size=n)\n        for i in demo_list:\n            self.print_example(i,\n                               body_text=body_text[i],\n                               title_text=title_text[i],\n                               url=url[i],\n                               threshold=threshold)\n            \n    def prepare_recommender(self, vectorized_array, original_df):\n        \"\"\"\n        Use the annoy library to build recommender\n        Parameters\n        ----------\n        vectorized_array : List[List[int]]\n            This is the list of list of integers that represents your corpus\n            that is fed into the seq2seq model for training.\n        original_df : pandas.DataFrame\n            This is the original dataframe that has the columns\n            ['issue_url', 'issue_title', 'body']\n        Returns\n        -------\n        annoy.AnnoyIndex  object (see https://github.com/spotify/annoy)\n        \"\"\"\n        self.rec_df = original_df\n        emb = self.encoder_model.predict(x=vectorized_array,\n                                         batch_size=vectorized_array.shape[0]//200)\n\n        f = emb.shape[1]\n        self.nn = AnnoyIndex(f)\n        logging.warning('Adding embeddings')\n        for i in tqdm(range(len(emb))):\n            self.nn.add_item(i, emb[i])\n        logging.warning('Building trees for similarity lookup.')\n        self.nn.build(50)\n        return self.nn\n    \n    def evaluate_model(self, holdout_bodies, holdout_titles):\n        \"\"\"\n        Method for calculating BLEU Score.\n        Parameters\n        ----------\n        holdout_bodies : List[str]\n            These are the issue bodies that we want to summarize\n        holdout_titles : List[str]\n            This is the ground truth we are trying to predict --> issue titles\n        Returns\n        -------\n        bleu : float\n            The BLEU Score\n        \"\"\"\n        actual, predicted = list(), list()\n        assert len(holdout_bodies) == len(holdout_titles)\n        num_examples = len(holdout_bodies)\n\n        logging.warning('Generating predictions.')\n        # step over the whole set TODO: parallelize this\n        for i in tqdm(range(num_examples)):\n            _, yhat = self.generate_issue_title(holdout_bodies[i])\n\n            actual.append(self.pp_title.process_text([holdout_titles[i]])[0])\n            predicted.append(self.pp_title.process_text([yhat])[0])\n        # calculate BLEU score\n        logging.warning('Calculating BLEU.')\n        \n        #must be careful with nltk api for corpus_bleu!, \n        # expects List[List[List[str]]] for ground truth, using List[List[str]] will give you\n        # erroneous results.\n        bleu = corpus_bleu([[a] for a in actual], predicted)\n        return bleu*4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing results"},{"metadata":{"_uuid":"8e85ab7f92c0314d58550b5d87e1b2a93eb6aebe","_cell_guid":"cfc5832e-58ee-4fcc-b3a8-b8f4987e9088","trusted":true},"cell_type":"code","source":"seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n                                 decoder_preprocessor=title_pp,\n                                 seq2seq_model=seq2seq_Model)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_uuid":"faa84da6a3a560f93fe4d5f4e2965077c985e988","_cell_guid":"40dac3f5-b9b8-4b44-98ee-4c81ba92391d","trusted":true},"cell_type":"code","source":"seq2seq_inf.demo_model_predictions(n=5, issue_df=testdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testdf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Custom Input"},{"metadata":{"trusted":true},"cell_type":"code","source":"url='https://github.com/github/hub/issues/2634'\ntitle='React-native cant run on AVD'\nbody='Hi there I recently started working with react native when I started I completely follow the setup docs at https://reactnative.dev/docs/getting-started so after that I tried running the app on an android emulator everything is good I also installed the SDK and have android revision 29 also installed intel 86_64 system image now I got this error also have the environment variables set up even though getting this and im using vs code for devloping'\ndata = [[url, title, body]]\ncustomdf=pd.DataFrame(data, columns = ['issue_url','issue_title', 'body'])\ncustomdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq2seq_inf.demo_model_predictions(n=1, issue_df=customdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"url='https://github.com/github/hub/issues/2627'\ntitle='How to display the data from the api response in flutter if its not in array format'\nbody='The problem Im trying to solve:I have problem in displaying the response data in flutter and i am not able to display it without list format How I imagine hub could expose this functionality:'\ndata = [[url, title, body]]\ncustomdf=pd.DataFrame(data, columns = ['issue_url','issue_title', 'body'])\ncustomdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq2seq_inf.demo_model_predictions(n=1, issue_df=customdf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Similar titles prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read All 5M data points\nall_data_df = pd.read_csv('../input/github_issues.csv').sample(n=200)\n# Extract the bodies from this dataframe\nall_data_bodies = all_data_df['body'].tolist()\n\n# transform all of the data using the ktext processor\nall_data_vectorized = body_pp.transform_parallel(all_data_bodies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import dill as dpickle\n# save transformed data\nwith open('all_data_vectorized.dpkl', 'wb') as f:\n    dpickle.dump(all_data_vectorized, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq2seq_inf_rec = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n                                    decoder_preprocessor=title_pp,\n                                    seq2seq_model=seq2seq_Model)\nrecsys_annoyobj = seq2seq_inf_rec.prepare_recommender(all_data_vectorized, all_data_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq2seq_inf_rec.demo_model_predictions(n=1, issue_df=customdf, threshold=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BLEU Score Calculation"},{"metadata":{"trusted":true},"cell_type":"code","source":"title='Have an option to Choose from one or more github accounts or a login feature and a logout feature '\nbody='I use two GitHub accounts in my system when I do hub create the hub is creating in the repo in my work account. to switch between them I need to remove the hub file and then re-auth. is there a fix for it already?'\ntitle_list=[title]\nbody_list=[body]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq2seq_inf.evaluate_model(holdout_bodies=body_list, holdout_titles = title_list)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}