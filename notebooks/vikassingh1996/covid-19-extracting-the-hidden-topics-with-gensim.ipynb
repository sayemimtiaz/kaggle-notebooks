{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# spacy for lemmatization\nimport spacy\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\n\n# nltk\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <font face=\"verdana\" color=\"dodgerblue\">Do we extract the hidden topics from large volumes of text ?\n### <font face=\"verdana\" color=\"red\"> Dataset Description </font>\nIn response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 44,000 scholarly articles, including over 29,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.\n\n### Contents\n* Exploration of Text In Articles \n* Tokenize words and Clean-up text\n* Creating Bigram and Trigram Models\n* Remove Stopwords, Make Bigrams and Lemmatize\n* Create the Dictionary and Corpus needed for Topic Modeling\n* Building the Topic Model\n* View the topics in LDA model\n* Compute Model Perplexity and Coherence Score\n* Visualize the topics-keywords\n* Building LDA Mallet Model\n* How to find the optimal number of topics for LDA?\n* Finding the dominant topic in each sentence\n* Find the most representative document for each topic\n* Topic distribution across documents"},{"metadata":{},"cell_type":"markdown","source":"# <font face=\"verdana\" color=\"dodgerblue\"> Exploration of Text In Articles </font>"},{"metadata":{},"cell_type":"markdown","source":"I load the output files from **[xhlulu's kernel](https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv)**, which contains a useful transformation of the json files in dictionaries to csv readable format. Go check it to give some credit and upvote the kernel!"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"biorxiv = pd.read_csv(\"/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/clean_comm_use.csv\")\nbiorxiv = biorxiv.fillna(\"No Information\")\nbiorxiv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"stop_words = set(stopwords.words(\"english\"))\n\ndef clean_text(s):\n    words = str(s).lower()\n    words = re.sub('\\[.*?\\]', '', words)\n    words = re.sub('https?://\\S+|www\\.\\S+', '', words)\n    words = re.sub('<.*?>+', '', words)\n    words = re.sub('[%s]' % re.escape(string.punctuation), '', words)\n    words = re.sub('\\n', '', words)\n    words = re.sub('\\w*\\d\\w*', '', words)\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    words =  ' '.join(words)\n    return words\n\n#source: https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\ndef get_top_unigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(1, 1)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_threegrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_fourgrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(4, 4)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font face=\"verdana\" color=\"dodgerblue\">Ngrams Analysis</font>\nIn the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus [source](https://en.wikipedia.org/wiki/N-gram).\n<p>Ngrams can be a very useful tool when trying to figure out which words and phrases are used in English. They can help show when certain phrases entered into the vernacular, and when they fell out of favor. But they have their limitations.</p>\n\n### <font face=\"verdana\" color=\"red\">Most Common Words in Title</font>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\ntitle = biorxiv['title'].apply(lambda x : clean_text(x))\n\nplt.style.use('ggplot')\nfig, axes = plt.subplots(2, 2, figsize=(18, 20), dpi=100)\n           \ntop_unigrams=get_top_unigrams(title)[:20]\nx,y=map(list,zip(*top_unigrams))\nsns.barplot(x=y,y=x, ax=axes[0,0], color='dodgerblue')\n\n\ntop_bigrams=get_top_bigrams(title)[:20]\nx,y=map(list,zip(*top_bigrams))\nsns.barplot(x=y,y=x, ax=axes[0,1], color='orangered')\n\ntop_threegrams=get_top_threegrams(title)[:20]\nx,y=map(list,zip(*top_threegrams))\nsns.barplot(x=y,y=x, ax=axes[1, 0], color='limegreen')\n\ntop_fourgrams=get_top_fourgrams(title)[:20]\nx,y=map(list,zip(*top_fourgrams))\nsns.barplot(x=y,y=x, ax=axes[1, 1], color='red')\n\n\naxes[0, 0].set_ylabel(' ')\naxes[0, 1].set_ylabel(' ')\naxes[1, 0].set_ylabel(' ')\naxes[1, 1].set_ylabel(' ')\n\naxes[0, 0].yaxis.set_tick_params(labelsize=15)\naxes[0, 1].yaxis.set_tick_params(labelsize=15)\naxes[1, 0].yaxis.set_tick_params(labelsize=15)\naxes[1, 1].yaxis.set_tick_params(labelsize=15)\n\naxes[0, 0].set_title('Top 20 most common unigrams in title', fontsize=15)\naxes[0, 1].set_title('Top 20 most common bigrams in title', fontsize=15)\naxes[1, 0].set_title('Top 20 most common threegrams in title', fontsize=15)\naxes[1, 1].set_title('Top 20 most common fourgrams in title', fontsize=15)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font face=\"verdana\" color=\"red\">Most Common Words in Abstract</font>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"abstract = biorxiv['abstract'].apply(lambda x : clean_text(x))\n\nplt.style.use('ggplot')\nfig, axes = plt.subplots(2, 2, figsize=(18, 20), dpi=100)\nplt.tight_layout()\n\ntop_unigrams=get_top_unigrams(abstract)[:20]\nx,y=map(list,zip(*top_unigrams))\nsns.barplot(x=y,y=x, ax=axes[0,0], color='dodgerblue')\n\n\ntop_bigrams=get_top_bigrams(abstract)[:20]\nx,y=map(list,zip(*top_bigrams))\nsns.barplot(x=y,y=x, ax=axes[0,1], color='orangered')\n\ntop_threegrams=get_top_threegrams(abstract)[:20]\nx,y=map(list,zip(*top_threegrams))\nsns.barplot(x=y,y=x, ax=axes[1, 0], color='limegreen')\n\ntop_fourgrams=get_top_fourgrams(abstract)[:20]\nx,y=map(list,zip(*top_fourgrams))\nsns.barplot(x=y,y=x, ax=axes[1, 1], color='red')\n\n\naxes[0, 0].set_ylabel(' ')\naxes[0, 1].set_ylabel(' ')\naxes[1, 0].set_ylabel(' ')\naxes[1, 1].set_ylabel(' ')\n\naxes[0, 0].yaxis.set_tick_params(labelsize=15)\naxes[0, 1].yaxis.set_tick_params(labelsize=15)\naxes[1, 0].yaxis.set_tick_params(labelsize=15)\naxes[1, 1].yaxis.set_tick_params(labelsize=15)\n\naxes[0, 0].set_title('Top 20 most common unigrams in abstract', fontsize=15)\naxes[0, 1].set_title('Top 20 most common bigrams in abstract', fontsize=15)\naxes[1, 0].set_title('Top 20 most common threegrams in abstract', fontsize=15)\naxes[1, 1].set_title('Top 20 most common fourgrams in abstract', fontsize=15)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font face=\"verdana\" color=\"red\">Most Common Words in Text Body</font>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"text = biorxiv['text'].apply(lambda x : clean_text(x))\n\nplt.style.use('ggplot')\nfig, axes = plt.subplots(2, 2, figsize=(18, 20), dpi=100)\nplt.tight_layout()\n\ntop_unigrams=get_top_unigrams(text)[:20]\nx,y=map(list,zip(*top_unigrams))\nsns.barplot(x=y,y=x, ax=axes[0,0], color='dodgerblue')\n\n\ntop_bigrams=get_top_bigrams(text)[:20]\nx,y=map(list,zip(*top_bigrams))\nsns.barplot(x=y,y=x, ax=axes[0,1], color='orangered')\n\ntop_threegrams=get_top_threegrams(text)[:20]\nx,y=map(list,zip(*top_threegrams))\nsns.barplot(x=y,y=x, ax=axes[1, 0], color='limegreen')\n\ntop_fourgrams=get_top_fourgrams(text)[:20]\nx,y=map(list,zip(*top_fourgrams))\nsns.barplot(x=y,y=x, ax=axes[1, 1], color='red')\n\n\naxes[0, 0].set_ylabel(' ')\naxes[0, 1].set_ylabel(' ')\naxes[1, 0].set_ylabel(' ')\naxes[1, 1].set_ylabel(' ')\n\naxes[0, 0].yaxis.set_tick_params(labelsize=15)\naxes[0, 1].yaxis.set_tick_params(labelsize=15)\naxes[1, 0].yaxis.set_tick_params(labelsize=15)\naxes[1, 1].yaxis.set_tick_params(labelsize=15)\n\naxes[0, 0].set_title('Top 20 most common unigrams in text', fontsize=15)\naxes[0, 1].set_title('Top 20 most common bigrams in text', fontsize=15)\naxes[1, 0].set_title('Top 20 most common threegrams in text', fontsize=15)\naxes[1, 1].set_title('Top 20 most common fourgrams in text', fontsize=15)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font face=\"verdana\" color=\"red\">Distribution of Word</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nfig,(ax1,ax2, ax3)= plt.subplots(ncols=3, figsize=(18, 5), dpi=100)\n\n\nlength=title.str.split().map(lambda x: len(x))\nax1.hist(length,bins = 20, color='black')\nax1.set_title('Tittle')\n\nlength=abstract.str.split().map(lambda x: len(x))\nax2.hist(length, bins = 20,  color='black')\nax2.set_title('Abstract')\n\nlength=text.str.split().map(lambda x: len(x))\nax3.hist(length, bins = 20,  color='black')\nax3.set_title('Text')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <font face=\"verdana\" color=\"dodgerblue\">Topic Modeling with Gensim</font>\n*Topic Modeling is a technique to extract the hidden topics from large volumes of text. Latent Dirichlet Allocation(LDA) is a popular algorithm for topic modeling with excellent implementations in the Python’s Gensim package. The challenge, however, is **how to extract good quality of topics that are clear, segregated and meaningful.** This depends heavily on the quality of text preprocessing and the strategy of finding the optimal number of topics. This tutorial attempts to tackle both of these problems.*\n\n<p>I will be using the Latent Dirichlet Allocation (LDA) from Gensim package along with the Mallet’s implementation (via Gensim). Mallet has an efficient implementation of the LDA. It is known to run faster and gives better topics segregation.</p>\n<p>Let’s begin!</p>"},{"metadata":{},"cell_type":"markdown","source":"### <font face=\"verdana\" color=\"red\">Tokenize words and Clean-up text</font>\nLet’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether.\n\nGensim’s simple_preprocess() is great for this. Additionally I have set deacc=True to remove the punctuations.\n\n<div class=\"alert alert-info\" role=\"alert\">\nOPEN THE HIDDEN BOX TO SEE THE OUTPUT\n</div>"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Covert the raw text into list\ntext = biorxiv.text.values.tolist()\nprint(text[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# cleaning the text\n\ntext = [re.sub('\\S*@\\S*\\s?', '', word) for word in text]\ntext = [re.sub('\\s+', ' ', word) for word in text]\ntext = [re.sub(\"\\'\", \"\", word) for word in text]\n\nprint(text[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Tokenize words\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ntext_words = list(sent_to_words(text))\n\nprint(text_words[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font face=\"verdana\" color=\"red\">Creating Bigram and Trigram Models</font>\nBigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.\n\nGensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\"\"\"Build the bigram and trigram models\"\"\"\n#bigram = gensim.models.Phrases(text_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n#trigram = gensim.models.Phrases(bigram[text_words], threshold=100)  \n\n\"\"\"Faster way to get a sentence clubbed as a trigram/bigram\"\"\"\n#bigram_mod = gensim.models.phrases.Phraser(bigram)\n#trigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\n#print(trigram_mod[bigram_mod[text_words[0]]])\n\n\"\"\"Save an exported collocation model.\"\"\"\n#bigram_mod.save(\"/kaggle/working/my_bigram_model.pkl\") \n#trigram_mod.save(\"/kaggle/working/my_trigram_model.pkl\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"\"\"\"load an exported collocation model\"\"\"\nbigram_reloaded = gensim.models.phrases.Phraser.load(\"../input/bi-and-tri-model/my_bigram_model.pkl\")\ntrigram_reloaded = gensim.models.phrases.Phraser.load(\"../input/bi-and-tri-model/my_trigram_model.pkl\")\nprint(trigram_reloaded[bigram_reloaded[text_words[0]]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font face=\"verdana\" color=\"red\">Remove Stopwords, Make Bigrams and Lemmatize</font>\nThe bigrams model is ready. Let’s define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially.\n\n\n<div class=\"alert alert-info\" role=\"alert\">\nOPEN THE OUTPUT BOX TO SEE THE HIDDEN OUTPUT\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# NLTK Stop words\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['et', 'al'])\n\n# Define functions for stopwords, bigrams, trigrams and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_reloaded[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_reloaded[bigram_reloaded[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for word in texts:\n        doc = nlp(\" \".join(word)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Remove Stop Words\ntext_words_nostops = remove_stopwords(text_words)\n\n# Form Bigrams\ntext_words_bigrams = make_bigrams(text_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ntext_words_lemmatized = lemmatization(text_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(text_words_lemmatized[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font face=\"verdana\" color=\"red\">Create the Dictionary and Corpus needed for Topic Modeling</font>\n\n<p>The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them. </p>\n<p>Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n\nFor example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n\nThis is used as the input by the LDA model.\n\nIf you want to see what word a given id corresponds to, pass the id as a key to the dictionary.<p>\n"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(text_words_lemmatized)\n\n# Create Corpus\ntexts = text_words_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Human readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright, without digressing further let’s jump back on track with the next step: Building the topic model."},{"metadata":{},"cell_type":"markdown","source":"### <font face=\"verdana\" color=\"red\">Building the Topic Model</font>\nWe have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n\nApart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n\nchunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes.\n\nSource: https://radimrehurek.com/gensim/models/ldamodel.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Build LDA model\"\"\"\n#lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           #id2word=id2word,\n                                           #num_topics=20, \n                                           #random_state=100,\n                                           #update_every=1,\n                                           #chunksize=100,\n                                           #passes=10,\n                                           #alpha='auto',\n                                           #per_word_topics=True)\n\"\"\"save model\"\"\"\n#lda_model.save('/kaggle/working/lda_model.model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font face=\"verdana\" color=\"red\">View the topics in LDA model</font>\nThe above LDA model is built with 20 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n\nYou can see the keywords for each topic and the weightage(importance) of each keyword using lda_model.print_topics() as shown next."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# load trained model from file\nmodel_reloaded =  gensim.models.LdaModel.load('../input/bi-and-tri-model/lda_model.model')\n\n# Print the Keyword in the 10 topics\nprint(model_reloaded.print_topics())\ndoc_lda = model_reloaded[corpus]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see How to interpret this?\n\nTopic 0 is a represented as _'0.049*\"vaccine\" + 0.032*\"antibody\" + 0.022*\"response\" + 0.017*\"epitope\" + 0.016*\"specific\" + 0.016*\"use\" + 0.014*\"immune\" + 0.013*\"mouse\" + 0.012*\"vaccination\" + 0.012*\"vector\"'.\n\nIt means the top 10 keywords that contribute to this topic are: ‘vaccine’, ‘antibody’, ‘response’, 'epitope'.. and so on and the weight of ‘vaccine’ on topic 0 is 0.049.\n\nThe weights reflect how important a keyword is to that topic.\n\n** <font face=\"verdana\" color=\"green\">Looking at these keywords, can we guess what this topic could be? I may summarise it either are ‘vaccine’ or ‘Antibody’.</font>**\nLikewise, can you go through the remaining topic keywords and judge what the topic is?\n\n<div class=\"alert alert-info\" role=\"alert\">\nDOMAIN KNOWLEDGE WILL BE HELPFUL \n</div>\n\n"},{"metadata":{},"cell_type":"markdown","source":"### <font face=\"verdana\" color=\"red\">Compute Model Perplexity and Coherence Score</font>\nModel perplexity and [topic coherence](https://rare-technologies.com/what-is-topic-coherence/) provide a convenient measure to judge how good a given topic model is. Topic coherence score, in particular, has been more helpful.\n\nsource: https://radimrehurek.com/gensim/models/coherencemodel.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Compute Perplexity\"\"\"\n#print('\\nPerplexity: ', model_reloaded.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n\"\"\"Compute Coherence Score\"\"\"\n#coherence_model_lda = CoherenceModel(model=model_reloaded, texts=text_words_lemmatized, dictionary=id2word, coherence='c_v')\n#coherence_lda = coherence_model_lda.get_coherence()\n#print('\\nCoherence Score: ', coherence_lda)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I going to train the CoherenceModel again. It is time taking, so, here is the score: \n**<p><font face=\"verdana\" color=\"green\">Perplexity:  -8.669969213095087</font></p>**\n**<p><font face=\"verdana\" color=\"green\">Coherence Score:  0.48574600791139133</font></p>**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}