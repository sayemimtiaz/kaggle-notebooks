{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport random\n\ndata = pd.read_csv(\"/kaggle/input/boardgamegeek-reviews/bgg-13m-reviews.csv\") \n# subset_percent = 0.01 # for running algo on this percent of data (used to reduce execution time)\n# data = pd.read_csv(\"/kaggle/input/boardgamegeek-reviews/bgg-13m-reviews.csv\", skiprows=lambda i: i>0 and random.random() > subset_percent, header=0) \ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding count of missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping missing values and extra columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna(how='any', subset=['comment'], inplace=True)\ndata.drop(['user', 'ID', 'name'], axis=1, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\ndata_train, data_test = train_test_split(data, test_size=0.2, random_state=42)\n\n\nvectorizer = CountVectorizer()\n# Didn't used stopwords or stemming as the reviews are from other languages as well.\n# Accuracy after removing stopwords and using stemmer: ~0.30\n# Accuracy with not removing stopwords nor using stemmer: ~0.32\n\n\ntrain_features = vectorizer.fit_transform(data_train['comment'])\ntest_features = vectorizer.transform(data_test['comment'])\n\ntest_label = [round(r) for r in data_test['rating']]\ntrain_label = [round(r) for r in data_train['rating']]\n\n\n# Below code was used to manipulate number of classes.Got good accuracy when the \n# number of classes were only 2 (positive and negative). But since it was not the goal, it was removed\n\n# for i in range(len(test_label)):\n#     if test_label[i] > 5:\n#         test_label[i] = 1\n#     else:\n#         test_label[i] = 0\n\n# for i in range(len(train_label)):\n#     if train_label[i] > 5:\n#         train_label[i] = 1\n#     else:\n#         train_label[i] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding the right model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"algos = [\"MulitnomialNB\", \"KNN\", \"MLP\", \"SVM\"]\nalgo_scores = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Multinomial Naive Bayes\nNaive Bayes works best when working with text data. Hence, this is my first choice. There are different types of Naive Bayes classifiers, but Multinomial Naive Bayes is to be used when there are multiple classes. Our dataset has 11 classes (0 - 10) hence it is most appropriate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB(alpha=1)\nnb.fit(train_features, train_label)\n\nscore = nb.score(test_features, test_label)\n\nalgo_scores.append(score)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using KNN Algorithm\nKNN, K nearest neighbors, algorithm is a simple algorithms which classifies based on it's neighbors. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=5)\nneigh.fit(train_features, train_label)\nknnscore = neigh.score(test_features, test_label)\nalgo_scores.append(knnscore)\nprint(knnscore)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using MultiLayer Perceptron Classifier\nMultilayer Perceptron Classifier is a type of artificial neural network. ANNs are used to solve complex problems. Since our dataset is complex, it is worth giving a try.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(solver='adam', alpha=1e-4, random_state=1, max_iter=500) # Increased max_iter to 500 as 200 was not enough\nclf.fit(train_features, train_label)\nscore = clf.score(test_features, test_label)\n\nalgo_scores.append(score)\nprint(score)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using SVM\nSVM is one of my favorite algorithms. It works by dividing the classes with a hyperplane. It gives considerably good results in many use cases. Hence worth a try.`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nclf = svm.SVC()\nclf.fit(train_features, train_label)\nscore = clf.score(test_features, test_label)\n\nalgo_scores.append(score)\nprint(score)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(algos,algo_scores)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, MultinomialNB gives the best result, we proceed with it for our hyper parameters tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Below accuracy was calculated by relaxing the output class by 1.\n# For ex. if the predicted value was 4 but the actual value was 3, then too it was considered correct.\n# Accuracy almost doubles when calculated in this way\n\npredictions = nb.predict(test_features)\n\ndef custom_accuracy(preds, actual):\n    count = 0\n    n = len(preds)\n    for i in range(n):\n        if abs(preds[i] - actual[i] < 1):\n            count += 1\n    return count / n\n\n# Compute the error\nprint(\"accuracy s \" + str(custom_accuracy(predictions, test_label)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyper parameter tuning for MultinomialNB\n\nFinding out the appropriate alpha parameter (smoothing value)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nimport numpy as np\n\nalpha_range = list(np.arange(1,50,5))\nlen(alpha_range)\n\nalpha_scores=[]\n\nfor a in alpha_range:\n    clf = MultinomialNB(alpha=a)\n    scores = cross_val_score(clf, train_features, train_label, cv=5, scoring='accuracy')\n    alpha_scores.append(scores.mean())\n    print(a,scores.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nMSE = [1 - x for x in alpha_scores]\n\n\noptimal_alpha_bnb = alpha_range[MSE.index(min(MSE))]\n\n# plot misclassification error vs alpha\nplt.plot(alpha_range, MSE)\n\nplt.xlabel('hyperparameter alpha')\nplt.ylabel('Misclassification Error')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The error is least for alpha = 1.\n\nWe can conclude that for alpha = 1, it gives the best result for this particular dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Visuals and parameters of the model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\n\ncm_test = confusion_matrix(test_label, predictions)\n\nsns.heatmap(cm_test,annot=True,fmt='d')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sample count in each class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nb.class_count_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting rating for a sample review","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_review = \"with in this is it\"\nprint(nb.predict(vectorizer.transform([test_review])))\nprint(nb.predict_proba(vectorizer.transform([test_review])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training with 100% of data to get better results when deployed\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nvectorizer = CountVectorizer()\n\ntrain_features = vectorizer.fit_transform(data['comment'])\n\ntrain_label = [round(r) for r in data['rating']]\n\nnb = MultinomialNB(alpha=1)\nnb.fit(train_features, train_label)\n\ntest_review = \"This is a sample review\"\nprint(nb.predict(vectorizer.transform([test_review])))\nprint(nb.predict_proba(vectorizer.transform([test_review])))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exporting model and vectorizer to deploy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nwith open('/kaggle/working/model.pk', 'wb') as file:\n    pickle.dump(nb, file)\nwith open('/kaggle/working/vect.pk', 'wb') as file:\n    pickle.dump(vectorizer, file)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Challenges faced:\n\n* Due to large dataset, execution time was too much. Solved it by writing a piece of code to read only a part of data after shuffling it.\n* The dataset contained many missing values i.e. 79.9%. All of them were removed which affected accuracy\n* The dataset contained reviews in different languages. Hence didn't removed stopwords nor used stemming. Improved accuracy by 2%\n* Accuracy was too low because of many classes to predict from. For ex. \"Good game\" review can have a rating anywhere from 7 to 10. Hence, \"custom\" accuracy was calculated which relaxed the rule a little (+- 1). Accuracy increased by ~30%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## References:\n\n* Official scikit learn documentation https://scikit-learn.org/ (Examples were referred of different classifiers)\n* https://github.com/krsatyam1996/IMDB-sentiment-analysis-using-naive-bayes/blob/master/movie_review.ipynb (Used many different classifiers other than NB and many modifications with different parameters)\n* https://blog.cambridgespark.com/deploying-a-machine-learning-model-to-the-web-725688b851c7 (Modified UI)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Important links\n\nThe model is deployed at https://saeedjassani.pythonanywhere.com/\n\nBlog post can be found at https://saeedjassani.uta.cloud/rating-predictor.html\n\nKaggle notebook link at https://www.kaggle.com/saeedjassani/ratings-predictor\n\nDemo video at https://youtu.be/a-uyXRdpHF4\n\nPlease feel free to play with it. Any suggestions are welcomed at saeedjassani@gmail.com","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}