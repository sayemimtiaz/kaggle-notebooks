{"cells":[{"metadata":{"_uuid":"6a3be378b1e0dbee1ce47c03d097c2342a931d88"},"cell_type":"markdown","source":"**Content**\n\n* **Linear Regression**\n* **KNN Algorithm**\n     * *Grid Search Cross Validation*\n* **Data Visualization**\n* **Confusion Matrix**\n\n> *Finding linear equations between two properties using **Linear Regression***\n> \n> *With the **KNN algorithm**, it is determined whether the non-classifiable data belong to the predetermined 2 classes.*\n> \n> **Grid Search Cross Validation**  *finds the best* **K** *value*\n> \n> **Data visualization** *enables data visualization*\n> \n> **Confusion Matrix** *It specifies which values are incorrectly estimated and which values are estimated correctly.*"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression #Linear Regression \nfrom sklearn.neighbors import KNeighborsClassifier #KNN Algorithm\nfrom sklearn.model_selection import train_test_split #Trian and Test Split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode,iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"dataframe1 = pd.read_csv('../input/column_2C_weka.csv') #read to file","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efd790e1cd9a61a2aef8efa54304690f0bb52270"},"cell_type":"markdown","source":"**Please see below**\n\n> *Let's look at the names of the column of our data*\n\n*It is important to know the names of the types that can cause problems with column names*"},{"metadata":{"trusted":true,"_uuid":"80d9cc617bc99739964eddc15518932a7d29d7e6"},"cell_type":"code","source":"print(dataframe1.columns) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a94c46bea3a8ccc0cf394d6ac41096d85d6d47b2"},"cell_type":"markdown","source":"**Please see below**\n\n**Let's read our file's information**\n\n> **sacral_slope andpelvic_radius values are very important for this kernel. Let's see if I have NaN(Missing Value) values in my set? We see this in the information screen below. However, it will be more useful to see visually.**"},{"metadata":{"trusted":true,"_uuid":"addd57246ff381b164f31f87a7e240b3a9caf554"},"cell_type":"code","source":"print(dataframe1.info())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"226d29f1d466f7a633328d6fb8988caba94de4cc"},"cell_type":"markdown","source":"**Please see below**\n\n> *Black: Available Data*\n> \n> *White: Missing Data*\n> \n> *sacral_slope: Column name1*\n> \n> *pelvic_radius: Column name2*\n\n**NOTE: No white band because there is no NaN value**"},{"metadata":{"trusted":true,"_uuid":"c2ab36798d951c7d5e6b0ba248dd87265cac7ee7"},"cell_type":"code","source":"newBioDataFrame = dataframe1.loc[:,[\"sacral_slope\",\"pelvic_radius\"]]\nimport missingno as msno\nmsno.matrix(newBioDataFrame)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"273e8ed702c808e6393779a334f7af8a671796c9"},"cell_type":"markdown","source":"**Please see below**\n\n> **Same as above but This is shown as a bar plot**"},{"metadata":{"trusted":true,"_uuid":"3f1f06a815295418ec072109028d99b37ffb8850"},"cell_type":"code","source":"msno.bar(newBioDataFrame)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76ae08f0483da390a198d629769f6f0c2d7ce1a0"},"cell_type":"markdown","source":"**Please see below**\n\n> **The first 10 data in the data set**"},{"metadata":{"trusted":true,"_uuid":"a08fe0e40c16b04a080919b11cbed463d744f36a"},"cell_type":"code","source":"dataframe1.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3824c1b1273298d19ae0c6c1c3e0d4d0c0efe7b6"},"cell_type":"markdown","source":"**Please see below**\n\n> **The last 10 data in the data set**"},{"metadata":{"trusted":true,"_uuid":"ed2d1d5be3bfcb5e6511b28ed86bd39c4b54fd0a"},"cell_type":"code","source":"dataframe1.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0931c2b0a72d0c75d9b001f21a646b6acec9fa0"},"cell_type":"markdown","source":"**Please see below**\n\n*Numerical Analysis*\n\n> The maximum values in the Pelvik_radius column are too high"},{"metadata":{"trusted":true,"_uuid":"65db38266b05c0a0ed7494fb4c37d83db8805334"},"cell_type":"code","source":"dataframe1.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e804c4d0aee7c1809cfc2d6642f4206d55c78435"},"cell_type":"code","source":"trace1 = go.Box(\n    y = dataframe1.sacral_slope,\n    name = \"sacral_slope\",\n    marker = dict(color = \"red\")\n)\ntrace2 = go.Box(\n    y = dataframe1.pelvic_radius,\n    name = \"pelvic_radius\",\n    marker = dict(color = \"blue\")\n)\nconcatTrace = [trace1,trace2]\niplot(concatTrace)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1100d6462422d8dceaf565c3988d073fd7f434a"},"cell_type":"markdown","source":"**Please see below**\n\n> **if you apply Linear Regression to the following graph**"},{"metadata":{"_uuid":"49d3d08f15ffec83522a462d122f922320ce21da"},"cell_type":"markdown","source":"**Please see below**\n\n> **We used a linear regression method to generate a straight line**"},{"metadata":{"trusted":true,"_uuid":"f70f86207f7a51577d303772fbe139a6b80a3e1f"},"cell_type":"code","source":"dataFilter = dataframe1[dataframe1['class'] == 'Abnormal']\nlinear_regression = LinearRegression()\nx = dataFilter.pelvic_incidence.values.reshape(-1,1)\ny = dataFilter.sacral_slope.values.reshape(-1,1)\nlinear_regression.fit(x,y)\n\ny_head = linear_regression.predict(x)\n\nplt.figure(figsize=[15,15])\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\n\nplt.plot(x,y_head,color=\"green\",linewidth=2)\nplt.show()\n\nfrom sklearn.metrics import r2_score\nprint('R^2 score: ',r2_score(y,y_head))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ce3532ef60de2a134010c9a2145e72dd064d321"},"cell_type":"markdown","source":"**Notes on Linear Regression**\n\n> Linear Regression,can be thought of as a simple reduction(Regression) of the dataset on complex computations and measurements of many. It is here, however, to be able to derive such an equilibrium equation from the values of the data in the data set.\n\n*R squared tells you how good the \"fit\" of your model is or better said how well the line you might draw through your points fits. Its a regression thing*\n\nThe score of R ^ 2 was found to be **0.6458410481075871**  **This value tells us that we need to improve our model**\n"},{"metadata":{"_uuid":"f00b2256ea1d858a1eef64e6337ff8f2861491a2"},"cell_type":"markdown","source":"**Notice That**\n\n*The features used in the CNN algorithm with Linear Regression are different.*"},{"metadata":{"_uuid":"c8ab7df4a5205f1d95e176f07888e2f29a5d32f6"},"cell_type":"markdown","source":"**Please see below**\n\n> *I collapsed the dataset according to the abnormal and normal key words*\n\n*I then plotted a graph based on the headings **pelvic_radius** and **lumbar_lordosis_angle**.*"},{"metadata":{"trusted":true,"_uuid":"01fe1494a2241eefce55fc11466e7cdc49396b63"},"cell_type":"code","source":"Abnormal = dataframe1[dataframe1[\"class\"] == \"Abnormal\"]\nNormal = dataframe1[dataframe1[\"class\"] == \"Normal\"]\n\nplt.figure(figsize=(15,15))\nplt.scatter(Abnormal.pelvic_radius,Abnormal.lumbar_lordosis_angle,color=\"blue\",label=\"pelvic_radius\")\nplt.scatter(Normal.pelvic_radius,Normal.lumbar_lordosis_angle,color=\"lime\",label=\"lumbar_lordosis_angle\")\nplt.legend()\nplt.xlabel(\"pelvic_radius\")\nplt.ylabel(\"lumbar_lordosis_angle\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe6666ff468d79ad5308d494023c6e2389cae9a9"},"cell_type":"markdown","source":"**Please see below**\n\n> *Abnormal and Normal values are converted from characters to integer.* (**y** values must be in integer type)\n\n**What is normalization:**\n*When there is a lot of difference between the data, the data is handled in a single order.*\n> If we have such values in my dataset, we can get an inaccurate analysis. So it is important to normalize\n\n**What is train data:**\n*I am making sure that my model knows the data by separating the data in the dataset*\n\n> Next, I would like my model to estimate the test data using test data."},{"metadata":{"trusted":true,"_uuid":"092798089f43a532b70d9971b6775d8de828af7a"},"cell_type":"code","source":"dataframe1[\"class\"] = [1 if(each == \"Abnormal\") else 0 for each in dataframe1[\"class\"]]\ny = dataframe1[\"class\"].values\nx_data = dataframe1.drop([\"class\"],axis=1)\n\n#Normalization\nx = (x_data - np.min(x_data))/(np.max(x_data) - np.min(x_data))\n\n#Train and Test values\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"646de58e49ec9446e3aed5132d7dba3b11a87e62"},"cell_type":"markdown","source":"**Please see below**\n\n***How do I choose the best K value in the KNN algorithm?***\n*I do this with the following lines of code*\n\n> The best value is like **13** "},{"metadata":{"trusted":true,"_uuid":"d926c0b5fd8b2972b883caa5ebae94b337bba32f"},"cell_type":"code","source":"score_list = []\nfor eachs in range(1,15):\n    knnAlgorithm1 = KNeighborsClassifier(n_neighbors = eachs)\n    knnAlgorithm1.fit(x_train,y_train) #Modeli eÄŸitiyorum\n    score_list.append(knnAlgorithm1.score(x_test,y_test))\nplt.figure(figsize=(15,15))\nplt.plot(range(1,15),score_list)\nplt.xlabel(\"K values\")\nplt.ylabel(\"Accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e081a70d55ef5126213190bf193f7d294b0c0b1"},"cell_type":"markdown","source":"**Please see below**\n\n**Notes on Grid Search Cross Validation**\n> One of the important problems in the **KNN** algorithm is that **K** is chosen.\n> \n> *In the above code line*, we can choose the value of **K** by trial. Of course we have to determine the *range*\n> \n> **Grid Search Cross-Validation algorithm** can be used to find the best **K** value\n> \n> The best **K** value from 1 to 50 is 31 dir\n\n*NOTICE THAT: We will still use the old value instead of completely changing the codes*"},{"metadata":{"trusted":true,"_uuid":"1592e1c54765078340b706c5ef19d7cce138defa"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nsection = {\"n_neighbors\":np.arange(1,50)}\nknnAlgorithm2 = KNeighborsClassifier()\nknnAlgorithm_cv = GridSearchCV(knnAlgorithm2,section,cv = 10)\nknnAlgorithm_cv.fit(x_train,y_train)\nprint(\"Best K value: \", knnAlgorithm_cv.best_params_)\nprint(\"And the best guess score: \",knnAlgorithm_cv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6174a9599181211da25bdbb1ebb2a5602f8109a1"},"cell_type":"markdown","source":"**Please see below**\n\n*I use the following **KNN** algorithm to find out which class my test data belongs to*\n\nNOTE: For detailed description of the algorithm **https://www.kaggle.com/kanncaa1/machine-learning-tutorial-for-beginners** (8.Chapter)\n\n**What is fit?** Use to train our model \n\n**What is predict?** *Take test data and make estimates based on the training received*\n\n**Notice That:** When K is 13, the estimated correct ratio: **0.7849462365591398**"},{"metadata":{"trusted":true,"_uuid":"7dfe009a4b385b10cee756f2d236611c95c1bb4d"},"cell_type":"code","source":"knnAlgorithm = KNeighborsClassifier(n_neighbors=13)\nknnAlgorithm.fit(x_train,y_train)\npredict = knnAlgorithm.predict(x_test)\nprint(\"{} nn Score {}: \".format(13,knnAlgorithm.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"077d3099e10e6386fa5143ed49c3aef160bebde8"},"cell_type":"markdown","source":"**Please see below**\n\nIf we calculate the ratio between **0.784** and **93**, the result is **72.912**. Approximately **73** is accepted.\n\nA list of estimated values is available below **(Predict:)**\nThe list of actual values is available below **(y_test:)**\n\n**Notice That:** A simple line of code found **73** correct guesses and 20 false predictions **Here, 73 matches the above ratio**"},{"metadata":{"trusted":true,"_uuid":"16b47572bb92cffc9d8d1dad05c41275bfe6deb4"},"cell_type":"code","source":"truePredict = 0\nfalsePredict = 0\nfor p in range(len(predict)):\n    for y in range(p,len(y_test)):\n        if (predict[p] == y_test[y]):\n            truePredict = truePredict +1\n            break\n        else:\n            falsePredict = falsePredict +1\n            break\nprint(\"True Predict: \",truePredict)\nprint(\"False Predict\",falsePredict)\nprint(\"-------------------------------------------------------------------------------------\")\nprint(\"Predict: \",predict)\nprint(\"-------------------------------------------------------------------------------------\")\nprint(\"y_test: \",y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07a992297eeed924c490dc881384e7bc040dab77"},"cell_type":"markdown","source":"**Please see below**\n> *Graph of True and False Estimates*"},{"metadata":{"trusted":true,"_uuid":"d2432e86aa31064403948a49fc9ed264820bf3ca"},"cell_type":"code","source":"x_Axis = [\"True\",\"False\"]\ny_Axis = [truePredict,falsePredict]\n\nplt.figure(figsize=(15,15))\nsns.barplot(x=x_Axis,y=y_Axis,palette = sns.cubehelix_palette(len(x_Axis)))\nplt.xlabel(\"Disease Class\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Abnormal and normal type diseases\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"461f55bfa12ca1d370e4b7a5ba029919af9125be"},"cell_type":"markdown","source":"**Please see below**\n\n***Notes on Confusion Matrix***\n\n> 1. *It specifies which values are incorrectly estimated and which values are estimated correctly.*\n> 2. *The complexity matrix is used to show us what our estimates are*\n\n*NOTE: The above code is an example of a confusion matrix without using the sklearn library.*\n\nNOTE: **Number 1: Abnormal **  ,  **Number 0: Normal**\n"},{"metadata":{"trusted":true,"_uuid":"80a8b2b44b2a9e740cdbc5276adb84a5bca5417d"},"cell_type":"code","source":"conf_matrix = confusion_matrix(y_test,predict)\nf,ax = plt.subplots(figsize=(15,15))\nsns.heatmap(conf_matrix,annot=True,linewidths=0.5,linecolor=\"white\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"predict\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"833b880c00d6bce50bfdc8432f8fd18faedae2a8"},"cell_type":"markdown","source":"**Notes on the KNN algorithm**\n\nThe KNN algorithm predicts which class the new data will belong to according to previously known graphs of classes A and B  \n\n> In our example, **pelvic_radius** and **lumbar_lordosis_angle** correspond to class A and B, respectively.\n\n> In our example, the ratio of the values we have correctly guessed is **0.7849462365591398**. This may be because our training datas are very close to each other. **You can see this in the above chart**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}