{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook deals with Linear Regression and its basic interpretation. Problems of multicollinearity, data transformations and new feature creation have been touched upon to keep balance between predictive power and interpretability.\n\nFinally a small comparison has been made between predictive power of a simple neural network and Linear Regression for this scenario"},{"metadata":{"id":"B80fEauKW40x","outputId":"4914a8cf-cc82-4a07-8473-8d0d3a972ff3","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#Initialization\n!pip install pingouin\nimport pandas as pd\nimport seaborn as sns\nimport pingouin as pg\nimport tensorflow as tf\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LogisticRegression as Log\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom scipy.stats import probplot\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix as cm\nfrom sklearn.decomposition import FactorAnalysis as FA\n\n\n# Tolerance function\ndef print_lintol(df):\n  for col in df.columns:\n    x = df.drop(col,axis=1)\n    y = df[col]\n    clf = LR()\n    clf.fit(x,y)\n    print(col)\n    print(1 - clf.score(x,y))\n\n#Stepwise Regression\n\ndef step_reg(df,target_col,significance_threshold,adj_r2_threshold):\n  remaining_col = df.columns.to_list()\n  remaining_col.remove(target_col)\n  included_cols = []\n  col_to_include = df.corr().drop(target_col).sort_values(target_col,ascending=False).index[0]\n  print(col_to_include)\n  included_cols.append(col_to_include)\n  remaining_col.remove(col_to_include)\n  clf = LR()\n  X = df[included_cols].to_numpy()#[...,np.newaxis]\n  y = df[target_col].to_numpy()\n  #*******optional***\n  scaler = StandardScaler()\n  scaler.fit_transform(X)\n  # #********\n  clf.fit(X,y)\n  max_adj_r2 = 1-(1-clf.score(X,y))*(X.shape[0]-1)/(X.shape[0]-X.shape[1]-1)\n  print(max_adj_r2)\n  while(len(included_cols) < (len(df.columns)-1)):\n    max_r = 0\n    col_to_include = None\n    for col in remaining_col:\n      df_temp = df[included_cols + [col,target_col]]\n      pc = pg.partial_corr(df_temp,col,target_col,covar=included_cols)\n      if (abs(pc['r'].iloc[0]) > max_r) and (pc['p-val'].iloc[0] < significance_threshold):\n        col_to_include = col\n        max_r = abs(pc['r'].iloc[0])\n    if col_to_include == None:\n      break\n    included_cols.append(col_to_include)\n    remaining_col.remove(col_to_include)\n    clf = LR()\n    #*******optional***\n    scaler = StandardScaler()\n    scaler.fit_transform(X)\n    #********\n    X = df[included_cols].to_numpy()\n    y = df[target_col].to_numpy()\n    clf.fit(X,y)\n    Adj_r2 = 1-(1-clf.score(X,y))*(X.shape[0]-1)/(X.shape[0]-X.shape[1]-1)\n    if (Adj_r2 - max_adj_r2) < adj_r2_threshold :\n      print('adjusted r2 voilated')\n      included_cols.remove(col_to_include)\n      remaining_col.append(col_to_include)\n      break\n    else:\n      print(col_to_include) \n      print(Adj_r2) \n      max_adj_r2 = Adj_r2\n      min_tol = 1\n      min_tol_col = None\n      for col in included_cols:\n        x = df[included_cols].drop(col,axis=1)\n        y = df[col]\n        clf = LR()\n        #*******optional***\n        scaler = StandardScaler()\n        scaler.fit_transform(X)\n        #********\n        clf.fit(x,y)\n        tol = 1 - clf.score(x,y)\n        if tol < min_tol:\n          min_tol = tol \n          min_tol_col = col\n      print('min tolerance - ' + min_tol_col + ' - ' + str(min_tol))\n  return(included_cols)\n\n\n\n# Neural net\nclass basic_net(tf.keras.Model):\n  def __init__(self,h1,h2,drop):\n    super(basic_net, self).__init__()\n    self.dense1 = tf.keras.layers.Dense(h1,'relu')\n    self.dense2 = tf.keras.layers.Dense(h2,'relu')\n    self.dense3 = tf.keras.layers.Dense(1)\n    self.drop = tf.keras.layers.Dropout(drop)\n  def call(self,x):\n    x = self.dense1(x)\n    x = self.dense2(x)\n    x = self.dense3(self.drop(x))\n    return(x)\n\ncheckpoint_filepath = '/tmp/checkpoint'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True)  \n\n\nloss = tf.keras.losses.MeanSquaredError()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ZOCt6GUactP9","trusted":true},"cell_type":"code","source":"# Load data\ndfo = pd.read_csv('../input/housesalesprediction/kc_house_data.csv',encoding='latin').sample(frac=1)\ndfo = dfo.drop(['id','date'],axis=1)\ndfo['age'] = dfo['yr_built'].max() - dfo['yr_built'] # Adding a new feature, which I think could be better than year built\nttr = 0.25\ndf_test = dfo.iloc[:int(ttr*len(dfo))]\ndf_train = dfo.iloc[int(ttr*len(dfo)):]\ndf_train_pristine = df_train.copy()\ndf_test_pristine = df_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"lHUlrUpgXbq2","outputId":"99f03090-eb35-41bd-e765-398890c15c36","trusted":true},"cell_type":"code","source":"# Visualization Borrowed from - https://www.kaggle.com/burhanykiyakoglu/predicting-house-prices - Thanks !\nfeatures = ['price','bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront',\n            'view','condition','grade','sqft_above','sqft_basement','yr_built','yr_renovated','lat','long','sqft_living15','sqft_lot15']\n\nmask = np.zeros_like(df_train[features].corr(), dtype=np.bool) \nmask[np.triu_indices_from(mask)] = True \n\nf, ax = plt.subplots(figsize=(16, 12))\nplt.title('Pearson Correlation Matrix',fontsize=25)\n\nsns.heatmap(df_train[features].corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"BuGn\", #\"BuGn_r\" to reverse \n            linecolor='w',annot=True,annot_kws={\"size\":8},mask=mask,cbar_kws={\"shrink\": .9});","execution_count":null,"outputs":[]},{"metadata":{"id":"Vq6s38quay4W","outputId":"4714650e-f881-49ea-8fc8-c3ae39ddda25","trusted":true},"cell_type":"code","source":"# Lets get the features having significant(I will take it as > 0.3) correlation with price. We may include the other features as well, but there is little hope they would be any useful.\nsignificant_features = ['price','bedrooms','bathrooms','sqft_living',\n            'view','grade','sqft_above','sqft_basement','lat','sqft_living15']\nsns.pairplot(df_train_pristine[significant_features])","execution_count":null,"outputs":[]},{"metadata":{"id":"0X6-5nnCapY8"},"cell_type":"markdown","source":"My aim is to create a Linear Model. So, I need to keep in mind the important assumptions that come with it. These are: \n1. Linear relationship: There exists a linear relationship between the independent variable, x, and the dependent variable, y.\n\n2. Independence: The residuals are independent. In particular, there is no correlation between consecutive residuals in time series data.\n\n3. Homoscedasticity: The residuals have constant variance at every level of x.\n\n4. Normality: The residuals of the model are normally distributed.\n\nSo, I need to try to do my best to cater all four assumptions through some transformations. It never guarantees, and in most cases, it will not be possible to completely abide by these principles with a mere human's limited knowledge of feature transformations.\n\nPairplots for 'Price'(dependant variable) shows fair amount of heteroscedasticity. Another point to note is, most of the variables shows a right skewed gaussian distribution. This is not a violation linear model, but fixing the skew with a log transformation, sometimes also fixes the heteroscedasticity. Thats exactly what I will be doing next. But before that, lets see a baseline model with all the variables."},{"metadata":{"id":"eakohAB37-Vy","outputId":"ced21d66-1193-446c-da34-2cf6d35e7737","trusted":true},"cell_type":"code","source":"#baseline Regression\nscaler = StandardScaler()\nX = df_train_pristine.drop('price',axis=1).to_numpy()\ny = df_train_pristine['price'].to_numpy()\nX = scaler.fit_transform(X)\nx_test = df_test_pristine.drop('price',axis=1).to_numpy()\nx_test = scaler.transform(x_test)\ny_test = df_test_pristine['price'].to_numpy()\n\nclf_baseline = LR()\nclf_baseline.fit(X,y)\n\nR2 = clf_baseline.score(x_test,y_test)\nprint('R2 - ' + str(R2))\nprint('adj R2 - ' + str(1-(1-R2)*(x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)))\nprint(df_test_pristine.drop('price',axis=1).columns.to_list())\nprint(clf_baseline.coef_)","execution_count":null,"outputs":[]},{"metadata":{"id":"f0siwyPaj7R6"},"cell_type":"markdown","source":"Let's Transform some features now."},{"metadata":{"id":"CG88IfuXeWZi","trusted":true},"cell_type":"code","source":"#Feature transformations on complete data\n# df_pristine = dfo.copy()\ndf = dfo\ndf['sqft_above'] = np.log(df['sqft_above'].to_numpy())\ndf['price'] = np.log(df['price'].to_numpy())\ndf['sqft_living'] = np.log(df['sqft_living'].to_numpy())\ndf['sqft_living15'] = np.log(df['sqft_living15'].to_numpy())\ndf['sqft_basement'] = np.log(df['sqft_basement'].to_numpy() + 1) # added 1, as log of zero is not defined.\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Bhyef_92f0dS"},"cell_type":"markdown","source":"Lets plot the graphs again and see the difference."},{"metadata":{"id":"jv7VQ26bfyPS","outputId":"46951d3d-ab7d-4ae5-a937-40938b05457a","trusted":true},"cell_type":"code","source":"sns.pairplot(df_train[significant_features])","execution_count":null,"outputs":[]},{"metadata":{"id":"Ji5ND6zagWDL"},"cell_type":"markdown","source":"Above plots look much better than the one with the unprocessed variables. Transformations have significantly improved the heteroscedasticity problem, and we can see a visible linear relationship. There is a lot of scope for improvement by modifying the transformations, which involves a lot of creativity. I will stick to simple transformations for now. \n\nBefore applying linear regression, I will check for multicollinearity problem. While not a problem for the predictive performance, multicollinearity can significantly hamper model interpretation by affecting the coefficient determination. I will simply print the tolerances of the different variables by regressing the independant variables against themselves."},{"metadata":{"id":"GCUJnrg4kH4q","outputId":"655f29c8-83af-4e02-91d2-0723867cf0f6","trusted":true},"cell_type":"code","source":"#Tolerance values\nprint('Tolerances for different variables')\nprint_lintol(df_train.drop('price',axis=1))","execution_count":null,"outputs":[]},{"metadata":{"id":"4HtnHbLTlPd7"},"cell_type":"markdown","source":"Significant tolerance problems with five variables - 'sqft_above', 'sqft_living' , 'sqft_basement' , 'yr_built' and 'age'. There is high degree of multicollinearity among these variables, which is not surprising as 'age' is derived from 'year_built' & 'sqft_above' + 'sqft_basement'  = 'sqft_living' .\nSo I would never enter 'age' if I choose 'year_built' OR all three of the square footages together. \n\nLets see if simple forward stepwise regression based on adjusted R squared of the model can take care of this. This will involve sequentially adding independant variables to the linear model based on the highest partial correlation it can achieve taking into account already entred variables, unless the model's adjusted r squared value wont significantly improve."},{"metadata":{"id":"YgpweB94ms2J","outputId":"205752ab-f09d-47bc-85ce-eeed39cfa2c9","trusted":true},"cell_type":"code","source":"#stepwise_regression\ntarget_col = 'price'\nsignificance_threshold = 0.01\nadj_r2_threshold = 0.003\n\nselected_cols = step_reg(df_train,target_col,significance_threshold,adj_r2_threshold)    ","execution_count":null,"outputs":[]},{"metadata":{"id":"hqHDhDJHrQCD"},"cell_type":"markdown","source":"Stepwise regression was successfull in avoiding selecting highly corelated variables together. First variable selected is grade, followed by 'sqft_living', 'lat' and 'yr_built'. Choosing adjusted r squared helps us maintain a balance between the degrees of freedom for model generalization and the predictive power. R squared simply improves with addition of extra dimensions at cost of generalizability and hence, may overfit. So, a tradeoff.\n\nLets try simple regression over a test and train set with the selected variables."},{"metadata":{"id":"NUxFCRNWrFts","outputId":"99b27c96-dd91-4332-838f-6108a95d60e1","trusted":true},"cell_type":"code","source":"# Regression on transformed data\nscaler = StandardScaler()\nX = df_train[selected_cols].to_numpy()\ny = df_train['price'].to_numpy()\nX = scaler.fit_transform(X)\nx_test = df_test[selected_cols].to_numpy()\nx_test = scaler.transform(x_test)\ny_test = df_test['price'].to_numpy()\n\nclf_lr = LR()\nclf_lr.fit(X,y)\n\nR2 = clf_lr.score(x_test,y_test)\nprint('R2 - ' + str(R2))\nprint('adj R2 - ' + str(1-(1-R2)*(x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)))\nprint(df_test[selected_cols].columns.to_list())\nprint(clf_lr.coef_)","execution_count":null,"outputs":[]},{"metadata":{"id":"7cvneZTMwDkf"},"cell_type":"markdown","source":"We achieved adj R squared of 0.77 with major important features as 'grade', 'lat', 'sqft_living', 'yr_built'.\n\nLets see the residual plot to see if the error terms are evenly split across the different values of x, in otherwords, homoscedasticity. Also, I will check if the error terms are normally distributed for basic linear regression. "},{"metadata":{"id":"U5a-gkIWVmak","outputId":"fa909d92-b1ae-42ff-daa1-b62672ba8364","trusted":true},"cell_type":"code","source":"#Training residuals\nres = (y - clf_lr.predict(X))\nfig, axs = plt.subplots(ncols=3,figsize=(26,7))\nqq,_ = probplot(res,sparams=(0,np.std(res)))\nsns.scatterplot(np.arange(X.shape[0]),res,ax=axs[0])\nsns.distplot(res,ax=axs[1])\nsns.scatterplot(qq[0],qq[1],ax=axs[2])","execution_count":null,"outputs":[]},{"metadata":{"id":"jz6JQ47JVAmH","outputId":"3dc50443-2467-4d34-e9eb-064b60bc72e5","trusted":true},"cell_type":"code","source":"#Test residuals\nres = (y_test - clf_lr.predict(x_test))\nfig, axs = plt.subplots(ncols=3,figsize=(26,7))\nqq,_ = probplot(res,sparams=(0,np.std(res)))\nsns.scatterplot(np.arange(x_test.shape[0]),res,ax=axs[0])\nsns.distplot(res,ax=axs[1])\nsns.scatterplot(qq[0],qq[1],ax=axs[2])","execution_count":null,"outputs":[]},{"metadata":{"id":"EjZdPk5PYV5v"},"cell_type":"markdown","source":"Residuals show similar spread across the different training and test samples. Error terms appear to be homoscedastic. The error distribution appears to be normal, but shows slight deviation(refer to QQ-plot) from the actual normal distribution centered at 0 with standard deviation same as the residual standard deviation. \n\nNow, lets try to do a simple interpretation of the results based on standardized coefficient values. I will be comparing scaled absolute values of coefficients from the baseline model and the final model. Final model does not take into account many variables fed into the baseline and those features can be assumed to have no(0) effect in the final model."},{"metadata":{"id":"svNs0_tZmqDJ","outputId":"eaf3ecbf-a621-489d-dc69-bb9e59d27b3f","trusted":true},"cell_type":"code","source":"#Coefficient visualization\ncoeff = np.abs(clf_baseline.coef_)\ncoef_baseline = (coeff- (np.min(coeff)))/ (np.max(coeff) - np.min(coeff))\nindex = np.argsort(coef_baseline)\ncoef_baseline = coef_baseline[index]\nbase_cols = df_test.drop('price',axis=1).columns\ncols = []\nfor i in index:\n  cols.append(base_cols[i])\ndf_baseline = pd.DataFrame({\"Coefficients\": coef_baseline },\n                  index=cols)\n\n\n\ncoeff = np.abs(clf_lr.coef_)\ncoef_final = (coeff- (np.min(coeff)))/ (np.max(coeff) - np.min(coeff))\nindex = np.argsort(coef_final)\ncoef_final = coef_final[index]\nfinal_cols = df_test[selected_cols].columns\ncols = []\nfor i in index:\n  cols.append(final_cols[i])\ndf_final = pd.DataFrame({\"Coefficients\": coef_final},\n                  index=cols)\n\n\nfig, axs = plt.subplots(ncols=2,figsize=(16,7))\nsns.heatmap(df_baseline, annot=True, fmt=\"g\", cmap='viridis',ax=axs[0])\nsns.heatmap(df_final, annot=True, fmt=\"g\", cmap='viridis',ax=axs[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"9LhPvLmZRPmJ"},"cell_type":"markdown","source":"There are significant differences between the baseline and the final model. 'grade' comes on top for the final model, whereas, baseline relies on square footages. Final model does not include any square footage but 'sqft_living', with a lesser importance as compared to baseline. But square footage and grade variables are significantly correlated for both the models and hence, their  actual contributions are obscured. This is the reason, we see such difference between the two(classic Multicollinearity problem here). However, one interesting observation is significant rise of the variable 'lat' in the final model. This is due to a significant bump in 'lat' correlation with 'price' after data transformations. This was not intended, but it happened anyway.\n\nLets see how correlations changed for variables(for whom the final correlation is greater than 0.3)after the data transformations, represented as percentages."},{"metadata":{"id":"bRKADpMb7T-F","outputId":"b367f344-064c-480e-8053-dedb9f335091","trusted":true},"cell_type":"code","source":"#Correlation differences after transformations\ncorr_at = df_train.corr().drop('price')['price']\ncorr_bt = df_train_pristine.corr().drop('price')['price']\n((corr_at - corr_bt)/corr_bt)*100*(corr_at>0.3).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"id":"sD_fL6zKnrUK"},"cell_type":"markdown","source":"'lat' has almost 50% increase in its correlation value. This looks logical, as location can be related to property prices due to several underlying factors. A better transformation for location may give us even better results.\n\nHowever, to interpret feature importance just by numbers is not sufficient and nothing can beat basic common sense and domain knowledge. You may use the number to back your claims, but numbers alone should not be trusted."},{"metadata":{"id":"47xWFxI6zPAw"},"cell_type":"markdown","source":"We may further improve the predictive power by adding polynomial features. But keep in mind, with more features, comes a problem of low interpretability and higher variance. Also, the variables are no longer independant from each other due to addition of interaction variables. Lets run a polynomial regression with degree 3."},{"metadata":{"id":"q3aORQDLyA_p","outputId":"df8e3a61-8718-43c5-beca-7df6eda29c1d","trusted":true},"cell_type":"code","source":"#Polynomial Features regression\nscaler = StandardScaler()\npoly = PolynomialFeatures(3)\nX = df_train[selected_cols].to_numpy()\ny = df_train['price'].to_numpy()\nX = poly.fit_transform(scaler.fit_transform(X))\nx_test = df_test[selected_cols].to_numpy()\nx_test = poly.fit_transform(scaler.transform(x_test))\ny_test = df_test['price'].to_numpy()\n\nclf_pr = LR()\nclf_pr.fit(X,y)\n\nr2 = clf_pr.score(x_test,y_test)\nadj_r2 = 1-(1-r2)*(x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)\nprint('R2 - ' + str(r2))\nprint('adj R2 - ' + str(adj_r2))","execution_count":null,"outputs":[]},{"metadata":{"id":"dv41RnXunxMb"},"cell_type":"markdown","source":"Polynomial features with degree 3 bumped the adjusted R squared to almost 0.829 on the test set.\n\n\nBut I think I am missing something here. Location seems important, atleast per my limited knowledge of the Real Estate, yet there is no effective utilization in my model. Selected features only contain latitude, but not longitude. Neither did zipcode come into picture. Latitude can hardly do much by itself. Zipcode seems promising, but being categorical with close to 70 distinct values makes me uncomfortable to include a one hot encoded version of it. \n\nSo, I will try to see if zipcode can be converted to an ordinal variable. \nApproach is simple, I will group the data by zipcodes and create another variable 'ordinal_zip' which would be a sorted version of the zipcode based on mean price value per zipcode. Instead of price, I will use price per squarefoot which I think is a better(again, a hunch) variable to base my new feature.\n\nAfter getting a ordinal version of zip code as per the price per squarefoot, I will try to regress this new variable on latitude and longitude, so that I dont have to take zipcodes into considerations at all. Latitude and Longitude should contain enough information to uniquely identify a zipcode. After all, you cant have two zipcodes for same geo coordinates(I hope the world works this way). For this regression, I will use a Random Forest Regressor, as the geomap of zipcodes looks like blocks of latitudes and longitude, which seems like ideal condition for a decision tree based approaches. "},{"metadata":{"id":"2uVwuOntwDQ_","outputId":"429d098b-9e3d-4077-c45c-ea3c8431ba41","trusted":true},"cell_type":"code","source":"# Build ordinal zip code\ndef get_ordinal_zip(x):\n  return(zip.loc[x]['ordinal_zip'])\n\n# df_train['zip_index'] = df_train['zipcode'].apply(get_ordinal_zip)\ndf_train['ppsf'] = df_train_pristine['price']/df_train_pristine['sqft_living'] #New variable, price per square foot\nzip = df_train.groupby('zipcode').mean()\nzip.sort_values('ppsf',inplace=True)\nzip['ordinal_zip'] = np.arange(len(zip))\n\ndf_train['ordinal_zip'] = df_train['zipcode'].apply(get_ordinal_zip) # New feature\nsns.scatterplot(df_train['ordinal_zip'],df_train['price'])","execution_count":null,"outputs":[]},{"metadata":{"id":"V1K8P2BpzE_y"},"cell_type":"markdown","source":"Now, we have a new variable, which is like zipcode, but ordered as per mean property prices in a particulat area. Higher the value of this code, higher the average prices in that area. Refer to the graph above.\n\nNow, we will create a random forest regressor to learn how latitude and longitude can be used to predict this new variable and use this regressor to predict these ordinal zip values in our test set."},{"metadata":{"id":"w-YU2S_qzk4a","outputId":"bbce1b15-d93d-4e69-e0f2-0bde13dec31f","trusted":true},"cell_type":"code","source":"#Learn to predict ordinal zipcode directly from latitude and longitude\nX = df_train[['lat','long']].to_numpy()\ny = df_train['ordinal_zip'].to_numpy()\n\n\nclf_zip = RFR()\nclf_zip.fit(X,y)\nprint('R2 for predicting ordinal zip from lat and long - ' + str(clf_zip.score(X,y)))\n\ndef reg_zip_index(abc):\n  x = abc.to_numpy()[np.newaxis,...]\n  return(clf_zip.predict(x)[0])\n\ndf_test['ordinal_zip'] = df_test[['lat','long']].apply(reg_zip_index,axis=1)\nsns.scatterplot(df_test['ordinal_zip'],df_test['price'])","execution_count":null,"outputs":[]},{"metadata":{"id":"qwPfn-Rf1f9o"},"cell_type":"markdown","source":"regressor trained on training data is able to replicate same relationship between ordinal zip and the price in the test data. So, now we have a function which can produce ordinal zip values from latitude and longitude.\n\nLets see if including this new variable can improve the performance of simple linear regression."},{"metadata":{"id":"6VlxrBiC2JYA","outputId":"853e3dee-0b96-4553-af76-905e58b63494","trusted":true},"cell_type":"code","source":"# regression with ordinal zipcode\nselected_cols1 = selected_cols + ['ordinal_zip'] # add new variable to already selected columns\nscaler = StandardScaler()\nX = df_train[selected_cols1].to_numpy()\ny = df_train['price'].to_numpy()\nX = scaler.fit_transform(X)\nx_test = df_test[selected_cols1].to_numpy()\nx_test = scaler.transform(x_test)\ny_test = df_test['price'].to_numpy()\n\nclf_lr = LR()\nclf_lr.fit(X,y)\n\nR2 = clf_lr.score(x_test,y_test)\nprint('R2 - ' + str(R2))\nprint('adj R2 - ' + str(1-(1-R2)*(x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)))\nprint(df_test.drop('price',axis=1).columns.to_list())\nprint(clf_baseline.coef_)","execution_count":null,"outputs":[]},{"metadata":{"id":"sAKqj65g3XOa"},"cell_type":"markdown","source":"This is a significant improvement in the performance of the linear regression, where we had to add only one ordinal variable instead of 70 categorical variables. Location sure is an important factor. I am also curious to know how the stepwise regression process changes after addition of the new variable. Let's see"},{"metadata":{"id":"YgyJ90gN_dOo","outputId":"4995b236-5bc8-46c3-a908-50c690112893","trusted":true},"cell_type":"code","source":"selected_cols1 = step_reg(df_train.drop('ppsf',axis=1),target_col,significance_threshold,adj_r2_threshold)   ","execution_count":null,"outputs":[]},{"metadata":{"id":"wxkeFgp3_91O"},"cell_type":"markdown","source":"Significant change. Infact, Latitude is gone, as its already coded in ordinal zip. our new feature achieves second place, with three most important features as - Grade, Location(zip, lat,long) and Sqft Area."},{"metadata":{"id":"01eYCfL8UupG"},"cell_type":"markdown","source":"\nLets see if a simple neural network can capture such a location feature. I would be using non transformed data with the neural network. However I would be standardizing the complete data for numerical stability of the network.\nThe network is composed of two hidden stacks and an output unit. Hidden units are Relu activated and loss is calculated as MSE. Dropout has been used to help prevent the overfitting."},{"metadata":{"id":"aSBCRzSjoSWk","outputId":"0bee6875-fa06-4540-c379-ec55eee2434b","trusted":true},"cell_type":"code","source":"# Simple Neural Net\nselected_cols1 = selected_cols\nif 'long' not in selected_cols1:\n  selected_cols1 = selected_cols + ['long']\nscalerx = StandardScaler()\nscalery = StandardScaler()\n# X = df_train.drop('price',axis=1).to_numpy()\nX = df_train_pristine[selected_cols1].to_numpy()\ny = df_train_pristine['price'].to_numpy()\nX = scalerx.fit_transform(X)\ny = scalery.fit_transform(y[...,np.newaxis])[:,0]\n# x_test = df_test.drop('price',axis=1).to_numpy()\nx_test = df_test_pristine[selected_cols1].to_numpy()\nx_test = scalerx.transform(x_test)\ny_test = df_test_pristine['price'].to_numpy()\ny_test = scalery.transform(y_test[...,np.newaxis])[:,0]\n\n\nmodel = basic_net(32,32,0.3)\nmodel.compile(optimizer='rmsprop', loss=loss)\nprint('Training model...')\nmodel.fit(X,y,epochs=75,batch_size=64,verbose=0,validation_split=0.2,callbacks=[model_checkpoint_callback])\nmodel.load_weights(checkpoint_filepath)\nprint('Testing...')\ny_pred = model.predict(x_test)\n\n\nr2 = r2_score(y_test,y_pred)\nadj_r2 = 1-(1-r2)*(x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)\nprint('R2 - ' + str(r2))\nprint('adj R2 - ' + str(adj_r2))","execution_count":null,"outputs":[]},{"metadata":{"id":"6ZRtU3E13u4h"},"cell_type":"markdown","source":"This model shows a significantly better performance with non transformed data with same features as the linear regression. A feature 'long' has been added(if it was not already selected) to see if location('lat' and 'long') could improve the results and it turns out, it does. You may re run this cell by excluding 'long' and see a significant drop in the performance.\nHowever, I would be interested to know if it gains any performance by adding our new feature - ordinal zip. Maybe, next time.\n\nWell, Neural network did save us from feature transformations. It might have learnt those transformations in the hidden units. There is no sure way of knowing this, but it definitely learnt some important transformations, which helped it's output unit(which essentially is just a simple linear regressor acting upon learnt transformations) achieve a good score without receiving any explicit data transformations from us."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}