{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport csv\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom numpy.random import seed\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.random import set_seed\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GRU, Reshape\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\nfrom tensorflow.keras.models import load_model\npd.set_option('display.max_rows', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/corpus-of-russian-news-articles-from-lenta/lenta-ru-news.csv').sample(frac = 0.3, \n                                                                                                 random_state = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA and data preparation ","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'][334]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.title('Missing data')\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['topic'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['topic'] == 'Ğ‘Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ°'].index, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tags'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 8))\ndf['title'].apply(len).hist(bins = 40)\nplt.title('Length of title distribution')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'].apply(type).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['text'].apply(type) == float]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['text'].apply(type) == float].index, inplace = True)\ndf['text'].apply(type).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['len'] = df['text'].apply(len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 8))\ndf['len'].hist(bins = 130)\nplt.title('Length of text distribution')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['len'] > 20000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 8))\ndf['len'][df['len'] < 6000].hist(bins = 130)\nplt.title('Length of text distribution')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['len'] < 300].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['len'] < 300].index, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_drop = {'\\x03', '\\x04', '\\x08',  '\\r', '\\x0f', '\\x13', '\\x1d', '\\x1e', '\\x1f', '<', '=', '>', '@',  '^',  '`', '{', '|', '}', \n '~', '\\x7f', '\\x97', '\\x98', '\\xa0', 'Â¡', 'Â©', 'Â«', '\\xad', 'Â®', 'Â¯', 'Â°', 'Â±', 'Â´', 'Âµ', 'Â·', 'Â¸', 'Â»', 'Â¼', 'Â½', 'Â¾', \n 'Â¿', 'Ã', 'Ã†', 'Ã‡', 'Ãˆ', 'Ã‰', 'Ã‹', 'Ã', 'Ã“', 'Ã”', 'Ã–', 'Ã—', 'Ã˜', 'Ãœ', 'Ã', 'ÃŸ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', 'Ã¦', 'Ã§', \n 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã°', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã·', 'Ã¸', 'Ãº', 'Ã»', 'Ã¼', 'Ã½', 'Ã¾', 'Ä', 'Äƒ', \n 'Ä…', 'Ä‡', 'ÄŒ', 'Ä', 'Ä', 'Ä“', 'Ä—', 'Ä™', 'Ä›', 'ÄŸ', 'Ä«', 'Ä°', 'Ä±', 'Å', 'Å‚', 'Å„', 'Å†', 'Å', 'Å', 'Å™', 'Å›', 'Å', 'ÅŸ', 'Å ', \n 'Å¡', 'Åª', 'Å³', 'Å·', 'Åº', 'Å¼', 'Å¾', 'È›', 'É”', 'É™', 'É¢', 'Éª', 'Ê’', 'ÊŸ', 'Ê¼', 'Ëš', 'Ì', 'Ì†', 'Ìˆ', 'Î”', 'Î˜', 'Î›', 'Î', 'Î£', 'Î­',\n 'Î¯', 'Î±', 'Î²', 'Î³', 'Î´', 'Îµ', 'Î¸', 'Î¹', 'Îº', 'Î»', 'Î¼', 'Î½', 'Î¿', 'Ï€', 'Ï', 'Ï‚', 'Ïƒ', 'Ï„', 'Ï…', 'Ï‡', 'Ïˆ', 'Ï‰', 'ÏŒ', 'Ï',\n 'Ï',  'Ğ‰',  'Ñ™', 'Ñ', 'Òš', 'Ò›', 'Ò£', 'Ò¯', 'Ò°', 'Ó§', '×', '×•', '×™', '×œ', '×', '×¤', '×¥', '×¦', '×§', '×¨', '×©', '×ª', 'ØŒ', 'Ø›', 'ØŸ', 'Ø¢', 'Ø¦', 'Ø§', 'Ø¨', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Øµ', 'Ø·', 'Ø¸', 'Ø¹', 'Ù', 'Ù‚', 'Ùƒ', 'Ù„', 'Ù…'\n , 'Ù†', 'Ù‡', 'Ùˆ', 'Ù‰', 'ÙŠ', 'Ù¾', 'Ú†', 'Ú©', 'Ú¯', 'ÛŒ', 'Û´', 'Ûµ', 'Û·', 'Û¹', 'àª–', 'àª«', 'àª¬', 'àª°', 'àª²', 'àª¸', 'àª¾', 'à«‹', 'à«', 'á´¥', \n 'áµ‰', 'áµ', 'áµ', 'áµ’', 'áµ˜', 'á¶…', 'á¶˜', 'á¶œ', 'á¶ ', 'á¸¤', 'á¸«', 'áº£', 'áº§', 'á»‡', 'á»‘', 'á»™', '\\u2002', '\\u2003', '\\u2009', '\\u200a', \n '\\u200b', '\\u200c', '\\u200d', '\\u200e', '\\u200f', 'â€˜', 'â€™', 'â€œ', 'â€', 'â€', 'â€¢', 'â€¦', '\\u2028', '\\u202a', '\\u202c', \n '\\u202d', '\\u202f', 'â€¼', '\\u2066', 'â±', 'â½', 'â¾', 'â¿', 'â‚¬', 'â‚½', 'â„–', 'â„¢', 'â†“', 'âˆ’', 'âˆ™', 'âˆ', 'âˆ©', 'â‰ ', 'â³', 'â‘ ', 'â’·',\n 'â’º', 'â’»', 'â“ƒ', 'â“„', 'â“‡', 'â“ˆ', 'â“‰', 'â”€', 'â–º', 'â˜€', 'â˜', 'â˜', 'â˜‘', 'â˜', 'â˜ ', 'â˜º', 'â™€', 'â™‚', 'â™¥', 'â™ª', 'âš”', 'âš–', \n 'âš¡', 'âšª', 'âš½', 'âœ…', 'âœˆ', 'âœŒ', 'âœ”', 'âœ¨', 'âœµ', 'â„', 'â—', 'â¤', 'â–', 'â¡', 'â°', 'â €', 'â¬…', 'â¬†', 'â­', 'â­•', 'ã€',\n 'ã€‚', 'ã€…', 'ã€Œ', 'ã€', 'ã€', 'ã€', 'ã€œ', 'ã€°', 'ã‚', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‹', 'ãŒ', 'ã', 'ã', 'ã', 'ã', 'ã‘', 'ã“', \n 'ã”', 'ã•', 'ã–', 'ã—', 'ã˜', 'ã™', 'ãš', 'ã›', 'ã', 'ãŸ', 'ã ', 'ã¡', 'ã£', 'ã¦', 'ã§', 'ã¨', 'ã©', 'ãª', 'ã«', 'ã­', 'ã®', \n 'ã¯', 'ãµ', 'ã½', 'ã¾', 'ã¿', 'ã‚€', 'ã‚', 'ã‚‚', 'ã‚ƒ', 'ã‚„', 'ã‚…', 'ã‚ˆ', 'ã‚‰', 'ã‚Š', 'ã‚‹', 'ã‚Œ', 'ã‚', 'ã‚', 'ã‚’', 'ã‚“', 'ã‚¢', \n 'ã‚¤', 'ã‚¦', 'ã‚§', 'ã‚¨', 'ã‚ª', 'ã‚«', 'ã‚¬', 'ã‚­', 'ã‚®', 'ã‚¯', 'ã‚±', 'ã‚³', 'ã‚´', 'ã‚µ', 'ã‚·', 'ã‚¸', 'ã‚¹', 'ã‚º', 'ã‚»', 'ã‚½', 'ãƒ€', \n 'ãƒ', 'ãƒƒ', 'ãƒ„', 'ãƒˆ', 'ãƒ‰', 'ãƒŠ', 'ãƒ‹', 'ãƒ', 'ãƒ', 'ãƒ', 'ãƒ‘', 'ãƒ”', 'ãƒ–', 'ãƒ—', 'ãƒ˜', 'ãƒš', 'ãƒœ', 'ãƒ', 'ãƒ', 'ãƒ ', 'ãƒ¡',\n 'ãƒ£', 'ãƒ¥', 'ãƒ§', 'ãƒ©', 'ãƒª', 'ãƒ«', 'ãƒ¬', 'ãƒ­', 'ãƒ³', 'ãƒ´', 'ãƒ»', 'ãƒ¼', 'ä¸€', 'ä¸‰', 'ä¸Š', 'ä¸‹', 'ä¹—', 'äº‹', 'äº”', 'äºº', 'ä»Š',\n 'ä»', 'ä»•', 'ä¼š', 'ä½•', 'ä½¿', 'ä¿', 'å…‰', 'å…¥', 'å†…', 'å†™', 'å‡º', 'åˆ†', 'åˆ°', 'åŠ ', 'å‹•', 'åŒ—', 'å', 'å£', 'å¯', 'å›½', 'åœ’', \n 'åœŸ', 'åŸº', 'å¤–', 'å¤©', 'å¥³', 'å¦™', 'å¬‰', 'å­', 'å¯', 'å±‹', 'å±±', 'å¸‚', 'å¹³', 'å¹¼', 'åº¦', 'å½“', 'å¾Œ', 'å¾®', 'æ€', 'æ€§', 'æ‚¦', \n 'æ‚²', 'æ„›', 'æ„Ÿ', 'æˆ', 'æ‹…', 'æŒ', 'æ•', 'æ–¹', 'æ—¥', 'æ™‚', 'æ™®', 'æš®', 'æœˆ', 'æœ‰', 'æœ¬', 'æ', 'æ±', 'æ', 'æš', 'æŸ’', 'æŸ¯', \n 'æ¤', 'æ¥µ', 'æ§˜', 'æ­§', 'æ®µ', 'æ°—', 'æ²™', 'æ²»', 'æ´ª', 'æ´»', 'æº€', 'ç„¶', 'ç…§', 'ç‰‡', 'ç‰©', 'çŠ¬', 'ç‹­', 'çŒ«', 'ç†', 'ç”Ÿ', 'ç”£',\n 'ç”¨', 'ç•ª', 'ç—¢', 'çš„', 'çš®', 'ç›®', 'ç›´', 'çœŸ', 'ç¤¾', 'ç¥­', 'ç«‹', 'ç´ ', 'çµ„', 'ç·¨', 'ç·´', 'ç½®', 'è€ƒ', 'è‚Œ', 'è‡ª', 'è‰¯', 'èŠ±',\n 'èŒ¶', 'è½', 'è™«', 'è™¾', 'è¦‹', 'è§†', 'èª¿', 'è² ', 'è²·', 'è³€', 'è³¼', 'è¶…', 'èº', 'è¿˜', 'é€š', 'éŠ', 'é”', 'éƒ¨', 'é›†', 'é›¨', 'é †', \n 'é¡”', 'é­š', 'ê±¸', 'ê³„', 'ê·¼', 'ê¸€', 'ê¸°', 'ë‹¤', 'ë‹¬', 'ëŒ€', 'ë™', 'ë¼', 'ë¡œ', 'ë¥´', 'ë¦¬', 'ë¦¼', 'ë§Œ', 'ë©”', 'ëª¨', 'ëª©', 'ë¬´', \n 'ë¬¼', 'ë³µ', 'ë¸Œ', 'ë¹Œ', 'ì˜', 'ì‚¬', 'ìŠ¤', 'ì‹¤', 'ì–´', 'ì—¬', 'ì˜ˆ', 'ì˜¤', 'ì˜¬', 'ìŒ', 'ì´', 'ì ‘', 'ì¤„', 'ì§€', 'ì§„', 'ì°½', 'ì½”', \n 'í¬', 'íˆ¬', 'í‹°', 'íŒ…', 'í‰', 'í”½', 'í•œ', 'í–‰', 'í—Œ', 'íšŒ', 'ï¬', 'ï¸', '\\ufeff',  'ğŸ†˜', 'ğŸŒ…', 'ğŸŒˆ', 'ğŸŒŠ', 'ğŸŒŒ', 'ğŸŒ', 'ğŸŒ', \n 'ğŸŒŸ', 'ğŸŒ ', 'ğŸŒ±', 'ğŸŒ´', 'ğŸŒ·', 'ğŸŒ¸', 'ğŸŒ¹', 'ğŸŒ»', 'ğŸ', 'ğŸ‚', 'ğŸƒ', 'ğŸ„', 'ğŸ‰', 'ğŸ‹', 'ğŸŒ', 'ğŸ', 'ğŸ', 'ğŸ‘', 'ğŸ’', \n 'ğŸ“', 'ğŸ•', 'ğŸ¡', 'ğŸ¥', 'ğŸ¦', 'ğŸ©', 'ğŸ¬', 'ğŸ´', 'ğŸ·', 'ğŸ¹', 'ğŸ»', 'ğŸ¼', 'ğŸ€', 'ğŸ', 'ğŸ„', 'ğŸ…', 'ğŸ†', 'ğŸ‡', 'ğŸ‰', 'ğŸŠ', \n 'ğŸ™', 'ğŸ¥', 'ğŸ­', 'ğŸ¶', 'ğŸƒ', 'ğŸ‹', 'ğŸ–', 'ğŸ', 'ğŸ¡', 'ğŸº', 'ğŸ»', 'ğŸ¼', 'ğŸ½', 'ğŸ¾', 'ğŸ¿', 'ğŸ…', 'ğŸˆ', 'ğŸ', 'ğŸ', 'ğŸ“',\n 'ğŸ ', 'ğŸ³', 'ğŸ´', 'ğŸ¶', 'ğŸ¸', 'ğŸº', 'ğŸ»', 'ğŸ¾', 'ğŸ‘€', 'ğŸ‘‡', 'ğŸ‘‰', 'ğŸ‘Š', 'ğŸ‘‹', 'ğŸ‘Œ', 'ğŸ‘', 'ğŸ‘', 'ğŸ‘‘', 'ğŸ‘’', 'ğŸ‘–', \n 'ğŸ‘—', 'ğŸ‘™', 'ğŸ‘›', 'ğŸ‘œ', 'ğŸ‘Ÿ', 'ğŸ‘ ', 'ğŸ‘£', 'ğŸ‘¨', 'ğŸ‘¯', 'ğŸ‘°', 'ğŸ‘¸', 'ğŸ‘¹', 'ğŸ‘½', 'ğŸ‘¿', 'ğŸ’ƒ', 'ğŸ’…', 'ğŸ’‡', 'ğŸ’‹', 'ğŸ’Œ', \n 'ğŸ’', 'ğŸ’', 'ğŸ’', 'ğŸ’“', 'ğŸ’”', 'ğŸ’•', 'ğŸ’–', 'ğŸ’—', 'ğŸ’˜', 'ğŸ’™', 'ğŸ’š', 'ğŸ’›', 'ğŸ’œ', 'ğŸ’', 'ğŸ’¥', 'ğŸ’¦', 'ğŸ’©', 'ğŸ’ª', 'ğŸ’«',\n 'ğŸ’­', 'ğŸ’¯', 'ğŸ’¸', 'ğŸ“', 'ğŸ“', 'ğŸ“²', 'ğŸ“·', 'ğŸ“¸', 'ğŸ“º', 'ğŸ“»', 'ğŸ”', 'ğŸ”™', 'ğŸ”¥', 'ğŸ”ª', 'ğŸ”´', 'ğŸ”¸', 'ğŸ”¹', 'ğŸ•Š', 'ğŸ•º', 'ğŸ–•',\n 'ğŸ–¤', 'ğŸ—£', 'ğŸ—¼', 'ğŸ˜€', 'ğŸ˜', 'ğŸ˜‚', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜…', 'ğŸ˜†', 'ğŸ˜‡', 'ğŸ˜ˆ', 'ğŸ˜‰', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜Œ', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', \n 'ğŸ˜', 'ğŸ˜’', 'ğŸ˜˜', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ˜¡', 'ğŸ˜¢', 'ğŸ˜¥', 'ğŸ˜¦', 'ğŸ˜¨', 'ğŸ˜©', 'ğŸ˜¬', 'ğŸ˜­', 'ğŸ˜¯', 'ğŸ˜±', 'ğŸ˜³', 'ğŸ˜´', 'ğŸ˜¶', 'ğŸ˜¸', \n 'ğŸ˜¹', 'ğŸ˜»', 'ğŸ˜¼', 'ğŸ˜½', 'ğŸ™€', 'ğŸ™ƒ', 'ğŸ™„', 'ğŸ™…', 'ğŸ™‡', 'ğŸ™ˆ', 'ğŸ™‹', 'ğŸ™Œ', 'ğŸ™', 'ğŸš€', 'ğŸš‚', 'ğŸš‘', 'ğŸš—', 'ğŸš¢', 'ğŸš¨',\n 'ğŸ›', 'ğŸ›’', 'ğŸ¤”', 'ğŸ¤–', 'ğŸ¤˜', 'ğŸ¤£', 'ğŸ¤¤', 'ğŸ¤¦', 'ğŸ¤©', 'ğŸ¤ª', 'ğŸ¤«', 'ğŸ¤­', 'ğŸ¤µ', 'ğŸ¤·', 'ğŸ¤¼', 'ğŸ¥‡', 'ğŸ¥ˆ', 'ğŸ¥‰', 'ğŸ¥Š', \n 'ğŸ¥°', 'ğŸ¥³', 'ğŸ¦€', 'ğŸ¦„', 'ğŸ¦…', 'ğŸ¦‹', 'ğŸ§›', 'ğŸ§œ'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_convert = {\n    'ï¼¨':'H',\n    'ï¼«':'K',\n    'ï¼®':'N',\n    'ğ€':'A',\n    'ğƒ':'D',\n    'ğŠ':'K',\n    'ğ':'N',\n    'ğ':'O',\n    'ğ“':'T',\n    'ğ•':'V',\n    'ğ’†':'e',\n    'ğ’Š':'i',\n    'ğ’':'l',\n    'ğ’':'m',\n    'ğ’':'o',\n    'ğ’”':'s',\n    'ğ’•':'t',\n    'ğ’—':'v',\n    'ğ’š':'y',\n    'ğ’›':'z',\n    'ğŸ‡¦':'A',\n    'ğŸ‡§':'B',\n    'ğŸ‡¨':'C',\n    'ğŸ‡©':'D',\n    'ğŸ‡ª':'E',\n    'ğŸ‡¬':'G',\n    'ğŸ‡®':'I',\n    'ğŸ‡°':'K',\n    'ğŸ‡²':'M',\n    'ğŸ‡·':'R',\n    'S':'U'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_clean(text):\n    for char in to_drop:\n        text = text.replace(char, ' ')\n    text = re.sub(r'\\s+', ' ', text)\n    \n    for to_be_replaced, to_replace in to_convert.items():\n        text = text.replace(to_be_replaced, to_replace)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_clean('ğ“ğ“ğ“ğ“ğ“ğ“ğ“75-Ğ»ĞµÑ‚Ğ½ÑÑ ĞĞ½Ğ¸ĞºĞ° ğŸºğŸºğŸºğŸºğŸºğŸºğŸºğŸºĞ”. Ğ¸Ğ· Ñ€ÑƒĞ¼Ñ‹Ğ½ÑĞºĞ¾Ğ¹ Ğ´ĞµÑ€ĞµĞ²Ğ½Ğ¸ ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(to_clean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text to batches convertion","metadata":{}},{"cell_type":"code","source":"vocab = set()\n\nfor news in df['text']:\n    vocab.update(set(news))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = sorted(vocab)\nprint(vocab)\nlen(vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind_to_char = np.array(vocab)\nchar_to_ind = {u:i for i, u in enumerate(vocab)}\nchar_to_ind","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoding = lambda text: np.array([char_to_ind[c] for c in text])\ndf['encoded text'] = df['text'].apply(encoding)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_batches(text, seq_len, step):\n    if len(text) < seq_len:\n        return []\n    else:\n        sequences = []\n        for i in range(0, len(text), step):\n            batch = text[i:i+seq_len+1]\n            if len(batch) > seq_len:\n                sequences.append(text[i:i+seq_len+1])\n        return sequences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test ='''ĞšĞ°Ğº ÑĞ¾Ğ¾Ğ±Ñ‰Ğ¸Ğ»Ğ¾ \"Ğ­Ñ…Ğ¾ ĞœĞ¾ÑĞºĞ²Ñ‹\", ĞšĞ¾Ğ½Ğ³Ñ€ĞµÑÑ Ñ€ÑƒÑÑĞºĞ¸Ñ… Ğ¾Ğ±Ñ‰Ğ¸Ğ½ (ĞšĞ Ğ) Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½ Ğ¸Ğ´Ñ‚Ğ¸ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ»Ğ°Ğ¼ĞµĞ½Ñ‚ÑĞºĞ¸Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñ‹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ±Ğ»Ğ¾ĞºĞµ\nÑ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ®Ñ€Ğ¸Ñ Ğ‘Ğ¾Ğ»Ğ´Ñ‹Ñ€ĞµĞ²Ğ°. ĞĞ± ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ°ÑĞ²Ğ¸Ğ» ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ Ğ½Ğ° Ğ¥I ÑÑŠĞµĞ·Ğ´Ğµ ĞšĞ Ğ ĞµĞ³Ğ¾ Ğ»Ğ¸Ğ´ĞµÑ€ Ğ”Ğ¼Ğ¸Ñ‚Ñ€Ğ¸Ğ¹ Ğ Ğ¾Ğ³Ğ¾Ğ·Ğ¸Ğ½. ĞĞ½ Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ³ Ñ€ĞµĞ·ĞºĞ¾Ğ¹\nĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞµ ĞšĞŸĞ Ğ¤, Ğ·Ğ°ÑĞ²Ğ¸Ğ², Ñ‡Ñ‚Ğ¾ \"Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ‚Ñ€Ğ¸Ğ¾Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ„Ñ€Ğ¾Ğ½Ñ‚Ğ° Ğ²ÑĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ²ÑÑ‚ÑƒĞ¿Ğ¸Ñ‚ÑŒ Ğ² ÑÑ‚Ñƒ \nĞ¿Ğ°Ñ€Ñ‚Ğ¸Ñ\". Ğ”Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ \"ĞÑ‚ĞµÑ‡ĞµÑÑ‚Ğ²Ğ¾\", Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼ Ğ Ğ¾Ğ³Ğ¾Ğ·Ğ¸Ğ½Ğ°, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ñ‚Ğ¸Ñ \"ĞµĞ»ÑŒÑ†Ğ¸Ğ½Ğ¸ÑÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ•Ğ»ÑŒÑ†Ğ¸Ğ½Ğ° Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑĞµÑ‚ \nĞ¿ÑƒÑ‚ÑŒ, ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğ¹ ĞĞ”Ğ \". Ğ’ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ¾Ğ² ĞšĞ Ğ Ğ”Ğ¼Ğ¸Ñ‚Ñ€Ğ¸Ğ¹ Ğ Ğ¾Ğ³Ğ¾Ğ·Ğ¸Ğ½ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ» \"Ğ±Ğ¾Ñ€ÑŒĞ±Ñƒ Ñ Ñ‚ĞµÑ€Ñ€Ğ¾Ñ€Ğ¾Ğ¼, ĞºĞ¾Ñ€Ñ€ÑƒĞ¿Ñ†Ğ¸ĞµĞ¹ Ğ¸ \nÑĞµĞ¿Ğ°Ñ€Ğ°Ñ‚Ğ¸Ğ·Ğ¼Ğ¾Ğ¼.\"  ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ´ĞµĞ»ĞµĞ³Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ»Ğ¸ Ğ·Ğ° ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ° ĞšĞ Ğ Ğ¸Ğ· ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ° ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… \nÑ‡Ğ»ĞµĞ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ \"ĞÑ‚ĞµÑ‡ĞµÑÑ‚Ğ²Ğ¾\".  ĞšĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑ ÑÑ‚Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, Ğ»Ğ¸Ğ´ĞµÑ€ ĞšĞ Ğ Ğ”Ğ¼Ğ¸Ñ‚Ñ€Ğ¸Ğ¹ Ğ Ğ¾Ğ³Ğ¾Ğ·Ğ¸Ğ½ Ğ·Ğ°ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ğ² Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ· \n\"ĞÑ‚ĞµÑ‡'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_len = 120\ngenerate_batches(test, seq_len = seq_len, step = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in generate_batches(test, seq_len = seq_len, step = 100):\n    print(len(batch))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences = []\nfor text in df['encoded text']:\n    batches = generate_batches(text, seq_len = seq_len, step = 400)\n    sequences.extend(batches)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of sequences: {len(sequences)}')\nprint(f'Example of sequence: {sequences[0]}')\nprint(f'Lenght of last sequence: {len(sequences[-1])}')  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sequences, test_sequences = train_test_split(\n    sequences,\n    test_size=0.3, \n    random_state=42\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sequences_array = []\nfor seq in train_sequences:\n    train_sequences_array.extend(seq)\n    \ntest_sequences_array = []\nfor seq in test_sequences:\n    test_sequences_array.extend(seq)\n    \nprint('for training len:', len(train_sequences_array))\nprint('for test len:', len(test_sequences_array))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sequences_array = np.array(train_sequences_array)\ntest_sequences_array = np.array(test_sequences_array)\nprint(train_sequences_array)\n\ntrain_char_dataset = tf.data.Dataset.from_tensor_slices(train_sequences_array)\ntrain_sequences = train_char_dataset.batch(seq_len+1, drop_remainder=True) \n\ntest_char_dataset = tf.data.Dataset.from_tensor_slices(test_sequences_array)\ntest_sequences = test_char_dataset.batch(seq_len+1, drop_remainder=True) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_seq_targets(seq):\n    input_txt = seq[:-1]\n    target_txt = seq[1:]\n    return input_txt, target_txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_sequences.map(create_seq_targets)\ntest_dataset = test_sequences.map(create_seq_targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\nbuffer_size = 10000\n\ntrain_dataset = train_dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\ntest_dataset = test_dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(vocab)\nvocab_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"def sparse_cat_loss(y_true,y_pred):\n    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(vocab_size, batch_size):\n    model = Sequential()\n    model.add(Embedding(vocab_size, 64, batch_input_shape=[batch_size, None]))\n    model.add(LSTM(512,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n    model.add(LSTM(512,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n    model.add(LSTM(512,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n    model.add(Dense(vocab_size))\n    model.compile(optimizer='adam', loss=sparse_cat_loss) \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_text(model, start_seed,num_generate=100,temperature=1.0):\n\n    input_eval = [char_to_ind[s] for s in start_seed]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    text_generated = []\n    model.reset_states()\n\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        predictions = tf.squeeze(predictions, 0)\n        predictions = predictions / temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n        input_eval = tf.expand_dims([predicted_id], 0)\n        text_generated.append(ind_to_char[predicted_id])\n\n    return (start_seed + ''.join(text_generated))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed(101)\nset_seed(101)\n\nes = EarlyStopping(monitor='val_loss',patience=4)\n\nmodel = create_model(\n    vocab_size = vocab_size,\n    batch_size=batch_size\n)\n\nmodel.compile(optimizer='adam', loss=sparse_cat_loss) \n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3 epochs","metadata":{}},{"cell_type":"code","source":"model.fit(\n    train_dataset,\n    epochs = 3,\n    validation_data = test_dataset,\n    callbacks=[es] \n)\n\nmodel.save('LSTM 3x512 3e.h5')\n\nhisto = pd.DataFrame(model.history.history)\nmetric = ['loss', 'val_loss']\nhisto[metric].plot()\nplt.title(' and '.join(metric))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pred = create_model(vocab_size, batch_size=1)\nmodel_pred.load_weights('LSTM 3x512 3e.h5')\nmodel_pred.build(tf.TensorShape([1, None]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 1\nprint(generate_text(model_pred,string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 0.5\nprint(generate_text(model_pred,string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 0.2\nprint(generate_text(model_pred,string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 0.1\nprint(generate_text(model_pred,string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 10 epochs","metadata":{}},{"cell_type":"code","source":"model.fit(\n    train_dataset,\n    epochs = 7,\n    validation_data = test_dataset,\n    callbacks=[es] \n)\n\nmodel.save('LSTM 3x512 10e.h5')\n\nhisto = pd.DataFrame(model.history.history)\nmetric = ['loss', 'val_loss']\nhisto[metric].plot()\nplt.title(' and '.join(metric))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pred10 = create_model(vocab_size, batch_size=1)\nmodel_pred10.load_weights('LSTM 3x512 10e.h5')\nmodel_pred10.build(tf.TensorShape([1, None]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 1\nprint(generate_text(model_pred10, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 0.5\nprint(generate_text(model_pred10, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 0.2\nprint(generate_text(model_pred10, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 0.1\nprint(generate_text(model_pred10, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 20 epochs","metadata":{}},{"cell_type":"code","source":"model.fit(\n    train_dataset,\n    epochs = 10,\n    validation_data = test_dataset,\n    callbacks=[es] \n)\n\nmodel.save('LSTM 3x512 20e.h5')\n\nhisto = pd.DataFrame(model.history.history)\nmetric = ['loss', 'val_loss']\nhisto[metric].plot()\nplt.title(' and '.join(metric))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pred20 = create_model(vocab_size, batch_size=1)\nmodel_pred20.load_weights('LSTM 3x512 20e.h5')\nmodel_pred20.build(tf.TensorShape([1, None]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 1\nprint(generate_text(model_pred20, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 0.5\nprint(generate_text(model_pred20, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 0.2\nprint(generate_text(model_pred20, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 0.1\nprint(generate_text(model_pred20, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 30 epochs","metadata":{}},{"cell_type":"code","source":"model.fit(\n    train_dataset,\n    epochs = 10,\n    validation_data = test_dataset,\n    callbacks=[es] \n)\n\nmodel.save('LSTM 3x512 30e.h5')\n\nhisto = pd.DataFrame(model.history.history)\nmetric = ['loss', 'val_loss']\nhisto[metric].plot()\nplt.title(' and '.join(metric))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pred30 = create_model(vocab_size, batch_size=1)\nmodel_pred30.load_weights('LSTM 3x512 30e.h5')\nmodel_pred30.build(tf.TensorShape([1, None]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 1\nprint(generate_text(model_pred30, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 0.5\nprint(generate_text(model_pred30, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 0.2\nprint(generate_text(model_pred30, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"Ğ\"\nsize = 10000\ntemp = 0.1\nprint(generate_text(model_pred30, string, size, temp))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}