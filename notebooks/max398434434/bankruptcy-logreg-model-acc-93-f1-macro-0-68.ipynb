{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport warnings\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score, learning_curve, ShuffleSplit\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures, OneHotEncoder, OrdinalEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.compose import make_column_transformer, make_column_selector, ColumnTransformer\nfrom sklearn.feature_selection import SelectPercentile, SelectorMixin\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom xgboost import XGBClassifier\n\npd.set_option('display.max_columns', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/company-bankruptcy-prediction/data.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Bankrupt?'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset is highly imbalanced","metadata":{}},{"cell_type":"code","source":"df[df.isnull().any(axis=1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.drop('Bankrupt?', axis = 1), \n                                                    df['Bankrupt?'], \n                                                    test_size=0.3, \n                                                    random_state=101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"markdown","source":"### Interactions and polinoms","metadata":{}},{"cell_type":"code","source":"class InterPolinomsFeatures(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, degree = 1, interaction_only = False):\n        self.degree = degree\n        self.interaction_only = interaction_only\n        self.encoder_inner = PolynomialFeatures(\n            degree = degree, \n            interaction_only = interaction_only,\n            include_bias = False\n        )\n        self.encoder = make_column_transformer(\n                    (self.encoder_inner,make_column_selector(dtype_exclude='int64')),\n                    remainder='passthrough'\n                )\n        \n    @staticmethod\n    def __convert_to_float(X):\n        X_copy = X.copy()\n        for name in X.columns:\n            if 'polynomialfeatures__' in name:\n                X_copy[name] = X_copy[name].astype(float)\n        return X_copy\n    \n    def __columns_name_change(self, old_name):\n        name = old_name.replace('polynomialfeatures__', '')\n        pattern_matches = re.findall(r'(?:\\b|_)(x\\d+\\b)', name)\n        for col_name in pattern_matches:\n            pattern_full = col_name + r'\\b'\n            feature_name = self.object_columns_dict[col_name]\n            name = re.sub(pattern_full, feature_name, name)\n        return name\n    \n    def fit(self, X, y = None):\n        X_copy = X.copy()\n        self.encoder.fit(X_copy)\n        object_columns = X_copy.select_dtypes(exclude='int64').columns\n        self.object_columns_dict = dict()\n        for i in enumerate(object_columns):\n            self.object_columns_dict[f'x{i[0]}'] = i[1]\n        return self\n                 \n    def transform(self, X, y = None):\n        X_copy = X.copy()\n        X_copy = self.encoder.transform(X_copy)\n        X_copy = pd.DataFrame(X_copy, columns = self.encoder.get_feature_names())\n        X_copy = self.__convert_to_float(X_copy)\n        X_copy.columns = [self.__columns_name_change(name) for name in X_copy.columns]\n        return X_copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InterPolinomsFeatures(\n    degree = 2, \n    interaction_only = False\n).fit_transform(X_train).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature selection","metadata":{}},{"cell_type":"code","source":"class Selector(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, percent = 50):\n        self.percent = percent\n        self.selector_inner = SelectPercentile(percentile=percent)\n        \n    def fit(self, X, y):\n        self.selector_inner.fit(X, y)\n        self.columns_names = X.columns[self.selector_inner.get_support()]\n        return self\n        \n    def transform(self, X, y = None):\n        X_copy = X.copy()\n        X_proc = self.selector_inner.transform(X_copy)\n        X_proc = pd.DataFrame(X_proc, columns = self.columns_names)\n        return X_proc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Selector(percent = 5).fit_transform(X_train, y_train).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scaling","metadata":{}},{"cell_type":"code","source":"class Scaler(BaseEstimator, TransformerMixin):\n    \n    scalers = {\n        'standart': StandardScaler(), \n        'minmax'  : MinMaxScaler(),\n        'none'    : None\n    }\n    \n    def __init__(self, mode = 'minmax'):\n        if mode in self.scalers.keys():\n            self.mode = mode\n            self.scaler_inner = self.scalers[self.mode]\n        else:\n            raise AttibuteError('Wrong mode name')\n        \n    def fit(self, X, y = None):\n        if self.mode != 'none':\n            self.scaler_inner.fit(X)\n            self.columns_names = X.columns\n        return self\n        \n    def transform(self, X, y = None):\n        X_copy = X.copy()\n        if self.mode != 'none':\n            X_copy = self.scaler_inner.transform(X_copy)\n            X_copy = pd.DataFrame(X_copy, columns = self.columns_names)\n        return X_copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Scaler(mode = 'minmax').fit_transform(X_train).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Scaler(mode = 'none').fit_transform(X_train).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"def plot_learning_curve(estimator, X, y, axes=None, ylim=None, cv=5,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    '''\n    https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\n    '''\n\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title('Learning curve')\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n    plt.show()\n\n\n\ndef precision_recall(model, X_test, y_test):\n    precision, recall, thresholds = precision_recall_curve(\n        y_test, \n        model.decision_function(X_test)\n    )\n    close_zero = np.argmin(np.abs(thresholds))\n    plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n     label=\"threshold 0\", fillstyle=\"none\", c='k', mew=2)\n    plt.plot(precision, recall, label=\"precision recall curve\")\n    plt.xlabel(\"Precision\")\n    plt.ylabel(\"Recall\")\n    plt.legend(loc=\"best\")\n    plt.show()\n\n\ndef eval_result(model, X_test, y_test, X_train, y_train, validation = False):\n    if type(model) == GridSearchCV:\n        model = model.best_estimator_\n    pipeline = False\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        pred = model.predict(X_test)\n        print(classification_report(y_test, pred, target_names = ['No Bankruptcy', 'Bankruptcy']))\n        display(pd.DataFrame(confusion_matrix(y_test, pred), \n                         columns = ['Bankruptcy Not Predicted', 'Bankruptcy Predicted'],\n                         index = ['No Bankruptcy', 'Bankruptcy']))\n        \n        if type(model) == Pipeline:\n            pipeline = True\n            pipe = model[:-1]\n            model = model[-1]\n        \n        if pipeline:\n            X_test = pipe.transform(X_test)\n            X_train = pipe.transform(X_train)\n            \n        if ((hasattr(model, 'feature_importances_') \n        or hasattr(model, 'coef_')) \n            and not validation):\n            try:\n                model_feat_imp = model.feature_importances_\n            except:\n                model_feat_imp = [abs(i) for i in model.coef_[0]]\n            \n                \n            features = pd.DataFrame({\n                'Variable'  :X_test.columns,\n                'Importance':model_feat_imp\n            })\n            features.sort_values('Importance', ascending=False, inplace=True)\n            display(features.head(20))\n        if not validation:\n            try:\n                precision_recall(model, X_test, y_test)\n                plot_learning_curve(model, X_train, y_train, n_jobs=-1)\n            except:\n                pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = Pipeline([\n    ('interpolinomsfeatures', InterPolinomsFeatures()),\n    ('selector', Selector()),\n    ('scaler', Scaler()),\n    ('classifier', LogisticRegression())\n])\npipe","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = [\n    {\n        'classifier': [LogisticRegression(random_state = 1)],\n        'interpolinomsfeatures__interaction_only': [True, False],\n        'interpolinomsfeatures__degree': [1, 2, 3],\n        'selector__percent': [10, 30, 50, 100],\n        'scaler__mode': ['standart', 'minmax'],\n        'classifier__class_weight': [\n            {0:1, 1:4},\n            {0:1, 1:5},\n            {0:1, 1:6},\n            {0:1, 1:7},\n            {0:1, 1:8},\n            {0:1, 1:10},\n            {0:1, 1:12},\n            {0:1, 1:15},\n        ],\n        'classifier__C': [0.1, 0.5, 0.7, 1, 2, 5]\n              },\n    {\n        'classifier': [XGBClassifier(random_state = 1,eval_metric = 'logloss')],\n        'interpolinomsfeatures__interaction_only': [True, False],\n        'interpolinomsfeatures__degree': [1, 2, 3],\n        'selector__percent': [100],\n        'scaler__mode': ['none'],\n        'classifier__scale_pos_weight': [1, 2, 3, 4 ,5, 7, 8, 9, 10],\n              }\n             ]\nparam_grid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe[:-1].fit_transform(X_train, y_train).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid = GridSearchCV(\n        pipe, \n        param_grid=param_grid, \n        cv=5, \n        n_jobs = -1, \n        #verbose = 2,\n        scoring = 'f1_macro'\n    )\ngrid.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best and worst parameters combination:","metadata":{}},{"cell_type":"code","source":"model_results = pd.DataFrame(grid.cv_results_)\nmodel_results.sort_values(by='mean_test_score', ascending = False, inplace = True)\n\ndisplay(model_results.head(4))\ndisplay(model_results.tail(4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Best cross val score: {grid.best_score_}\")\nprint(f\"\\nBest params:\")\nfor param, val in grid.best_params_.items():\n    print(f'{param}: {val}')\nprint('\\n')\neval_result(grid, X_test, y_test, X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}