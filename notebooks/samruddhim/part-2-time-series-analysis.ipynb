{"cells":[{"metadata":{},"cell_type":"markdown","source":"Let us continue from where we left at [Part 1: Expolratory Data Analysis](https://www.kaggle.com/samruddhim/part-1-exploratory-data-analysis)"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, we will perform time series analysis to get the sales for forecasting for next 7 days."},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the necessary Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the train dataset \ndf = pd.read_csv('../input/sales-forecasting/train.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understanding the distribution of the concerned data. This will display information about numeric columns only.\ndf.describe()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Understanding the type of data in every columns of the data set that we will be dealing with.\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the column 'Row ID', as it does not help much in the process of data analysis of the dataset.\ndf.drop('Row ID',axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Order Date'] = pd.to_datetime(df['Order Date'], format='%d/%m/%Y') #converting the data type of 'Order Date' column to date time format\ndf['Ship Date'] = pd.to_datetime(df['Ship Date'], format='%d/%m/%Y') #converting the data type of 'Ship Date' column to date time format\ndf.info() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Order Date Description\\n') \nprint(df['Order Date'].describe()) #Displays the distribution of dates in 'Order Data' column\nprint('\\nShip Date Description\\n')\nprint(df['Ship Date'].describe()) #Displays the distribution of dates in 'Ship Data' column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sorting data by order date\ndf.sort_values(by=['Order Date'], inplace=True, ascending=True) #Sorting data by  ascending order of the coloumn values 'Order Date'\ndf.set_index(\"Order Date\", inplace = True) #Setting 'Order Date' as index of the dataframe 'df' for ease of Time Series Analysis\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To forecast sales seven days later of the order date, let us create a new dataframe with only the target column i.e, \n# the 'Sales' column and 'Order Date' as the index \n\nnew_data = pd.DataFrame(df['Sales'])\nnew_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the data to understand the sales distribution from the year 2015-2018\nnew_data.plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A series is said to be stationary when its mean and variance do not change over time. From the above distribution of the sales it is not clear whether the sales distribution is stationary or not. Let us perform some stationarity tests to check whether the time series is stationary or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Checkting for Stationarity\nnew_data =  pd.DataFrame(new_data['Sales'].resample('D').mean())\nnew_data = new_data.interpolate(method='linear') #The interpolate() function is used to interpolate values according to \n#different methods. It ignore the index and treats the values as equally spaced.\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Method 1\n# To check for stationarity by comparing the change in mean and variance over time, let us split the data into train, test and validate.\ntrain, test, validate = np.split(new_data['Sales'].sample(frac=1), [int(.6*len(new_data['Sales'])),int(.8*len(new_data['Sales']))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Dataset')\nprint(train)\nprint('Test Dataset')\nprint(test)\nprint('Validate Dataset')\nprint(validate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean1, mean2, mean3 = train.mean(), test.mean(), validate.mean() #taking mean of train, test and validate data\nvar1, var2, var3 = train.var(), test.var(), validate.var() #taking variance of train, test and validate data\n\nprint('Mean:')\nprint(mean1, mean2, mean3)\nprint('Variance:')\nprint(var1, var2, var3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above values of mean and variance, it can be inferred that their is not much difference in the three values of mean and variance, indicating that the series is stationary. However, to verify our observations, let us perform a standard stationarity test, called Augmented Dicky Fuller test."},{"metadata":{},"cell_type":"markdown","source":"**Augmented Dicky Fuller test**\n\n\n\n* The Augmented Dickey-Fuller test is a type of statistical test alsocalled a unit root test.The base of unit root test is that it helps in determining how strongly a time series is defined by a trend.\n\n* The null hypothesis of the test is that the time series can be represented by a unit root, that it is not stationary. The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.\n\n    1. Null Hypothesis(H0): Time series is not stationary\n    2. Alternate Hypothesis (H1): Time series is stationary\n\n* This result is interpreted using the p-value from the test. \n\n   1. p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n   2. p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Method 2\n# Augmented Dicky Fuller Test\n\nfrom statsmodels.tsa.stattools import adfuller #importing adfuller tool from statsmodels\n#statsmodels provide adfuller() fucntion to implement stationarity test of a time series\n\nadf = adfuller(new_data) \n\nprint(adf)\nprint('\\nADF = ', str(adf[0])) #more towards negative value the better\nprint('\\np-value = ', str(adf[1]))\nprint('\\nCritical Values: ')\n\nfor key, val in adf[4].items(): #for loop to print the p-value (1%, 5% and 10%) and their respective values\n    print(key,':',val)\n    \n\n    if adf[0] < val:\n        print('Null Hypothesis Rejected. Time Series is Stationary')\n    else:\n        print('Null Hypothesis Accepted. Time Series is not Stationary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pylab import rcParams\nrcParams['figure.figsize'] = 20, 10\n\nimport statsmodels.api as sm\ndecomposition = sm.tsa.seasonal_decompose(new_data, model='additive') #function used to decompose Time Series Data into Trend and Seasonality\nfig = decomposition.plot()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we know our time series is data is stationary. Let us begin with model training for forecasting the sales.\nWe have chosen SARIMA model to forecast the sales.\n\nSeasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that supports univariate time series data with a seasonal component.\n\nSARIMA requires selecting hyperparameters for both the trend and seasonal elements of the series.\n\n1. Trend Elements\nThere are three trend elements that require configuration.\n\np: Trend autoregression order.\nd: Trend difference order.\nq: Trend moving average order.\n\n2. Seasonal Elements\nThere are four seasonal elements:\n\nP: Seasonal autoregressive order.\nD: Seasonal difference order.\nQ: Seasonal moving average order.\nm: The number of time steps for a single seasonal period.\n\nThe notation for a SARIMA model is specified as:\nSARIMA(p,d,q)(P,D,Q)m"},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\np = d = q = range(0, 2) \npdq = list(itertools.product(p, d, q))\nseasonal_pdq_comb = [(i[0], i[1], i[2], 12) for i in list(itertools.product(p, d, q))] #for loop for creating combinations of seasonal parameters of SARIMA\nprint('Examples of parameter combinations for Seasonal ARIMA:')\nprint('SARIMA: {} x {}'.format(pdq[1], seasonal_pdq_comb[1]))\nprint('SARIMA: {} x {}'.format(pdq[1], seasonal_pdq_comb[2]))\nprint('SARIMA: {} x {}'.format(pdq[2], seasonal_pdq_comb[3]))\nprint('SARIMA: {} x {}'.format(pdq[2], seasonal_pdq_comb[4]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for parameters in pdq: #for loop for determining the best combination of seasonal parameters for SARIMA\n    for seasonal_param in seasonal_pdq_comb:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(new_data,\n                                            order=parameters,\n                                            seasonal_param_order=seasonal_param,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False) #determines the AIC value of the model**\n            results = mod.fit()\n            print('SARIMA{}x{}12 - AIC:{}'.format(parameters, seasonal_param, results.aic))\n        except:\n            continue\n\n# **The Akaike information criterion (AIC) is an estimator of out-of-sample prediction error and thereby relative \n# quality of statistical models for a given set of data. AIC estimates the relative amount of information lost \n# by a given model. The less information a model loses, the higher the quality of that model.        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After choosing the combination of seasonal parameters with least AIC value, let us train the SARIMA model\nmod = sm.tsa.statespace.SARIMAX(new_data,\n                                order=(1, 1, 1),\n                                seasonal_order=(1, 1, 1, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False) #model defintion\nresults = mod.fit() #model fitting\nprint(results.summary().tables[1]) # displaying the result ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.plot_diagnostics(figsize=(16, 8)) #Produces a plot grid of: 1. Standardized residuals over time \n# 2. Histogram plus estimated density of standardized residulas and along with a Normal(0,1) density plotted for reference.\n# 3. Normal Q-Q plot, with Normal reference line and, 4. Correlogram.\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = results.get_prediction(start=pd.to_datetime('2015-01-03'), dynamic=False) # variable to display plot for predicted values\npred_val = pred.conf_int()\nax = new_data['2014':].plot(label='observed') # displays plot for original values\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7)) # displays plot for predicted values\nax.fill_between(pred_val.index,\n                pred_val.iloc[:, 0],\n                pred_val.iloc[:, 1], color='k', alpha=.2)\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_forecasted = pred.predicted_mean\ny_truth = new_data['Sales']\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nmse = mean_squared_error(y_forecasted, y_truth)\nrmse = sqrt(mse)\nprint('The Mean Squared Error of the forecasts is {}'.format(round(rmse, 2))) # displays the root mean squared error of the forecast with rounding it up to 2 decimals","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Out of Sample forecast:\n\nTo forecast sales values after some time period of the given data. In our case, we have to forecast sales with time period of 7 days."},{"metadata":{"trusted":true},"cell_type":"code","source":"# mod = sm.tsa.statespace.SARIMAX(new_data,\n#                                 order=(1, 1, 1),\n#                                 seasonal_order=(1, 1, 1, 12),\n#                                 enforce_stationarity=False,\n#                                 enforce_invertibility=False) #model defintion\n# results = mod.fit() #model fitting\n\nforecast = results.forecast(steps=7) # making a forecast of 7 days later of the last date in the 'Order Date' column\nprint(forecast.astype('int')) #displays the sales forecast as type integer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = forecast.astype('int') #saving the sales values as type integer\nforecast_df = forecast.to_frame() # forecast is in Series form, converting it to DataFrame\nforecast_df.reset_index(level=0, inplace=True) # converting the index to column\nforecast_df.columns = ['Prediction Date', 'Predicted Sales'] # giving appropriate names to the output columns\nprediction = pd.DataFrame(forecast_df).to_csv('prediction.csv',index=False) # saving the output as a csv file with name 'prediction.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**If you like my notebook, kindly upvote it.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}