{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" <div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\">\n    \n     Welcome\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           background-color:Beige;\n           font-size:110%;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:black;\">\n    \nHello Kagglers, <br>\n\nIn this notebook, I am going to predict whether it is going to rain in australia. But, first I am going to do deal with missing values in the dataset and then perform exploratory data analysis and learn more about the features. Then, I am going to use H2O classification models on our dataset and select the best performing one. <br>\n    So, let's get started.\n</p>\n</div> ","metadata":{}},{"cell_type":"markdown","source":"### **Table of Contents:**\n1. [Importing Libraries](#1)<a href='1' ></a> <br>\n2. [Importing Dataset](#2)<a href='2' ></a> <br>\n3. [Taking care of Missing Values](#3)<a href='3' ></a> <br>\n    3.1. [Visualizing Missing Values](#3.1)<a href='3.1' ></a> <br>\n    3.2. [Categorical Data](#3.2)<a href='3.2' ></a> <br>\n    3.3. [Numerical Data](#3.3)<a href='3.3' ></a> <br>\n4. [Data Visualization and Analysis](#4)<a href='4' ></a> <br>\n    4.1. [Heat Map Correlation](#4.1)<a href='4.1' ></a> <br> \n    4.2. [Distribution Plot](#4.2)<a href='4.2' ></a> <br>\n    4.3. [Date Plot](#4.3)<a href='4.3' ></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; a. [MinTemp and MaxTemp](#4.3.1)<a href='4.3.1' ></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; b. [Rainfall](#4.3.2)<a href='4.3.2' ></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; c. [WindGustSpeed](#4.3.3)<a href='4.3.3' ></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; d. [WindSpeed9am and WindSpeed3pm](#4.3.4)<a href='4.3.4' ></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; e. [Humidity9am and Humidity3pm](#4.3.5)<a href='4.3.5' ></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; f. &nbsp;[Pressure9am and Pressure3am](#4.3.6)<a href='4.3.6' ></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; g. [Cloud9am and Cloud3am](#4.3.7)<a href='4.3.7' ></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; h. [Temp9am and Temp3pm](#4.3.8)<a href='4.3.8' ></a> <br>\n    4.4. [Outliers](#4.4)<a href='4.4' ></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; a. [Detection](#4.4.1)<a href='4.4.1' ></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; b. [Removal](#4.4.2)<a href='4.4.2' ></a> <br>\n    4.5. [Pair Plot](#4.5)<a href='4.5' ></a> <br>\n5. [Data Preprocessing](#5)<a href='5' ></a> <br>\n    5.1. [Label Encoding](#5.1)<a href='5.1' ></a> <br>\n    5.2. [Splitting data into Train and Test Set](#5.2)<a href='5.2' ></a> <br>\n    5.3. [BorderlineSMOTE and RandomUnderSampler](#5.3)<a href='5.3' ></a> <br>\n6. [H2O](#6)<a href='6' ></a> <br>\n    6.1. [H2OGeneralizedLinearEstimator](#6.1)<a href='6.1' ></a> <br>\n    6.2. [H2ORandomForestEstimator](#6.2)<a href='6.2' ></a> <br>\n    6.3. [H2OGradientBoostingEstimator](#6.3)<a href='6.3' ></a> <br>\n    6.4. [H2ODeepLearningEstimator](#6.4)<a href='6.4' ></a> <br>\n    6.5. [H2OStackedEnsembleEstimator](#6.5)<a href='6.5' ></a> <br>\n7. [Conclusion](#7)<a href='7' ></a> <br>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n            letter-spacing:0.5px\"> <a id='1'></a>\n    \n    Importing Libraries \n</div>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '2'></a>\n    \n    Importing Dataset \n</div>","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå We have 145460 rows and 23 columns in our dataset. <br>\nüìå We can see that the dataset contains mixture of *categorical* and *numerical* variables. <br>\nüìå Also, there are some missing values in the dataset. Let's check it out. ","metadata":{}},{"cell_type":"code","source":"dataset.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå Wow! There are lot of missing values in our dataset.","metadata":{}},{"cell_type":"code","source":"dataset.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå The above command *df.describe()* helps us to view the statistical properties of numerical variables. It excludes character variables.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '3'></a>\n\n    Taking care of Missing Values \n</div>","metadata":{}},{"cell_type":"markdown","source":"## **Visualizing Missing Values** <a id='3.1' ></a>","metadata":{}},{"cell_type":"code","source":"import missingno as msno\nmsno.matrix(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.bar(dataset, sort= 'descending')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå From the above plot, we can see that *Cloud9am*, *Cloud3pm*, *Evaporation*, and *Sunshine* have a lot of missing values.","metadata":{}},{"cell_type":"markdown","source":"üìå Splitting our dataset in Categorical and Numerical values.","metadata":{}},{"cell_type":"code","source":"categorical = [i for i in dataset.columns if dataset[i].dtype=='object']\ncategorical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical = [i for i in dataset.columns if dataset[i].dtype=='float64']\nnumerical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[categorical].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Categorical Data** <a id='3.2' ></a>","metadata":{}},{"cell_type":"markdown","source":"### Date ","metadata":{}},{"cell_type":"markdown","source":"üìå Converting Date into datetime and then splitting it into Day, Month and Year columns.","metadata":{}},{"cell_type":"code","source":"dataset['Date'] = pd.to_datetime(dataset['Date'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['Day'] = dataset['Date'].dt.day\ndataset.Day","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['Month'] = dataset['Date'].dt.month\ndataset.Month","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['Year'] = dataset['Date'].dt.year\ndataset.Year","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Location ","metadata":{}},{"cell_type":"markdown","source":"üìå There are no missing values in Location but let's visualize the unique values in it. ","metadata":{}},{"cell_type":"code","source":"dataset.Location.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.Location.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå We can see that the number of unique values in Location variable is 49.","metadata":{}},{"cell_type":"code","source":"dataset.Location.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use(\"classic\")\nsns.countplot(data=dataset, x=\"Location\")\nplt.grid(linewidth = 0.7)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### WindGustDir ","metadata":{}},{"cell_type":"code","source":"dataset.WindGustDir.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå We can see that the number of unique values in WindGustDir variable is 16 and NaN values.","metadata":{}},{"cell_type":"code","source":"dataset.WindGustDir.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå Replacing the missing values with the most frequent values.","metadata":{}},{"cell_type":"code","source":"dataset['WindGustDir'].fillna(dataset['WindGustDir'].mode()[0], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.WindGustDir.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use(\"classic\")\nsns.countplot(data=dataset, x=\"WindGustDir\")\nplt.grid(linewidth = 0.7)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### WindDir9am ","metadata":{}},{"cell_type":"code","source":"dataset.WindDir9am.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå We can see that the number of unique values in WindDir9am variable is 16 and NaN values.","metadata":{}},{"cell_type":"code","source":"dataset.WindDir9am.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå Replacing the missing values with the most frequent values.","metadata":{}},{"cell_type":"code","source":"dataset['WindDir9am'].fillna(dataset['WindDir9am'].mode()[0], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.WindDir9am.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use(\"classic\")\nsns.countplot(data=dataset, x=\"WindDir9am\")\nplt.grid(linewidth = 0.7)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### WindDir3pm","metadata":{}},{"cell_type":"code","source":"dataset.WindDir3pm.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå We can see that the number of unique values in WindDir3pm variable is 16 and NaN values.","metadata":{}},{"cell_type":"code","source":"dataset.WindDir3pm.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå Replacing the missing values with the most frequent values.","metadata":{}},{"cell_type":"code","source":"dataset['WindDir3pm'].fillna(dataset['WindDir3pm'].mode()[0], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.WindDir3pm.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use(\"classic\")\nsns.countplot(data=dataset, x=\"WindDir3pm\")\nplt.grid(linewidth = 0.7)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RainToday","metadata":{}},{"cell_type":"code","source":"dataset.RainToday.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå We can see that the number of unique values in RainToday variable is 2 and NaN values.","metadata":{}},{"cell_type":"code","source":"dataset.RainToday.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå Replacing the missing values with the most frequent values.","metadata":{}},{"cell_type":"code","source":"dataset['RainToday'].fillna(dataset['RainToday'].mode()[0], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.RainToday.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use(\"classic\")\nsns.countplot(data=dataset, x=\"RainToday\")\nplt.grid(linewidth = 0.7)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RainTomorrow","metadata":{}},{"cell_type":"code","source":"dataset.RainTomorrow.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå We can see that the number of unique values in RainTomorrow variable is 2 and NaN values.","metadata":{}},{"cell_type":"code","source":"dataset.RainTomorrow.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå Replacing the missing values with the most frequent values.","metadata":{}},{"cell_type":"code","source":"dataset['RainTomorrow'].fillna(dataset['RainTomorrow'].mode()[0], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.RainTomorrow.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use(\"classic\")\nsns.countplot(data=dataset, x=\"RainTomorrow\")\nplt.grid(linewidth = 0.7)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå Above plot shows that our data seems imbalance. We will take care of it later.","metadata":{}},{"cell_type":"code","source":"dataset[categorical].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå As you can see there are no missing values now.","metadata":{}},{"cell_type":"markdown","source":"## **Numerical Data** <a id='3.2' ></a>","metadata":{}},{"cell_type":"markdown","source":"üìå Missing values in numerical data.","metadata":{}},{"cell_type":"code","source":"dataset[numerical].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå Imputing missing values in numerical data with the median.","metadata":{}},{"cell_type":"code","source":"for i in dataset:\n    if dataset[i].dtype=='float64':\n        dataset[i].replace(to_replace=np.nan, value=dataset[i].median(), inplace=True)\n    else:\n        continue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå We can see that there are no missing values present now in our dataset. ","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n            letter-spacing:0.5px\"> <a id='4'></a>\n    \n    Data Visualization and Analysis \n</div>","metadata":{}},{"cell_type":"markdown","source":"## **Heat Map Correlation** <a id='4.1' ></a>","metadata":{}},{"cell_type":"code","source":" plt.style.use('fivethirtyeight')\n# Compute the correlation matrix\ncorr = dataset.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(8, 8))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,12))\nax = sns.heatmap(corr, square=True, annot=True, fmt='.2f')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)          \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above correlation map we can see that: <br>\nüìå *MinTemp* and *MaxTemp* features are highly correlated (correlation coefficient = 0.73). <br>\nüìå *MinTemp* and *Temp9am* features are highly correlated (correlation coefficient = 0.90). <br>\nüìå *MinTemp* and *Temp3pm* features are highly correlated (correlation coefficient = 0.70). <br>\nüìå *MaxTemp* and *Temp9am* features are highly correlated (correlation coefficient = 0.88). <br>\nüìå *MaxTemp* and *Temp3pm* features are highly correlated (correlation coefficient = 0.97). <br>\nüìå *Pressure9am* and *Pressure3pm* features are highly correlated (correlation coefficient = 0.96). <br>\nüìå *Temp9am* and *Temp3pm* features are highly correlated (correlation coefficient = 0.85).","metadata":{}},{"cell_type":"markdown","source":"## **Distribution Plot** <a id='4.2' ></a>","metadata":{}},{"cell_type":"code","source":"#  plot Numerical Data\na = 4  # number of rows\nb = 4  # number of columns\nc = 1  # initialize plot counter\nfig = plt.figure(figsize=(35,32))\nfor i in dataset:\n    if dataset[i].dtype=='float64':\n        plt.subplot(a, b, c)\n        sns.distplot(dataset[i])\n        c = c+1\n    else:\n        continue\nplt.tight_layout()\nplt.show()       ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå From above distribution plot, we can see that certain features are same around few parts of x-axis like WindSpeed9am and WindSpeed3pm is same at 15-20 etc.  <br>","metadata":{}},{"cell_type":"markdown","source":"## **Date Plot** <a id='4.3' ></a>","metadata":{}},{"cell_type":"markdown","source":"üìå We are going to plot features with datetime. Here, I am going to use date from last 3 years.","metadata":{}},{"cell_type":"markdown","source":"### MinTemp and MaxTemp <a id='4.3.1' ></a>","metadata":{}},{"cell_type":"code","source":"#Soure: https://www.kaggle.com/momincks/rain-tomorrow-in-aus-by-xgboostclassifier : Do check him out.\n\ndf_dateplot = dataset.iloc[-950:,:]\nplt.figure(figsize=[20,5])\nplt.plot(df_dateplot['Date'],df_dateplot['MinTemp'],color='blue',linewidth=1, label= 'MinTemp')\nplt.plot(df_dateplot['Date'],df_dateplot['MaxTemp'],color='red',linewidth=1, label= 'MaxTemp')\nplt.fill_between(df_dateplot['Date'],df_dateplot['MinTemp'],df_dateplot['MaxTemp'], facecolor = '#EBF78F')\nplt.title('MinTemp vs MaxTemp by Date')\nplt.legend(loc='lower left', frameon=False)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå Above plot shows that the *MinTemp* and *MaxTemp* relatively increases and decreases every year. <br>\nüìå The weather conditions are always opposite in the two hemispheres. As, the Australia is situated in the southern hemisphere. The seasons are bit different. <br>\nüìå As you can see that, December to February is summer; March to May is autumn; June to August is winter; and September to November is spring.","metadata":{}},{"cell_type":"markdown","source":"### Rainfall <a id='4.3.2' ></a>","metadata":{}},{"cell_type":"code","source":"df_dateplot = dataset.iloc[-950:,:]\nplt.figure(figsize=[20,5])\nplt.plot(df_dateplot['Date'],df_dateplot['Rainfall'],color='violet', linewidth=2, label= 'Rainfall')\nplt.legend(loc='upper left', frameon=False)\nplt.title('Rainfall by Date')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå Being situated in southern hemisphere, the majority of rainfall occurs between December and March. <br>\nüìå As you can see from above plot, we can see that Dec-Jan does get a lot of rainfall but there are months like Jun-Jul when rainfall occurs too.","metadata":{}},{"cell_type":"markdown","source":"### WindGustSpeed <a id='4.3.3' ></a>","metadata":{}},{"cell_type":"code","source":"df_dateplot = dataset.iloc[-950:,:]\nplt.figure(figsize=[20,5])\nplt.plot(df_dateplot['Date'],df_dateplot['WindGustSpeed'],color='violet', linewidth=2, label= 'WindGustSpeed')\nplt.legend(loc='upper left', frameon=False)\nplt.title('WindGustSpeed by Date')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå In Australia, wind speed is usually moderate. But, from above plot we can see that Dec-Feb is the windiest months.","metadata":{}},{"cell_type":"markdown","source":"### WindSpeed9am and WindSpeed3pm <a id='4.3.4' ></a>","metadata":{}},{"cell_type":"code","source":"df_dateplot = dataset.iloc[-950:,:]\nplt.figure(figsize=[20,5])\nplt.plot(df_dateplot['Date'],df_dateplot['WindSpeed9am'],color='blue', linewidth=2, label= 'WindSpeed9am')\nplt.plot(df_dateplot['Date'],df_dateplot['WindSpeed3pm'],color='green', linewidth=2, label= 'WindSpeed3pm')\nplt.legend(loc='upper left', frameon=False)\nplt.title('WindSpeed9am vs WindSpeed3pm by Date')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå *WindSpeed9am* and *WindSpeed3pm* are relatively same around certain months.","metadata":{}},{"cell_type":"markdown","source":"### Humidity9am and Humidity3pm <a id='4.3.5' ></a>","metadata":{}},{"cell_type":"code","source":"df_dateplot = dataset.iloc[-950:,:]\nplt.figure(figsize=[20,5])\nplt.plot(df_dateplot['Date'],df_dateplot['Humidity9am'],color='blue', linewidth=2, label= 'Humidity9am')\nplt.plot(df_dateplot['Date'],df_dateplot['Humidity3pm'],color='green', linewidth=2, label= 'Humidity3pm')\nplt.legend(loc='upper left', frameon=False)\nplt.title('Humidity9am vs Humidity3pm by Date')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå From above plot we can see that the Humidity is high around Jun-Jul and also during that time, there is good difference between humidity around 9am and 3pm. ","metadata":{}},{"cell_type":"markdown","source":"### Pressure9am and Pressure3am <a id='4.3.6' ></a>","metadata":{}},{"cell_type":"code","source":"df_dateplot = dataset.iloc[-950:,:]\nplt.figure(figsize=[20,5])\nplt.plot(df_dateplot['Date'],df_dateplot['Pressure9am'],color='blue', linewidth=2, label= 'Pressure9am')\nplt.plot(df_dateplot['Date'],df_dateplot['Pressure3pm'],color='green', linewidth=2, label= 'Pressure3pm')\nplt.legend(loc='upper left', frameon=False)\nplt.title('Pressure9am vs Pressure3pm by Date')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå Pressure is high around the months of Jun-Aug and around Dec-Jan you can see that the pressure is low. <br>\nüìå In a low pressure area the rising air cools and this is likely to condense water vapour and form clouds, and consequently rain. <br>","metadata":{}},{"cell_type":"markdown","source":"### Cloud9am and Cloud3am <a id='4.3.7' ></a>","metadata":{}},{"cell_type":"code","source":"df_dateplot = dataset.iloc[-950:,:]\nplt.figure(figsize=[20,5])\nplt.plot(df_dateplot['Date'],df_dateplot['Cloud9am'],color='blue', linewidth=2, label= 'Cloud9am')\nplt.plot(df_dateplot['Date'],df_dateplot['Cloud3pm'],color='green', linewidth=2, label= 'Cloud3pm')\nplt.legend(loc='upper left', frameon=False)\nplt.title('Cloud9am vs Cloud3pm by Date')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå Cloud is the same at 5 all-around years but, there are certain months when it falls or rises. ","metadata":{}},{"cell_type":"markdown","source":"### Temp9am and Temp3pm <a id='4.3.8' ></a>","metadata":{}},{"cell_type":"code","source":"df_dateplot = dataset.iloc[-950:,:]\nplt.figure(figsize=[20,5])\nplt.plot(df_dateplot['Date'],df_dateplot['Temp9am'],color='blue', linewidth=2, label= 'Temp9am')\nplt.plot(df_dateplot['Date'],df_dateplot['Temp3pm'],color='green', linewidth=2, label= 'Temp3pm')\nplt.legend(loc='lower left', frameon=False)\nplt.title('Temp9am vs Temp3pm by Date')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå As I mentioned in the above plots, that Dec-Jan are months when the temperature is high but these are the months when the difference between temperature around 9am and 3pm is less as compare to the months of Jun-Aug when the difference is high.","metadata":{}},{"cell_type":"code","source":"dataset.drop(['Date'], axis=1, inplace=True)\ncategorical.remove('Date')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Outliers** <a id='4.4' ></a>","metadata":{}},{"cell_type":"markdown","source":"üìå An outlier is an observation that lies an abnormal distance from other values in a random sample from a population.","metadata":{}},{"cell_type":"markdown","source":"### Detection <a id='4.4.1' ></a>","metadata":{}},{"cell_type":"markdown","source":"üìå In this notebook, we are using Box Plot to detect the outliers of each features in our dataset, where any point above or below the whiskers represent an outlier. This is also known as ‚ÄúUnivariate method‚Äù as here we are using one variable outlier analysis. <br>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=[25,15])\ndataset.boxplot(column= ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm'])\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removal <a id='4.4.2' ></a>","metadata":{}},{"cell_type":"markdown","source":"üìå After detecting, we are using Median Imputation to take care of outliers. In this technique, we replace the extreme values with median values. <br>\nüìå It is represented by the formula IQR = Q3 ‚àí Q1. The lines of code below calculate and print the interquartile range for each of the variables in the dataset. <br>\nüìå It is advised to not use mean values as they are affected by outliers.","metadata":{}},{"cell_type":"code","source":"for i in dataset:\n    if dataset[i].dtype=='float64':\n        q1 = dataset[i].quantile(0.25)\n        q3 = dataset[i].quantile(0.75)\n        iqr = q3-q1\n        Lower_tail = q1 - 1.5 * iqr\n        Upper_tail = q3 + 1.5 * iqr\n        med = np.median(dataset[i])\n        for j in dataset[i]:\n            if j > Upper_tail or j < Lower_tail:\n                dataset[i] = dataset[i].replace(j, med)\n    else:\n        continue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=[25,15])\ndataset.boxplot(column= ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm'])\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pair Plot <a id='4.5' ></a>","metadata":{}},{"cell_type":"code","source":"sns.pairplot(data=dataset)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå From the above graphs, we can see that the outliers in our dataset have been taken care of.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '5'></a>\n    \n    Data Preprocessing \n</div>","metadata":{}},{"cell_type":"markdown","source":"## **Label Encoding** <a id='5.1'></a>","metadata":{}},{"cell_type":"markdown","source":"üìå Now, I am going to encode categorical data. Label encode will convert target labels with value between 0 and n_classes-1.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le =  LabelEncoder()\nfor i in dataset:\n    if dataset[i].dtype=='object':\n        dataset[i] = le.fit_transform(dataset[i])\n    else:\n        continue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå As you can see, *object* data type is converted to *int64* data type.","metadata":{}},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Splitting data into Train and Test Set** <a id='5.2'></a>","metadata":{}},{"cell_type":"code","source":"x = dataset.drop('RainTomorrow', axis=1).values\ny = dataset['RainTomorrow']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **BorderlineSMOTE and RandomUnderSampler** <a id='5.3'></a>","metadata":{}},{"cell_type":"markdown","source":"üìå BorderlineSMOTE is a popular extension to SMOTE involves selecting those instances of the minority class that are misclassified, such as with a k-nearest neighbor classification model. <br>\nüìå Instead of generating new synthetic examples for the minority class blindly, we would expect the Borderline-SMOTE method to only create synthetic examples along the decision boundary between the two classes.","metadata":{}},{"cell_type":"markdown","source":"üìå Random undersampling involves randomly selecting examples from the majority class to delete from the training dataset. <br>\nüìå This has the effect of reducing the number of examples in the majority class in the transformed version of the training dataset. This process can be repeated until the desired class distribution is achieved, such as an equal number of examples for each class.","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom imblearn.pipeline import Pipeline\n\nfrom collections import Counter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"over = BorderlineSMOTE(sampling_strategy=0.3)\nunder = RandomUnderSampler(sampling_strategy=0.6)\n\nsteps = [('o', over), ('u', under)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline = Pipeline(steps=steps)\n\n# transform the dataset\nx_sm, y_sm = pipeline.fit_resample(x_train, y_train)\n\nprint(Counter(y_train))\nprint(Counter(y_sm))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Names of the independent variables\nfeature_names = list(dataset.drop('RainTomorrow', axis=1).columns)\n\n# Concatenate resampling train sets and test sets to build the new dataframe \nnew_x = np.concatenate((x_sm, x_test))\nnew_y = np.concatenate((y_sm, y_test))\n\n#New DataFrame with sampling data\nsm_dataset = pd.DataFrame(np.column_stack([new_y, new_x]), columns=['RainTomorrow'] + feature_names)\nsm_dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå From the above dataset, we can see that our Encoded features have become float type. Let's convert them into int type.","metadata":{}},{"cell_type":"code","source":"cols = ['Location','WindGustDir','WindDir9am','WindDir3pm','RainToday','RainTomorrow','Day','Month','Year']\nfor i in cols:\n    sm_dataset[i] = sm_dataset[i].astype(int)\nsm_dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '6'></a>\n\n    H2O \n</div>","metadata":{}},{"cell_type":"markdown","source":"üìå H2O from Python is a tool for rapidly turning over models, doing data munging, and building applications in a fast, scalable environment without any of the mental anguish about parallelism and distribution of work.","metadata":{}},{"cell_type":"code","source":"# Start and connect to a local H2O cluster\nimport h2o\nh2o.init(nthreads = -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather = h2o.H2OFrame(sm_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert 'Location','WindGustDir','WindDir9am','WindDir3pm','RainToday' and 'RainTomorrow' to categorical values\nweather['Location'] = weather['Location'].asfactor()\nweather['WindGustDir'] = weather['WindGustDir'].asfactor()\nweather['WindDir9am'] = weather['WindDir9am'].asfactor()\nweather['WindDir3pm'] = weather['WindDir3pm'].asfactor()\nweather['RainToday'] = weather['RainToday'].asfactor()\nweather['RainTomorrow'] = weather['RainTomorrow'].asfactor()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define features (or predictors) manually\nfeatures = ['Location','MinTemp','MaxTemp','Rainfall','Evaporation','Sunshine','WindGustDir','WindGustSpeed','WindDir9am','WindDir3pm','WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','Cloud9am','Cloud3pm','Temp9am','Temp3pm','RainToday','Day','Month','Year']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the H2O data frame into training/test sets\n# so we can evaluate out-of-bag performance\nweather_split = weather.split_frame(ratios = [0.8], seed = 1234)\n\nweather_train = weather_split[0] # using 80% for training\nweather_test = weather_split[1]  # using the rest 20% for out-of-bag evaluation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **H2OGeneralizedLinearEstimator** <a id = '6.1'></a>","metadata":{}},{"cell_type":"markdown","source":"üìå Build a Generalized Linear Model Fit a generalized linear model, specified by a response variable, a set of predictors, and a description of the error distribution.","metadata":{}},{"cell_type":"code","source":"# Build a Generalized Linear Model (GLM) with default settings\n\n# Import the function for GLM\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\n\n# Set up GLM for binary classification\nglm_default = H2OGeneralizedLinearEstimator(family = 'binomial', model_id = 'glm_default', keep_cross_validation_predictions = True, nfolds =5, fold_assignment = \"stratified\", balance_classes=True)\n\n# Use .train() to build the model\nglm_default.train(x = features, \n                  y = 'RainTomorrow', \n                  training_frame = weather_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the model performance on training dataset\nglm_default","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nvariable = glm_default.varimp_plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"performace = glm_default.model_performance(train=True)\nperformace.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coef = glm_default.std_coef_plot()\nplt.yticks(size=6)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the model performance on test dataset\nglm_default.model_performance(weather_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"performace = glm_default.model_performance(weather_test)\nperformace.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use GLM model to make predictions\nyhat_test_glm = glm_default.predict(weather_test)\nyhat_test_glm.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **H2ORandomForestEstimator** <a id = '6.2'></a>","metadata":{}},{"cell_type":"markdown","source":"üìå Builds a Random Forest Model on an H2OFrame.","metadata":{}},{"cell_type":"code","source":"# Build a Distributed Random Forest (DRF) model with default settings\n\n# Import the function for DRF\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator\n\n# Set up DRF for classification\n# Add a seed for reproducibility\ndrf_default = H2ORandomForestEstimator(model_id = 'drf_default', seed = 1234, keep_cross_validation_predictions = True, nfolds =5, fold_assignment = \"stratified\", balance_classes=True)\n\n# Use .train() to build the model\ndrf_default.train(x = features, \n                  y = 'RainTomorrow', \n                  training_frame = weather_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the DRF model summary\ndrf_default","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nvariable = drf_default.varimp_plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"performace = drf_default.model_performance(train=True)\nperformace.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the model performance on test dataset\ndrf_default.model_performance(weather_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"performace = drf_default.model_performance(weather_test)\nperformace.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use DRF model to make predictions\nyhat_test_drf = drf_default.predict(weather_test)\nyhat_test_drf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **H2OGradientBoostingEstimator** <a id = '6.3'></a>","metadata":{}},{"cell_type":"markdown","source":"üìå Builds gradient boosted classification trees, and gradient boosted regression trees on a parsed data set. The default distribution function will guess the model type based on the response column type run properly the response column must be an numeric for ‚Äúgaussian‚Äù or an enum for ‚Äúbernoulli‚Äù or ‚Äúmultinomial‚Äù.","metadata":{}},{"cell_type":"code","source":"# Build a Gradient Boosting Machines (GBM) model with default settings\n\n# Import the function for GBM\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\n\n# Set up GBM for classification\n# Add a seed for reproducibility\ngbm_default = H2OGradientBoostingEstimator(model_id = 'gbm_default', seed = 1234, keep_cross_validation_predictions = True, nfolds =5, fold_assignment = \"stratified\", balance_classes=True)\n\n# Use .train() to build the model\ngbm_default.train(x = features, \n                  y = 'RainTomorrow', \n                  training_frame = weather_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the GBM model summary\ngbm_default","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nvariable = gbm_default.varimp_plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"performace = gbm_default.model_performance(train=True)\nperformace.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the model performance on test dataset\ngbm_default.model_performance(weather_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"performace = gbm_default.model_performance(weather_test)\nperformace.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use GBM model to make predictions\nyhat_test_gbm = gbm_default.predict(weather_test)\nyhat_test_gbm.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **H2ODeepLearningEstimator** <a id = '6.4'></a>","metadata":{}},{"cell_type":"markdown","source":"üìå Build a supervised Deep Neural Network model Builds a feed-forward multilayer artificial neural network on an H2OFrame.","metadata":{}},{"cell_type":"code","source":"# Build a Deep Learning (Deep Neural Networks, DNN) model with default settings\n\n# Import the function for DNN\nfrom h2o.estimators.deeplearning import H2ODeepLearningEstimator\n\n# Set up DNN for regression\ndnn_default = H2ODeepLearningEstimator(model_id = 'dnn_default', keep_cross_validation_predictions = True, nfolds =5,fold_assignment='stratified', balance_classes=True)\n\n# (not run) Change 'reproducible' to True if you want to reproduce the results\n# The model will be built using a single thread (could be very slow)\n# dnn_default = H2ODeepLearningEstimator(model_id = 'dnn_default', reproducible = True)\n\n# Use .train() to build the model\ndnn_default.train(x = features, \n                  y = 'RainTomorrow', \n                  training_frame = weather_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the DNN model summary\ndnn_default","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nvariable = dnn_default.varimp_plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"performace = dnn_default.model_performance(train=True)\nperformace.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the model performance on test dataset\ndnn_default.model_performance(weather_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"performace = dnn_default.model_performance(weather_test)\nperformace.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use DNN model to make predictions\nyhat_test_dnn = dnn_default.predict(weather_test)\nyhat_test_dnn.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **H2OStackedEnsembleEstimator** <a id = '6.5'></a>","metadata":{}},{"cell_type":"markdown","source":"üìå Builds a stacked ensemble (aka ‚Äúsuper learner‚Äù) machine learning method that uses two or more H2O learning algorithms to improve predictive performance. It is a loss-based supervised learning method that finds the optimal combination of a collection of prediction algorithms.This method supports regression and binary classification.","metadata":{}},{"cell_type":"code","source":"# Import the function for StackedEnsemble\nfrom h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator\n\n# Set up Stacked Ensemble\nensemble = H2OStackedEnsembleEstimator(model_id = \"my_ensemble\",\n                                       base_models = [glm_default, drf_default, gbm_default, dnn_default])\n\n# use .train to start model stacking\n# GLM as the default metalearner\nensemble.train(x = features, \n               y = 'RainTomorrow', \n               training_frame = weather_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the Stacked model summary\nensemble","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the model performance on test dataset\nensemble.model_performance(weather_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"performace = ensemble.model_performance(weather_test)\nperformace.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use Stacked Ensemble to make predictions\nyhat_test_ensemble = ensemble.predict(weather_test)\nyhat_test_ensemble.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå Comparison of Model Performance on Test Data","metadata":{}},{"cell_type":"code","source":"print('GLM model (AUC) : ', glm_default.model_performance(weather_test).auc())\nprint('DRF model (AUC) : ', drf_default.model_performance(weather_test).auc())\nprint('GBM model (AUC) : ', gbm_default.model_performance(weather_test).auc())\nprint('DNN model (AUC) : ', dnn_default.model_performance(weather_test).auc())\nprint('Stacked Ensembles (AUC) : ', ensemble.model_performance(weather_test).auc())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:LightSlateGray;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\"> <a id = '7'></a>\n\n    Conclusion\n</div>","metadata":{}},{"cell_type":"markdown","source":"üìå After dealing with missing values in the dataset and extensive data analysis of the features. I got to know more about how features are correlated to each other. <br>\nüìå I implemented BorderlineSMOTE and RandomUnderSampler techniques on our features to deal with the imbalance data. <br>\nüìå Then, I used different classification models from H2O to see how it performs on the dataset. I got pretty good results with AUC, precision, and recall score. <br>\nüìå With that, I concluded that the Stacked Ensembles model is the best fit for our dataset. But, as I have used default parameters for our models. So, the other models could be tuned and make the other models better.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           border:2px solid DodgerBlue;\n           background-color:white;\n           font-size:150%;\n           text-align:center;\n           letter-spacing:0.5px\">\n    \n    Thank You!\n</div>","metadata":{}}]}