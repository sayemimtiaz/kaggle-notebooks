{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task for Today  \n\n***\n\n## Kiva Loan Type Prediction  \n\nGiven *data about Kiva crowdfunding loans*, let's try to predict the **type** of a given loan.  \n  \nWe will use a TensorFlow neural network to make our predictions."},{"metadata":{},"cell_type":"markdown","source":"# Getting Started"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\n\nfrom sklearn.metrics import confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/data-science-for-good-kiva-crowdfunding/kiva_loans.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_dates(df, column):\n    df = df.copy()\n    \n    df[column] = pd.to_datetime(df[column])\n    \n    df[column + \"_year\"] = df[column].apply(lambda x: x.year)\n    df[column + \"_month\"] = df[column].apply(lambda x: x.month)\n    df[column + \"_day\"] = df[column].apply(lambda x: x.day)\n    \n    df[column + \"_hour\"] = df[column].apply(lambda x: x.hour)\n    df[column + \"_minute\"] = df[column].apply(lambda x: x.minute)\n    df[column + \"_second\"] = df[column].apply(lambda x: x.second)\n    \n    df = df.drop(column, axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_male_count(x):\n    count = 0\n    for gender in str(x).split(', '):\n        if gender == 'male':\n            count += 1\n    return count\n\ndef get_female_count(x):\n    count = 0\n    for gender in str(x).split(', '):\n        if gender == 'female':\n            count += 1\n    return count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def onehot_encode(df, columns_with_prefixes):\n    df = df.copy()\n    \n    for column, prefix in columns_with_prefixes:\n        dummies = pd.get_dummies(df[column], prefix=prefix)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # Drop id column\n    df = df.drop('id', axis=1)\n    \n    # Drop use and tags columns (avoiding NLP)\n    df = df.drop(['use', 'tags'], axis=1)\n    \n    # Drop country and date columns (redundant information)\n    df = df.drop(['country', 'date'], axis=1)\n    \n    # Drop region column (high-cardinality)\n    df = df.drop('region', axis=1)\n    \n    # Extract date features\n    df = encode_dates(df, column='posted_time')\n    df = encode_dates(df, column='disbursed_time')\n    df = encode_dates(df, column='funded_time')\n    \n    # Engineer gender count features\n    df['male_count'] = df['borrower_genders'].apply(get_male_count)\n    df['female_count'] = df['borrower_genders'].apply(get_female_count)\n    df = df.drop('borrower_genders', axis=1)\n    \n    # One-hot encode nominal features\n    nominal_features = [\n        ('activity', \"act\"),\n        ('sector', \"sec\"),\n        ('country_code', \"ctc\"),\n        ('currency', \"cur\"),\n        ('partner_id', \"pid\")\n    ]\n    df = onehot_encode(df, columns_with_prefixes=nominal_features)\n    \n    # Split df into X and y\n    y = df['repayment_interval']\n    X = df.drop('repayment_interval', axis=1)\n    \n    # Encode labels\n    label_mapping = {\n        'bullet': 0,\n        'weekly': 1,\n        'monthly': 2,\n        'irregular': 3\n    }\n    y = y.replace(label_mapping)\n    \n    # Fill in remaining missing values with column means\n    missing_value_columns = X.loc[:, X.isna().sum() > 0].columns\n    for column in missing_value_columns:\n        X[column] = X[column].fillna(X[column].mean())\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X with a standard scaler\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    \n    X_train = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n    \n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_model(model, X_test, y_test):\n    \n    results = model.evaluate(X_test, y_test, verbose=0)\n    print(\"    Test Loss: {:.4f}\".format(results[0]))\n    print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))\n    \n    y_pred = np.argmax(model.predict(X_test), axis=1)\n    \n    cm = confusion_matrix(y_test, y_pred)\n    clr = classification_report(y_test, y_pred, target_names=['bullet', 'weekly', 'monthly', 'irregular'])\n    \n    plt.figure(figsize=(10, 10))\n    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)\n    plt.xticks(np.arange(4) + 0.5, ['bullet', 'weekly', 'monthly', 'irregular'])\n    plt.yticks(np.arange(4) + 0.5, ['bullet', 'weekly', 'monthly', 'irregular'])\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n    \n    print(\"Classification Report:\\n----------------------\\n\", clr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = preprocess_inputs(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = tf.keras.Input(shape=(X_train.shape[1],))\nx = tf.keras.layers.Dense(128, activation='relu')(inputs)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\noutputs = tf.keras.layers.Dense(4, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n\nmodel.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(model, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps://youtu.be/Mi-MF1p40h8"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}