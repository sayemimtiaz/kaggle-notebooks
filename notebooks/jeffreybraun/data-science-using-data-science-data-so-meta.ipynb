{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning + Initial Augmentation\nThanks to Jaganadh Gopinadhan (https://www.kaggle.com/jaganadhg) for his Kernel wherein he cleaned the title and subtitle columns as well as added the title count and subtitle count columns (https://www.kaggle.com/jaganadhg/clapmediumclap). All code in the following block is courtesy of Jaganadh","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport unicodedata\nimport warnings\nwarnings.simplefilter(action='ignore')\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport sklearn as sl\n\ndf = pd.read_csv(\"../input/medium-articles-dataset/medium_data.csv\")\n\ndef normalize_text(text : str) -> str:\n    \"\"\" Normalize the unicode string\n        :param text: text data\n        :retrns clean_text: clean text\n    \"\"\"\n    if text != np.nan:\n        clean_text = unicodedata.normalize(\"NFKD\",text)\n    else:\n        clean_text = text\n    return clean_text\n\ndef create_wc(text : str) -> int:\n    \"\"\" Count words in a text\n        :param text: String to check the len\n        :retirns wc: Word count\n    \"\"\"\n    wc = 0\n    norm_text = text.lower()\n    wc = len(norm_text.split(\" \"))\n    return wc\n\ndf['clean_title'] = df.title.apply(lambda x: normalize_text(x) if x!= np.nan else x)\ndf['clean_subtitle'] = df.subtitle.apply(lambda x: normalize_text(x) if x!= np.nan and type(x) == str else x)\ndf['title_wc'] = df.title.apply(lambda x: create_wc(x) if x!= np.nan else 0)\ndf['subtitle_wc'] = df.subtitle.apply(lambda x: create_wc(x) if x!= np.nan and type(x) == str else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA\nFirst, let's do an overview of the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how the claps are distributed across different categories:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Publication vs. Claps","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Value Counts for each publication:\")\nprint(df.publication.value_counts())\n\nsns.set(style=\"ticks\", color_codes=True)\nplt.style.use('fivethirtyeight')\n\ng = sns.catplot(x=\"claps\", y=\"publication\", kind=\"box\", data=df, order=df.publication.value_counts().iloc[:25].index)\ng.set(xlim=(-25, 2500))\ng.fig.set_size_inches(28, 10)\ng.ax.set_xticks([0,250,500,750,1000,1250,1500,1750,2000,2250,2500], minor=True)\nplt.title(\"Claps vs. Publication - Ordered by Most Common Publications\")\nplt.show()\n\ndf_ordered = df.groupby(\"publication\").median().sort_values(by = 'claps', ascending=False)\n\ng = sns.catplot(x=\"claps\", y=\"publication\", kind=\"box\", data=df, order=df_ordered.iloc[:25].index)\ng.set(xlim=(-25, 2500))\ng.fig.set_size_inches(28, 10)\ng.ax.set_xticks([0,250,500,750,1000,1250,1500,1750,2000,2250,2500], minor=True)\nplt.title(\"Claps vs. Publication - Ordered by Median Claps\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading Time vs. Claps\n","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,10))\ng = sns.scatterplot(x=\"claps\", y=\"reading_time\", data=df, legend='full')\ng.set(xlim=(-25, 1050))\nplt.title(\"Claps vs. Reading Time\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It might be useful to bin the reading times into categories and then observe the data","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Value Counts for each reading time:\")\nprint(df.reading_time.value_counts())\nprint('\\n')\n\nplt.style.use('fivethirtyeight')\n\ndef bin_time(row):\n    read_time = row['reading_time']\n    if read_time <= 3:\n        return 'Very Short (0-3 minutes)'\n    elif read_time > 3 and read_time <= 7:\n        return 'Short (4-7 minutes)'\n    elif read_time > 7 and read_time <= 12:\n        return 'Substantial (8-12 minutes)'\n    elif read_time > 12:\n        return 'Long (13+ minutes)'\n\ndf['reading_time_bin'] = df.apply(lambda row: bin_time(row), axis=1)\nprint(\"Value Counts for each reading time bin:\")\nprint(df.reading_time_bin.value_counts())\n\n\ndf_ordered = df.groupby(\"reading_time_bin\").median().sort_values(by = 'claps', ascending=False)\n\ng = sns.catplot(x=\"claps\", y=\"reading_time_bin\", kind=\"box\", data=df, order=df_ordered.iloc[:25].index)\ng.set(xlim=(-25, 1050))\ng.fig.set_size_inches(28, 10)\ng.ax.set_xticks([0,100,200,300,400,500,600,700,800,900,1000], minor=True)\nplt.title(\"Claps vs. Reading Time - Ordered by Median Claps\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom PIL import Image\n\nprint(torch.__version__)\ntorch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\n\ndf_img = df.dropna(subset=['image']).reset_index()\n\nimage_data = np.zeros([df_img.shape[0], 1, 15, 50], dtype=float)\nimage_labels = np.zeros([df_img.shape[0]], dtype=float)\n\nfor i in tqdm(range(df_img.shape[0])):\n    img_file = df_img.loc[i, 'image']\n    if img_file[-1] == '.':\n        img_file = img_file.strip('.')\n    img = Image.open('/kaggle/input/medium-articles-dataset/images/' + img_file).convert('LA').resize((50,15),resample=Image.BILINEAR)\n    img_data = np.asarray(img)\n    image_data[i,0,:,:] = img_data[:,:,0]\n    num_clap = df_img.loc[i, 'claps']\n    if num_clap < 50:\n        image_labels[i] = 0\n    elif num_clap >= 50 and num_clap < 100:\n        image_labels[i] = 1\n    elif num_clap >= 100 and num_clap < 400:\n        image_labels[i] = 2\n    else:\n        image_labels[i] = 3\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cut = int(np.floor(image_data.shape[0] * 5/7))\nval_cut = int(np.floor(image_data.shape[0] * 1/7))\n        \nimage_data = np.float32(image_data)\nimage_labels = np.float32(image_labels)\nimage_data = image_data/255.\ntrain_dat = image_data[0:train_cut]\ntrain_labels = image_labels[0:train_cut]\nval_dat = image_data[train_cut:train_cut + val_cut]\nval_labels = image_labels[train_cut:train_cut + val_cut]\ntest_dat = image_data[train_cut + val_cut:]\ntest_labels = image_labels[train_cut + val_cut:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MNIST_Net(nn.Module):\n    \"\"\" \n    Original Model\n    \"\"\"\n    def __init__(self,p=0.5,minimizer='Adam',step_size=0.001):\n        super(MNIST_Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d(p)\n        self.fc1 = nn.Linear(640, 64)\n        self.fc2 = nn.Linear(64, 4)\n        if minimizer == 'Adam':\n            self.optimizer = torch.optim.Adam(self.parameters(), lr = step_size)\n        else:\n            optimizer = torch.optim.SGD(self.parameters(), lr = step_size, momentum=0.9)\n         \n        self.criterion=nn.CrossEntropyLoss()\n            \n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 640)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return x\n    \n    def get_acc_and_loss(self, data, targ):\n        output = self.forward(data)\n        loss = self.criterion(output, targ)\n        pred = torch.max(output,1)[1]\n        correct = torch.eq(pred,targ).sum()\n        \n        return loss,correct\n        \n    def run_grad(self,data,targ):\n    \n        loss, correct=self.get_acc_and_loss(data,targ)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        return loss, correct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_epoch(net,epoch,train,batch_size, num=None, ttype=\"train\"):\n    \n    error_rates_batch = []\n    epoch_indices = []\n    net.train()\n    if ttype=='train':\n        t1=time.time()\n        n=train[0].shape[0]\n        if (num is not None):\n            n=np.minimum(n,num)\n        ii=np.array(np.arange(0,n,1))\n        tr=train[0][ii]\n        y=train[1][ii]\n        train_loss=0; train_correct=0\n        with tqdm(total=len(y)) as progress_bar:\n            for j in np.arange(0,len(y),batch_size):\n                data=torch.torch.from_numpy(tr[j:j+batch_size]).to(device)\n                targ=torch.torch.from_numpy(y[j:j+batch_size]).type(torch.long).to(device)\n                loss, correct = net.run_grad(data,targ) \n                train_loss += loss.item()\n                train_correct += correct.item()\n\n                error_rates_batch.append(1 - correct.item() / batch_size)\n                epoch_indices.append(epoch + j/len(y))\n                \n                progress_bar.set_postfix(loss=loss.item())\n                progress_bar.update(data.size(0))\n        train_loss /= len(y)\n        print('\\nTraining set epoch {}: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(epoch,\n            train_loss, train_correct, len(y),\n            100. * train_correct / len(y)))\n        \n    return error_rates_batch, epoch_indices\n        \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def net_test(net,val,batch_size,ttype='val'):\n\n    error_rates_batch = []\n    batch_indices = []\n    net.eval()\n    with torch.no_grad():\n                test_loss = 0\n                test_correct = 0\n                vald=val[0]\n                yval=val[1]\n                for j in np.arange(0,len(yval),batch_size):\n                    data=torch.torch.from_numpy(vald[j:j+batch_size]).to(device)\n                    targ = torch.torch.from_numpy(yval[j:j+batch_size]).type(torch.long).to(device)\n                    loss,correct=net.get_acc_and_loss(data,targ)\n\n                    test_loss += loss.item()\n                    test_correct += correct.item()\n\n                    error_rates_batch.append(1 - correct.item()/batch_size)\n                    batch_indices.append(j/len(yval))\n\n                test_loss /= len(yval)\n                SSS='Validation'\n                if (ttype=='test'):\n                    SSS='Test'\n                print('\\n{} set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(SSS,\n                    test_loss, test_correct, len(yval),\n                    100. * test_correct / len(yval)))\n    return error_rates_batch, batch_indices\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(device)\n\ndef create_model(net_id, model_name):\n    batch_size=64\n    step_size=.001\n    num_epochs=20\n    numtrain=50000\n    minimizer=\"Adam\"\n    dropout_p=0.5\n    dim=28\n    nchannels=1\n\n    # use GPU when possible\n    train = (train_dat, train_labels)\n    val = (val_dat, val_labels)\n    test = (test_dat, test_labels)\n    if net_id == 0:\n        net = MNIST_Net(p = dropout_p, minimizer=minimizer,step_size=step_size)\n    net.to(device)\n    #define optimizer\n\n    ers_train = []\n    eis_train = []\n    ers_val = []\n    eis_val = []\n    \n    for i in range(num_epochs):\n        [error_rates, epoch_indices] = run_epoch(net,i,train,batch_size, num=numtrain, ttype=\"train\")\n        ers_train.extend(error_rates)\n        eis_train.extend(epoch_indices)\n        [error_rates, batch_indices] = net_test(net,val,batch_size)\n        ers_val.extend(error_rates)\n        eis_val.extend([x+i for x in batch_indices])\n\n    plt.plot(eis_train, ers_train)\n    plt.title(\"Training Set\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Error Rate\")\n    plt.show()\n\n    plt.plot(eis_val, ers_val)\n    plt.title(\"Validation Set\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Eror_Rate\")\n    plt.show()\n\n    net_test(net,test,batch_size,ttype='test')\n\n    #torch.save(net.state_dict(), datadir+'models/'+model_name)\n\n    return net","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_net = create_model(0, \"original_model\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}