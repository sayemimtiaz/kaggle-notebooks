{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Insights from Hotel Reviews\n\n**Goal**\n\nThis notebook aims to explore the Trip Advisor hotel reviews dataset, to show a combination of techniques that can reveal what makes a guest enjoy or dislike their hotel stay.  \n\nSome of the techniques that will be applied are:\n* **Topic Modeling** - This is the most common approach to reveal what makes a hotel good or bad, but the problem is that topics are often hard to interpret.  Even when they can be interpreted, they often overlap so much that they are not useful.  This notebook will explore ways to make the topics easier to understand.\n* **Sentiment Analysis** - Sentiment can overlap with rating, but it may not.  This notebook will explore how sentiment and rating relate, and how sentiment can be used to enhance topic modeling.\n* **Rating prediction** - There is no value in predicting rating.  The reviewers do that for us!  The true usefulness of a rating prediction model is in exploring which features steer it to a rating prediction.  That is, which features make the hotel rating good or bad?  This notebook will explore how deep learning can be used to create a solid prediction model, and how the black box can be peeled open to reveal the features that matter.\n\n**Note About the Data**\n\nThe reviews themselves have been cleaned already.  The text does not read cleanly, as words have been removed.  I would have preferred the full, raw text, but the pre-cleaning will speed things up a bit."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install glimpy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport spacy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pyLDAvis\nimport pyLDAvis.sklearn\nimport tensorflow as tf\nimport itertools\nimport plotly.express as px\nfrom collections import Counter\nfrom tqdm import tqdm\nfrom textblob import TextBlob\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, confusion_matrix, accuracy_score, f1_score, log_loss\nfrom glimpy import GLM, Poisson\nfrom transformers import DistilBertTokenizer, DistilBertConfig, TFDistilBertModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hardcoded values used throughout the script\nNBR_EPOCHS = 2\nTEST_SET_FRAC = 0.2\nSEED = 14\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nSince this is text data, I want to check for nulls, empty strings, new line characters, and tabs that could cause trouble parsing the text.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"d = pd.read_csv('/kaggle/input/trip-advisor-hotel-reviews/tripadvisor_hotel_reviews.csv')\nd.columns = [c.lower() for c in d.columns]\n\n# OPTIONAL - reduce dataset size in case the kernel runs out of memory\nd = d.loc[:3000]\n\nprint(len(d))\nprint(d.columns.to_list())\nd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for nulls\nd.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for empty string reviews\nlen(d[d.review == ''])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for new lines and tabs\nd.review.str.contains('[\\n\\t]', regex=True).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sentiment Analysis\n\nAs part of the data exploration process, it would be interesting to see how sentiment plays into review ratings.  I want to do 2 levels of analysis:\n\n* Sentiment at the sence level\n* Sentiment at the document (review) level\n\nFor the document level, I will average the sentiments of the sentences in each document.  To do this, I will use Spacy and TextBlob."},{"metadata":{"trusted":true},"cell_type":"code","source":"# load Spacy language model\nsplg = spacy.load('en_core_web_lg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of reviews to iterate over\nreviews = d.review.to_list()\n\n# create a list of tuples containing the sentences and sentiments\nreviews_info = []\nfor r in tqdm(reviews):\n    text = splg(r)\n    sents = [str(s) for s in text.sents]\n    sent_sentiments = [TextBlob(str(s)).sentiment[0] for s in text.sents]  # polarity is index 0\n    reviews_info.append((sents, sent_sentiments))\n\n# inspect the first review: the first tuple element should be a list of sentences\nprint(\"Nbr Sents:\", len(reviews_info[0][0]))\nreviews_info[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create doc to sentence, and doc to sentence sentiment maps\n# also create flat lists containing all sentences and sentiments\ndoc_sents = dict.fromkeys([i for i in range(len(reviews_info))])\ndoc_sent_sentiments = dict.fromkeys([i for i in range(len(reviews_info))])\nall_sents = []\nall_sent_sentiments = []\n\nfor doc_id, doc_info in enumerate(reviews_info):\n    doc_sents[doc_id] = doc_info[0]  # sentences are index 0\n    doc_sent_sentiments[doc_id] = doc_info[1]  # sentiments are index 1\n    all_sents += doc_info[0]\n    all_sent_sentiments += doc_info[1]\n\n# create a doc to sentiment map by averaging the sentiments of a doc's sentences\ndoc_sentiments = {k: np.mean(v) for k, v in doc_sent_sentiments.items()}\n\n# explore the document level sentiment distribution by rating\nd['review_sentiment'] = d.index.map(doc_sentiments)\nsns.boxplot(x=d.rating, y=d.review_sentiment)\nplt.ylabel('Sentiment Distribution (Averaged Sentence Polarity per Review)')\nplt.title('Sentiment Distribution by Rating')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Document sentiment tends to rise with the rating, which is to be expected.  Document sentiment could be a good feature to use when predicting rating.  There are times, however, when there are positive sentiments but low ratings, and times when there are negative sentiments but high ratings.  But from the appearance of these box plots, one could almost draw a line at doc sentiment = 0.1 to separate low (1-3) and high (3-5) ratings.  \n\nNext I want to view examples of negative sentiments with high ratings, and examples of positive sentiments with low ratings.  I will look for high sentiment sentences that might influence the document sentiment too much."},{"metadata":{"trusted":true},"cell_type":"code","source":"# view examples of negative sentiment but high rating\nd[((d.rating==5) & (d.review_sentiment<0.1))].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# render the first review for inspection\nspacy.displacy.render(\n    splg(d[((d.rating==5) & (d.review_sentiment<0.1))]['review'].to_list()[0]),\n    style='ent', \n    jupyter=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view examples of positive sentiment but low rating\nd[((d.rating==1) & (d.review_sentiment>0.1))].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy.displacy.render(\n    splg(d[((d.rating==1) & (d.review_sentiment>0.1))]['review'].to_list()[0]),\n    style='ent', \n    jupyter=True\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like some of the high ratings with negative sentiment are complaining about things that are irrelevant to the hotel, like the flights to the city.  Others complaing about 1 aspect of the hotel, like parking, but it did not impact the overall review.  It looks like a combination of sentiment and topic might be insightful.\n\nIt looks like some of the low ratings with high sentiment are actually more neutral sounding.  Since 0 is neutral sentiment, it may be worth changing the threshold for what defines positive.  But it is interesting to see that even neutral sentiment reviews can have very low ratings.  Perhaps high ratings are given when people are pleasantly surprised?  Again, it looks like a combination of topic and sentiment would be interesting.\n\nBefore exploring topics, I want to look at the sentence length distribution and rating distribution.  If there are unusually short or long sentences, they might be useless to analyze.  The short ones won't contain enough information, and the long ones will contain so much that it becomes too noisy (conflicting sentiments, embeddings that smooth towards 0, etc.) To do this, I need to assign each sentence an ID that is unique across all sentences, and then map each document to the global IDs of the sentences they contain."},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign each sentence a unique, global ID\nsent_global_id = {i: v for i, v in enumerate(all_sents)}\n\n# map each document to the global IDs of the sents within it\ndoc_global_id = dict.fromkeys([n for n in range(len(doc_sents))])\ndoc_max_global_id = 0\nfor k, v in doc_sents.items():\n    nbr_sents = len(v)\n    new_max_global_id = doc_max_global_id + nbr_sents\n    doc_global_id[k] = [n for n in range(doc_max_global_id, new_max_global_id)]\n    doc_max_global_id = new_max_global_id\n\ndoc_global_id[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check sentence length distribution\nsns.distplot([len(s.split()) for s in all_sents], kde=False, bins=60)\nplt.xlabel('Sentence Length (Nbr Words)')\nplt.title('Sentence Length Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rating distribution\nsns.distplot(d.rating, kde=False)\nplt.title('Rating Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sentence length distribution shows that there are a handful of long sentences, and some with under 10 words.  \n\nThe rating distribution shows that there are more high ratings than low, overall.  I could group some of the ratings together as categories, such as 1-2 as negative, 4-5 as high, and 3 as neutral.  But from a business perspective, ratings are vital to landing new customers, so perhaps anything < 4 should be labeled as negative.  \n\nAre there rare words that might be able to be removed?  Words that only appear once might not be useful for modeling."},{"metadata":{"trusted":true},"cell_type":"code","source":"# find rare words\nminimum_word_count = 2\nword_freqs = Counter(' '.join(all_sents).split())\nsorted_word_freqs = sorted(word_freqs.items(), key=lambda i: i[1])\n[i for i in sorted_word_freqs if i[1] < minimum_word_count][:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for curiosity's sake, inspect most frequent words (hopefully these are not stop words)\nword_freqs.most_common(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Topic Modeling (LDA vs Manually Created Topics)\n\nTopic modeling may help identify features of a hotel that could be improved, or that are a major draw for customers.  When combined with ratings, these topics could help the business identify areas of improvement.\n\nWithout writing a single line of code, I would bet that the topics that will surface might be something like:\n\n['hotel', 'room', 'stay', 'great', 'good']\n\n['restaurant', 'food']\n\n['staff', 'friendly']\n\n\nYou don't need data science to figure out what people want in a hotel.  What is truly valuable is to find the non-obvious things, or the things a hotel might not realize are a problem.  Sometimes you might get lucky and find something, but it can require sifting through many topics to find nuggets of gold.  Who has time for that?  After taking the standard approach to topic modeling (LDA), I will look at manually creating topics.  **Spoiler: in this case, the manual approach is much better.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Limit number of words/features to use in LDA to 1000\nnbr_features = 1000\n\n# LDA uses raw term frequencies\ntf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=nbr_features, stop_words='english')\nterm_freqs = tf_vectorizer.fit_transform(all_sents)\ntf_feature_names = tf_vectorizer.get_feature_names()\n\nnbr_topics = 20\nlda = LatentDirichletAllocation(n_components=nbr_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=SEED).fit(term_freqs)\nlda_topics = lda.transform(term_freqs)\n\n# Plot pretty LDA output\nlda_vis_data = pyLDAvis.sklearn.prepare(lda, term_freqs, tf_vectorizer)\nlda_vis_data_html = pyLDAvis.prepared_data_to_html(lda_vis_data)\npyLDAvis.display(lda_vis_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows the problem with topic modeling: the topics are often hard to interpret, and they have significant overlap in their term components.  There may be some topics that pop out, like one that talks about the beds, or another that deals with beach hotels, but what pratical use are these?  You don't need data science to figure out that a comfortable bed and beachside hotel are things that customers want.  Unfortunately, it seems like topic modeling has not revealed anything useful.\n\n### Topic Exploration\n\nIf the topics were easy to distinguish, then a more detailed exploration of them would be insightful.  In this case, they are not, but I'll do this analysis anyway."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inspect the most frequent n words for the top 10 topics\ntop_n = 10\ntotal_rows = nbr_topics*top_n\ntopic = []\ntopic_top_ten = []\ntopic_top_ten_scores = []\nfor tid, t in enumerate(lda.components_):\n    topic.append([tid+1]*top_n)\n    topic_top_ten.append([tf_feature_names[i] for i in t.argsort()[:-top_n -1:-1]])\n    topic_top_ten_scores.append(t[t.argsort()[:-top_n -1:-1]])\ntop_words = np.concatenate([np.array(topic).reshape(total_rows,1), np.array(topic_top_ten).reshape(total_rows,1), np.array(topic_top_ten_scores).reshape(total_rows,1)], axis=1)\ntopwordsdf = pd.DataFrame(top_words, columns=['topic', 'word', 'score'])\ntopwordsdf['topic'] = topwordsdf['topic'].astype('int64')\ntopwordsdf['score'] = topwordsdf['score'].astype('float64')\n\n# The higher this threshold, the fewer results will be returned when data is filtered\nlda_score_threshold = 0.2\n\n\n# Set up bar chart of top words by topic\ndef create_bar_top_words(topwords_df, topic):\n    topwords_df[topwords_df['topic']==topic].sort_values(by=['score'], ascending=True).plot.barh(x='word', y='score', title='Top 10 Words for Topic '+str(topic), colormap='Paired', legend=False)\n    plt.xlabel('score')\n    plt.tight_layout()\n    plt.show()\n    return\n\ncreate_bar_top_words(topwordsdf, topic=1)\n\n# Set up heatmap of document to topic probability score\ndef create_doc_topic_heatmap(topics_input):\n    heatmap_x_labels = [\"Topic %d\" % i for i in range(1, topics_input.shape[1]+1)]\n    heatmap_y_labels = [\"%d\" % i for i in range(1, topics_input.shape[0]+1)]\n    sns.heatmap(topics_input, cmap='Greys', xticklabels=heatmap_x_labels, yticklabels=heatmap_y_labels)\n    plt.title('Probability that Records are Related to Topics')\n    plt.tight_layout()\n    plt.show()\n    return\n\ncreate_doc_topic_heatmap(lda_topics[:24,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While the topics are not easy to interpret, might they be useful when combined with sentiment?  To find out, I will map each topic to a sentiment by averaging the sentence sentiments of the sentences.  Sentences will be assigned to topics based on their highest topic probability scores.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# get sentiment by LDA topic\n\n# map each document to its most probable topic (assign 1 topic per document using the max probability score from LDA)\ndoc_topic_mapping = {doc_id: topic for doc_id, topic in enumerate(list(np.argmax(lda_topics, axis=1)))}\n\n# reverse the mapping to get a list of documents for each topic\ntopic_doc_mapping = {}\nfor k, v in doc_topic_mapping.items():\n    topic_doc_mapping.setdefault(v, set()).add(k)\ntopic_doc_mapping\n\n# now get the topic to sentiment mapping, by averaging the sentiments of the sentences contained in the documents \n#   that were mapped to each topic\ntopic_sentiment_mapping = {}\nfor k, v in topic_doc_mapping.items():\n    topic_sentiment_mapping[k] = []\n    for doc_id in v:\n        topic_sentiment_mapping[k].append(all_sent_sentiments[doc_id])\ntopic_sentiment_mapping = {k: np.mean(v) for k, v in topic_sentiment_mapping.items()}\ntopic_sentiment_mapping","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit Rating to Topic\n\nBy modeling review rating on topic, I can determine if there is any relationship between them, and if there are any topics that influence rating, a regression will show their coefficient weighting."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a dataframe of review and lda_topics\ndnew = pd.DataFrame(lda_topics)\n\n\n# create map of sentence ID (global ID) to document\nglobal_id_to_doc = dict.fromkeys([i for i in range(lda_topics.shape[0])])\nfor sent_id in global_id_to_doc.keys():\n    for doc_id, sent_id_list in doc_global_id.items():\n        if sent_id in sent_id_list:\n            global_id_to_doc[sent_id] = doc_id\n    \ndnew['doc_id'] = dnew.index.map(global_id_to_doc)\n\n# average probability that a document relates to each topic\ndnew = dnew.groupby('doc_id').mean().reset_index()\ndnew['rating'] = dnew.doc_id.map(dict(zip(d.index, d.rating)))\ndnew.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# is there any relationship between the prob of a topic appearing and the rating?\n\nglm = GLM(fit_intercept=True, family=Poisson())\nglm.fit(X=dnew.drop(['doc_id', 'rating'], axis=1), y=dnew.rating)\npred_rating = glm.predict(dnew.drop(['doc_id', 'rating'], axis=1))\nprint(glm.summary())\nprint(\"RMSE:\", np.sqrt(mean_squared_error(dnew.rating, pred_rating)))\n\nsns.scatterplot(pred_rating, dnew.rating)\nplt.title(\"Predicted vs Actual Rating\")\nplt.xlabel(\"Predicted Rating\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The predicted vs actual rating shows that there is a lot of overlap between the range of the predicted ratings.  This lends support to the idea that binning them and doing classification might be more useful than trying to predict the precise rating.\n\nThis regression shows that the topic probabilities alone are not enough to reliably predict actual review rating.  There are some interesting findings.  For example, topic 14 (x15) has a large negative coefficient, suggesting that its presence indicates a low review.  Going back up to the LDA visualization, you can see that this topic contains words like \"disappointed\" and \"issues\". However, it also contains the words \"enjoyed\" and \"friend\".  This further complicates the interpretation of topics.  For this data, a better approach is to manually create topics, or basically just find sentences containing words you want to explore.  It's simple and dumb, but it works better than fancy data science in this case.\n\n### Manually Specified Topics\n\nAlthough LDA's topics were not very helpful, we could manually search for a word or string to create a topic.  For instance, if we extract all the sentences containing the word \"bed\", then average their sentiments, we can get an idea for what customers think of the hotel bed.  This would be much easier to interpret than topics modeled from LDA."},{"metadata":{"trusted":true},"cell_type":"code","source":"# find sentiment of all sentences containing a word or string - this can serve as a proxy to topic sentiment\nsents_containing_string = [si for si, s in enumerate(all_sents) if 'bed' in s]\ntopic_sentiment = np.mean([all_sent_sentiments[s] for s in sents_containing_string])\ntopic_sentiment","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So sentences containing the word \"bed\" were generally positive.  Note that these review cover many hotels.  If it were possible to focus on a single hotel, then this approach would be extremely useful for the business.\n\n## Predicting Review Rating\n\nThere is not much business use for predicting hotel ratings.  After all, the customer will provide a rating with the review, so there is no need to predict it.  But it still can be useful for finding the features that most contributed towards the rating.  By embedding the text and predicting the rating, I can explore the embeddings to see which features triggered the activations for very positive and very negative reviews.  This will be the best way to find hidden issues that the hotel may have.\n\n\n### Fitting DistilBERT and a CNN Classifier\n\nI want to combine the text data with the review sentiment score for classification.  So the model will need to take multiple inputs, since there is no need to embed the sentiment, but the text needs to be embedded.\n\nThe model will take the Bert tokenized text and masks as input to a transformer that will produce the embedding.  The embedding will be fed to a bi-directional LSTM layer.  The bi-directional aspect will allow the context of the entire review to assist with the interpretation of the embedding.  The output from this layer is down-sampled in the time dimension using max pooling. This will be the text processing piece.\n\nAnother piece of the model will look at review sentiment.  Each review has an average sentiment, and this will be concatenated with the downsampled output of the bi-directional LSTM.  The concatenated vectors will be fed to another dense layer, before ending up in the final layer with softmax activation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# bin the ratings:\n#  1-2 = bad\n#  3   = neutral\n#  4-5 = good\ndnew = d.copy()\ndnew['rating'] = dnew['rating'].map({1: 0, 2: 0, 3: 1, 4: 2, 5: 2})\ndnew.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encode the target and create a train/test split\n\ndnew_onehot = dnew.drop(['rating'], axis=1).join(pd.get_dummies(dnew.rating)).copy()\ndnew_onehot.rename(columns={0: 'bad', 1: 'neutral', 2: 'good'}, inplace=True)\n\n# uncomment to troubleshoot\n#dnew = dnew[dnew['rating']==0].head(1)\n\nx_train, x_test, y_train, y_test = train_test_split(\n    dnew_onehot.drop(['bad', 'neutral', 'good'], axis=1).values, dnew_onehot[['bad', 'neutral', 'good']].values, \n    test_size=TEST_SET_FRAC, random_state=SEED\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# free up memory\ndel d, dnew, dnew_onehot, all_sents, all_sent_sentiments, doc_topic_mapping, topic_doc_mapping, topic_sentiment_mapping, topwordsdf, lda_topics, word_freqs, sorted_word_freqs, reviews, reviews_info, doc_sents, doc_sent_sentiments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distil_bert = 'distilbert-base-uncased'\n\n# tokenize the text to prepare it for modeling, using Bert's tokenization method\n\ntokenizer = DistilBertTokenizer.from_pretrained(\n    distil_bert, \n    do_lower_case=True, \n    add_special_tokens=True, \n    max_length=128, \n    pad_to_max_length=True\n)\n\ndef tokenize(sentences, tokenizer):\n    input_ids, input_masks, input_segments = [],[],[]\n    for sentence in tqdm(sentences):\n        inputs = tokenizer.encode_plus(\n            sentence, \n            add_special_tokens=True,   \n            max_length=128, \n            pad_to_max_length=True,\n            return_attention_mask=True, \n            return_token_type_ids=True\n        )\n\n        input_ids.append(inputs['input_ids'])\n        input_masks.append(inputs['attention_mask'])\n        input_segments.append(inputs['token_type_ids'])\n\n    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n\n\n# only tokenize the text input, leave the sentiment part of the array alone\ntrain_tokens, train_masks, train_segments = tokenize(sentences=list(x_train[:,0]), tokenizer=tokenizer)\ntest_tokens, test_masks, test_segments = tokenize(sentences=list(x_test[:,0]), tokenizer=tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model with text and sentiment\n\nsentiment_dims = 1 or x_train[:,1].shape[1]\nnbr_classes = y_train.shape[1]\n\ndistil_bert = 'distilbert-base-uncased'\n\nconfig = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\nconfig.output_hidden_states = False\ntransformer_model = TFDistilBertModel.from_pretrained(distil_bert, config=config)\n   \ninput_layer_tokens = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\ninput_layer_masks = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32')\ninput_layer_sentiment = tf.keras.layers.Input(shape=(sentiment_dims,), name='sentiment', dtype='float32')\n\nembedding_layer = transformer_model(input_layer_tokens, attention_mask=input_layer_masks)[0]\nx = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\nx = tf.keras.layers.GlobalMaxPool1D()(x)\nx = tf.keras.layers.Dense(50, activation='relu')(x)\nx = tf.keras.Model(inputs=[input_layer_tokens, input_layer_masks], outputs=x)\n\ny = tf.keras.layers.Dense(50, activation='relu')(input_layer_sentiment)\ny = tf.keras.Model(inputs=input_layer_sentiment, outputs=y)\n\ncombined = tf.keras.layers.concatenate([x.output, y.output])\ncombined = tf.keras.layers.Dense(10, activation='relu')(combined)\n#combined = tf.keras.layers.Dropout(0.2)(combined)\ncombined = tf.keras.layers.Dense(nbr_classes, activation='softmax')(combined)\nmodel = tf.keras.Model(inputs=[x.input, y.input], outputs=combined)\n\n# freeze the first 3 layers of the combined model (the input layers and Bert embeddings)\nfor layer in model.layers[:3]:\n  layer.trainable = False\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = tf.keras.optimizers.Adam(learning_rate=5e-3)\nmodel.compile(\n    optimizer=opt,\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Train the model\n\nmodel_inputs = [train_tokens, train_masks, x_train[:,1].astype('float32').reshape(-1,)]\n\ntrain_history = model.fit(\n    model_inputs,\n    y_train,\n    validation_split=TEST_SET_FRAC,\n    batch_size=16,\n    epochs=NBR_EPOCHS,\n)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n  \"\"\"\n  This function prints and plots the confusion matrix.\n  Normalization can be applied by setting `normalize=True`.\n  \"\"\"\n  if normalize:\n      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n      print(\"Normalized confusion matrix\")\n  else:\n      print('Confusion matrix, without normalization')\n\n  print(cm)\n\n  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n  plt.title(title)\n  plt.colorbar()\n  tick_marks = np.arange(len(classes))\n  plt.xticks(tick_marks, classes, rotation=45)\n  plt.yticks(tick_marks, classes)\n\n  fmt = '.2f' if normalize else 'd'\n  thresh = cm.max() / 2.\n  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n      plt.text(j, i, format(cm[i, j], fmt),\n               horizontalalignment=\"center\",\n               color=\"white\" if cm[i, j] > thresh else \"black\")\n\n  plt.tight_layout()\n  plt.ylabel('True label')\n  plt.xlabel('Predicted label')\n  plt.show()\n\n\n# make predictions\np_test = model.predict([test_tokens, test_masks, x_test[:,1].astype('float32')]).argmax(axis=1)\n\n# convert y labels back to useable form\ny_test_clean = []\nfor p in y_test:\n    predicted_class = 1  # use neutral as baseline\n    for ci, c in enumerate(p):\n        if c == 1:\n            predicted_class = ci\n    y_test_clean.append(predicted_class)\ny_test_clean = np.array(y_test_clean)\n\n# evaluate model\ncm = confusion_matrix(y_test_clean, p_test)\nplot_confusion_matrix(cm, classes=['bad', 'neutral', 'good'])\nprint(\n    \"Accuracy:\", accuracy_score(y_true=y_test_clean, y_pred=p_test), \"\\n\",\n    \"F1 Score:\", f1_score(y_true=y_test_clean, y_pred=p_test, average='weighted'), \"\\n\",\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show some misclassified examples: predicted bad rating but it was actually good\nmisclassified_idx = np.where(((p_test == 0) & (y_test_clean == 2)))[0]\ni = np.random.choice(misclassified_idx)\nx_test[i,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show some misclassified examples: predicted good rating but it was actually bad\nmisclassified_idx = np.where(((p_test == 2) & (y_test_clean == 0)))[0]\ni = np.random.choice(misclassified_idx)\nx_test[i, 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The misclassified examples make sense.  The are tricky to classify because they say some good things about the hotel, despite the overall experience being bad.  It's like the reviewer is giving criticism to a friend and wants to soften the blow.  This confuses the model.\n\n### Embedding Visualization\n\nBy visualizing the embeddings, it will be easier to determine which features contribute to a rating.  TensorBoard would be perfect for this, but there seems to be a bug with the projector in TF2 (https://github.com/tensorflow/tensorboard/issues/2471).  So instead, I will create the 3D scatterplot manually, using PCA."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_embeddings(t_text=\"hello world\", t_inputs=None):\n    \"\"\"\n    Tokenizes provided text and returns BERT embeddings.  \n    BERT was never trained - its weights were frozen, so the initial pretrained weights \n    can be used to create the embeddings and they will match what came out of the model.\n    \n    Hugging Face's BERT returns a tuple.  The first item contains the embeddings.  The \n    second item contains the transformer's hidden states.  The final hidden state should \n    equal the embeddings: t_model(t_inputs)[0] == t_model(t_inputs)[1][-1]\n    \n    :params:\n        t_text: A string to be embedded.  This argument is ignored if t_inputs is provided.\n        t_inputs: A Numpy array of int32 type that contains the tokenized input for BERT.\n    \"\"\"\n    t_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n    t_config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n    # be sure to return hidden states\n    t_config.output_hidden_states = True\n    t_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=t_config)\n    if t_inputs is not None:\n        return t_model(t_inputs)[0]\n    else:\n        t_inputs = tokenize(t_text, tokenizer=t_tokenizer)[0]  # ignore the masks and segments, only return tokens\n        return t_model(t_inputs)[0]\n\n\nembeds = get_embeddings(t_inputs=test_tokens)\n\n# BERT output is a tuple of (batch_size, sequence_length, 768)\n# convert the embeds to a numpy array, and average them over the sequence axis (axis 1)\n# this will produce 1 vector per review\nreview_embeds = np.mean(embeds.numpy(), axis=1).squeeze()\n\n# apply PCA to reduce the dimensionality to 3\npca = PCA(n_components=3)\ncomponents = pca.fit_transform(review_embeds)\ntotal_var = pca.explained_variance_ratio_.sum() * 100\n\n# convert class labels to readable categories\nclass_labels = []\nfor c in y_test_clean.tolist():\n    if c == 0:\n        class_labels.append(\"Bad (Rating 1-2)\")\n    elif c == 1:\n        class_labels.append(\"Neutral (Rating 3)\")\n    else:\n        class_labels.append(\"Good (Rating 4-5)\")\n\n# convert reviews to readable format when they are hovered over\nreview_texts = x_test[:,0].tolist()\nsplit_after_n_words = 12\nreview_texts_formatted = []\nfor r in review_texts:\n    words = r.split()\n    total_words = len(words)\n    nbr_segments = np.ceil(total_words/split_after_n_words)\n    # insert line break after every nth word\n    words_new = [\n        x for y in (words[i:i+split_after_n_words] + ['<br>'] * (i < len(words) - 2) \n                    for i in range(0, len(words), split_after_n_words)) for x in y\n    ]\n    r_new = ' '.join(words_new)\n    review_texts_formatted.append(r_new)\n\n# plot with Plotly\nfig = px.scatter_3d(\n    components, x=0, y=1, z=2, color=class_labels, \n    title=f'First 3 Principal Components by Class, with Total Explained Variance: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'},\n    hover_name=review_texts_formatted\n)\nfig.update_traces(marker=dict(size=3))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot gives insight into possible areas for a hotel to improve, as well as the most liked features of a hotel.  The color shows the rating.  Reviews that are closer together are more similar.  \n\nRight away, a region of bad reviews becomes visible, where they all comment on the rudeness of the hotel staff.  There are some neutral reviews in that region, but it is clear that rude staff will not lead to a good review.  Another region shows good reviews that all reflect the quality of the room.  Things like \"elegant fixtures\" and \"large clean room\" are mentioned.  \n\nThere are regions that contain mixed reviews.  One of these concerns the noise level in the hotel.  One reviewer gave a bad review, complaining about the noise.  Another reviewer gave a good review, but said that it was a nice hotel if you don't mind the noise.  This is interesting, because a hotel could look at reviews like these to determine what kind of people would likely enjoy their stay more. \n\nThese are the kinds of insights that one might hope to gain from data science.  Many of them are still obvious (who wouldn't give rude staff a bad rating), but it is less about the rating and more about discovery.  Perhaps a hotel does not realize its staff are rude, and there are too many reviews to sift through to discover it.  Maybe a guest had a bedbug problem in one of the rooms, and the hotel needs to get on top of it, before it spreads.  Or maybe several good reviews mention the restaurant, and the hotel could benefit from marketing that."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}