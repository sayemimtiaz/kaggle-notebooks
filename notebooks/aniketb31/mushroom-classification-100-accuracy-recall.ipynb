{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Introduction and Approach\n\nThe study intends to identify whether a particular species of mushroom is edible or poisonous, on the basis of a number of features. The dataset includes hypothetical instances of mushrooms along with their values for all the features and identification of them as either edible or poisonous.\n\nThe below analysis intends to identify the features which are most indicative of mushroom edibility. For this purpose, the below techniques will be employed:\n1. Correlation Matrix\n2. Extra Trees Classifier for feature importance\n3. Detailed EDA on the identified features\n\n\nAdditionally, the analysis will identify the machine learning (ML) models which will perform best on the given dataset. The below ML models will analysed:\n1. SGD Classifier\n2. Logistic Regression\n3. K Nearest Neighbors\n4. SVC (Linear and RBF)\n5. Decision Tree\n6. Random Forest\n\nThe dataset will be split into train, test and validation sets. The models will be trained on train set and evaluated on validation set. The final model/s will give the results for test set. \n\nThe performance measures used are Accuracy, Precision and Recall. A study of this nature will be much more concerned with keeping the false negatives (identification of poisonous mushrooms as edible) as low as possible even at the cost of an increase in false positives (identification of edible mushrooms as poisonous). Hence, a high Recall will be much more important than a high Precision. \n\nThus, the final model/s will be shortlisted on the basis of their Accuracy and Recall. \n"},{"metadata":{},"cell_type":"markdown","source":"# Initial Impressions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the essential libraries \n\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom collections import defaultdict\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV, RandomizedSearchCV\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, SelectPercentile, f_classif\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\nfrom sklearn.ensemble import ExtraTreesClassifier \nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, precision_recall_curve, classification_report, roc_curve, roc_auc_score\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.calibration import CalibratedClassifierCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the data\n\ndf = pd.read_csv('../input/mushroom-classification/mushrooms.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the values are categorical. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"About 8000 instances. There are no null values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for any out of place values \n\nfor col in df.columns:\n    print(col, df[col].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stalk root has some values populated as ?. Let's address it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Zoom into the 'stalk root' column\n\ndf['stalk-root'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Significant number of rows in 'stalk root' are populated with '?'. The best approach will be to delete this column."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the stalk root column\n\ndf.drop('stalk-root', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rename the columns \n\ndf.rename(columns={'cap-shape':'cap_shape', 'cap-surface':'cap_surface', 'cap-color':'cap_color', \n                   'gill-attachment':'gill_attachment', 'gill-spacing':'gill_spacing', 'gill-size':'gill_size', \n                   'gill-color':'gill_color', 'stalk-shape':'stalk_shape', 'stalk-surface-above-ring':'stalk_surface_above_ring', \n                   'stalk-surface-below-ring':'stalk_surface_below_ring', \n                   'stalk-color-above-ring':'stalk_color_above_ring', 'stalk-color-below-ring':'stalk_color_below_ring', \n                   'veil-type':'veil_type', 'veil-color':'veil_color', 'ring-number':'ring_number', \n                   'ring-type':'ring_type', 'spore-print-color':'spore_print_color'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Done! Let's create a test set and keep it aside before conducting EDA. Moreover, it is a better practice to conduct EDA on the train set to prevent any observer bias. "},{"metadata":{},"cell_type":"markdown","source":"# Create a train, test and validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a train, test and validation set\n\n# The test and validation size is set at 1000. There isn't any particular scientific reason for this choice. It just felt right.\n# The shuffle is kept as True to ensure random distribution of instances in all sets. \n\n# Split out the test set from the original dataset\ndf_train_val, df_test = train_test_split(df, test_size=1000, random_state=42, shuffle=True)\n\n# Split the remaining data into train and validation sets\ndf_train, df_validation = train_test_split(df_train_val, test_size=1000, random_state=42, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a separate train set for EDA\n\n# A copy of train set is kept aside for conducting EDA since categories are to be encoded later.\n# Conducting EDA on alphabetical categories, rather than encoded ones, is easier for understanding the data. \n\ndf_train_eda = df_train.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create functions to encode the data \n\n# Reason for creating elaborate functions to encode the data:\n# 1. If LabelEncoder is applied directly on the entire data without splitting it, there will be information leakage into the test set. Hence, encoding had to be done after splitting.\n# 2. However, applying LabelEncoder object with fit_transform method on train set and later applying the same object with transform method on test and validation set didn't work.\n# 3. Hence, the need for elaborate functions which create encoded values out of the training data and later encode the train, test and validation set.\n# 4. If anyone knows a more efficient approach, please let me know. \n\n# A function to create and return a dictionary of alphabetical categories mapped to their numerical codes for all train set columns\n\ndef create_list_of_encoded_values(df): # Input will be the dataset on which encoder object will be fit\n    \n    le = LabelEncoder() # Labelencoder object\n    d_list = [] # An empty dictionary to store the alphabetical categories:codes mapping\n    \n    for col in df.columns: # For all columns, create the necessary mapping and add to the dictionary\n        le.fit(df[col]) \n        d_list.append(dict(zip(le.classes_, le.transform(le.classes_))))\n        \n    return d_list # Return the dictionary \n\n# A function to encode other datasets (of the same family as that which acted as input in the above function) on the basis of mapping done by previous function.\n\ndef encode_datasets(d_list, df): # The dictionary output by above function and the dataset on which encoding is to be done are the inputs\n    i=0\n    for col in df.columns:\n        df[col].replace(d_list[i], inplace=True)\n        i+=1\n        \n    return (df) # Return the encoded dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As planned, Create list of alphabetical categories:codes mapping from training data\n\nlist_encoded_values = create_list_of_encoded_values(df_train)\n\n# Encode train, test and validation data\n\ndf_train = encode_datasets(list_encoded_values, df_train)\ndf_test = encode_datasets(list_encoded_values, df_test)\ndf_validation = encode_datasets(list_encoded_values, df_validation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With that done, let's start the EDA now. "},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Which features are most indicative of a poisonous mushroom?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify the most important features using correlation matrix \n\ndf_train.corr()['class'].sort_values() # Check correlation of all features with 'class' ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, the features with an absolute correlation above 0.4 will be considered as most important identifier of mushroom edibility. Such features are Gill size, gill color, bruises and ring type. Let us analyze these features."},{"metadata":{},"cell_type":"markdown","source":"### Gill Size"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gill size\n\nfig, ax = plt.subplots(figsize=(10,7))\nplt.style.use('ggplot')\n\n# Plot the data\nsns.countplot(x='gill_size', data=df_train_eda, hue='class')\n\n# Manage the axes and title\nax.set_xlabel(\"Gill Size\",fontsize=20)\nax.set_ylabel('No. of Mushrooms',fontsize=20)\nax.set_title('Mushroom Gill Size vis-a-vis Edibility',fontsize=22)\nax.set_xticklabels(('broad', 'narrow'), fontsize = 12)\nax.grid(False)\n\n# Change the legend text\nL = plt.legend()\nL.get_texts()[0].set_text('Edible')\nL.get_texts()[1].set_text('Poisonous')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Most of edible mushrooms are broad while most of poisonous mushrooms are narrow. \n2. If one encounters a broad mushroom, there is a high probability that it is edible.\n3. If one encounters a narrow mushroom, there is a bery high probability that it is poisonous. "},{"metadata":{},"cell_type":"markdown","source":"### Gill Color"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gill color\n\nfig, ax = plt.subplots(figsize=(10,7))\nplt.style.use('ggplot')\n\nsns.countplot(x='gill_color', data=df_train_eda, hue='class')\n\nax.set_xlabel(\"Gill Color\",fontsize=20)\nax.set_ylabel('No. of Mushrooms',fontsize=20)\nax.set_title('Mushroom Gill Color vis-a-vis Edibility',fontsize=22)\nax.set_xticklabels(('purple', 'pink', 'red', 'brown', 'gray', 'buff', 'white', 'black', 'chocolate', 'yellow', 'orange', 'green'), fontsize = 12)\nax.grid(False)\n\nL = plt.legend()\nL.get_texts()[0].set_text('Edible')\nL.get_texts()[1].set_text('Poisonous')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Most of edible mushrooms are purple, pink, brown or white.\n2. Most of poisonous mushrooms are pink, gray, buff or chocolate.\n3. If one encounters a purple, brown, white or black mushroom, there is a high probability that it is edible.\n4. If one encounters a gray, buff or chocolate mushroom, there is a high probability that it is poisonous."},{"metadata":{},"cell_type":"markdown","source":"### Bruises"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bruises\n\nfig, ax = plt.subplots(figsize=(10,7))\nplt.style.use('ggplot')\n\nsns.countplot(x='bruises', data=df_train_eda, hue='class')\n\nax.set_xlabel(\"Bruises\",fontsize=20)\nax.set_ylabel('No. of Mushrooms',fontsize=20)\nax.set_title('Mushroom Bruises vis-a-vis Edibility',fontsize=22)\nax.set_xticklabels(('yes', 'no'), fontsize = 12)\nax.grid(False)\n\n#gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n\nL = plt.legend()\nL.get_texts()[0].set_text('Edible')\nL.get_texts()[1].set_text('Poisonous')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Most edible mushrooms are bruised while most poisonous ones are not.\n2. If one encounters bruised mushroom, there is a high probability that it is edible.\n3. If one encounters a non-bruised mushroom, there is a high probability that it is poisonous."},{"metadata":{},"cell_type":"markdown","source":"### Ring Type"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ring type\n\nfig, ax = plt.subplots(figsize=(10,7))\nplt.style.use('ggplot')\n\nsns.countplot(x='ring_type', data=df_train_eda, hue='class')\n\nax.set_xlabel(\"Ring Type\",fontsize=20)\nax.set_ylabel('No. of Mushrooms',fontsize=20)\nax.set_title('Mushroom Ring Types vis-a-vis Edibility',fontsize=22)\nax.set_xticklabels(('pendant', 'evanescent', 'large', 'none', 'flaring'), fontsize = 12)\nax.grid(False)\n\nL = plt.legend()\nL.get_texts()[0].set_text('Edible')\nL.get_texts()[1].set_text('Poisonous')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Most edible mushrooms have ring types - pendant and evanescent.\n2. Most poisonous mushrooms have ring types - evanescent and large. \n3. If one encounters a mushroom with ring type pendant, there is a high probability that it is edible.\n3. If one encounters a mushroom with ring types pendant or large , there is a high probability that it is poisonous."},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use ExtraTreesClassifier to find out the most important features which will be used as input for the model\n\nplt.figure(figsize=(20,15))\nplt.style.use('fivethirtyeight')\n\net_clf = ExtraTreesClassifier(random_state=42)\net_clf.fit(df_train.drop('class', axis=1), df_train['class'])\n\npd.Series(et_clf.feature_importances_, index=df_train.drop('class', axis=1).columns).nlargest(22).plot(kind='barh')\nplt.xlabel('Feature Importance')\nplt.title('Features and their Importance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The top 12 features (i.e. habitat and later) will be used as input for the models. The decision to shortlist 12 features is an arbitrary one. This number of input features can be considered as a 'hyperparameter' and can be varied to arrive at different models. \n\nThe Classifier identifies gill size, bruises, gill color and ring type as important features in line with the output of the correlation matrix. However, it identifies odor and spore print color as more important some of the above 4 features. I need to understand the mechanism behing the 2 techniques - ET Classifier and Corr matrix - to understand why the output differs as such. If someone already knows the reason behind this, please let me know."},{"metadata":{"trusted":true},"cell_type":"code","source":"# A function to create a train, test and validation set that contains only the top features \n\ndef new_set(x, old_set): # x is the number of features to be shortlisted, old_set is the parent set\n    \n    nue_set = pd.DataFrame()\n    \n    for col in pd.Series(et_clf.feature_importances_, index=df_train.drop('class', axis=1).columns).nlargest(x).index:\n        nue_set[col] = old_set[col]\n    nue_set['class'] = old_set['class']\n    \n    return (nue_set) # The 'reconstructed' set will be returned ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the new train and validation sets and split them into X and y. Test set will be dealt with later.\n\n# X train and y train \n\ndf_train_new = new_set(12, df_train)\n\nX_train = df_train_new.drop('class', axis=1)\ny_train = df_train_new['class']\n\n# X validation and y validation \n\ndf_validation_new = new_set(12, df_validation)\n\nX_val = df_validation_new.drop('class', axis=1)\ny_val = df_validation_new['class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the train and validation data\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train) # Fit the scaler object on X_train and transform it\n\nX_val = scaler.transform(X_val) # Use the object already fitted on X_train to transform X_val","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this done, let's start the modelling."},{"metadata":{},"cell_type":"markdown","source":"# What types of machine learning models perform best on this dataset?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check the bare bones classifiers first\n\nsgd_clf = SGDClassifier(random_state=42) # Standard SGD Classifier with hinge loss; this is equivalent to linear SVM\nlog_clf = LogisticRegression(random_state=42) # Standard logistic regression\nknn_clf = KNeighborsClassifier() # Standard KNN classifier \nsvc_clf = SVC(random_state=42) # Standard SVC with RBF kernel\nlsvc_clf = LinearSVC(random_state=42) # SVC with linear kernel\ndt_clf = DecisionTreeClassifier() # Standard decision tree \nrf_clf = RandomForestClassifier() # Standard random forest\n\nmodels = [sgd_clf, log_clf, knn_clf, svc_clf, lsvc_clf, dt_clf, rf_clf]\naccuracy_scores = [] \nrecall_scores = []\n\nfor clf in [sgd_clf, log_clf, knn_clf, svc_clf, lsvc_clf, dt_clf, rf_clf]:\n    \n    # Fit the classifier on training data\n    clf.fit(X_train, y_train)\n    \n    # Make predictions for validation data\n    y_pred = clf.predict(X_val)\n    \n    # Performance measures for validation data\n    print('confusion matrix for {}:'.format(clf.__class__.__name__), '\\n', confusion_matrix(y_val, y_pred))\n    print('precision score for {}:'.format(clf.__class__.__name__), precision_score(y_val, y_pred))\n    print('recall score for {}:'.format(clf.__class__.__name__), recall_score(y_val, y_pred))\n    print('accuracy score for {}:'.format(clf.__class__.__name__), accuracy_score(y_val, y_pred))\n    print('-'*100)\n    \n    accuracy_scores.append(accuracy_score(y_val, y_pred))\n    recall_scores.append(recall_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KNN, SVC, Decision tree and Random forest classifiers show 100% accuracy, precision and recall. The classifiers have been trained and validated on different datasets. So, even if the models are 'overfitting', they are doing a perfect job. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us compare the Accuracy and Recall scores for all the models.\n# As mentioned in the Introduction, Recall is more important than Precision in this study.\n\nplt.figure(figsize=(15,6))\nplt.style.use('fivethirtyeight')\n\nmylist = ['SGD Classifier', 'Logistic Regression', 'KNN', 'SVC', 'Linear SVC', 'Decision Tree', 'Random Forest']\n\nsns.lineplot(x=mylist, y=accuracy_scores, label='accuracy')\nsns.lineplot(x=mylist, y=recall_scores, label='recall')\n\nplt.title('Accuracy & Recall for Classifiers')\nplt.xlabel('Classifiers')\nplt.ylabel('Accuracy/Recall score')\n\nplt.legend(loc='center left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though Decision tree, KNN, Random forest and SVC are working perfectly, other classifiers - Linear SVC, SGD Classifier and Logistic regression - are not doing a bad jobs either with more than 92% accuracy and recall.\n\nLet's see if we can improve these 3 'underperforming' classifiers with hyperparameter tuning."},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter tuning"},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter tuning for SGD Classifier "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter tuning of SGDClassifier\n\n# Tuning of 'penalty' and 'alpha' hyperparameters is intended to modify the regularization of the classifier so that it fits more snugly to the data.\n# Tuning of 'max_iter' is done so that the optimization doesn't stop prematurely for lack of iterations allowed.\n\nparameters = [{'penalty':['l1', 'l2'], 'alpha':np.arange(0.00005, 0.001, 0.00005), 'max_iter':range(1000, 2000, 100)}]\n\nsgd_clf = SGDClassifier(random_state=42)\n\ngrid_search_sgd = GridSearchCV(sgd_clf, parameters, cv=3, scoring='accuracy') # cv is 5 by default, n_iter is 10 by default\n\ngrid_search_sgd.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters for SGD Classifier\n\ngrid_search_sgd.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's fit the best estimator on validation data and see if there is any improvement vis-a-vis previous SGD classifier model\n\n# Best estimator \nsgd_best = grid_search_sgd.best_estimator_\n\n# Make predictions for validation data\ny_pred = sgd_best.predict(X_val)\n    \n# Performance measures for validation data\nprint('confusion matrix:', '\\n', confusion_matrix(y_val, y_pred))\nprint('precision score:', precision_score(y_val, y_pred))\nprint('recall score:', recall_score(y_val, y_pred))\nprint('accuracy score:', accuracy_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A slight improvement in precision but decline in recall performance, which is of more importance to us than precision. The accuracy is same as before. The 'tuned' model doesn't help us much."},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter tuning for Logistic Regression Classifier "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter tuning for Logistic Regression\n\n# Tuning of 'penalty', 'C' and 'solver' hyperparameters is intended to modify the regularization of the classifier so that it fits more snugly to the data.\n\nparameters = [{'penalty':['l1', 'l2'], 'C':np.arange(0.1, 2.0, 0.1), 'solver':['liblinear', 'saga']},\n              {'penalty':['l2'], 'C':np.arange(0.1, 2.0, 0.1), 'solver':['newton-cg', 'lbfgs', 'sag']}]\n\nlog_clf = LogisticRegression(random_state=42)\n\ngrid_search_log = GridSearchCV(log_clf, parameters, cv=3, scoring='accuracy') # cv is 5 by default, n_iter is 10 by default\n\ngrid_search_log.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters for Logistic Regression\n\ngrid_search_log.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's fit the best estimator on validation data and see if there is any improvement vis-a-vis previous SGD classifier model\n\n# Best estimator\nlog_best = grid_search_log.best_estimator_\n\n# Make predictions for validation data\ny_pred = log_best.predict(X_val)\n    \n# Performance measures for validation data\nprint('confusion matrix:', '\\n', confusion_matrix(y_val, y_pred))\nprint('precision score:', precision_score(y_val, y_pred))\nprint('recall score:', recall_score(y_val, y_pred))\nprint('accuracy score:', accuracy_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy, Precision and Recall results are exactly same as before. 'Tuned' model isn't of much use."},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter tuning for Linear SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameter tuning for linear SVC \n\n# Tuning of 'penalty' and 'C' hyperparameters is intended to modify the regularization of the classifier so that it fits more snugly to the data.\n# Tuning of 'max_iter' is done so that the optimization doesn't stop prematurely for lack of iterations allowed.\n\nparameters = [{'penalty':['l1', 'l2'], 'C':np.arange(0.1, 5.0, 0.2), 'dual':[False], 'max_iter':range(1000, 2000, 100)},\n             {'penalty':['l2'], 'C':np.arange(0.1, 5.0, 0.2), 'loss':['squared_hinge'], 'max_iter':range(1000, 2000, 100)}]\n\nlsvc_clf = LinearSVC(random_state=42)\n\ngrid_search_lsvc = GridSearchCV(lsvc_clf, parameters, cv=3, scoring='accuracy') # cv is 5 by default, n_iter is 10 by default\n\ngrid_search_lsvc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best parameters for Linear SVC\n\ngrid_search_lsvc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's fit the best estimator on validation data and see if there is any improvement vis-a-vis previous SGD classifier model\n\n# Best estimator\n\nlsvc_best = grid_search_lsvc.best_estimator_\n\n# Make predictions for validation data\ny_pred = lsvc_best.predict(X_val)\n    \n# Performance measures for validation data\nprint('confusion matrix:', '\\n', confusion_matrix(y_val, y_pred))\nprint('precision score:', precision_score(y_val, y_pred))\nprint('recall score:', recall_score(y_val, y_pred))\nprint('accuracy score:', accuracy_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, there is no improvement at all for the 'tuned' classifier over the 'untuned' one. "},{"metadata":{},"cell_type":"markdown","source":"## Ensemble methods \n\nSince Hyperparameter tuning hasn't delivered, let us see if ensemble methods can help."},{"metadata":{},"cell_type":"markdown","source":"### Voting Classifier\n\nLet us create a voting classifier with the 3 'tuned and underperforming' classifiers - SGD Classifier, Logistic regression and Linear SVC - and evaluate it on the validation data.\n\nVoting classifier aggregates the predictions of each classifier for each instance and predict the class, for that instance, that gets the most votes. Theoretically, even if each classifier only has a 51% accuracy, voting classifier can give upto 75% accuracy. Let's check it out. (Ref: Hands on ML with Scikit Learn and Tensorflow by Aurelien Geron)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Voting Classifier \n\nvoting_clf = VotingClassifier(estimators=[('lr', log_best), ('sg', sgd_best), ('lsvc', lsvc_best)], voting='hard')\n\nvoting_clf.fit(X_train, y_train)\n\n# Make predictions for validation data\ny_pred = voting_clf.predict(X_val)\n    \n# Performance measures for validation data\nprint('confusion matrix:', '\\n', confusion_matrix(y_val, y_pred))\nprint('precision score:', precision_score(y_val, y_pred))\nprint('recall score:', recall_score(y_val, y_pred))\nprint('accuracy score:', accuracy_score(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nope. No improvement at all. "},{"metadata":{},"cell_type":"markdown","source":"### Adaboost Classifier \n\nIn an AdaBoost classifier, a first base classifier (such as Linear SVC) is trained and used to make predictions on the training set. The relative weight of misclassified training instances is then increased. A second classifier is trained using the updated weights and again it makes predictions on the training set, weights are updated, and so on. (Ref: Hands on ML with Scikit Learn and Tensorflow by Aurelien Geron)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# AdaBoost Classifier \n\nfor clf in [log_best, lsvc_best]: # Haven't considered SGD classifier here as an error is encountered during its execution which I am not able to understand. \n    ada_clf = AdaBoostClassifier(clf, n_estimators=200, algorithm=\"SAMME\", learning_rate=0.5)\n    ada_clf.fit(X_train, y_train)\n    \n    # Make predictions for validation data\n    y_pred = ada_clf.predict(X_val)\n    \n    # Performance measures for validation data\n    print('confusion matrix for {}:'.format(clf.__class__.__name__), '\\n', confusion_matrix(y_val, y_pred))\n    print('precision score for {}:'.format(clf.__class__.__name__), precision_score(y_val, y_pred))\n    print('recall score for {}:'.format(clf.__class__.__name__), recall_score(y_val, y_pred))\n    print('accuracy score for {}:'.format(clf.__class__.__name__), accuracy_score(y_val, y_pred))\n    print('-'*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No improvement. Instead, the performance decreases for Linear SVC. \n\nLet us move on with out best performing classifiers - KNN, SVC, Decision tree and Random forest. Mind you, the other 3 'underperforming' classifiers aren't performing bad either and we can evaluate them on test data too. However, for sake of simplicity, let's keep them aside for now."},{"metadata":{},"cell_type":"markdown","source":"Thus, the best performing ML models for the dataset are KNN, SVC, Decision tree and Random forest. "},{"metadata":{},"cell_type":"markdown","source":"### Evaluation on Test set\n\nLet us evaluate our best performing models on Test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Process the test set data \n\n# X test and y test \n\ndf_test_new = new_set(12, df_test)\n\nX_test = df_test_new.drop('class', axis=1)\ny_test = df_test_new['class']\n\n# Scale the X test values\n\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate the Classifiers on the Test data\n\nfor clf in [knn_clf, svc_clf, dt_clf, rf_clf]:\n    \n    # Make predictions for test data\n    y_pred = clf.predict(X_test)\n    \n    # Performance measures for test data\n    print('confusion matrix for {}:'.format(clf.__class__.__name__), '\\n', confusion_matrix(y_test, y_pred))\n    print('precision score for {}:'.format(clf.__class__.__name__), precision_score(y_test, y_pred))\n    print('recall score for {}:'.format(clf.__class__.__name__), recall_score(y_test, y_pred))\n    print('accuracy score for {}:'.format(clf.__class__.__name__), accuracy_score(y_test, y_pred))\n    print('-'*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A 100% Accuracy, Precision and Recall on the Test data!"},{"metadata":{},"cell_type":"markdown","source":"### Summary \n\n1. KNN, SVC, Decision tree and Random forest are best performing Classifiers on the data with a 100% Accuracy and Recall. \n2. The features - Gill size, gill color, bruises and ring type - are most important indicators of mushroom edibility. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}