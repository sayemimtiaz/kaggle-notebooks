{"cells":[{"metadata":{"_uuid":"36fd2490ab780dd1298aae2c14631950aefab8ef"},"cell_type":"markdown","source":"# Time Series For beginners with ARIMA\n\n**Author:** Arindam Chatterjee  \n**Start Date:** 10th July, 2018\n\n**Purpose:** The objective of this notebook is to have a simplified template to understand Time series forecasting with ARIMA model in python, acting more as a tutorial.\n\n**Objective:** Build a model to forecast the demand(passenger traffic) in Airplanes. The data is classified in date/time and the passengers travelling per month"},{"metadata":{"_uuid":"e025f18744ae4c8e784aec0d1ea92a48dde55995"},"cell_type":"markdown","source":"## Table of contents\n1. [QnA](#QnA)\n2. [Import Libraries](#Import-Libraries)\n3. [Read Data](#read-data)\n4. [Data Transformation to achieve Stationarity](#data-transform)\n    1. [Log Scale Transformation](#log)\n    2. [Exponential Decay Transformation](#exp)\n    3. [Time Shift Transformation](#shift)    \n5. [Plotting ACF & PACF](#acf-pacf)\n6. [Building Models](#model)\n7. [Prediction & Reverse transformations](#prediction)\n"},{"metadata":{"_uuid":"302d7e4e30dca4b7c11e2e50f966eb064c7b1f74"},"cell_type":"markdown","source":"## QnA <a name=\"QnA\"></a>\n\n**1. What is Time series analysis?**  \nA. Time Series is a series of observations taken at specified time intervals usually equal intervals. Analysis of the series helps us to predict future values based on previous observed values. In Time series, we have only 2 variables, time & the variable we want to forecast.  \n\n  \n**2. Why & where Time Series is used?**  \nA. Time series data can be analysed in order to extract meaningful statistics and other charecteristsics. It's used in atleast the 4 scenarios:  \n    a) Business Forecasting  \n    b) Understand past behavior  \n    c) Plan the future  \n    d) Evaluate current accomplishment  \n  \n**3. When shouldn't we use Time Series Analysis?**  \nA. We don't need to apply Time series in atleast the following 2 cases:  \n    a) The dependant variable(y) (that is supposed to vary with time) is constant. Eq: y=f(x)=4, a line parallel to x-axis(time) will always remain the same.  \n    b) The dependant variable(y) represent values that can be denoted as a mathematical function. Eq: sin(x), log(x), Polynomials etc. Thus, we can directly get value at some time using the function itself. No need of forecasting.  \n  \n**4. What are the components of Time Series?**  \nA. There are 4 components:  \n    a) Trend - Upward & downward movement of the data with time over a large period of time. Eq: Appreciation of Dollar vs rupee.  \n    b) Seasonality - seasonal variances. Eq: Ice cream sales increases in Summer only  \n    c) Noise or Irregularity - Spikes & troughs at random intervals  \n    d) Cyclicity - behavior that repeats itself after large interval of time, like months, years etc.  \n    \n**5. What is Stationarity?**    \nA. Before applying any statistical model on a Time Series, the series has to be staionary, which means that, over different time  periods,  \n    a) It should have constant mean.  \n    b) It should have constant variance or standard deviation.  \n    c) Auto-covariance should not depend on time.  \n\nTrend & Seasonality are two reasons why a Time Series is not stationaru & hence need to be corrected.\n    \n**6. Why does Time Series(TS) need to be stationary?**  \nA. It is because of the following reasons:  \n    a) If a TS has a particular behavior over a time interval, then there's a high probability that over a different interval, it will have same behavior, provided TS is stationary. This helps in forecasting accurately.  \n    b) Theories & Mathematical formulas ae more mature & easier to apply for as TS which is stationary.  \n\n**7. Tests to check if a series is stationary or not**  \nA. There are 2 ways to check for Stationarity of a TS:  \n    a) Rolling Statistics - Plot the moving avg or moving standard deviation to see if it varies with time. Its a visual technique.  \n    b) ADCF Test - Augmented Dickey–Fuller test is used to gives us various values that can help in identifying stationarity. The Null hypothesis says that a TS is non-stationary. It comprises of a **Test Statistics** & some **critical values** for some confidence levels. If the Test statistics is less than the critical values, we can reject the null hypothesis & say that the series is stationary. THE ADCF test also gives us a **p-value**. Acc to the null hypothesis, lower values of p is better.\n    \n**8. What is ARIMA model?**      \nA. ARIMA(Auto Regressive Integrated Moving Average) is a combination of 2 models AR(Auto Regressive) & MA(Moving Average). It has 3 hyperparameters - P(auto regressive lags),d(order of differentiation),Q(moving avg.) which respectively comes from the AR, I & MA components. The AR part is correlation between prev & current time periods. To smooth out the noise, the MA part is used. The I part binds together the AR & MA parts. \n\n**9. How to find value of P & Q for ARIMA ?**  \nA. We need to take help of ACF(Auto Correlation Function) & PACF(Partial Auto Correlation Function) plots.\nACF & PACF graphs are used to find value of P & Q for ARIMA. We need to check, for which value in x-axis, graph line drops to 0 in y-axis for 1st time.  \nFrom PACF(at y=0), get P  \nFrom ACF(at y=0), get Q  \n\n**10. What Is ADCF test?**  \nA. In statistics and econometrics, an augmented Dickey–Fuller test (ADF) tests the null hypothesis that a unit root is present in a time series sample. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity. It is an augmented version of the Dickey–Fuller test for a larger and more complicated set of time series models.\n\nThe augmented Dickey–Fuller (ADF) statistic, used in the test, is a negative number. The more negative it is, the stronger the rejection of the hypothesis that there is a unit root at some level of confidence.\n\np value(0<=p<=1) should be as low as possible. Critical values at different confidence intervals should be close to the Test statistics value.\n\n**11. What is Exponential Smoothing?**  \nA. *Exponential smoothing* is a rule of thumb technique for smoothing time series data using the exponential window function. Whereas in the simple moving average the past observations are weighted equally, exponential functions are used to assign exponentially decreasing weights over time. It is an easily learned and easily applied procedure for making some determination based on prior assumptions by the user, such as seasonality. Exponential smoothing is often used for analysis of time-series data.\n\nThe raw data sequence is often represented by ${x_{t}}$ beginning at time $t=0$, and the output of the exponential smoothing algorithm is commonly written as ${s_{t}}$, which may be regarded as a best estimate of what the next value of $x$ will be. When the sequence of observations begins at time $t=0$, the simplest form of exponential smoothing is given by the formulas:  \n\n$s_{0} = x_{0}$  \n$s_{t} = α*x_{t} + (1-α)*s_{t-1}$  , $t>0$  \n\nwhere $α$ is the smoothing factor, and $0<α<1$.\n\n**12. What is Exponential decay?**  \nA. A quantity is subject to exponential decay if it decreases at a rate proportional to its current value. Symbolically, this process can be expressed by the following differential equation, where N is the quantity and λ (lambda) is a positive rate called the exponential decay constant:\n\n$dN/dt = -λN$\n\nThe solution to this equation (see derivation below) is:  \n$N(t) = N_{0}*e^{-λt}$  \n\nwhere N(t) is the quantity at time t, and N0 = N(0) is the initial quantity, i.e. the quantity at time t = 0.  \n\n**Half Life** is the time required for the decaying quantity to fall to one half of its initial value. It is denoted by $t_{1/2}$. The half-life can be written in terms of the decay constant as:  \n\n$t_{1/2} = ln(2)/λ$  \n"},{"metadata":{"_uuid":"c451af2e223b8e288924bf3a5d71f75d31956e53"},"cell_type":"markdown","source":"## Import Libraries <a name=\"Import-Libraries\"></a>"},{"metadata":{"trusted":false,"_uuid":"abd1fe2388290c5f4be239536f3eaed563639fa7"},"cell_type":"code","source":"from datetime import datetime\nimport numpy as np             #for numerical computations like log,exp,sqrt etc\nimport pandas as pd            #for reading & storing data, pre-processing\nimport matplotlib.pylab as plt #for visualization\n#for making sure matplotlib plots are generated in Jupyter notebook itself\n%matplotlib inline             \nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 10, 6","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f696360018577b8163554b95454e24edb04cdb43"},"cell_type":"markdown","source":"## Read Data  <a name=\"read-data\"></a>\nTime series deals with 2 columns, one is temporal ie: month in this case & another is the value to be forecasted ie: airplane passengers. To make plotting graphs easier, we set the index of pandas dataframe to the Month. During plots, the index will act by default as the x-axis & since it has only 1 more column, that will be automatically taken as the y-axis"},{"metadata":{"trusted":false,"_uuid":"1d2a64e9b0d8b8485936e973468b2ccb25e706c2"},"cell_type":"code","source":"#path = \"input/AirPassengers.csv\" #For local\npath = \"../input/AirPassengers.csv\" #For Kaggle\ndataset = pd.read_csv(path)\n#Parse strings to datetime type\ndataset['Month'] = pd.to_datetime(dataset['Month'],infer_datetime_format=True) #convert from string to datetime\nindexedDataset = dataset.set_index(['Month'])\nindexedDataset.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e71de03beca98002540379de58943270006a6ae"},"cell_type":"markdown","source":"From the plot below, we can see that there is a Trend compoenent in th series. Hence, we now check for stationarity of the data"},{"metadata":{"trusted":false,"_uuid":"761cd88f2629f435defbe769b7bb0b0e320c97d1"},"cell_type":"code","source":"## plot graph\nplt.xlabel('Date')\nplt.ylabel('Number of air passengers')\nplt.plot(indexedDataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"720f4fff3d03fc7daa98d4412f0bcddf1f2d89fa"},"cell_type":"code","source":"#Determine rolling statistics\nrolmean = indexedDataset.rolling(window=12).mean() #window size 12 denotes 12 months, giving rolling mean at yearly level\nrolstd = indexedDataset.rolling(window=12).std()\nprint(rolmean,rolstd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"286648e049e9ec95c2c648e2421ede8d3208d060"},"cell_type":"code","source":"#Plot rolling statistics\norig = plt.plot(indexedDataset, color='blue', label='Original')\nmean = plt.plot(rolmean, color='red', label='Rolling Mean')\nstd = plt.plot(rolstd, color='black', label='Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')\nplt.show(block=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9dd9317997eaf9d86c1615b144d832d9a2547cb9"},"cell_type":"markdown","source":"From the above graph, we see that rolling mean itself has a trend component even though rolling standard deviation is fairly constant with time. For our time series to be stationary, we need to ensure that both the rolling statistics ie: mean & std. dev. remain time invariant or constant with time. Thus the curves for both of them have to be parallel to the x-axis, which in our case is not so. \n\nTo further augment our hypothesis that the time series is not stationary, let us perform the ADCF test."},{"metadata":{"trusted":false,"_uuid":"424d7f23806906f44e793442fdd451987c9b33d7"},"cell_type":"code","source":"#Perform Augmented Dickey–Fuller test:\nprint('Results of Dickey Fuller Test:')\ndftest = adfuller(indexedDataset['#Passengers'], autolag='AIC')\n\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\n    \nprint(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe828fcb190363b6aba127ecea7f5b7e1fd554bc"},"cell_type":"markdown","source":"For a Time series to be stationary, its ADCF test should have:\n1. p-value to be low (according to the null hypothesis)\n2. The critical values at 1%,5%,10% confidence intervals should be as close as possible to the Test Statistics\n\nFrom the above ADCF test result, we see that p-value(at max can be 1.0) is very large. Also critical values are no where close to the Test Statistics. Hence, we can safely say that **our Time Series at the moment is not stationary**"},{"metadata":{"_uuid":"ff2e442d4169e7c979efbd402e835c01c0b2c6fc"},"cell_type":"markdown","source":"## Data Transformation to achieve Stationarity <a name=\"data-transform\"></a>\n\nThere are a couple of ways to achieve stationarity through data transformation like taking $log_{10}$,$log_{e}$, square, square root, cube, cube root, exponential decay, time shift and so on ...\n\nIn our notebook, lets start of with log transformations. Our objective is to remove the trend component. Hence,  flatter curves( ie: paralle to x-axis) for time series and rolling mean after taking log would say that our data transformation did a good job."},{"metadata":{"_uuid":"2daca62f26cb0736d99a037b8cfac78f4e2c3178"},"cell_type":"markdown","source":"### Log Scale Transformation  <a name=\"log\"></a>"},{"metadata":{"trusted":false,"_uuid":"90510fb0b230444f44eee765824a7c6dd2fb4b5a"},"cell_type":"code","source":"#Estimating trend\nindexedDataset_logScale = np.log(indexedDataset)\nplt.plot(indexedDataset_logScale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"391c21fba03415cc9a0d79bba92c353b13faf6fe"},"cell_type":"code","source":"#The below transformation is required to make series stationary\nmovingAverage = indexedDataset_logScale.rolling(window=12).mean()\nmovingSTD = indexedDataset_logScale.rolling(window=12).std()\nplt.plot(indexedDataset_logScale)\nplt.plot(movingAverage, color='red')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ac9dc414702d89049c3ad226e4f8fc7d6514dee"},"cell_type":"markdown","source":"From above graph, we see that even though rolling mean is not stationary, it is still better than the previous case, where no transfromation were applied to series. So we can atleast say that we are heading in the right direction.\n\nWe know from above graph that both the Time series with log scale as well as its moving average have a trend component. Thus we can apply a elementary intuition: subtraction one from the other should remove the trend component of both. Its like:  \n\n$log scale L = stationary part(L1) + trend(LT)$   \n$moving avg of log scale A = stationary part(A1) + trend(AT)$   \n$result series R = L - A = (L1+LT) - (A1+AT) = (L1-A1) + (LT-AT)$\n\nSince, L & A are series & it moving avg, their trend will be more or less same, Hence  \nLT-AT nearly equals to 0  \n\nThus trend component will be almost removed. And we have,  \n  \n$R = L1 - A1$, our final non-trend curve"},{"metadata":{"trusted":false,"_uuid":"b022e936f5b22d9e1847190abe37c68012fd3b95"},"cell_type":"code","source":"datasetLogScaleMinusMovingAverage = indexedDataset_logScale - movingAverage\ndatasetLogScaleMinusMovingAverage.head(12)\n\n#Remove NAN values\ndatasetLogScaleMinusMovingAverage.dropna(inplace=True)\ndatasetLogScaleMinusMovingAverage.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fdbbf1c96364c98206ba49f454c0c4e200945738"},"cell_type":"code","source":"def test_stationarity(timeseries):\n    \n    #Determine rolling statistics\n    movingAverage = timeseries.rolling(window=12).mean()\n    movingSTD = timeseries.rolling(window=12).std()\n    \n    #Plot rolling statistics\n    orig = plt.plot(timeseries, color='blue', label='Original')\n    mean = plt.plot(movingAverage, color='red', label='Rolling Mean')\n    std = plt.plot(movingSTD, color='black', label='Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey–Fuller test:\n    print('Results of Dickey Fuller Test:')\n    dftest = adfuller(timeseries['#Passengers'], autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3022a9239558aca693cf4607f00a9eaa1cfce37b"},"cell_type":"code","source":"test_stationarity(datasetLogScaleMinusMovingAverage)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8df1ecf50a8e19504cdd513930364fb23307baa"},"cell_type":"markdown","source":"From above graph, we observe that our intuition that *\"subtracting two related series having similar trend components will make the result stationary\"* is true. We find that:  \n\n1. p-value has reduced from 0.99 to 0.022.  \n2. The critical values at 1%,5%,10% confidence intervals are pretty close to the Test Statistic.\nThus, from above 2 points, we can say that our given series is stationary.  \n\nBut, in the spirit of getting higher accuracy, let us explore & try to find a better scale than our current log.\n\nLet us try out Exponential decay.  \nFor further info, refer to my answer 12 at the top of the notebook on it."},{"metadata":{"_uuid":"35a7bdf0fa22bf5776a4d5a715e0531b2688a889"},"cell_type":"markdown","source":"### Exponential Decay Transformation   <a name=\"exp\"></a>"},{"metadata":{"trusted":false,"_uuid":"b0a6d6f496870b20abaf04d889496d6db921759e"},"cell_type":"code","source":"exponentialDecayWeightedAverage = indexedDataset_logScale.ewm(halflife=12, min_periods=0, adjust=True).mean()\nplt.plot(indexedDataset_logScale)\nplt.plot(exponentialDecayWeightedAverage, color='red')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4443cd76dc7af382438df8d0ab10217560891ff8"},"cell_type":"markdown","source":"From above graph, it seems that exponential decay is not holding any advantage over log scale as both the corresponding curves are similar. But, in statistics, inferences cannot be drawn simply by looking at the curves. Hence, we perform the ADCF test again on the decay series below."},{"metadata":{"trusted":false,"_uuid":"c2e6f7b573fd615b18fbcd3c8ea5ef1e0549d28e"},"cell_type":"code","source":"datasetLogScaleMinusExponentialMovingAverage = indexedDataset_logScale - exponentialDecayWeightedAverage\ntest_stationarity(datasetLogScaleMinusExponentialMovingAverage)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14d7b74db220e1f9ae575d8eaa5cbd4b78e996ed"},"cell_type":"markdown","source":"We observe that the Time Series is stationary & also the series for moving avg & std. dev. is almost parallel to x-axis thus they also have no trend.  \nAlso,     \n1. p-value has decreased from 0.022 to 0.005.  \n2. Test Statistic value is very much closer to the Critical values.  \nBoth the points say that our current transformation is better than the previous logarithmic transformation. Even though, we couldn't observe any differences by visually looking at the graphs, the tests confirmed decay to be much better.\n\nBut lets try one more time & find if an even better solution exists. We will try out the simple time shift technique, which is simply:  \n\nGiven a set of observation on the time series:  \n$ x0, x1, x2, x3, .... xn $  \n\nThe shifted values will be:    \n$ null, x0, x1, x2,.... xn $                             <---- basically all xi's shifted by 1 pos to right  \n\nThus, the time series with time shifted values are:   \n$ null, (x1-x0), (x2-x1), (x3-x2), (x4-x3),.... (xn-x_{n-1}) $   "},{"metadata":{"_uuid":"62d6495976ac9a3e825fc033982db13b60635150"},"cell_type":"markdown","source":"### Time Shift Transformation  <a name=\"shift\"></a>"},{"metadata":{"trusted":false,"_uuid":"c6ebf0391698cf9dc74b7f550c2e9b6eb74d17d9"},"cell_type":"code","source":"datasetLogDiffShifting = indexedDataset_logScale - indexedDataset_logScale.shift()\nplt.plot(datasetLogDiffShifting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4670612db47e5c43b47405e2e56129a95e055a16"},"cell_type":"code","source":"datasetLogDiffShifting.dropna(inplace=True)\ntest_stationarity(datasetLogDiffShifting)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df10f819ee7cd6475a527db13acbbdb6958d2e1f"},"cell_type":"markdown","source":"From above 2 graphs, we can see that, visually this is the best result as our series along with rolling statistic values of moving avg & moving std. dev. is very much flat & stationary. But, the ADCF test shows us that:\n1. p-value of 0.07 is not as good as 0.005 of exponential decay.  \n2. Test Statistic value not as close to the critical values as that for exponential decay.  \n  \nWe have thus tried out 3 different transformation: log, exp decay & time shift. For simplicity, we will go with the log scale. The reason for doing this is that we can revert back to the original scale during forecasting.\n\nLet us now break down the 3 components of the log scale series using a system libary function. Once, we separate our the components, we can simply ignore trend & seasonality and check on the nature of the residual part."},{"metadata":{"trusted":false,"_uuid":"72e319a859df4156f25348c49c205af72bc2f6bc"},"cell_type":"code","source":"decomposition = seasonal_decompose(indexedDataset_logScale) \n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(indexedDataset_logScale, label='Original')\nplt.legend(loc='best')\n\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\n\nplt.subplot(411)\nplt.plot(seasonal, label='Seasonality')\nplt.legend(loc='best')\n\nplt.subplot(411)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\n\nplt.tight_layout()\n\n#there can be cases where an observation simply consisted of trend & seasonality. In that case, there won't be \n#any residual component & that would be a null or NaN. Hence, we also remove such cases.\ndecomposedLogData = residual\ndecomposedLogData.dropna(inplace=True)\ntest_stationarity(decomposedLogData)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5b746fabed53cba2647787734713196ef21d20ee"},"cell_type":"code","source":"decomposedLogData = residual\ndecomposedLogData.dropna(inplace=True)\ntest_stationarity(decomposedLogData)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02256626044f733610cfe6920509385abc89dadd"},"cell_type":"markdown","source":"## Plotting ACF & PACF <a name=\"acf-pacf\"></a>"},{"metadata":{"trusted":false,"_uuid":"9f7e2bd9d15b36aa5bfbd41a3ea5b99ef9d2124e"},"cell_type":"code","source":"#ACF & PACF plots\n\nlag_acf = acf(datasetLogDiffShifting, nlags=20)\nlag_pacf = pacf(datasetLogDiffShifting, nlags=20, method='ols')\n\n#Plot ACF:\nplt.subplot(121)\nplt.plot(lag_acf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.title('Autocorrelation Function')            \n\n#Plot PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.axhline(y=1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.title('Partial Autocorrelation Function')\n            \nplt.tight_layout()            ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccbd3aa17eb33306ec7f17fa54bfe2bbcad98f11"},"cell_type":"markdown","source":"From the ACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, Q = 2\nFrom the PACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, P = 2\n\nARIMA is AR + I + MA. Before, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models will give a value of RSS. Lower RSS values indicate a better model."},{"metadata":{"_uuid":"2f8a4b371f1678d3fd84b715c433438f74b91e1f"},"cell_type":"markdown","source":"## Building Models <a name=\"model\"></a>"},{"metadata":{"trusted":false,"_uuid":"c1fcf782c93eb35a746d0a107aa44d41f62290e4"},"cell_type":"code","source":"#AR Model\n#making order=(2,1,0) gives RSS=1.5023\nmodel = ARIMA(indexedDataset_logScale, order=(2,1,0))\nresults_AR = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_AR.fittedvalues - datasetLogDiffShifting['#Passengers'])**2))\nprint('Plotting AR model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ef6f3af5945c7ba92eda2c0cba9dd4b5b3e6b02a"},"cell_type":"code","source":"#MA Model\nmodel = ARIMA(indexedDataset_logScale, order=(0,1,2))\nresults_MA = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting)\nplt.plot(results_MA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_MA.fittedvalues - datasetLogDiffShifting['#Passengers'])**2))\nprint('Plotting MA model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b2955ce1f6a872d1347241becd2a90cbea81693f"},"cell_type":"code","source":"# AR+I+MA = ARIMA model\nmodel = ARIMA(indexedDataset_logScale, order=(2,1,2))\nresults_ARIMA = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_ARIMA.fittedvalues - datasetLogDiffShifting['#Passengers'])**2))\nprint('Plotting ARIMA model')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b19bb5e6d62e6c061af14ee480f20488021ee72"},"cell_type":"markdown","source":"By combining AR & MA into ARIMA, we see that RSS value has decreased from either case to 1.0292, indicating ARIMA to be better than its individual component models.   \n\nWith the ARIMA model built, we will now generate predictions. But, before we do any plots for predictions ,we need to reconvert the predictions back to original form. This is because, our model was built on log transformed data."},{"metadata":{"_uuid":"9aad48b39d784b198a2ceb64f6525d8bf010df78"},"cell_type":"markdown","source":"## Prediction & Reverse transformations <a name=\"prediction\"></a>"},{"metadata":{"trusted":false,"_uuid":"c891f91814a921a23eb825e13a1abc222190ad91"},"cell_type":"code","source":"predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\nprint(predictions_ARIMA_diff.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7cf992d6d71486192d35f85faf03199ab17262d7"},"cell_type":"code","source":"#Convert to cumulative sum\npredictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\nprint(predictions_ARIMA_diff_cumsum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b329774737d721d61b1b6ce5c884250e4e65e0aa"},"cell_type":"code","source":"predictions_ARIMA_log = pd.Series(indexedDataset_logScale['#Passengers'].iloc[0], index=indexedDataset_logScale.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)\npredictions_ARIMA_log.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2b9a7714e0a493c405896ff3175d723853ed84b7"},"cell_type":"code","source":"# Inverse of log is exp.\npredictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(indexedDataset)\nplt.plot(predictions_ARIMA)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0bcbd64aef08232010e737293b88405d4d5f662"},"cell_type":"markdown","source":"We see that our predicted forecasts are very close to the real time series values indicating a fairly accurate model."},{"metadata":{"trusted":false,"_uuid":"6f5d70181159172c5b4c51dbf1758982278163af"},"cell_type":"code","source":"indexedDataset_logScale","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ca204ccf1ab4ba28a83dcffdb50d6d300123fc20"},"cell_type":"code","source":"#We have 144(existing data of 12 yrs in months) data points. \n#And we want to forecast for additional 120 data points or 10 yrs.\nresults_ARIMA.plot_predict(1,264) \n#x=results_ARIMA.forecast(steps=120)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"37c295300418e5cadc6d11864f8326b0e05557a1"},"cell_type":"code","source":"#print(x[1])\n#print(len(x[1]))\n#print(np.exp(x[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fad66003073e3ca600d1c159b8c0968c50f4377f"},"cell_type":"markdown","source":"## The End"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}