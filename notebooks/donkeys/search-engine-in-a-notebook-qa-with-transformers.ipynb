{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Questions and Answers on the COVID-19 articles\n### - Find Documents with TF-IDF, Ask Questions with Transformers\n\nThis notebook searches over the COVID research papers dataset and tries to find some answers to the questions posed for the related Kaggle tasks. These are not intended to be direct answers (in all cases) but to point to relevant papers and highlight potentially interesting points that specific papers cover related to the task questions.\n\nFrom the technical perspective this uses [TF-IDF scores](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) across the papers to find papers related to the questions, sorts them according to these scores, and then uses the [Huggingface Transformers library](https://github.com/huggingface/transformers) and its [Question-Answers model (pipeline)](https://huggingface.co/transformers/main_classes/pipelines.html#questionansweringpipeline) to answer the questions. \n\nThe answers given by the model are ranked by the confidence given by the model. Only answers above a specific confidence threshold are selected. The threshold confidence is varied across questions based on running the model with the question on a smaller subset and tuning the threshold to what produces useful results for that questions (in my opinion).\n\nI believe the results are most useful to help someone going through a large body of research to find interesting papers to look into that they might have otherwise not noticed, or to help focus search over large sets of documents. Perhaps to highlights some ideas that could otherwise be missed."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd \nfrom memory_profiler import profile\nfrom typing import List\nimport pickle\nfrom gensim.models.word2vec import Word2Vec\n\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Install some profiling tools to help track where the memory and CPU goes:"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install memory_utils","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install codeprofile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import memory_utils\nfrom codeprofile import profiler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set a few parameters that are used later in the notebook. These limit the resource use to fit in the limits of a Kaggle kernel. They determine how many documents at most to parse looking for answers to a question (DOC_LIMIT), and the maximum size of a document that is parsed for question-answering (DOC_SIZE_CAP):"},{"metadata":{"trusted":true},"cell_type":"code","source":"#limit number of documents if DOC_LIMIT != None. \n#allows testing the code without waiting 2 days, and to run a large set of questions in the notebook timelimit / resources\nDOC_LIMIT = 42\nDOC_SIZE_CAP = 100000 #caps document length at 100k characters for processing, saves memory and processing time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessed data and libraries\n\n## Transformers\n\nTransformers are a deep learning model for natural language processing. Here I import and later use the one by [Hugging Face](https://github.com/huggingface/transformers/)."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install --upgrade transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from transformers import pipeline\n\nnlp = pipeline(\"question-answering\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessed Datasets"},{"metadata":{},"cell_type":"markdown","source":"This notebook uses a number of datasets I created previously. They save me resources as they provide a lot of preprocessed data to use, and allow focusing this notebook on the question-answering:\n\n- word2vec: Relations of the dataset words in 300-dimensional vector space. Useful to find related words, such as synonyms for queries.\n- Inverted index: Maps words to their TF-IDF scores in different documents. Useful to find highly relevant documents for a set of keywords. Those docs can then be fed to the question-answer model.\n- TF-IDF matrix: A set of statistical scores on how frequent words are across all documents vs specific document. Used as input to the inverted index.\n- Doc ids: Maps the document identifiers in the inverted index back to the original Kaggle data, and metadata such as publication dates, authors, and journal."},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"/kaggle/input/covid-word2vec/word2vec.pickle\", \"rb\") as f:\n    w2v = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"/kaggle/input/covid-tfidf/i_index.pickle\", \"rb\") as f:\n    i_index = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"/kaggle/input/covid-tfidf/tfidf_matrix.pickle\", \"rb\") as f:\n    tfidf_matrix = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"/kaggle/input/covid-tfidf/doc_ids.pickle\", \"rb\") as f:\n    doc_ids = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the document metadata provided by Kaggle"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#need to be able to load all the documents for the question-answering later, so load up all the paths first to identify where to find the document later.\nimport glob, os, json\n\ndef load_doc_paths():\n    all_file_paths = []\n    base_paths = [\n        \"/kaggle/input/CORD-19-research-challenge/arxiv/arxiv/pdf_json/*\",\n        \"/kaggle/input/CORD-19-research-challenge/arxiv/arxiv/pmc_json/*\",\n        \"/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/*\",\n        \"/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pmc_json/*\",\n        \"/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/pdf_json/*\",\n        \"/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/pmc_json/*\",\n        \"/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pdf_json/*\",\n        \"/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pmc_json/*\",\n        \"/kaggle/input/CORD-19-research-challenge/custom_license/custom_license/pdf_json/*\",\n        \"/kaggle/input/CORD-19-research-challenge/custom_license/custom_license/pmc_json/*\",\n    ]\n    for base_path in base_paths:\n        file_paths_glob = glob.glob(base_path)\n        all_file_paths.extend(file_paths_glob)\n    return all_file_paths","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"all_doc_paths = load_doc_paths()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The metadata describes the journal, authors, and similar metadata about each article in the dataset. Need it to give a more meaningful context for the answers once they are found."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_metadata = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\")\ndf_metadata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inverted Index\n\nThe inverted index comes from one of my [previous notebooks](https://www.kaggle.com/donkeys/tf-idf-and-inverted-index-creation-for-covid19), and is made available as a [dataset](https://www.kaggle.com/donkeys/covid-tfidf) I import in this notebook. What does an [inverted index](https://en.wikipedia.org/wiki/Inverted_index) mean? I would consider a normal index to map documents to words it contains. This inverted index contains word-weight pairs mapped to documents.\n\nFor example, if we take the word \"patient\" from the  inverted index *i_index*:"},{"metadata":{"trusted":true},"cell_type":"code","source":"i_index[\"patient\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Depending on how the dataset evolves, the specific numbers above might change over time. But at the time of writing this, it said the following for the first two rows:\n\n```\narray([[3.2264000e+04, 6.1301637e-01],\n       [9.9830000e+03, 6.1036140e-01],\n       ...,\n```\n\nThis shows an array where each element is another array of two elements. The above is basically saying:\n- Document at index 32264 has a TF-IDF score of 0.61301637 for the word \"patient\"\n- Document at index  9983 has a TF-IDF score of 0.61036140 for the word \"patient\"\n- And so on...\n\nNotice that all the documents are sorted by their TF-IDF score, so in this case the first item in the array is the document with the highest score for \"patient\". And the last one would be the lowest score. Which I may have capped at some small value when the index was created, to save space and computation.\n\nIn any case, this index allows to quickly look up related documents with highest scores for words of interest. The idea was to use this to to build the set of documents for question-answering per specific terms, but in this case I first translate it to a word-doc weight dictionary."},{"metadata":{},"cell_type":"markdown","source":"**Weight Dictionaries**\n\nBesides having words mapped to documents (as in the inverted index), it is also useful to have the other type of mapping available. From words to documents to their weights for that word. The following builds that. For example:\n\n```\n  word_dicts[\"bob\"]={1:0.5,\n                     2:0.1,\n                     7:0.3}\n```\n\nThe above shows an example where word \"bob\" maps to 3 documents with ID's of 1,2, and 7. Each having a weight of 0.5, 0.1, and 0.3 respectively, for the word \"bob\".\n\nThis weight dictionary is actually what gets used in this notebook later to find the docs for a set of (key)words of interest.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def build_dicts(threshold):\n    word_weight_dicts = {}\n    for word in tqdm(i_index.keys()):\n        doc_weights = i_index[word]\n        doc_weight_dict = {}\n        word_weight_dicts[word] = doc_weight_dict\n        for doc_idx, doc_weight in doc_weights:\n            doc_idx = int(doc_idx)\n            doc_weight_dict[doc_idx] = doc_weight\n            #reduct sizes by capping on some number of docs\n            if doc_weight < threshold and len(doc_weight_dict) > 1000:\n                break\n    return word_weight_dicts\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word2Vec\n\nWord2vec is quite a traditional NLP model these days. It describes relations of words across documents. For example, maybe \"patient\" and some other words are used similarly? Example:"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"w2v.init_sims()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v.similar_by_vector(\"patient\", 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I use word2vec to find pairs of words used similarly. Looking at the top 10 words reported (as for \"patient\" above), loop through them and pick the ones that higher than 0.5 score for similarity. Later use these as synonyms to identify related documents to a topic of interest. Why 10 and 0.5? Consider those as hyperparameters I chose based on some experiments with this data."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def find_pairs(words_sentence):\n    words = words_sentence.split(\" \")\n    word_lists = []\n    for word in words:\n        synonyms = w2v.similar_by_vector(word, topn = 10)\n        selected = [word]\n        for synonym in synonyms:\n            #if word2vec distance factor is less than 0.5, stop adding. expect input to be sorted..\n            if synonym[1] < 0.5:\n                break\n            selected.append(synonym[0])\n        word_lists.append(selected)\n    \n    for word_list in word_lists:\n        print(\"synonyms found:\")\n        print(f\"{word_list[0]}: {word_list[1:]}\")\n    \n    return word_lists\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there can be quite many documents with the keywords of interest, I need to filter them. A threshold search is done to find which score to use as a cut-off point. In this case (or when I wrote this..) I use a score threshold of 70% out of what all the documents score. So if the word is in a document, but has a TF-IDF lower than max 30% for that word, for that document, the document is not included.\n\nFor example:\n\n- Imagine 70% of documents have a TF-IDF score for patient at 0.6 or less. 0.6 is then selected as a threshold, and any document having a TF-IDF sore less than, or equal to 0.6, is not included."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def find_weight_threshold():\n    arrays = [weights for weights in i_index.values()]\n    print(f\"loaded doc weights for {len(arrays)} words.\")\n    all_data = np.concatenate(arrays)\n    print(f\"a total of {all_data.shape} doc-word weights loaded.\")\n    d_mean = np.mean(all_data[:,1])\n    d_med = np.median(all_data[:,1])\n    d_max = np.max(all_data[:,1])\n    d_min = np.min(all_data[:,1])\n    p80 = np.percentile(all_data[:,1],80)\n    p70 = np.percentile(all_data[:,1],70)\n    p30 = np.percentile(all_data[:,1],30)\n    print(f\"min={d_min}, max={d_max}\\n\"+\n          f\"avg={d_mean}, median={d_med}\\n\"+\n          f\"p30={p30}, p80=(p80)\")\n    with np.printoptions(precision=20, suppress=True):\n        print(np.array([d_min, d_max, d_mean, d_med, p30, p80]))\n    #threshold = np.max([d_mean, d_med])\n    threshold = p70 #using 70 to limit the size of the notebook\n    print(f\"threshold: {threshold}\")\n    return threshold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%time\nthreshold = find_weight_threshold()\nword_dicts = build_dicts(threshold)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate weights for documents given the set of search terms. For example, assume search \"incubation period\":\n- get list of synonyms for \"incubation\" and \"period\". call this list \"synonyms\".\n- two examples what it could be for each (will use these as examples):\n   - incubation: preincubation, incubating\n   - period: interval, duration\n- loop every word/synonym:\n   - use the above built dictionary of word weights for all docs to find TFIDF weight for every doc\n   - build a list of weights for all docs for each word / synonym\n   - sum all per doc\n   - example doc 1:\n      - incubation: incubation=0.2, preincubation=0.14, total=0.2+0.14=0.34\n      - period: period=0.2, internal=0.1, duration=0.22, total=0.2+0.1+0.22=0.52\n      - sum: 0.34+0.52=0.86\n   - example doc 2:\n      - incubation: incubation=0.5, preincubation=0.3, total=0.5+0.3=0.8\n      - period: None, total=None  <- doc 2 does not have word \"period\" in this example, or the score is too low\n      - sum: 0.8+None=None <- if sore for one word is missing, the doc gets removed\n\nIn selecting the actual documents for question-answering, I use the above formula to calculate scores for documents, and then sort them by their scores. Ask the question from each of the top N of those documents.\n\nNote that all the documents that make it into this formula should already have quite high TF-IDF score due to earlier threshold filtering. So I just take the ones I find here and sum their keyword-scores into one score per document..\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from collections import defaultdict\n\n@profiler.profile_func\ndef find_docs_for_words(synonym_lists):\n    total_scores = defaultdict(lambda: [])\n    #assume query (keyword) string \"incubation period\"\n    #word_list1 = incubation and its synonyms\n    #word_list2 = period and its synonyms\n    for word_list in synonym_lists:\n        doc_scores = defaultdict(lambda: [])\n        #each word list represent one base word and its synonyms, \n        #so first sum up all weights for a single word and its synonyms\n        #note that above we filtered by threshold and number of words, so should not have very small scores in it\n        #TODO: improve score weights and filter count threshold\n        for word in word_list:\n            if word not in word_dicts:\n                #some of the synonyms from word2vec may be rare and were dropped by earlier preprocessing steps\n                #this prints those so we can see if it is a real loss (should we go back and add it) or not\n                print(f\"missed word: {word}\")\n                continue\n            word_doc_scores = word_dicts[word]\n            for doc_idx in word_doc_scores:\n                #get weights for this word for each document the word appears in\n                doc_scores[doc_idx].append(word_doc_scores[doc_idx])\n        for doc_idx in doc_scores:\n            #sum up all synonyms into one score for each document. \n            #after this each doc has as many lists as it has base words with weights\n            total_scores[doc_idx].append(sum(doc_scores[doc_idx]))\n        #so at this point total_scores has one entry per doc_id: (weights1, weights2). \n        #if there are less than 2, it did not have weight in one of the two, and will be removed later\n        #TODO: nicer filtering schema        \n    return total_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Often a large number of documents is found to have one or more of the keywords. To be included for question-answering, the document must have scores for all the keywords, or their synonyms. In the above example that would be \"incubation\" and \"period\", or their synonyms."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"@profiler.profile_func\ndef filter_weighted_docs(n_words, total_scores):\n    #remove all docs that do not have a weight for one of the N base words\n    to_remove = []\n    for doc_id in total_scores:\n        ds = total_scores[doc_id]\n        if len(ds) < n_words:\n            to_remove.append(doc_id)\n            continue\n        total_scores[doc_id] = sum(ds)\n    print(f\"removing {len(to_remove)} docs\")\n    for key in to_remove:\n        del total_scores[key]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For further processing, we build a list of the documents that match the given keyword search criteria, highest scoring first."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"@profiler.profile_func\ndef doc_scores_to_df(total_scores):\n    #NOTE: here we sort the docs by score so from this on highest scoring will be first\n    #filter_weighted_docs has summed all the word weights for keywords into one, stores in item[1] here \n    ts_dict = {k: v for k, v in sorted(total_scores.items(), key=lambda item: item[1], reverse=True)}\n    #print(ts_dict)\n    df_ts = pd.DataFrame(ts_dict.items(), columns=['DocID', 'WeightScore'])\n    return df_ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, need to match the numerical document id's from the preprocessed datasets to the Kaggle provided metadata. This allows us to load the original file, and to access metadata for final presentation of results in this notebook (to include author, journal, etc. info)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"@profiler.profile_func\ndef get_doc_ids_to_load(doc_ids, df_total_scores):\n    doc_ids_to_load = []\n    for index, row in df_total_scores.iterrows():\n        doc_idx = int(row[\"DocID\"])\n        #print(doc_idx)\n        doc_ids_to_load.append(doc_ids[doc_idx])\n    return doc_ids_to_load","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Matching the document ID's also allows us to load the original, full-text documents in the Kaggle dataset."},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"@profiler.profile_func\ndef load_docs(doc_ids_to_load, filepaths, df_metadata):\n    loaded_docs = {}\n    new_ids = []\n    \n    for doc_id in tqdm(doc_ids_to_load):\n        if doc_id in loaded_docs:\n            print(f\"WARNING: duplicate doc id to load: {doc_id}, skipping\")\n            continue\n         \n        #TODO: this should not work if SHA is nan, why does it?\n        doc_sha = df_metadata[df_metadata[\"cord_uid\"] == doc_id][\"sha\"]\n        if doc_sha.shape[0] > 0:\n            doc_sha = doc_sha.values[0]\n        else:\n            doc_sha = None\n        #print(doc_sha)\n        #TODO: this should not work if PMCID is nan, why does it?\n        doc_pmcid = df_metadata[df_metadata[\"cord_uid\"] == doc_id][\"pmcid\"]\n        if doc_pmcid.shape[0] > 0:\n            doc_pmcid = doc_pmcid.values[0]\n        else:\n            doc_pmcid = None\n        pmc_path = None\n        sha_path = None\n        for filepath in filepaths:\n            if isinstance(doc_pmcid, str) and doc_pmcid in filepath:\n                pmc_path = filepath\n                break\n            if isinstance(doc_sha, str) and doc_sha in filepath:\n                sha_path = filepath\n        if pmc_path is not None:\n            #always favour PMC docs since they are described as higher quality (not scanned from PDF but direct machine format)\n            filepath = pmc_path\n        else:\n            filepath = sha_path\n        #print(filepath)\n        if filepath is None:\n            print(f\"WARNING: cannot find path for doc id {doc_id}. Possibly Kaggle dataset has changed?\")\n            continue\n        with open(filepath) as f:\n            d = json.load(f)\n            body = \"\"\n            for idx, paragraph in enumerate(d[\"body_text\"]):\n                body += f\"{paragraph['text']}\\n\"\n                #print(paragraph)\n                #print(\"---------\")\n            loaded_docs[doc_id] = body\n            new_ids.append(doc_id)\n            \n    return loaded_docs, new_ids","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Search Functions\n\nThere are actually two types of \"queries\" used in this notebook. The first one consists of keywords I selected that I thought could have a high TF-IDF score for documents related to a question. The second query is the question-answering query to prodive the final answers based on the selected documents.\n\nSo, first we find and load the docs for the keywords."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"@profiler.profile_func\ndef find_docs_for_query(query):\n    print(f\"query: {query}\")\n    pairs = find_pairs(query)\n    n_words = len(pairs)\n    print(f\"query has {n_words} words\")\n    total_scores = find_docs_for_words(pairs)\n    #print(total_scores)\n    print(f\"number of docs with some search terms (at high score): {len(total_scores)}\")\n    filter_weighted_docs(n_words, total_scores)\n    print(f\"number of docs with all search terms (at high score): {len(total_scores)}\")\n    #this also sorts the scores before creating the dataframe. so highest scoring are first\n    df_scores = doc_scores_to_df(total_scores)\n    query_doc_ids = get_doc_ids_to_load(doc_ids, df_scores)\n    print(f\"num. doc ids to load for the final docs: {len(query_doc_ids)}\")\n    if DOC_LIMIT is not None:\n        #this avoid the overhead of loading thousands of extra documents. memory+processing time\n        #the multiplier is to give it an extra chance to find answers that meet the confidence level\n        query_doc_ids = query_doc_ids[:DOC_LIMIT*2]\n    print(f\"num. doc ids to load for the final docs after capping: {len(query_doc_ids)}\")\n    loaded_docs, query_doc_ids = load_docs(query_doc_ids, all_doc_paths, df_metadata)\n    return query_doc_ids, loaded_docs\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Asking the actual question(s) is possible once we have found the source set of documents of interest. We can then present the question to the question-answer model, using each of the selected documents as the question context, one at a time."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"@profiler.profile_func\ndef run_query(loaded_docs, doc_ids, question):\n    scores = []\n    answers = []\n    processed_ids = set()\n    for doc_id in tqdm(doc_ids):\n        if doc_id in processed_ids:\n            print(f\"skipping already processed doc id: {doc_id}\")\n            continue\n        processed_ids.add(doc_id)\n        #doc_id = doc_ids[idx]\n        context = loaded_docs[doc_id]\n        #print(len(scores))\n        #memory_utils.print_memory()\n        #print(len(context))\n        context = context[:DOC_SIZE_CAP]\n        #print(len(context))\n        if context is None:\n            print(f\"skipping doc id {doc_id}, not found\")\n            continue\n        with profiler.profile(\"nlp question\"):\n            answer = nlp(question=question, context=context)\n        score = answer[\"score\"]\n        answer_text = answer[\"answer\"]\n        print(f\"question: {question}\")\n        print(f\"  doc id: {doc_id}\")\n        print(f\"  answer: {answer_text}\")\n        print(f\"  score: {score}\")\n        scores.append(score)\n        answers.append(answer_text)\n        if DOC_LIMIT is not None and len(answers) > DOC_LIMIT:\n            break\n    return scores, answers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A nice results presentation also needs to be built to show the questions and answers along with the name of the article, its authors, and journal it was published in."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"@profiler.profile_func\ndef build_results_df(query_doc_ids, df_metadata, scores, answers, score_limit):\n    titles = []\n    publish_times = []\n    journals = []\n    author_lists = []\n    filtered_scores = []\n    filtered_answers = []\n    for idx, doc_id in enumerate(query_doc_ids):\n        if DOC_LIMIT is not None and idx > DOC_LIMIT:\n            break\n        \n        doc_meta = df_metadata[df_metadata[\"cord_uid\"] == doc_id]\n        title = doc_meta[\"title\"].values[0]\n        publish_time = doc_meta[\"publish_time\"].values[0]\n        journal = doc_meta[\"journal\"].values[0]\n        authors = doc_meta[\"authors\"].values[0]\n        score = scores[idx]\n        answer = answers[idx]\n        if (score < score_limit):\n            continue\n\n        titles.append(title)\n        publish_times.append(publish_time)\n        journals.append(journal)\n        author_lists.append(authors)\n        filtered_scores.append(score)\n        filtered_answers.append(answer)\n    \n    df_result = pd.DataFrame({\n        \"Article title\": titles,\n        \"Published\": publish_times,\n        \"Journal\": journals,\n        \"Authors\": author_lists,\n        \"Confidence\": filtered_scores,\n        \"Answer\": filtered_answers\n    })\n    return df_result\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def answer_a_question(tfidf_sentence, question, df_metadata, score_limit):\n    query_doc_ids, query_docs = find_docs_for_query(tfidf_sentence)\n    print(f\"doc_ids={len(query_doc_ids)}, docs={len(query_docs)}\")\n    scores, answers = run_query(query_docs, query_doc_ids, question)\n    df = build_results_df(query_doc_ids, df_metadata, scores, answers, score_limit)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"memory_utils.print_memory()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Questions and Answers\n\nThe following sections are coarsely divided to roughly match four of the Kaggle tasks and their related questions.\n\nIn each section, a number of questions formulated to match some aspects of the Kaggle task questions are presented to the question-answer (QA) model, along with a set of keywords that are first used to filter a set of documents to use as context for the questions. These questions are a result of my own experiments with different questions.\n\nBased on the confidence level given by the QA model for the question and each presented document, a set of answers is collected. The answers with high enough confidence are presented as the final results. The threshold confidence is set as a hyperparater based on earlier experiments, and varies by question.\n\nThe answers are combined with metadata for the article from the Kaggle dataset. This includes the following:\n\n- Article title: Well, the title given by the authors to their article.. :)\n- Published: The publication date as listed in the metadata for this article.\n- Journal: The journal given in the metadata for this article.\n- Authors: Given article authors from metadata.\n- Confidence: The confidence score given for the answer by the question-answer model.\n- Answer: The actual answer to the posed question, based on the article text, as given by the question-answer model.\n\nThe TF-IDF score for the document is not shown in the table, but the entries are sorted so the the ones with the highest TF-IDF score are first in the table. Articles with low confidence score are filtered out, so there might be others in the dataset with higher TF-IDF but not included here. That would be because the QA model was not as confident on its answer for those, and the threshold (hyperparameter) set for this question filtered it out."},{"metadata":{},"cell_type":"markdown","source":"# Task 1\n\nTask 1 refers to the task that was the highest ranked task in the list of tasks at the time. This section tries to answer questions related to this task. The task description:\n\nTask Details\n\nWhat is known about transmission, incubation, and environmental stability? What do we know about natural history, transmission, and diagnostics for the virus? What have we learned about infection prevention and control?\n\nSpecifically, we want to know what the literature reports about:\n\n- Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\n- Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\n- Seasonality of transmission.\n- Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\n- Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\n- Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\n- Natural history of the virus and shedding of it from an infected person\n- Implementation of diagnostics and products to improve clinical processes\n- Disease models, including animal models for infection, disease and transmission\n- Tools and studies to monitor phenotypic change and potential adaptation of the virus\n- Immune response and immunity\n- Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\n- Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\n- Role of the environment in transmission\n\n"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What is the incubation period?\"\ndf = answer_a_question(\"covid19 incubation period\",  q, df_metadata, 0.7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"How prevalent is asymptomatic shedding and transmission?\"\ndf = answer_a_question(\"asymptomatic shedding transmission\",  q, df_metadata, 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What is the transmission seasonality?\"\ndf = answer_a_question(\"covid19 transmission seasonality\",  q, df_metadata, 0.02)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What is covid19 chemical structure?\"\ndf = answer_a_question(\"covid19 chemical structure\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"How long does covid19 survive?\"\ndf = answer_a_question(\"covid19 persistent host\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"How long is host infectous?\"\ndf = answer_a_question(\"covid19 infect host\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What does covid19 persist on?\"\ndf = answer_a_question(\"covid19 copper steel plastic\",  q, df_metadata, 0.005)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What is the history of covid19?\"\ndf = answer_a_question(\"covid19 history\",  q, df_metadata, 0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What is the disease model?\"\ndf = answer_a_question(\"covid19 disease model\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What are effective diagnostics processes?\"\ndf = answer_a_question(\"covid19 diagnostic process\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"How does the virus change and adapt?\"\ndf = answer_a_question(\"covid19 phenotypic change adaptation\",  q, df_metadata, 0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What is effective movement control strategy?\"\ndf = answer_a_question(\"covid19 movement control strategy\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What is effective protective equipment?\"\ndf = answer_a_question(\"covid19 personal protective equipment\",  q, df_metadata, 0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What is the role of environment in transmission?\"\ndf = answer_a_question(\"covid19 environment transmission\",  q, df_metadata, 0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What is immune response?\"\ndf = answer_a_question(\"covid19 immune response\",  q, df_metadata, 0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"How long is immunity?\"\ndf = answer_a_question(\"covid19 immunity period\",  q, df_metadata, 0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 2\n\n2nd ranked in the list, at the time:\n\nTask Details\n\nWhat do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\n\nSpecifically, we want to know what the literature reports about:\n\n- Data on potential risks factors\n- Smoking, pre-existing pulmonary disease\n- Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities\n- Neonates and pregnant women\n- Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n- Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n- Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n- Susceptibility of populations\n- Public health mitigation measures that could be effective for control\n"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What are risk factors?\"\ndf = answer_a_question(\"covid19 risk factor\",  q, df_metadata, 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"Does smoking increase risk?\"\ndf = answer_a_question(\"covid19 smoke risk\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"How does coninfection affect transmission?\"\ndf = answer_a_question(\"covid19 coinfection transmission\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What is risk to pregnant women?\"\ndf = answer_a_question(\"covid19 pregnant woman\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"Does social status affect risk?\"\ndf = answer_a_question(\"covid19 social economic\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"How is the virus transmitted?\"\ndf = answer_a_question(\"covid19 transmission dynamic\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What is the reproductive number?\"\ndf = answer_a_question(\"covid19 reproduction number\",  q, df_metadata, 0.7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"How is covid19 transmitted?\"\ndf = answer_a_question(\"covid19 transmission mode\",  q, df_metadata, 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What the impact of environment on transmission?\"\ndf = answer_a_question(\"covid19 environment factor\",  q, df_metadata, 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What is the severity of covid19?\"\ndf = answer_a_question(\"covid19 severity risk\",  q, df_metadata, 0.03)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"Who is most at risk?\"\ndf = answer_a_question(\"covid19 risk population\",  q, df_metadata, 0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What are effective mitigation measures?\"\ndf = answer_a_question(\"covid19 mitigation measure\",  q, df_metadata, 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What the asymptotic fatality rate?\"\ndf = answer_a_question(\"covid19 asymptotic fatality\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 4\n\n4th in the list, with questions:\n\n- Effectiveness of drugs being developed and tried to treat COVID-19 patients.\n- Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\n- Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\n- Exploration of use of best animal models and their predictive value for a human vaccine.\n- Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\n- Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\n- Efforts targeted at a universal coronavirus vaccine.\n- Efforts to develop animal models and standardize challenge studies\n- Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\n- Approaches to evaluate risk for enhanced disease after vaccination\n- Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What drugs are effective?\"\ndf = answer_a_question(\"covid19 drug effective\",  q, df_metadata, 0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"How effective are drugs?\"\ndf = answer_a_question(\"covid19 drug effective\", q, df_metadata, 0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What inhibitors are effective?\"\ndf = answer_a_question(\"covid19 viral inhibitor\",  q, df_metadata, 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What is the effectiveness of inhibitors?\"\ndf = answer_a_question(\"covid19 viral inhibitor\",  q, df_metadata, 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"How effective is antibiotic enhancement?\"\ndf = answer_a_question(\"covid19 antibiotic enhancement\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"Are animal models effective for humans?\"\ndf = answer_a_question(\"covid19 animal model\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"#q = \"How can production capacity be expanded?\"\n#df = answer_a_question(\"covid19 production capacity\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#print(f\"Q: {q}\")\n#df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q  = \"How to get people to use masks?\"\ndf = answer_a_question(\"covid19 mask respirator\",  q, df_metadata, 0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q  = \"What is effective vaccine?\"\ndf = answer_a_question(\"covid19 vaccine develop\",  q, df_metadata, 0.35)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What are vaccine risks?\"\ndf = answer_a_question(\"covid19 vaccine risk\",  q, df_metadata, 0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Task 5\n\n5th in the list, questions given:\n\n- Resources to support skilled nursing facilities and long term care facilities.\n- Mobilization of surge medical staff to address shortages in overwhelmed communities\n- Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with/without other organ failure  particularly for viral etiologies\n- Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients\n- Outcomes data for COVID-19 after mechanical ventilation adjusted for age.\n- Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest.\n- Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level.\n- Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks.\n- Best telemedicine practices, barriers and faciitators, and specific actions to remove/expand them within and across state boundaries.\n- Guidance on the simple things people can do at home to take care of sick people and manage disease.\n- Oral medications that might potentially work.\n- Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually.\n- Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes.\n- Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials\n- Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials\n- Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"How can we support nursing facilities?\"\ndf = answer_a_question(\"covid19 support nurse facility\",  q, df_metadata, 0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"Does age have effect?\"\ndf = answer_a_question(\"organ failure mortality\",  q, df_metadata, 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"How do ethical principles map to covid19?\"\ndf = answer_a_question(\"covid19 ethical principle\",  q, df_metadata, 0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What do people fear?\"\ndf = answer_a_question(\"covid19 fear anxiety\",  q, df_metadata, 0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"What are local barriers and enablers?\"\ndf = answer_a_question(\"covid19 barrier enabler\",  q, df_metadata, 0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"#q = \"How are healthcare providers affected?\"\n#df = answer_a_question(\"covid19 provider health psychological\",  q, df_metadata, 0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#print(f\"Q: {q}\")\n#df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"what is the experiment outcome?\"\ndf = answer_a_question(\"membrane oxygenation\",  q, df_metadata, 0.02)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"Is heart attack more likely?\"\ndf = answer_a_question(\"covid19 heart attack\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"How does regulation affect care level?\"\ndf = answer_a_question(\"covid19 regulatory regulation\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"#q = \"How to get people to use masks?\"\n#df = answer_a_question(\"covid19 mask respirator\",  q, df_metadata, 0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"q = \"How to provide effective remote support?\"\ndf = answer_a_question(\"covid19 telemedicine support\",  q, df_metadata, 0.35)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"q = \"What can people do at home?\"\ndf = answer_a_question(\"covid19 home guidance\",  q, df_metadata, 0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"q = \"What are effective diagnostics processes?\"\ndf = answer_a_question(\"covid19 diagnostic process\",  q, df_metadata, 0.002)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"q = \"What oral medication works?\"\ndf = answer_a_question(\"covid19 oral medication\",  q, df_metadata, 0.35)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"q = \"What are best practices at hospitals?\"\ndf = answer_a_question(\"covid19 hospital best practice\",  q, df_metadata, 0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"q = \"What is effective protective equipment?\"\ndf = answer_a_question(\"covid19 personal protective equipment\",  q, df_metadata, 0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f\"Q: {q}\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#q = \"How is artificial intelligence used?\"\n#df = answer_a_question(\"covid19 intervention automation\",  q, df_metadata, 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#print(f\"Q: {q}\")\n#df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's all folks."},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n\nI found it quite impressive with how well the transformers question-answering model was able to find matching information in large sets of text. And my approach to find relevant documents to ask about, using TF-IDF scores for keywords across the documents seemed to work quite well. \n\nNext time I do a literature search on something, I will definitely look into trying this in my own areas of research. In that situation, I would likely be better off finding ways to finetune and identify when it finds the most relevant papers and information, and how to best apply it. Of course, the research community still ha some way to go to having nice sets of papers available in a suitable format for this type of processing. Here, Kaggle has done it for us, but in my experience raw PDF is what you mostly get.\n\nBut as I said, I found the results and their quality very interesting. I think such an approach could be quite helpful to identify documents and topics in large sets of text when researching some specific problems. For example, to\n- Find missed documents that could be highly relevant\n- Group different insights quickly\n- Reduce a large set of documents to go through manually\n- Highlight new ideas and viewpoints that could otherwise be missed\n\nI believe this type of service would be a useful helping tool for literature search in any domain. For academic texts, further improvements could include:\n- Ranking by journal (higher ranked journals get more points)\n- In quite many cases I see titles give good indicators of usefulness as well. Weighting different article elements in general, in scoring, could be an option.\n- Fancy UI for a search engine to let you explore the results\n- Integrating explorative data analysis approaches with tuning the keywords, questions, hyperparameters, and results\n- Running multiple queries in parallel, assuming you have the resources\n- Using previous question answers to produce more detailed questions. For example, when running this notebook, a question about vaccines lists many random looking items in the answers, but also things like rVSV-ZEBOV, which seems to be an experimental Ebola vaccine. Such terms could further be used to guide the search.\n- The answers given by the transformer model are very short and concide. Slicing larger parts of text near the answer could make it easire to directly deduce if it is worth the effor to explore that answer with a more in-depth manual review of the paper.\n\nHope this is helpful for some researcher :)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"profiler.print_run_stats()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}