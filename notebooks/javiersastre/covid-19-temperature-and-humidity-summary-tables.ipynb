{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction <a class=\"anchor\" id=\"intro-header\"></a>\n\nThis notebook provides extensive search and information extraction functionality for relevant factors related to Sars-CoV-2, Covid-19, and provides an example of information extraction on how temperature and humidity affects the transmission of 2019-nCoV. It filters relevant papers using an Apache Lucene based search engine over an index compiled on the full text of publications in the Kaggle competition metadata and then performs extensive text mining of unstructured text through a robust [GrapeNLP](https://github.com/GrapeNLP) grammar with fuzzy matching capabilities.\n\nThe techniques provided here would be of interest to researchers and policy makers seeking to automatically find answers to questions such as the effect of temperature and humidity on disease transmission. Answers to such questions would help policy makers to tailor their response to the pandemic, based on geographic and seasonal difereences. This would aid ongoing COVID-19 response efforts worldwide and attempts at economic recovery. These techniques can also be reconfigured to explore other factors.\n\nThis submission is able to: (1) recreate the target tables; (2) append new rows to the old tables in order to add: (A) newly published articles; or (B) previously overlooked articles.\n\nIt will use the following workflow:\n\n* [Notebook parameters](#parameters-header) - Set the parameters to apply to the entire notebook\n* [Install libraries and load metadata](#install-header) - Install the necessary components and load the CORD-19 metadata.\n* [Load Lucene index and searcher](#load-lucene-header) - Load the CORD-19 Lucene index and instantiate the Lucene searcher\n* [Search for relevant publications](#search-header) - Search for papers that may contain relevant Covid-19 factors. To implement this component, we use PyLucene, which is a Python extension for accessing Java Lucene.\n* [Extraction grammar description](#grammar-description-header) - Overview of the GrapeNLP grammar used to find and extract the target datapoints.\n* [Load grammar](#load-grammar-header) - Instantiation of the grammar engine.\n* [Extract information from text](#extract-header) - Extract datapoints from full text of papers.\n* [Display results](#display-header) Save target CSV tables and display them.\n* [Conclusion](#conclusion-header) Conclusions and future directions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Notebook parameters <a class=\"anchor\" id=\"parameters-header\"></a>\n\nSet the parameters to use accross the entire notebook.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"relevant_factor_terms = {\n    'temperature_or_humidity': ['air', 'clammy', 'climate', 'cool', 'cold', 'hotness', 'humid', 'humidity', 'precipitation', 'rainfall', 'temperature', 'temperatures', 'warm']\n}\n\ncovid19_synonyms = [\"coronavirus disease 19\", \"sars cov 2\", \"2019 ncov\", \"2019ncov\", \"coronavirus 2019\", \"wuhan pneumonia\", \"wuhan virus\", \"wuhan coronavirus\", \"covid19\", \"covid-19\"]\n\n# Max documents to search per Lucene query (set it to e.g. 100000 to return all possible matches)\nMAX_SEARCH_RESULTS = 1000000\nMIN_GRAMMAR_SCORE = -600\n\n# We allow for at least 40 tokens in between (roughly the lenght of 2 sentences), which will add -15*40=-600 points to the overall match score\nMIN_GRAMMAR_SCORE = -600\n# Left and right context size in characters to extract upon a grammar match (roughly 40 words, assuming 5 chars per word on average)\nCONTEXT_SIZE = 200\n# Styles for highlighting the matched risk factor and severe measures within the full text of the papers\nFACTORS_STYLE = \"background-color: #EC1163\"\nEXCERPT_STYLE = \"background-color: #80FF32\"\n\nLUCENE_INDEX_DIR = \"documentLevel\"\nLUCENE_BASE_DIR = \"/kaggle/working\"\nCOVID_FULLTEXT_DF = \"../input/covidfulltext/metadata_and_fulltext_2020-04-17.csv\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Install Libraries <a class=\"anchor\" id=\"install-header\"></a>\n\nA number of external components are used, including Apache Lucene with pre-compiled indexes and the [GrapeNLP](https://github.com/GrapeNLP) grammar engine for information extraction. In this kernel, we provide an installation/ configuration/ compilation package for PyLucene 8.1.1 as an external data called “compiledlucene”, which provides all the required software dependencies for installation and deployment of PyLucene. For the installation of GrapeNLP, we reuse the libgrapenlp dataset, which provides the required Debian packages, then we install from Pypi the GrapeNLP Python interface package: pygrapenlp. More details on how to install and use the GrapeNLP grammar engine in a Kaggle notebook can be found here: https://www.kaggle.com/javiersastre/grapenlp-grammar-engine-in-a-kaggle-notebook","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install and import relevant libraries\n!python -m easy_install ../input/compiledlucene/bk/lucene-8.1.1-py3.6-linux-x86_64.egg\n!cp -r ../input/compiledlucene/bk/JCC-3.7-py3.6-linux-x86_64.egg /opt/conda/lib/python3.6/site-packages/\nimport sys\nsys.path\nsys.path.append('/opt/conda/lib/python3.6/site-packages/JCC-3.7-py3.6-linux-x86_64.egg')\nsys.path.append('/opt/conda/lib/python3.6/site-packages/lucene-8.1.1-py3.6-linux-x86_64.egg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!dpkg -i ../input/libgrapenlp/libgrapenlp_2.8.0-0ubuntu1_xenial_amd64.deb\n!dpkg -i ../input/libgrapenlp/libgrapenlp-dev_2.8.0-0ubuntu1_xenial_amd64.deb\n!pip install pygrapenlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys, os, lucene, threading, time, html\nfrom datetime import datetime\nfrom java.nio.file import Paths\nfrom org.apache.lucene.analysis.miscellaneous import LimitTokenCountAnalyzer\nfrom org.apache.lucene.document import Document, Field, FieldType\nfrom org.apache.lucene.index import FieldInfo, IndexWriter, IndexWriterConfig, IndexOptions\nfrom org.apache.lucene.analysis.standard import StandardAnalyzer\nfrom org.apache.lucene.index import DirectoryReader\nfrom org.apache.lucene.queryparser.classic import QueryParser\nfrom org.apache.lucene.store import SimpleFSDirectory\nfrom org.apache.lucene.search import IndexSearcher","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import OrderedDict\nfrom pygrapenlp import u_out_bound_trie_string_to_string\nfrom pygrapenlp.grammar_engine import GrammarEngine","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.auto import tqdm\nimport pandas as pd\nfrom IPython.display import display, HTML, Image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Lucene index <a class=\"anchor\" id=\"load-lucene-header\"></a>\n\nLoad the Lucene index using IndexWriter. This component manages an index over a dynamic collection of documents and provides very rapid updates to the index as documents are added and deleted from the collection. This index provides a mapping from terms to documents, which is called an “inverted index. Document indexing consists of first constructing a Lucene Document that contains the fields to be indexed, then adding that Document to the inverted index”, see figure below. The index is maintained as a set of segments in a storage abstraction called SimpleFSDirectory which provides an interface similar to an OS file system.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load indexed metadata \nfinal_df = pd.read_csv('../input/covidfulltext/metadata_and_fulltext_2020-04-17.csv')\nfinal_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture --no-display\n# This section loads the indexs and takes about 4 minutes to run\nclass Ticker(object):\n\n    def __init__(self):\n        self.tick = True\n\n    def run(self):\n        while self.tick:\n            sys.stdout.write('.')\n            sys.stdout.flush()\n            time.sleep(1.0)\n\nclass IndexFiles(object):\n    \"\"\"Usage: python IndexFiles <doc_directory>\"\"\"\n\n    def __init__(self, root, storeDir, analyzer):\n        ##print(\"before store\")\n        if not os.path.exists(storeDir):\n            os.mkdir(storeDir)\n        ##print(\"after store\")\n\n        store = SimpleFSDirectory(Paths.get(storeDir))\n        ##print(storeDir)\n        analyzer = LimitTokenCountAnalyzer(analyzer, 1048576)\n        ##print(\"after analyzer \")\n\n        config = IndexWriterConfig(analyzer)\n        ##print(\"after config\")\n\n        config.setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n        ##print(\"before writer\")\n        writer = IndexWriter(store, config)\n        ##print(\"after writer\")\n        self.indexDocs(root, writer)\n        ticker = Ticker()\n        ##print ('commit index')\n        threading.Thread(target=ticker.run).start()\n        writer.commit()\n        writer.close()\n        ticker.tick = False\n        ##print ('done')\n\n    def indexDocs(self, root, writer):\n\n        t1 = FieldType()\n        t1.setStored(True)\n        t1.setTokenized(False)\n        t1.setStoreTermVectors(True)\n        t1.setStoreTermVectorOffsets(True)\n        t1.setStoreTermVectorPositions(True)\n        t1.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS)\n        \n\n        t2 = FieldType()\n        t2.setStored(True)\n        t2.setTokenized(True)\n        t2.setStoreTermVectors(True)\n        t2.setStoreTermVectorOffsets(True)\n        t2.setStoreTermVectorPositions(True)\n        t2.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS)\n                \n        i = 1\n        for index, row in tqdm(final_df.iterrows(), desc='Indexing: ', total=len(final_df.index)):\n            print (\"adding \", i , \"th document:\", row['paper_id'])\n            try :\n                doc = Document()\n                doc.add(Field(\"paper_id\", row['paper_id'], t1))\n                doc.add(Field(\"title\", row['title'], t2))\n                doc.add(Field(\"doi\",row['doi'], t1))\n                doc.add(Field(\"pmcid\", row['pmcid'], t1))\n                doc.add(Field(\"publish_time\", row['publish_time'], t1))\n                doc.add(Field(\"journal\", row['journal'], t1))\n                doc.add(Field(\"url\", row['url'], t1))\n                \n                if len(row['text']) > 0:\n                    doc.add(Field(\"full_text\", row['text'], t2))\n                else :\n                    print (\"warning: no fulltext available in %s\", row['title'])\n                    \n                if len(row['abstract_y']) > 0:\n                    doc.add(Field(\"abstract\", row['abstract_y'], t2))\n                else :\n                    print (\"warning: no abstract available in %s\", row['title'])\n                writer.addDocument(doc)\n            except (RuntimeError, TypeError, NameError):\n                pass\n            i=i+1\n            \n\nlucene.initVM()\nstart = datetime.now()\ntry:\n    IndexFiles(LUCENE_BASE_DIR, os.path.join(LUCENE_BASE_DIR, LUCENE_INDEX_DIR),StandardAnalyzer())\n    end = datetime.now()\n    print (end - start)\nexcept (RuntimeError, TypeError, NameError):\n    print (\"Failed: \")\n    raise","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Search relevant publications <a class=\"anchor\" id=\"search-header\"></a>\n\nThe following section uses Apache Lucene to search for publications that contain expressions of temperature and/or humidity as well as mentions to Covid-19.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_or_term_query(terms):\n    quoted_terms = terms.copy()\n    for i in range(len(quoted_terms)):\n        if ' ' in quoted_terms[i]:\n            quoted_terms[i] = '\"' + quoted_terms[i] + '\"'\n    return ' OR '.join(quoted_terms)\n\ndef make_and_query(subqueries):\n    return \"(\" + \") AND (\".join(subqueries) + \")\"\n\ndef search(searcher, analyzer, query_expression):\n    query = QueryParser(\"full_text\", analyzer).parse(query_expression)\n    scoreDocs = searcher.search(query, MAX_SEARCH_RESULTS).scoreDocs\n    results = []\n    for scoreDoc in scoreDocs:\n        doc = searcher.doc(scoreDoc.doc)\n        result = {\n            'date': doc.get(\"publish_time\"),\n            'study': doc.get(\"title\"),\n            'study_link': doc.get(\"url\"),\n            'journal': doc.get(\"journal\"),\n            'paper_id': doc.get('paper_id'),\n            'paper_full_text': doc.get('full_text'),\n            'pmcid': doc.get(\"pmcid\")\n        }\n        results.append(result)\n    return pd.DataFrame(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"directory = SimpleFSDirectory(Paths.get(os.path.join(LUCENE_BASE_DIR, LUCENE_INDEX_DIR)))\nsearcher = IndexSearcher(DirectoryReader.open(directory))\nanalyzer = StandardAnalyzer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_19_query = make_or_term_query(covid19_synonyms)\nrelevant_factor_queries = {relevant_factor: make_and_query([make_or_term_query(terms), covid_19_query]) for relevant_factor, terms in relevant_factor_terms.items()}\nrelevant_factor_queries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"papers_by_relevant_factor = {}\nfor relevant_factor, query_expression in tqdm(relevant_factor_queries.items(), desc='Searching: '):\n    results = search(searcher, analyzer, query_expression)\n    papers_by_relevant_factor[relevant_factor] = results\n    print(\"{}: {} documents found\".format(relevant_factor, len(results.index)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extraction Grammar Description <a class=\"anchor\" id=\"grammar-description-header\"></a>\n\nGrammars have greater expressive power compared to regular expressions, so we have developed a [GrapeNLP](https://github.com/GrapeNLP) grammar for the extraction of causal relations between temperature and humidity related factors and effects such as spread of the virus or death. This grammar performs a similar operation than a regular expression with extraction groups: it will try to match the entire text of each paper, then upon a match it will extract segments of the matched text that are bounded in the grammar by XML tags. The XML tags act as parenthesis in regular expressions for defining extraction groups, allowing later to retrieve the matched portions of text in each group using the tag names instead of group indexes. We extract the following segments from the text:\n\n* The detected relevant factor (e.g. temperature)\n* The minimum excerpt of text that contains the causal relation statement\n\nApart from these, we generate an empty XML tag when negation of the causal relation is present, in order to classify the excerpt as influential or non-influential.\n\nThe grammars can be structured into reusable components (equivalent to non-terminal symbols of context-free grammars) as illustrated by the grammar axiom:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image('../input/relevant-factor-grammars/axiom_temp_or_humidity.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This grammar relies on sub-grammar **causal_relation**  to detect the causal or negated causal relation. The matched fragment of text expressing the causal relation is delimited by XML tags &lt;excerpt&gt; in order to extract it. Since the grammar engine performs exact matching on the entire paper full text, a call to grammar **null-insert** is added before and after the causal relation to match the beginning and end of the paper. This grammar recognizes an arbitrary sequence of 0, 1 or more tokens:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image('../input/relevant-factor-grammars/null_insert.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apart from generating XML tags, the grammar engine generates as well matching scores per token, which by default depend on the specificity degree of the lexical mask used to match each token: lexical mask &lt;TOKEN&gt; is given by default 0 points, while lexical masks requiring a specific word, digit or symbol are given 14 points. The grammar engine generates an efficient representation of all posible matches as a kind of weighted finite-state automaton with output, then uses a Viterbi-like algorithm to efficiently extract the match with the highest overall score.\n\nThe **causal_relation** grammar below detects expressions of correlations or results of a factor or list of factors (grammar **list_of_factors**) with an effect (grammar **efect**). If present, it also detects negation particles and generates an empty &lt;negation&gt; tag for classifying the relation as non-influential. Meta code &lt;E&gt; represents the empty string, and is used to make a box optional (e.g. tokens \"positive\" and \"positively\" may appear but are not mandatory).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image('../input/relevant-factor-grammars/causal_relation.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grammar **penalizing_insert** is similar to grammar **null_insert**: it recognizes 0, 1 or more arbitrary tokens, but overwrites the default 0 score per token by a -15 score, letting the causal expressions to contain unknown token inserts but favoring a minimal occurrence of these.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image('../input/relevant-factor-grammars/penalizing_insert.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to avoid recognizing too large chunks of texts as causal relations, matches that are below an overall grammar score will be rejected.\n\nThe **list_of_factor** grammar recognizes lists of 1 or more relevant factors, allowing as well mentions in between of unknown tokens. However, the matched list must start and end with a known factor. The matched list of factors is delimited by tag &lt;factors&gt; to extract it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image('../input/relevant-factor-grammars/list_of_factors.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grammar **temp_or_humidity** detects one single relevant factor, in our case different expressions related to temperature and humidity:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image('../input/relevant-factor-grammars/temp_or_humidity.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, grammar **effect** detects different expressions of effects such as spread, transmission or death. Additionally, it also recognizes optional modifiers such as \"increases\" or \"decreases\", and in the later case generates the &lt;negation&gt; tag to classify the relation as non-influential:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Image('../input/relevant-factor-grammars/effect.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load grammar <a class=\"anchor\" id=\"load-grammar-header\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_delaf_pathname = os.path.join('..', 'input', 'test-delaf', 'dictionary.bin')\ngrammar_dir = os.path.join('..', 'input', 'relevant-factor-grammars')\nrelevant_factors_grammar_pathname = os.path.join(grammar_dir, 'grammar_relevant_factors.fst2')\nrelevant_factors_grammar_engine = GrammarEngine(relevant_factors_grammar_pathname, bin_delaf_pathname)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extract datapoints <a class=\"anchor\" id=\"extract-header\"></a>\n\nWe apply here the loaded grammar to the full text of each paper found, for each relevant factor defined in the first section of this notebook.\n\nThe grammar engine is implemented in C++, and the corresponding native match object accessed from Python using SWIG. We use the following function to convert this native object into a Python dictionary which is easier to process. Upon multiple matches of the same expression, we retrieve the top scored match. The grammar not only matches expressions but tags the fragmets to extract (what we call \"segments\" of the input text). For the top scored match, this method returns a Python dictionary for all the tagged text segments, using as key the label of the tag and as value the text segment itself. The total match score is also returned in order to be able to enforce a minimum score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def grapenlp_results_to_python_dic(sentence, native_results):\n    top_segments = OrderedDict()\n    score = -sys.maxsize - 1 # Minimum integer value\n    if not native_results.empty():\n        top_native_result = native_results.get_elem_at(0)\n        score = top_native_result.w\n        top_native_result_segments = top_native_result.ssa\n        for i in range(0, top_native_result_segments.size()):\n            native_segment = top_native_result_segments.get_elem_at(i)\n            native_segment_label = native_segment.name\n            segment_label = u_out_bound_trie_string_to_string(native_segment_label)\n            segment = OrderedDict()\n            segment['value'] = sentence[native_segment.begin:native_segment.end]\n            segment['start'] = native_segment.begin\n            segment['end'] = native_segment.end\n            top_segments[segment_label] = segment\n    return top_segments, score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function below generates the final datapoints to present in the summary table, given the extracted segments of text:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_segments(text, segments):\n    study_type = None\n    factors = None\n    influential = 'Y'\n    excerpt = None\n    measure_of_evidence = None\n\n    if 'study_type' in segments:\n        study_type = segments['study_type']['value']\n    if 'factors' in segments:\n        factors = segments['factors']['value']\n    if 'negation' in segments:\n        influential = 'N'\n    if 'measure_of_evidence' in segments:\n        measure_of_evidence = segments['measure_of_evidence']['value']\n\n    if factors and 'excerpt' in segments:\n        excerpt_start = segments['excerpt']['start']\n        excerpt_end = segments['excerpt']['end']\n        factors_start = segments['factors']['start']\n        factors_end = segments['factors']['end']\n        factors_value = segments['factors']['value']\n        \n        left_excerpt = text[excerpt_start:factors_start]\n        right_excerpt = text[factors_end:excerpt_end]\n        left_context = text[excerpt_start - CONTEXT_SIZE:excerpt_start]\n        right_context = text[excerpt_end:excerpt_end + CONTEXT_SIZE]\n        excerpt = left_context + \\\n                  '<span style=\"' + EXCERPT_STYLE + '\">' + left_excerpt + '</span>' + \\\n                  '<span style=\"' + FACTORS_STYLE + '\">' + factors + '</span>' + \\\n                  '<span style=\"' + EXCERPT_STYLE + '\">' + right_excerpt + '</span>' + \\\n                  right_context\n\n    return study_type, factors, influential, excerpt, measure_of_evidence","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function applies the grammar to the entire paper and returns the summary table as a Pandas dataframe:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_from_text(papers, grammar_engine):\n# Extracts from the paper full text the target datapoints for the given risk factor\n    result_df = pd.DataFrame(columns=['Date', 'Study','Study Link','Journal','Study Type','Factors','Influential', 'Score', 'Excerpt', 'Measure of Evidence'])\n    context = {}\n    matched_papers = 0\n    for id, paper in tqdm(papers.iterrows(), total=len(papers.index)):\n        text = paper['paper_full_text']\n        native_matches = grammar_engine.tag(text, context)\n        segments, score = grapenlp_results_to_python_dic(text, native_matches)\n        if segments:\n            matched_papers += 1\n            if score >= MIN_GRAMMAR_SCORE:\n                print(\"Score:\", score)\n                study_type, factors, influential, excerpt, measure_of_evidence = parse_segments(text, segments)\n                if excerpt:\n                    paper_data_dict = {\n                        'Date': html.escape(paper.date),\n                        'Study': html.escape(paper.study),\n                        'Study Link': '<a href=\"' + paper.study_link + '\">' + html.escape(paper.study_link) + '</a>',\n                        'Journal': html.escape(paper.journal),\n                        'Study Type': study_type,\n                        'Factors': factors,\n                        'Influential': influential,\n                        'Score': score,\n                        'Excerpt': excerpt,\n                        'Measure of Evidence': measure_of_evidence\n                    }\n                    result_df = result_df.append(paper_data_dict, ignore_index=True)\n    matches_above_threshold = len(result_df.index)\n    total_papers = len(papers.index)\n    display(HTML(\"<p>total papers/total matches/matches above threshold: {}/{}/{}</p>\".format(total_papers, matched_papers, matches_above_threshold)))\n    return result_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we apply the extraction function to all the papers returned by Lucene. While this notebook focuses on temperature humidity, the loop below could be used in a future work to extract tables for the other proposed relevant factors.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture --no-display\n# For each relevant factor\ntables_by_relevant_factor = {}\nfor relevant_factor, papers in tqdm(papers_by_relevant_factor.items(), desc='Extracting: '):\n    table = extract_from_text(papers, relevant_factors_grammar_engine)\n    tables_by_relevant_factor[relevant_factor] = table","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Display tables <a class=\"anchor\" id=\"display-header\"></a>\n\nGiven a list of papers, a relevant factor and a grammar engine instance, this method returns a DataFrame for the target summary tables containing an entry per paper, where an effect was found close to a mention of the factor. The table includes the target datapoints, indicating whether the factor was influential or not depending on the presence of the absence or presence of the &lt;negation&gt; tag. The excerpt of text with some left and right context is included in a last column for manual review, as well as the overall matching score, which can be used to fine tune the matching threshold.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture --no-display\nfor relevant_factor, table in tables_by_relevant_factor.items():\n    display(HTML(table.to_html(escape=False)))\n    table.to_csv(relevant_factor + \".csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion <a class=\"anchor\" id=\"conclusion-header\"></a>\n\nWe set out to provide a reproducible pipeline to accurately search and extract valuable information from Covid-19 research studies for relevant factors. We have demonstrated the ability to automatically extract tables for relevent factors realted to temoerature and humidity. This pipeline enables researchers, clinicians, policy makers and many others to rapidly mine tens of thousands of publications in order to find highly relevant factors from studies related to Covid-19. This pipeline is fully automatic and is highly configurable and can include grammars for many other factors. Our inspection of the result show that they are highly relevant and we can tune precision and recall by tuning the grammar threshold for accepting a match.\n\n## Pros and cons of this approach\n\n### Pros\n\nOur approach provides the following features:\n\n* We provide a robust grammar engine to extract factors related to Covid-19.\n* The generated results are highly relevant\n* We can tune precision and recall by tuning the grammar threshold for accepting a match.\n* We automate all of the steps needed to extract this crucial information in one reusable pipeline.\n* The system is highly configurable, allowing researchers to target other factors and issues.\n* The analysis is highly reproducible, allowing other researchers to replicate and reuse the pipeline.\n* The system can be re-run as more data becomes available.\n\n### Cons\n\nThe following are the shortfalls of our approach:\n\n* More factors to be explored\n\n## Future Work\n\n* Extend approach to more factors\n* Further refinements of the grammar to improve precision and recall","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}