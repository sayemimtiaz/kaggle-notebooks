{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm"},{"cell_type":"markdown","metadata":{},"source":"# Scripty McScriptface the Lazy Kaggler\n\nKaggle Scripts were launched in April 2015 and have received both positive and negative feedback.\n\nOne of the common critiques of the Scripts is that they allow people to achieve high leaderboard positions with very little effort - literally by clicking a button.\n\nI want to investigate what kind of results could be achieved by using only the public scripts. Enter Scripty McScriptface, a Kaggler who does nothing but submit public scripts for every competition where they are enabled.\n\nWhere would this person be today in the Kaggle global ranking?\n\n## Load the data\n\nRead in competitions data, use only competitions that award points."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"competitions = (pd.read_csv('../input/Competitions.csv')\n                .rename(columns={'Id':'CompetitionId'}))\ncompetitions = competitions[(competitions.UserRankMultiplier > 0)]"},{"cell_type":"markdown","metadata":{},"source":"Figure out if competition's evaluation metric should be maximized or minimized:"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"evals = (pd.read_csv('../input/EvaluationAlgorithms.csv')\n           .rename(columns={'Id':'EvaluationAlgorithmId'}))\ncompetitions = competitions.merge(evals[['EvaluationAlgorithmId','IsMax']], \n                                  how='left',on='EvaluationAlgorithmId')\n# Fill missing values for two competitions\ncompetitions.loc[competitions.CompetitionId==4488,'IsMax'] = True # Flavours of physics\ncompetitions.loc[competitions.CompetitionId==4704,'IsMax'] = False # Santa's Stolen Sleigh"},{"cell_type":"markdown","metadata":{},"source":"Which competitions have scripts?"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"scriptprojects = pd.read_csv('../input/ScriptProjects.csv')\ncompetitions = competitions[competitions.CompetitionId.isin(scriptprojects.CompetitionId)]\nprint(\"Found {} competitions with scripts enabled.\".format(competitions.shape[0]))\nif competitions.IsMax.isnull().any():\n    # in case this is rerun after more competitions are added\n    print(\"Please fill IsMax value for:\")\n    print(competitions.loc[competitions.IsMax.isnull(),['CompetitionId','Title']])"},{"cell_type":"markdown","metadata":{},"source":"Read in teams data and filter by competition."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"teams = (pd.read_csv('../input/Teams.csv')\n         .rename(columns={'Id':'TeamId'}))\nteams = teams[teams.CompetitionId.isin(competitions.CompetitionId)]\nteams['Score'] = teams.Score.astype(float)"},{"cell_type":"markdown","metadata":{},"source":"Read in submissions data and filter by competition (through teams selected previously). Filter out submissions with errors and submissions made after the competition deadline."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"submissions = pd.read_csv('../input/Submissions.csv')\nsubmissions = submissions[(submissions.TeamId.isin(teams.TeamId))\n                         &(submissions.IsAfterDeadline==False)\n                         &(~(submissions.PublicScore.isnull()))]\nsubmissions = submissions.merge(teams[['TeamId','CompetitionId']],\n                                how='left',on='TeamId')\nsubmissions = submissions.merge(competitions[['CompetitionId','IsMax']],\n                                how='left',on='CompetitionId')"},{"cell_type":"markdown","metadata":{},"source":"## How many teams use script submissions?\nCalculate how many teams participated in each competition and how many submitted at least one script result. Submissions have a `SourceScriptVersionId` field that lets us determine if a submission was from a script. We don't catch submissions that come from downloading the script's results and submitting a .csv file. "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"competitions.set_index(\"CompetitionId\",inplace=True)\n# How many teams participated in a competition?\ncompetitions['Nteams'] = (submissions.groupby('CompetitionId')\n                          ['TeamId'].nunique())\n# How many teams used at least one script submission?\ncompetitions['TeamsSubmittedScripts'] = (submissions\n                                         [~(submissions.SourceScriptVersionId.isnull())]\n                                         .groupby('CompetitionId')['TeamId'].nunique())"},{"cell_type":"markdown","metadata":{},"source":"Another interesting quantity is how many teams actually selected their script submission for scoring. We have an IsSelected field in the Submissions table, so we can use that. Usually any team may select up to two submissions for scoring. If a team selects less than two then their best public submissions are used instead.\n\nHow many submissions do teams usually select?"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"submissions.groupby('TeamId')['IsSelected'].sum().value_counts()"},{"cell_type":"markdown","metadata":{},"source":"So a lot of teams did not select any submissions at all. We need to figure out which two submissions were used for scoring those teams."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def isscored(group):\n    # if two or less submissions select all\n    if group.shape[0] <= 2:\n        pd.Series(np.ones(group.shape[0],dtype=np.bool),index=group.index)\n    nsel = group.IsSelected.sum()\n    # if two selected return them\n    if nsel == 2:\n        return group.IsSelected\n    # if need to select more - choose by highest public score\n    toselect = list(group.IsSelected.values.nonzero()[0])\n    ismax = group['IsMax'].iloc[0]\n    ind = np.argsort(group['PublicScore'].values)\n    scored = group.IsSelected.copy()\n    if ismax:\n        ind = ind[::-1]\n    for i in ind:\n        if i not in toselect:\n            toselect.append(i)\n        if len(toselect)==2:\n            break\n    scored.iloc[toselect] = True\n    return scored"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"submissions['PublicScore'] = submissions['PublicScore'].astype(float)\nsubmissions['PrivateScore'] = submissions['PrivateScore'].astype(float)\nscored = submissions.groupby('TeamId',sort=False).apply(isscored)\nscored.index = scored.index.droplevel()\nsubmissions['IsScored'] = scored"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# How many teams selected a script submission for private LB scoring?\ncompetitions['TeamsSelectedScripts'] = (submissions\n                                        [~(submissions.SourceScriptVersionId.isnull())&\n                                          (submissions.IsScored)]\n                                        .groupby('CompetitionId')['TeamId'].nunique())"},{"cell_type":"markdown","metadata":{},"source":"Finally, let's look at the plot of our results."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"competitions.sort_values(by='Nteams',inplace=True)\nfig, ax = plt.subplots(figsize=(10,8))\nh = np.arange(len(competitions))\ncolors = cm.Blues(np.linspace(0.5, 1, 3))\nax.barh(h, competitions.Nteams,color=colors[0])\nax.barh(h, competitions.TeamsSubmittedScripts,color=colors[1])\nax.barh(h, competitions.TeamsSelectedScripts,color=colors[2])\nax.set_yticks(h+0.4)\nax.set_yticklabels(competitions.Title.values);\nax.set_ylabel('');\nax.legend(['Total teams',\n           'Submitted from a script',\n           'Selected a script submission'],loc=4,fontsize='large');\nax.set_title('Usage of script submissions by teams');\nax.set_ylim(0,h.max()+1);"},{"cell_type":"markdown","metadata":{},"source":"Things to note here:\n\n* Some competitions like Otto Group and Restaurant Revenue Prediction have scripts enabled but don't have any script submissions. This is because \"submit to competition\" button was not yet available at the time.\n* Around 25% of teams use script submissions.\n* More than half of the teams that submit script results actually select these submissions for scoring."},{"cell_type":"markdown","metadata":{},"source":"## Scripty's results\n\n### Submit two best scripts from the public LB\nFor estimating what kind of results we might expect from using public scripts I'll assume that Scripty looks for best performing scripts on the public LB and selects the best two for scoring on the private LB."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"competitions[\"NScriptSubs\"] = (submissions\n                               [~(submissions.SourceScriptVersionId.isnull())]\n                               .groupby('CompetitionId')['Id'].count())\nscriptycomps = competitions[competitions.NScriptSubs > 0].copy()\nscriptycomps.shape"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def find_private_score(df):\n    if df.SourceScriptVersionId.isnull().all():\n        # no scripts\n        return\n    ismax = df.IsMax.iloc[0]\n    submit = (df.loc[~(df.SourceScriptVersionId.isnull())]\n                .groupby('SourceScriptVersionId')\n                [['PublicScore','PrivateScore']]\n                .agg('first')\n                .sort_values(by='PublicScore',ascending = not ismax)\n                .iloc[:2])\n    score = submit.PrivateScore.max() if ismax else submit.PrivateScore.min()\n    # Find scores from all teams\n    results = (df.loc[df.IsScored]\n                 .groupby('TeamId')\n                 ['PrivateScore']\n                 .agg('max' if ismax else 'min')\n                 .sort_values(ascending = not ismax)\n                 .values)\n    if ismax:\n        ranktail = (results <  score).nonzero()[0][0] + 1\n        rankhead = (results <= score).nonzero()[0][0] + 1\n    else:\n        ranktail = (results >  score).nonzero()[0][0] + 1\n        rankhead = (results >= score).nonzero()[0][0] + 1\n    rank = int(0.5*(ranktail+rankhead))\n    return pd.Series({'Rank':rank,'Score':score})\n\nscriptycomps[['Rank','Score']] = (submissions.groupby('CompetitionId')\n                                             .apply(find_private_score))\nscriptycomps['TopPerc'] = np.ceil(100*scriptycomps['Rank']\n                                  /scriptycomps['Nteams'])\nscriptycomps['Points'] = (1.0e5*((scriptycomps.Rank)**(-0.75))\n                          *np.log10(1+np.log10(scriptycomps.Nteams))\n                          *scriptycomps.UserRankMultiplier)\nscriptycomps[['Title','Score','Nteams',\n              'Rank','TopPerc','Points']].sort_values(by='Rank')"},{"cell_type":"markdown","metadata":{},"source":"The result in Liberty Mutual Group competition seems unbelievably good. [Here](https://www.kaggle.com/chriscc/liberty-mutual-group-property-inspection-prediction/blah-xgb/run/45838) is the responsible script. Apparently even the script's author did not guess how well it was likely to perform on the private Leaderboard as it was not chosen. The best actual result that comes from a script submission has rank 150 in this competition.\n\nHow many badges did Scripty earn?"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"top10p = (scriptycomps.TopPerc <= 10).sum()\ntop25p = ((scriptycomps.TopPerc > 10)&(scriptycomps.TopPerc <= 25)).sum()\nprint(\"{} Top10% badges and {} Top25% badges\".format(top10p, top25p))"},{"cell_type":"markdown","metadata":{},"source":"Not bad... Well, at least he didn't earn a master status this way.\n\nHow many points does he have now?"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"lastdeadline = pd.to_datetime(competitions.Deadline.max())\ndecay = np.exp((pd.to_datetime(scriptycomps.Deadline) - lastdeadline).dt.days/500)\ntotalpoints = (decay*scriptycomps.Points).sum()\ntotalpoints"},{"cell_type":"markdown","metadata":{},"source":"Well, shame... it's more than I have. What's his global ranking now?"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"users = pd.read_csv('../input/Users.csv').sort_values(by='Points',ascending=False)\nrank = (users.Points < totalpoints).nonzero()[0][0] + 1\nprint(\"Number {} in the global ranking\".format(rank))"},{"cell_type":"markdown","metadata":{},"source":"We could argue that the single excellent result in Liberty Mutual competition affects our calculations too strongly. If we change the 15th place to 150th (best script submission result on private LB) then the results become:"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"scriptycomps.loc[4471,'Rank'] = 150\nscriptycomps['TopPerc'] = np.ceil(100*scriptycomps['Rank']/scriptycomps['Nteams'])\nscriptycomps['Points'] = 1.0e5*((scriptycomps.Rank)**(-0.75))*np.log10(1+np.log10(scriptycomps.Nteams))*scriptycomps.UserRankMultiplier\ntotalpoints1 = (decay*scriptycomps.Points).sum()\ntotalpoints1"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"rank1 = (users.Points < totalpoints1).nonzero()[0][0] + 1\nrank1"},{"cell_type":"markdown","metadata":{},"source":"The rank is predictably worse but still in the top 500.\n\n### Submit what everyone else submits\n\nA valid point was raised in the comments that it's not always possible to find the best scoring script (it may be hidden), and also it's not possible for Scripty to be the first to submit anything. So I came up with another way of calculating Scripty's rank.\n\nI look at script submissions that were actually used for scoring the participants. \nAmong these I find two most popular script versions, and that's what Scripty selects. \nThen his score is the best private score of these two submissions. \nSince there's a lot of people with the same score I take the rank of the middle of this group.\nI think this deals with both stated problems."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def find_private_score(df):\n    if df.SourceScriptVersionId.isnull().all():\n        # no scripts\n        return\n    ismax = df.IsMax.iloc[0]\n    competition = df.name\n    submit = (df.loc[~(df.SourceScriptVersionId.isnull())\n                     &(df.IsScored)]\n                .groupby('SourceScriptVersionId')\n                .agg({'PublicScore':'first','PrivateScore':'first','Id':'size'})\n                .rename(columns={'Id':'Nteams'})\n                .sort_values(by='Nteams',ascending = False)\n                .iloc[:2])\n    score = submit.PrivateScore.max() if ismax else submit.PrivateScore.min()\n    # Find scores from all teams\n    results = (df.loc[df.IsScored]\n                 .groupby('TeamId')\n                 ['PrivateScore']\n                 .agg('max' if ismax else 'min')\n                 .sort_values(ascending = not ismax)\n                 .values)\n    rank = int(np.median((results==score).nonzero()[0])) + 1\n    return pd.Series({'Rank':rank,'Score':score})\n\nscriptycomps[['Rank','Score']] = (submissions.groupby('CompetitionId')\n                                             .apply(find_private_score))\nscriptycomps['TopPerc'] = np.ceil(100*scriptycomps['Rank']\n                                  /scriptycomps['Nteams'])\nscriptycomps['Points'] = (1.0e5*((scriptycomps.Rank)**(-0.75))\n                          *np.log10(1+np.log10(scriptycomps.Nteams))\n                          *scriptycomps.UserRankMultiplier)\nscriptycomps[['Title','Score','Nteams',\n              'Rank','TopPerc','Points']].sort_values(by='Rank')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"top10p = (scriptycomps.TopPerc <= 10).sum()\ntop25p = ((scriptycomps.TopPerc > 10)&(scriptycomps.TopPerc <= 25)).sum()\nprint(\"{} Top10% badges and {} Top25% badges\".format(top10p, top25p))\ntotalpoints2 = (decay*scriptycomps.Points).sum()\nrank2 = (users.Points < totalpoints2).nonzero()[0][0] + 1\nprint(\"Ranked {} with {:.1f} points.\".format(rank2,totalpoints2))"},{"cell_type":"markdown","metadata":{},"source":"Scripty's performance became less stellar with this approach, but he's still in \nthe global top 1000.\n\n\n===\n\nI guess this does prove the point that prowling the leaderboards and looking for \nbest performing scripts could bring you well into the top 1000 Kagglers. \nEven into the top 500 with some luck. \n\nI can see why people who don't have time for participating in every competition would be frustrated about being pushed down in the ratings by the script chasers.\n\nThis could potentially devalue Kaggle global rankings. Still for me the main benefit of every competition was the things I learned along the way and not the final placement. I value being able to discuss the competition's dataset properties and quirks, which kinds of models worked and why. And this is something a script chaser would lack. Also \"beating that damn script\" can be a nice motivator.\n\n\nI think the scripts do provide great opportunities for code sharing and learning. I wish there was some parallel ranking system that would reward posting useful and interesting scripts. Then Kaggle could be a step closer to the Home for Data Science =)."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}