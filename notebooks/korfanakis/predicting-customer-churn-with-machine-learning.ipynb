{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font size=+3 color=\"#3D3D3D\"><center><b>Predicting Customer Churn with Machine Learning üè¶üí∞</b></center></font>\n\n<img src=\"https://images.unsplash.com/photo-1520033906782-1684d0e7498e?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1334&q=80\" width = 600>\n<center><em>Photo by Michael Jasmund (Unsplash)</em></center>\n\n<br>\n\n**Table of Contents**\n\n- [Introduction](#Introduction)\n- [Objective](#Objective)\n- [Libraries](#Libraries)\n- [Parameters and Variables](#Parameters-and-Variables)\n- [Functions](#Functions)\n- [A Quick Look at our Data](#A-Quick-Look-at-our-Data)\n- [Creating a Test Set](#Creating-a-Test-Set)\n- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n    - [Target Variable: Exited](#Target-Variable:-Exited)\n    - [Continuous Variables](#Continuous-Variables)\n    - [Categorical Variables](#Categorical-Variables)\n- [Data Preprocessing](#Data-Preprocessing)\n    - [Feature Selection](#Feature-Selection)\n    - [Encoding Categorical Features](#Encoding-Categorical-Features)\n    - [Scaling](#Scaling)\n    - [Addressing Class Imbalance](#Addressing-Class-Imbalance)\n- [Building Machine Learning Models](#Building-Machine-Learning-Models)\n    - [Baseline Models](#Baseline-Models)\n    - [Model Tuning](#Model-Tuning)\n- [Results](#Results)\n    - [Learning Curves](#Learning-Curves)\n    - [Feature Importance](#Feature-Importance)\n    - [Performance Comparison](#Performance-Comparison)\n- [Evaluating the Test Set](#Evaluating-the-Test-Set)\n- [Bibliography](#Bibliography)\n- [Future Development](#Future-Development)\n- [Conclusions](#Conclusions)\n  \n\n<br>\n\n# Introduction\n\n\nCustomer churn (also known as customer attrition) occurs when a customer stops using a company's products or services. \n\nCustomer churn affects profitability, especially in industries where revenues are heavily dependent on subscriptions (e.g. banks, telephone and internet service providers, pay-TV companies, insurance firms, etc.). It is estimated that acquiring a new customer can cost up to five times more than retaining an existing one.\n\nTherefore, customer churn analysis is essential as it can help a business:\n\n- identify problems in its services (e.g. poor quality product/service, poor customer support, wrong target audience, etc.), and \n- make correct strategic decisions that would lead to higher customer satisfaction and consequently higher customer retention.\n\nIf you would like to know more about this topic, please refer to the references in the [Bibliography](#Bibliography) section.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n# Objective\n\nThe goal of this notebook is to understand and predict customer churn for a bank. Specifically, we will initially perform **Exploratory Data Analysis** (**EDA**) to identify and visualise the factors contributing to customer churn. This analysis will later help us build **Machine Learning** models to predict whether a customer will churn or not. \n\nThis problem is a typical **classification** task. The task does not specify which performance metric to use for optimising our machine learning models. I decided to use **recall** since correctly classifying elements of the positive class (customers who will churn) is more critical for the bank.\n\n<br>\n\n*Skills: Exploratory Data Analysis, Data Visualisation, Data Preprocessing (Feature Selection, Encoding Categorical Features, Feature Scaling), Addressing Class Imbalance (SMOTE), Model Tuning.*\n\n*Models Used: Logistic Regression, Support Vector Machines, Random Forests, Gradient Boosting, XGBoost, and Light Gradient Boosting Machine.*\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n# Libraries","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport numpy as np\nimport pandas as pd\npd.set_option('precision', 3)\n\n# Data Visualisation Libraries\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\n\n!pip install seaborn --upgrade\nimport seaborn as sns\nsns.set_style('darkgrid')\n\n# Statistics\nfrom scipy.stats import chi2_contingency\nfrom imblearn.over_sampling import SMOTE\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.model_selection import learning_curve\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, auc, roc_auc_score, roc_curve\nfrom sklearn.metrics import confusion_matrix\nimport scikitplot as skplt\n\nprint('‚úîÔ∏è Libraries Imported!')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.286749Z","start_time":"2021-08-25T06:37:02.535173Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:44.544136Z","iopub.execute_input":"2021-08-25T07:32:44.544462Z","iopub.status.idle":"2021-08-25T07:32:50.594088Z","shell.execute_reply.started":"2021-08-25T07:32:44.544435Z","shell.execute_reply":"2021-08-25T07:32:50.591825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n# Parameters and Variables\n\nIt is convenient to set some (default) parameters and variables for the whole notebook.  ","metadata":{}},{"cell_type":"code","source":"font_size = 20\nplt.rcParams['axes.labelsize'] = font_size\nplt.rcParams['axes.titlesize'] = font_size + 2\nplt.rcParams['xtick.labelsize'] = font_size - 2\nplt.rcParams['ytick.labelsize'] = font_size - 2\nplt.rcParams['legend.fontsize'] = font_size - 2\n\ncolors = ['#00A5E0', '#DD403A']\ncolors_cat = ['#E8907E', '#D5CABD', '#7A6F86', '#C34A36', '#B0A8B9', '#845EC2', '#8f9aaa', '#FFB86F', '#63BAAA', '#9D88B3', '#38c4e3']\ncolors_comp = ['steelblue', 'seagreen', 'black', 'darkorange', 'purple', 'firebrick', 'slategrey']\n\nrandom_state = 42\nscoring_metric = 'recall'\ncomparison_dict, comparison_test_dict = {}, {}\n\nprint('‚úîÔ∏è Default Parameters and Variables Set!')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.296672Z","start_time":"2021-08-25T06:37:16.28923Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:50.595979Z","iopub.execute_input":"2021-08-25T07:32:50.596265Z","iopub.status.idle":"2021-08-25T07:32:50.604886Z","shell.execute_reply.started":"2021-08-25T07:32:50.596235Z","shell.execute_reply":"2021-08-25T07:32:50.603754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n# Functions\n\nSince we will reuse parts of the code, it will be helpful to define some functions.\n\n## `plot_continuous()`","metadata":{}},{"cell_type":"code","source":"def plot_continuous(feature):\n    '''Plot a histogram and boxplot for the churned and retained distributions for the specified feature.'''\n    df_func = train_df.copy()\n    df_func['Exited'] = df_func['Exited'].astype('category')\n\n    fig, (ax1, ax2) = plt.subplots(2,\n                                   figsize=(9, 7),\n                                   sharex=True,\n                                   gridspec_kw={'height_ratios': (.7, .3)})\n\n    for df, color, label in zip([df_retained, df_churned], colors, ['Retained', 'Churned']):\n        sns.histplot(data=df,\n                     x=feature,\n                     bins=15,\n                     color=color,\n                     alpha=0.66,\n                     edgecolor='firebrick',\n                     label=label,\n                     kde=False,\n                     ax=ax1)\n    ax1.legend()\n\n    sns.boxplot(x=feature, y='Exited', data=df_func, palette=colors, ax=ax2)\n    ax2.set_ylabel('')\n    ax2.set_yticklabels(['Retained', 'Churned'])\n\n    plt.tight_layout();\n\n\nprint('‚úîÔ∏è Function Defined!')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.310064Z","start_time":"2021-08-25T06:37:16.300145Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:50.606567Z","iopub.execute_input":"2021-08-25T07:32:50.606997Z","iopub.status.idle":"2021-08-25T07:32:50.618271Z","shell.execute_reply.started":"2021-08-25T07:32:50.606955Z","shell.execute_reply":"2021-08-25T07:32:50.617289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## `plot_categorical()`","metadata":{}},{"cell_type":"code","source":"def plot_categorical(feature):\n    '''For a categorical feature, plot a seaborn.countplot for the total counts of each category next to a barplot for the churn rate.'''\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n    sns.countplot(x=feature,\n                  hue='Exited',\n                  data=train_df,\n                  palette=colors,\n                  ax=ax1)\n    ax1.set_ylabel('Count')\n    ax1.legend(labels=['Retained', 'Churned'])\n\n    sns.barplot(x=feature,\n                y='Exited',\n                data=train_df,\n                palette=colors_cat,\n                ax=ax2)\n    ax2.set_ylabel('Churn rate')\n\n    if (feature == 'HasCrCard' or feature == 'IsActiveMember'):\n        ax1.set_xticklabels(['No', 'Yes'])\n        ax2.set_xticklabels(['No', 'Yes'])\n\n    plt.tight_layout();\n\n\nprint('‚úîÔ∏è Function Defined!')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.32349Z","start_time":"2021-08-25T06:37:16.313535Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:50.619759Z","iopub.execute_input":"2021-08-25T07:32:50.620187Z","iopub.status.idle":"2021-08-25T07:32:50.633304Z","shell.execute_reply.started":"2021-08-25T07:32:50.620146Z","shell.execute_reply":"2021-08-25T07:32:50.632428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## `plot_conf_mx()`","metadata":{}},{"cell_type":"code","source":"def plot_conf_mx(cm, ax):\n    '''Plot a confusion matrix in the specified axes object.'''\n    sns.heatmap(data=cm,\n                annot=True,\n                cmap='Blues',\n                annot_kws={'fontsize': 30},\n                ax=ax)\n\n    ax.set_xlabel('Predicted Label')\n    ax.set_xticks([0.5, 1.5])\n    ax.set_xticklabels(['Retained', 'Churned'])\n\n    ax.set_ylabel('True Label')\n    ax.set_yticks([0.25, 1.25])\n    ax.set_yticklabels(['Retained', 'Churned']);\n\n\nprint('‚úîÔ∏è Function Defined!')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.334862Z","start_time":"2021-08-25T06:37:16.326927Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:50.636074Z","iopub.execute_input":"2021-08-25T07:32:50.636381Z","iopub.status.idle":"2021-08-25T07:32:50.64422Z","shell.execute_reply.started":"2021-08-25T07:32:50.636349Z","shell.execute_reply":"2021-08-25T07:32:50.643075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## `plot_learning_curve()`","metadata":{}},{"cell_type":"code","source":"def plot_learning_curve(estimator,\n                        X,\n                        y,\n                        ax,\n                        cv=None,\n                        train_sizes=np.linspace(0.1, 1.0, 5)):\n    '''Plot the learning curves for an estimator in the specified axes object.'''\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator,\n        X,\n        y,\n        cv=cv,\n        n_jobs=-1,\n        train_sizes=train_sizes,\n        scoring='accuracy')\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.fill_between(train_sizes,\n                    train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std,\n                    alpha=0.1,\n                    color='dodgerblue')\n    ax.fill_between(train_sizes,\n                    test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std,\n                    alpha=0.1,\n                    color='darkorange')\n\n    ax.plot(train_sizes,\n            train_scores_mean,\n            color='dodgerblue',\n            marker='o',\n            linestyle='-',\n            label='Training Score')\n    ax.plot(train_sizes,\n            test_scores_mean,\n            color='darkorange',\n            marker='o',\n            linestyle='-',\n            label='Cross-validation Score')\n\n    ax.set_xlabel('Training Examples')\n    ax.set_ylabel('Score')\n    ax.legend(loc='best', fontsize=14);\n\n\nprint('‚úîÔ∏è Function Defined!')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.349247Z","start_time":"2021-08-25T06:37:16.337839Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:50.647726Z","iopub.execute_input":"2021-08-25T07:32:50.647979Z","iopub.status.idle":"2021-08-25T07:32:50.658766Z","shell.execute_reply.started":"2021-08-25T07:32:50.647955Z","shell.execute_reply":"2021-08-25T07:32:50.657733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## `clf_performance()`","metadata":{}},{"cell_type":"code","source":"def clf_performance(classifier, classifier_name, classifier_name_abv):\n    '''Display the overall performance of a classifier with this template.'''\n    print('\\n', classifier_name)\n    print('-------------------------------')\n    print('   Best Score ({}): '.format(scoring_metric) + str(np.round(classifier.best_score_, 3)))\n    print('   Best Parameters: ')\n    for key, value in classifier.best_params_.items():\n        print('      {}: {}'.format(key, value))\n\n    y_pred_pp = cross_val_predict(estimator=classifier.best_estimator_,\n                                  X=X_train,\n                                  y=y_train,\n                                  cv=5,\n                                  method='predict_proba')[:, 1]\n    y_pred = y_pred_pp.round()\n\n    cm = confusion_matrix(y_train, y_pred, normalize='true')\n\n    fpr, tpr, _ = roc_curve(y_train, y_pred_pp)\n    comparison_dict[classifier_name_abv] = [\n        accuracy_score(y_train, y_pred),\n        precision_score(y_train, y_pred),\n        recall_score(y_train, y_pred),\n        roc_auc_score(y_train, y_pred_pp), fpr, tpr\n    ]\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    plot_conf_mx(cm, ax1)\n    plot_learning_curve(classifier.best_estimator_, X_train, y_train, ax2)\n\n    plt.tight_layout();\n\n\nprint('‚úîÔ∏è Function Defined!')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.367599Z","start_time":"2021-08-25T06:37:16.352719Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:50.660186Z","iopub.execute_input":"2021-08-25T07:32:50.660546Z","iopub.status.idle":"2021-08-25T07:32:50.671712Z","shell.execute_reply.started":"2021-08-25T07:32:50.660509Z","shell.execute_reply":"2021-08-25T07:32:50.671003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## `plot_feature_imp()`","metadata":{}},{"cell_type":"code","source":"def plot_feature_imp(classifier, classifier_name, color, ax):\n    '''Plot the importance of features for a classifier as a barplot.'''\n    importances = pd.DataFrame({'Feature': X_train.columns,\n                                'Importance': np.round(classifier.best_estimator_.feature_importances_, 3)})\n\n    importances = importances.sort_values('Importance', ascending=True).set_index('Feature')\n\n    importances.plot.barh(color=color,\n                          edgecolor='firebrick',\n                          legend=False,\n                          ax=ax)\n    ax.set_title(classifier_name)\n    ax.set_xlabel('Importance');\n\n\nprint('‚úîÔ∏è Function Defined!')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.379999Z","start_time":"2021-08-25T06:37:16.372064Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:50.672762Z","iopub.execute_input":"2021-08-25T07:32:50.673006Z","iopub.status.idle":"2021-08-25T07:32:50.685852Z","shell.execute_reply.started":"2021-08-25T07:32:50.672984Z","shell.execute_reply":"2021-08-25T07:32:50.684998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## `test_func()`","metadata":{}},{"cell_type":"code","source":"def test_func(classifier, classifier_name, ax):\n    '''Assess the performance on the test set and plot the confusion matrix.'''\n    y_pred = classifier.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred, normalize='true')\n\n    comparison_test_dict[classifier_name] = [accuracy_score(y_test, y_pred),\n                                             precision_score(y_test, y_pred),\n                                             recall_score(y_test, y_pred)]\n\n    sns.heatmap(cm,\n                annot=True,\n                annot_kws={'fontsize': 24},\n                cmap='Blues',\n                ax=ax)\n\n    ax.set_title(classifier_name)\n\n    ax.set_xlabel('Predicted Label')\n    ax.set_xticks([0.5, 1.5])\n    ax.set_xticklabels(['Retained', 'Churned'])\n\n    ax.set_ylabel('True Label')\n    ax.set_yticks([0.2, 1.4])\n    ax.set_yticklabels(['Retained', 'Churned']);\n\n\nprint('‚úîÔ∏è Function Defined!')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.394878Z","start_time":"2021-08-25T06:37:16.386447Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:50.68711Z","iopub.execute_input":"2021-08-25T07:32:50.68738Z","iopub.status.idle":"2021-08-25T07:32:50.696127Z","shell.execute_reply.started":"2021-08-25T07:32:50.687355Z","shell.execute_reply":"2021-08-25T07:32:50.695314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n# A Quick Look at our Data\n\nWe start by importing the dataset as a Pandas DataFrame. We can also take a look at the top five rows using the `head()` method:","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/predicting-churn-for-bank-customers/Churn_Modelling.csv')\n\nprint('‚úîÔ∏è Dataset Imported Successfully!\\n')\nprint('It contains {} rows and {} columns.'.format(df.shape[0], df.shape[1]))\ndf.head()","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.443025Z","start_time":"2021-08-25T06:37:16.397358Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:50.697465Z","iopub.execute_input":"2021-08-25T07:32:50.697862Z","iopub.status.idle":"2021-08-25T07:32:50.743613Z","shell.execute_reply.started":"2021-08-25T07:32:50.697802Z","shell.execute_reply":"2021-08-25T07:32:50.742665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our DataFrame has 14 features/attributes and 10K customers/instances. The last feature, '**Exited**', is the **target variable** and indicates whether the customer has churned (0 = No, 1 = Yes). The meaning of the rest of the features can be easily inferred from their name.\n\nFeatures 'RowNumber', 'CustomerId', and 'Surname' are specific to each customer and can be dropped:","metadata":{}},{"cell_type":"code","source":"df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)\ndf.columns","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.453935Z","start_time":"2021-08-25T06:37:16.445508Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:50.744615Z","iopub.execute_input":"2021-08-25T07:32:50.744882Z","iopub.status.idle":"2021-08-25T07:32:50.752067Z","shell.execute_reply.started":"2021-08-25T07:32:50.744854Z","shell.execute_reply":"2021-08-25T07:32:50.751018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `info()` method can give us valuable information such as the number of non-null values and the type of each feature:","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.473281Z","start_time":"2021-08-25T06:37:16.456911Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:50.753276Z","iopub.execute_input":"2021-08-25T07:32:50.753578Z","iopub.status.idle":"2021-08-25T07:32:50.768386Z","shell.execute_reply.started":"2021-08-25T07:32:50.753554Z","shell.execute_reply":"2021-08-25T07:32:50.767375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thankfully, there are **no missing values** in our DataFrame. The `describe()` method gives us a statistical summary of the numerical features:","metadata":{}},{"cell_type":"code","source":"df.describe().T","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.517921Z","start_time":"2021-08-25T06:37:16.475266Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:50.769546Z","iopub.execute_input":"2021-08-25T07:32:50.770075Z","iopub.status.idle":"2021-08-25T07:32:50.806682Z","shell.execute_reply.started":"2021-08-25T07:32:50.770033Z","shell.execute_reply":"2021-08-25T07:32:50.805806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The most important things to note are:\n\n- The age of customers ranges from 18 to 92, with a mean value approximately equal to 40,\n- The mean (and median) tenure is 5 years, so the majority of customers is loyal (tenure > 3), and\n- Approximately 50% of customers are active.\n\nEDA will help us understand our dataset better. However, before we look at the data any further, we need to create a **test set**, put it aside, and use it only to evaluate our Machine Learning models. This practice protects our models from **data snooping bias** (you can read more on page 51 of [[1](#Bibliography)]) and ensures that evaluation will be performed using unseen data. ","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n# Creating a Test Set\n\nWe will split our dataset into a train and test set using scikit-learn's `train_test_split()` function, which implements random sampling. Our dataset is large enough (especially relative to the number of features), so we do **not** risk introducing *sampling bias*.","metadata":{}},{"cell_type":"code","source":"train_df, test_df = train_test_split(df, test_size=0.2, random_state=random_state)\n\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)\n\nprint('Train set: {} rows x {} columns'.format(train_df.shape[0],\n                                               train_df.shape[1]))\nprint(' Test set: {} rows x {} columns'.format(test_df.shape[0],\n                                               test_df.shape[1]))","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.535777Z","start_time":"2021-08-25T06:37:16.5204Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:50.807762Z","iopub.execute_input":"2021-08-25T07:32:50.808014Z","iopub.status.idle":"2021-08-25T07:32:50.819684Z","shell.execute_reply.started":"2021-08-25T07:32:50.80799Z","shell.execute_reply":"2021-08-25T07:32:50.817921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n# Exploratory Data Analysis\n\n## Target Variable: Exited\n\nAs we mentioned earlier, the target variable is already encoded and can take two possible values:\n\n- Zero (0) for a customer that has **not** churned, and\n- One (1) for a customer that has churned.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 6))\n\nsns.countplot(x='Exited', data=train_df, palette=colors, ax=ax)\n\nfor index, value in enumerate(train_df['Exited'].value_counts()):\n    label = '{}%'.format(round((value / train_df['Exited'].shape[0]) * 100, 2))\n    ax.annotate(label,\n                xy=(index, value + 250),\n                ha='center',\n                va='center',\n                color=colors[index],\n                fontweight='bold',\n                size=font_size + 4)\n\nax.set_xticklabels(['Retained', 'Churned'])\nax.set_xlabel('Status')\nax.set_ylabel('Count')\nax.set_ylim([0, 7000]);","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.838543Z","start_time":"2021-08-25T06:37:16.538256Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:50.820897Z","iopub.execute_input":"2021-08-25T07:32:50.821549Z","iopub.status.idle":"2021-08-25T07:32:51.000662Z","shell.execute_reply.started":"2021-08-25T07:32:50.821511Z","shell.execute_reply":"2021-08-25T07:32:50.999652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The bank kept 80% of its clientele.\n\nNotice that our dataset is **skewed/imbalanced** since the number of instances in the 'Retained' class outnumbers the number of instances in the 'Churned' class by a lot. Therefore, accuracy is probably not the best metric for model performance.\n\n\nDifferent visualisation techniques apply to different types of variables, so it's helpful to differentiate between continuous and categorical variables and look at them separately.","metadata":{}},{"cell_type":"code","source":"continuous = ['Age', 'CreditScore', 'Balance', 'EstimatedSalary']\ncategorical = ['Geography', 'Gender', 'Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember']\n\nprint('Continuous: ', ', '.join(continuous))\nprint('Categorical: ', ', '.join(categorical))","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:16.849421Z","start_time":"2021-08-25T06:37:16.841483Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:51.00178Z","iopub.execute_input":"2021-08-25T07:32:51.002037Z","iopub.status.idle":"2021-08-25T07:32:51.00791Z","shell.execute_reply.started":"2021-08-25T07:32:51.002012Z","shell.execute_reply":"2021-08-25T07:32:51.006987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Continuous Variables\n\nBy calling the `hist()` method, we can plot a histogram for each of the four continuous numeric features:","metadata":{}},{"cell_type":"code","source":"train_df[continuous].hist(figsize=(12, 10),\n                          bins=20,\n                          layout=(2, 2),\n                          color='steelblue',\n                          edgecolor='firebrick',\n                          linewidth=1.5);","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:18.05321Z","start_time":"2021-08-25T06:37:16.853388Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:51.00898Z","iopub.execute_input":"2021-08-25T07:32:51.009314Z","iopub.status.idle":"2021-08-25T07:32:52.021546Z","shell.execute_reply.started":"2021-08-25T07:32:51.009255Z","shell.execute_reply":"2021-08-25T07:32:52.020595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 'Age' is slightly tail-heavy, i.e. it extends more further to the right of the median than to the left,\n- Most values for 'CreditScore' are above 600,\n- If we ignore the first bin, 'Balance' follows a fairly normal distribution, and\n- The distribution of 'EstimatedSalary' is more or less uniform and provides little information.\n\n## Looking for Correlations\n\nWe can compute the standard correlation coefficient between every pair of (continuous) features using the pandas' `corr()` method and plot it as a matrix:","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(7, 6))\n\nsns.heatmap(train_df[continuous].corr(),\n            annot=True,\n            annot_kws={'fontsize': 16},\n            cmap='Blues',\n            ax=ax)\n\nax.tick_params(axis='x', rotation=45)\nax.tick_params(axis='y', rotation=360);","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:18.563593Z","start_time":"2021-08-25T06:37:18.056186Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:52.023089Z","iopub.execute_input":"2021-08-25T07:32:52.023451Z","iopub.status.idle":"2021-08-25T07:32:52.318484Z","shell.execute_reply.started":"2021-08-25T07:32:52.023405Z","shell.execute_reply":"2021-08-25T07:32:52.317506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no significant intercorrelation between our features, so we do **not** have to worry about multicollinearity.\n\nLet's look at these features in greater detail.\n\n### Age","metadata":{}},{"cell_type":"code","source":"df_churned = train_df[train_df['Exited'] == 1]\ndf_retained = train_df[train_df['Exited'] == 0]\n\nplot_continuous('Age')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:19.207401Z","start_time":"2021-08-25T06:37:18.566576Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:52.319591Z","iopub.execute_input":"2021-08-25T07:32:52.319852Z","iopub.status.idle":"2021-08-25T07:32:53.121659Z","shell.execute_reply.started":"2021-08-25T07:32:52.319826Z","shell.execute_reply":"2021-08-25T07:32:53.120878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, there is a clear difference between age groups since older customers are more likely to churn. This observation could potentially indicate that preferences change with age, and the bank hasn't adapted its strategy to meet the requirements of older customers.","metadata":{}},{"cell_type":"markdown","source":"### Credit Score","metadata":{}},{"cell_type":"code","source":"plot_continuous('CreditScore')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:19.798633Z","start_time":"2021-08-25T06:37:19.210378Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:53.122898Z","iopub.execute_input":"2021-08-25T07:32:53.123158Z","iopub.status.idle":"2021-08-25T07:32:53.770103Z","shell.execute_reply.started":"2021-08-25T07:32:53.123132Z","shell.execute_reply":"2021-08-25T07:32:53.769447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no significant difference between retained and churned customers in terms of their credit scores.","metadata":{}},{"cell_type":"markdown","source":"### Balance","metadata":{}},{"cell_type":"code","source":"plot_continuous('Balance')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:20.387913Z","start_time":"2021-08-25T06:37:19.801113Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:53.771195Z","iopub.execute_input":"2021-08-25T07:32:53.771645Z","iopub.status.idle":"2021-08-25T07:32:54.410212Z","shell.execute_reply.started":"2021-08-25T07:32:53.771615Z","shell.execute_reply":"2021-08-25T07:32:54.409557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, the two distributions are quite similar. There is a big percentage of non-churned customers with a low account balance.","metadata":{}},{"cell_type":"markdown","source":"### Estimated Salary","metadata":{}},{"cell_type":"code","source":"plot_continuous('EstimatedSalary')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:21.0466Z","start_time":"2021-08-25T06:37:20.39036Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:54.411294Z","iopub.execute_input":"2021-08-25T07:32:54.411748Z","iopub.status.idle":"2021-08-25T07:32:55.096432Z","shell.execute_reply.started":"2021-08-25T07:32:54.411717Z","shell.execute_reply":"2021-08-25T07:32:55.095753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both churned and retained customers display a similar uniform distribution in their salaries. Consequently, we can conclude that salary doesn't have a significant effect on the likelihood to churn.","metadata":{}},{"cell_type":"markdown","source":"## Categorical Variables\n\nLet's plot a seaborn.countplot for each categorical feature:","metadata":{}},{"cell_type":"code","source":"df_cat = train_df[categorical]\n\nfig, ax = plt.subplots(2, 3, figsize=(12, 8))\n\nfor index, column in enumerate(df_cat.columns):\n\n    plt.subplot(2, 3, index + 1)\n    sns.countplot(x=column, data=train_df, palette=colors_cat)\n\n    plt.ylabel('Count')\n    if (column == 'HasCrCard' or column == 'IsActiveMember'):\n        plt.xticks([0, 1], ['No', 'Yes'])\n\nplt.tight_layout();","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:22.42346Z","start_time":"2021-08-25T06:37:21.053016Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:55.09756Z","iopub.execute_input":"2021-08-25T07:32:55.098008Z","iopub.status.idle":"2021-08-25T07:32:56.057152Z","shell.execute_reply.started":"2021-08-25T07:32:55.097978Z","shell.execute_reply":"2021-08-25T07:32:56.056447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Important points:\n\n- The bank has customers in three countries (France, Spain, and Germany). Most customers are in France.\n- There are more male customers than females,\n- Only a small percentage leaves within the first year. The count of customers in tenure years between 1 and 9 is almost the same,\n- Most of the customers have purchased 1 or 2 products, while a small portion has purchased 3 and 4,\n- A significant majority of customers has a credit card, and\n- Almost 50% of customers are not active.\n\nAgain, we will look at these features in greater detail.","metadata":{}},{"cell_type":"markdown","source":"### Geography","metadata":{}},{"cell_type":"code","source":"plot_categorical('Geography')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:23.042468Z","start_time":"2021-08-25T06:37:22.427429Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:56.058299Z","iopub.execute_input":"2021-08-25T07:32:56.058759Z","iopub.status.idle":"2021-08-25T07:32:56.521681Z","shell.execute_reply.started":"2021-08-25T07:32:56.058727Z","shell.execute_reply":"2021-08-25T07:32:56.520984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Customers in Germany are more likely to churn than customers in the other two countries (the churn rate is almost double compared to Spain and France). Many reasons could explain this finding, such as higher competition or different preferences for German customers.","metadata":{}},{"cell_type":"markdown","source":"### Gender","metadata":{}},{"cell_type":"code","source":"plot_categorical('Gender')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:23.624806Z","start_time":"2021-08-25T06:37:23.044948Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:56.522799Z","iopub.execute_input":"2021-08-25T07:32:56.523259Z","iopub.status.idle":"2021-08-25T07:32:57.05158Z","shell.execute_reply.started":"2021-08-25T07:32:56.523227Z","shell.execute_reply":"2021-08-25T07:32:57.050907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Female customers are more likely to churn.","metadata":{}},{"cell_type":"markdown","source":"### Tenure","metadata":{}},{"cell_type":"code","source":"plot_categorical('Tenure')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:24.722948Z","start_time":"2021-08-25T06:37:23.626758Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:57.052902Z","iopub.execute_input":"2021-08-25T07:32:57.05339Z","iopub.status.idle":"2021-08-25T07:32:57.908564Z","shell.execute_reply.started":"2021-08-25T07:32:57.053358Z","shell.execute_reply":"2021-08-25T07:32:57.907886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of years (tenure) does not seem to affect the churn rate.","metadata":{}},{"cell_type":"markdown","source":"### Number of Products","metadata":{}},{"cell_type":"code","source":"plot_categorical('NumOfProducts')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:25.79034Z","start_time":"2021-08-25T06:37:24.725392Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:57.909649Z","iopub.execute_input":"2021-08-25T07:32:57.910188Z","iopub.status.idle":"2021-08-25T07:32:58.422107Z","shell.execute_reply.started":"2021-08-25T07:32:57.910143Z","shell.execute_reply":"2021-08-25T07:32:58.421451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, having 3 or 4 products significantly increases the likelihood of churn. I am not sure how to interpret this result. It could potentially mean that the bank cannot properly support customers with more products which in turn increases customer dissatisfaction.","metadata":{}},{"cell_type":"markdown","source":"### Card Holders","metadata":{}},{"cell_type":"code","source":"plot_categorical('HasCrCard')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:26.466385Z","start_time":"2021-08-25T06:37:25.793279Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:58.423191Z","iopub.execute_input":"2021-08-25T07:32:58.423642Z","iopub.status.idle":"2021-08-25T07:32:58.830763Z","shell.execute_reply.started":"2021-08-25T07:32:58.423611Z","shell.execute_reply":"2021-08-25T07:32:58.830096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Having a credit card does not seem to affect the churn rate.","metadata":{}},{"cell_type":"markdown","source":"### Active Members","metadata":{}},{"cell_type":"code","source":"plot_categorical('IsActiveMember')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:27.088945Z","start_time":"2021-08-25T06:37:26.469327Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:58.831882Z","iopub.execute_input":"2021-08-25T07:32:58.8325Z","iopub.status.idle":"2021-08-25T07:32:59.245692Z","shell.execute_reply.started":"2021-08-25T07:32:58.832463Z","shell.execute_reply":"2021-08-25T07:32:59.245012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's not a surprise that inactive customers are more likely to churn. A significant portion of the clientele is inactive; therefore, the bank will benefit from changing its policy so that more customers become active.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n# Data Preprocessing\n\nData preprocessing is the process of converting raw data into a well-readable format that is suitable for building and training Machine Learning models.\n\nLet's complete this process step-by-step.\n\n## Feature Selection\n\nWe have already performed feature selection by dropping columns 'RowNumber', 'CustomerId', and 'Surname' at the beginning of our notebook. EDA revealed several more features that can be dropped as they do not provide any value in predicting our target variable:\n\n- 'EstimatedSalary' displays a uniform distribution for both types of customers and can be dropped.\n- The categories in 'Tenure' and 'HasCrCard' have a similar churn rate and are deemed redundant. This can be confirmed from a chi-square test [[2](#Bibliography)]:","metadata":{}},{"cell_type":"code","source":"chi2_array, p_array = [], []\nfor column in categorical:\n\n    crosstab = pd.crosstab(train_df[column], train_df['Exited'])\n    chi2, p, dof, expected = chi2_contingency(crosstab)\n    chi2_array.append(chi2)\n    p_array.append(p)\n\ndf_chi = pd.DataFrame({\n    'Variable': categorical,\n    'Chi-square': chi2_array,\n    'p-value': p_array\n})\ndf_chi.sort_values(by='Chi-square', ascending=False)","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:27.316572Z","start_time":"2021-08-25T06:37:27.091389Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:59.24678Z","iopub.execute_input":"2021-08-25T07:32:59.247243Z","iopub.status.idle":"2021-08-25T07:32:59.325546Z","shell.execute_reply.started":"2021-08-25T07:32:59.247211Z","shell.execute_reply":"2021-08-25T07:32:59.324694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'Tenure' and 'HasCrCard' have a small chi-square and a p-value greater than 0.05 (the standard cut-off value), confirming our initial hypothesis that these two features do not convey any useful information.\n\nWe can use the `drop()` method to remove these three features from the train set:","metadata":{}},{"cell_type":"code","source":"features_drop = ['Tenure', 'HasCrCard', 'EstimatedSalary']\ntrain_df = train_df.drop(features_drop, axis=1)\n\nprint('‚úîÔ∏è Features Dropped!')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:27.326493Z","start_time":"2021-08-25T06:37:27.31905Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:59.3269Z","iopub.execute_input":"2021-08-25T07:32:59.327172Z","iopub.status.idle":"2021-08-25T07:32:59.335317Z","shell.execute_reply.started":"2021-08-25T07:32:59.327145Z","shell.execute_reply":"2021-08-25T07:32:59.33411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding Categorical Features\n\nMachine learning algorithms usually require that all input (and output) features are numeric. Consequently, categorical features need to be converted (encoded) to numbers before building models.\n\nOur dataset contains two features that require encoding.\n\n- For 'Gender', we will use scikit-learn's `LabelEncoder()` which maps each unique label to an integer (Male --> 1 and Female --> 0).\n- For 'Geography', we will manually map values so that customers in Germany have the value of one (1) and all other customers (France and Spain) have zero (0). I chose this method since the churn rate for customers in the other two countries is almost equal and considerably lower than in Germany. Therefore, it makes sense to encode this feature so that it differentiates between German and non-German customers. Additionally, I tried one-hot encoding (`get_dummies()`) this feature, and the two new features for France and Spain had small feature importance.\n","metadata":{}},{"cell_type":"code","source":"train_df['Gender'] = LabelEncoder().fit_transform(train_df['Gender'])\n\ntrain_df['Geography'] = train_df['Geography'].map({\n    'Germany': 1,\n    'Spain': 0,\n    'France': 0\n})\n\nprint('‚úîÔ∏è Features Encoded!')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:27.342362Z","start_time":"2021-08-25T06:37:27.329465Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:59.339905Z","iopub.execute_input":"2021-08-25T07:32:59.340205Z","iopub.status.idle":"2021-08-25T07:32:59.350398Z","shell.execute_reply.started":"2021-08-25T07:32:59.34018Z","shell.execute_reply":"2021-08-25T07:32:59.349328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scaling\n\nFeature scaling is a technique used to normalise the range of features in a dataset. Some algorithms are sensitive to feature scaling (e.g. SVMs), while others are invariant (e.g. Random Forests). \n\nI decided to use `StandardScaler()`, which standardises features by subtracting the mean and dividing by the standard deviation. This transformation results in features with zero mean and unit variance.","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\n\nscl_columns = ['CreditScore', 'Age', 'Balance']\ntrain_df[scl_columns] = scaler.fit_transform(train_df[scl_columns])\n\nprint('‚úîÔ∏è Features Scaled!')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:27.373115Z","start_time":"2021-08-25T06:37:27.344842Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:59.351669Z","iopub.execute_input":"2021-08-25T07:32:59.351952Z","iopub.status.idle":"2021-08-25T07:32:59.37215Z","shell.execute_reply.started":"2021-08-25T07:32:59.351919Z","shell.execute_reply":"2021-08-25T07:32:59.371374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we will split the train set into 'X_train' and 'y_train' sets:","metadata":{}},{"cell_type":"code","source":"y_train = train_df['Exited']\nX_train = train_df.drop('Exited', 1)\n\nprint('‚úîÔ∏è Sets Created!')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:27.38353Z","start_time":"2021-08-25T06:37:27.375595Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:59.374909Z","iopub.execute_input":"2021-08-25T07:32:59.377001Z","iopub.status.idle":"2021-08-25T07:32:59.382973Z","shell.execute_reply.started":"2021-08-25T07:32:59.376939Z","shell.execute_reply":"2021-08-25T07:32:59.382359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Addressing Class Imbalance\n\nAs we have seen previously, there is an imbalance in the classes to be predicted, with one class (0 ‚Äì retained) much more prevalent than the other (1 - churned):","metadata":{}},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:27.397417Z","start_time":"2021-08-25T06:37:27.387498Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:59.383799Z","iopub.execute_input":"2021-08-25T07:32:59.384154Z","iopub.status.idle":"2021-08-25T07:32:59.394381Z","shell.execute_reply.started":"2021-08-25T07:32:59.384129Z","shell.execute_reply":"2021-08-25T07:32:59.393416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Class imbalance is usually a problem and occurs in many real-world tasks. Classification using imbalanced data is biased in favour of the majority class, meaning that machine learning algorithms will likely result in models that do little more than predict the most common class. Additionally, common metrics can be misleading when handling class-imbalanced data (e.g. if a dataset contains 99.9% 0s and 0.01% 1s, a classifier that always predicts 0 will have 99.9% accuracy).\n\nThankfully, some strategies can address this problem. I decided to use the SMOTE ('Synthetic Minority Oversampling Technique') algorithm, which, as we read in [[2](#Bibliography)], <br>\n'*finds a record that is similar to the record being upsampled and creates a synthetic record that is a randomly weighted average of the original record and the neighbouring record, where the weight is generated separately for each predictor*'.\n\nI‚Äôll use the `SMOTE` function from [imblearn](https://imbalanced-learn.readthedocs.io/en/stable/api.html) with the `sampling_strategy` set to 'auto'.","metadata":{}},{"cell_type":"code","source":"over = SMOTE(sampling_strategy='auto', random_state=random_state)\nX_train, y_train = over.fit_resample(X_train, y_train)\n\ny_train.value_counts()","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:27.449994Z","start_time":"2021-08-25T06:37:27.399897Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:59.39521Z","iopub.execute_input":"2021-08-25T07:32:59.395449Z","iopub.status.idle":"2021-08-25T07:32:59.434888Z","shell.execute_reply.started":"2021-08-25T07:32:59.395425Z","shell.execute_reply":"2021-08-25T07:32:59.434006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n# Building Machine Learning Models\n\n## Baseline Models\n\nWe start this section by first creating two simple models to estimate the **baseline performance** on the training set. Specifically, we will use Gaussian Na√Øve Bayes and Logistic Regression. We will use their default parameters and evaluate their (mean) recall by performing **k-fold cross-validation**. The idea behind k-fold cross-validation, which is illustrated in [this figure](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png), is simple: it splits the (training) set into k subsets/folds, trains the models using k-1 folds, and evaluates the model on the remaining one fold. This process is repeated until every fold is tested once.","metadata":{}},{"cell_type":"code","source":"clf_list = [('Gaussian Naive Bayes', GaussianNB()),\n            ('Logistic Regression', LogisticRegression(random_state=random_state))]\n\ncv_base_mean, cv_std = [], []\nfor clf in clf_list:\n\n    cv = cross_val_score(estimator=clf[1],\n                         X=X_train,\n                         y=y_train,\n                         scoring=scoring_metric,\n                         cv=5,\n                         n_jobs=-1)\n\n    cv_base_mean.append(cv.mean())\n    cv_std.append(cv.std())\n\nprint('Baseline Models (Recall):')\n\nfor i in range(len(clf_list)):\n    print('   {}: {}'.format(clf_list[i][0], np.round(cv_base_mean[i], 2)))","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:34.14896Z","start_time":"2021-08-25T06:37:27.453962Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:32:59.436169Z","iopub.execute_input":"2021-08-25T07:32:59.43645Z","iopub.status.idle":"2021-08-25T07:33:01.439177Z","shell.execute_reply.started":"2021-08-25T07:32:59.436422Z","shell.execute_reply":"2021-08-25T07:33:01.437758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note**: We could have used more (powerful) classifiers such as Random Forests or/and XGBoost. However, I preferred to exclude them at this stage as their default parameters make them more susceptible to overfitting the training set and hence provide inaccurate baseline performance. \n\n## Model Tuning\n\nWe are now ready to start building machine learning models. The six classifiers I have selected are the following:\n\n1) [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), <br>\n2) [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), <br>\n3) [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), <br> \n4) [Gradient Boosting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html), <br>\n5) [Xtreme Gradient Boosting Classifier](https://xgboost.readthedocs.io/en/latest/), and <br>\n6) [Light Gradient Boosting Machine](https://lightgbm.readthedocs.io/en/latest/).\n\nI won't go into detail about how these algorithms work. You can read more in [[1](#Bibliography)] or the corresponding documentation.\n\nUsing default hyperparameters usually results in non-optimised models that overfit or underfit the dataset. **Hyperparameter tuning** is the process of finding the set of hyperparameter values that achieves optimal performance. For this purpose, we will first define which hyperparameters we want to experiment with and what values to try out. We will pass this information to Scikit-Learn‚Äôs `GridSearchCV`, which then evaluates all the possible combinations of hyperparameter values. As mentioned [earlier](#Objective), **recall** will be used as the scoring metric for optimising our models. Note that `GridSearchCV` evaluates performance by performing k-fold cross-validation (therefore, a number of folds, `cv`, needs to be provided).\n\nApart from a confusion matrix, a plot of the **learning curves** will be provided for each classifier. Learning curves are plots of a model's performance on the training set and the validation set as a function of the training set size. They can help us visualise overfitting/underfitting and the effect of the training size on a model's error.","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(random_state=random_state)\n\nparam_grid = {\n    'max_iter': [100],\n    'penalty': ['l1', 'l2'],\n    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n    'solver': ['lbfgs', 'liblinear']\n}\n\nlr_clf = GridSearchCV(estimator=lr,\n                      param_grid=param_grid,\n                      scoring=scoring_metric,\n                      cv=5,\n                      verbose=False,\n                      n_jobs=-1)\n\nbest_lr_clf = lr_clf.fit(X_train, y_train)\nclf_performance(best_lr_clf, 'Logistic Regression', 'LR')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:37:37.681964Z","start_time":"2021-08-25T06:37:34.152433Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:33:01.440677Z","iopub.execute_input":"2021-08-25T07:33:01.441052Z","iopub.status.idle":"2021-08-25T07:33:03.456713Z","shell.execute_reply.started":"2021-08-25T07:33:01.441015Z","shell.execute_reply":"2021-08-25T07:33:03.45592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Support Vector Classifier","metadata":{}},{"cell_type":"code","source":"svc = SVC(probability=True, random_state=random_state)\n\nparam_grid = tuned_parameters = [{'kernel': ['rbf'],\n                                  'gamma': ['scale', 'auto'],\n                                  'C': [.1, 1, 2]},\n                                 {'kernel': ['linear'],\n                                  'C': [.1, 1, 10]}\n                                ]\n\nsvc_clf = GridSearchCV(estimator=svc,\n                       param_grid=param_grid,\n                       scoring=scoring_metric,\n                       cv=5,\n                       verbose=False,\n                       n_jobs=-1)\n\nbest_svc_clf = svc_clf.fit(X_train, y_train)\nclf_performance(best_svc_clf, 'Support Vector Classifier', 'SVC')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:44:06.433805Z","start_time":"2021-08-25T06:37:37.684444Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:33:03.457784Z","iopub.execute_input":"2021-08-25T07:33:03.458059Z","iopub.status.idle":"2021-08-25T07:41:58.769097Z","shell.execute_reply.started":"2021-08-25T07:33:03.458033Z","shell.execute_reply":"2021-08-25T07:41:58.768001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(random_state=random_state)\nparam_grid = {\n    'n_estimators': [100],\n    'criterion': ['entropy', 'gini'],\n    'bootstrap': [True, False],\n    'max_depth': [6],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [2, 3, 5],\n    'min_samples_split': [2, 3, 5]\n}\n\nrf_clf = GridSearchCV(estimator=rf,\n                      param_grid=param_grid,\n                      scoring=scoring_metric,\n                      cv=5,\n                      verbose=False,\n                      n_jobs=-1)\n\nbest_rf_clf = rf_clf.fit(X_train, y_train)\nclf_performance(best_rf_clf, 'Random Forest', 'RF')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:47:02.376124Z","start_time":"2021-08-25T06:44:06.435744Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:41:58.771022Z","iopub.execute_input":"2021-08-25T07:41:58.771447Z","iopub.status.idle":"2021-08-25T07:44:03.010456Z","shell.execute_reply.started":"2021-08-25T07:41:58.771402Z","shell.execute_reply":"2021-08-25T07:44:03.009354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Boosting Classifier","metadata":{}},{"cell_type":"code","source":"gbc = GradientBoostingClassifier(random_state=random_state)\nparam_grid = {\n    'n_estimators': [600],\n    'subsample': [0.66, 0.75],\n    'learning_rate': [0.001, 0.01],\n    'max_depth': [3],  # default=3\n    'min_samples_split': [5, 7],\n    'min_samples_leaf': [3, 5],\n    'max_features': ['auto', 'log2', None],\n    'n_iter_no_change': [20],\n    'validation_fraction': [0.2],\n    'tol': [0.01]\n}\n\ngbc_clf = GridSearchCV(estimator=gbc,\n                       param_grid=param_grid,\n                       scoring=scoring_metric,\n                       cv=5,\n                       verbose=False,\n                       n_jobs=-1)\n\nbest_gbc_clf = gbc_clf.fit(X_train, y_train)\nclf_performance(best_gbc_clf, 'Gradient Boosting Classifier', 'GBC')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:49:59.950362Z","start_time":"2021-08-25T06:47:02.379597Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:44:03.011941Z","iopub.execute_input":"2021-08-25T07:44:03.012336Z","iopub.status.idle":"2021-08-25T07:46:19.749435Z","shell.execute_reply.started":"2021-08-25T07:44:03.012295Z","shell.execute_reply":"2021-08-25T07:46:19.748779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of estimators after early stopping is: ","metadata":{}},{"cell_type":"code","source":"best_gbc_clf.best_estimator_.n_estimators_","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:49:59.960246Z","start_time":"2021-08-25T06:49:59.953303Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:46:19.75038Z","iopub.execute_input":"2021-08-25T07:46:19.750729Z","iopub.status.idle":"2021-08-25T07:46:19.755861Z","shell.execute_reply.started":"2021-08-25T07:46:19.750702Z","shell.execute_reply":"2021-08-25T07:46:19.755136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(random_state=random_state)\n\nparam_grid = {\n    'n_estimators': [50],\n    'learning_rate': [0.001, 0.01],\n    'max_depth': [3, 4],  # default=6\n    'reg_alpha': [1, 2],\n    'reg_lambda': [1, 2],\n    'subsample': [0.5, 0.75],\n    'colsample_bytree': [0.50, 0.75],\n    'gamma': [0.1, 0.5, 1],\n    'min_child_weight': [1]\n}\n\nxgb_clf = GridSearchCV(estimator=xgb,\n                       param_grid=param_grid,\n                       scoring=scoring_metric,\n                       cv=5,\n                       verbose=False,\n                       n_jobs=-1)\n\nbest_xgb_clf = xgb_clf.fit(X_train, y_train)\nclf_performance(best_xgb_clf, 'XGBoost Classifier', 'XGB')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:52:23.838736Z","start_time":"2021-08-25T06:49:59.96372Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:46:19.757015Z","iopub.execute_input":"2021-08-25T07:46:19.757278Z","iopub.status.idle":"2021-08-25T07:47:57.179057Z","shell.execute_reply.started":"2021-08-25T07:46:19.757252Z","shell.execute_reply":"2021-08-25T07:47:57.177907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LGBMClassifier","metadata":{}},{"cell_type":"code","source":"lgbmc = LGBMClassifier(random_state=random_state)\n\nparam_grid = {\n    'max_depth': [5],\n    'num_leaves': [5, 10],\n    'learning_rate': [0.001, 0.01],\n    'n_estimators': [200],\n    'feature_fraction': [0.5],\n    'min_child_samples': [5, 10],\n    'reg_alpha': [0.1, 0.5],\n    'reg_lambda': [0.1, 0.5]\n}\n\nlgbmc_clf = GridSearchCV(estimator=lgbmc,\n                         param_grid=param_grid,\n                         scoring=scoring_metric,\n                         cv=5,\n                         verbose=False,\n                         n_jobs=-1)\n\nbest_lgbmc_clf = lgbmc_clf.fit(X_train, y_train)\nclf_performance(best_lgbmc_clf, 'LGBMClassifier', 'LGBMC')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:52:51.422249Z","start_time":"2021-08-25T06:52:23.841218Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:47:57.180892Z","iopub.execute_input":"2021-08-25T07:47:57.181274Z","iopub.status.idle":"2021-08-25T07:48:14.886439Z","shell.execute_reply.started":"2021-08-25T07:47:57.18123Z","shell.execute_reply":"2021-08-25T07:48:14.885449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensemble Learning\n\nWe can combine the predictions of all these classifiers to determine if we get better predictive performance compared to each individual constituent classifier. This practice is the main motivation behind Ensemble Learning.\n\nSpecifically, I will use **Soft Voting**. In this case, every individual classifier provides a probability value that a specific data point belongs to a particular target class. The predictions are weighted by the classifier's importance and summed up. Then the target label with the greatest sum of weighted probabilities wins the vote.\n","metadata":{}},{"cell_type":"code","source":"estimators = [('LR', best_lr_clf.best_estimator_),\n              ('SCV', best_svc_clf.best_estimator_),\n              ('RF', best_rf_clf.best_estimator_),\n              ('GBC', best_gbc_clf.best_estimator_),\n              ('XGB', best_xgb_clf.best_estimator_),\n              ('LGBMC', best_lgbmc_clf.best_estimator_)]\n\ntuned_voting_soft = VotingClassifier(estimators=estimators[1:],\n                                     voting='soft',\n                                     n_jobs=-1)\nestimators.append(('SoftV', tuned_voting_soft))\n\ny_pred_pp = cross_val_predict(tuned_voting_soft,\n                              X_train,\n                              y_train,\n                              cv=5,\n                              method='predict_proba')[:, 1]\ny_pred = y_pred_pp.round()\n\ncm = confusion_matrix(y_train, y_pred, normalize='true')\nfpr, tpr, _ = roc_curve(y_train, y_pred_pp)\ncomparison_dict['SVot'] = [\n    accuracy_score(y_train, y_pred),\n    precision_score(y_train, y_pred),\n    recall_score(y_train, y_pred),\n    roc_auc_score(y_train, y_pred_pp), fpr, tpr\n]\n\nprint('Soft Voting\\n-----------------')\nprint('  Recall: ', np.round(recall_score(y_train, y_pred), 3))\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nplot_conf_mx(cm, ax1)\nplot_learning_curve(tuned_voting_soft, X_train, y_train, ax2)","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:58:20.884684Z","start_time":"2021-08-25T06:52:51.425723Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:48:14.888106Z","iopub.execute_input":"2021-08-25T07:48:14.888617Z","iopub.status.idle":"2021-08-25T07:51:11.940417Z","shell.execute_reply.started":"2021-08-25T07:48:14.888576Z","shell.execute_reply":"2021-08-25T07:51:11.939726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n# Results\n\n## Learning Curves\n\nFor all models, there is a tiny gap between the two curves at the end of training. This observation indicates that we do **not** overfit the training set. \n\n## Feature Importance\n\nSome classifiers allow us to visualise feature importance:","metadata":{}},{"cell_type":"code","source":"colors_fi = ['steelblue', 'darkgray', 'cadetblue', 'bisque']\n\nfig = plt.subplots(2, 2, figsize=(12, 10))\n\nfor i, (name, clf) in enumerate(zip(['RF', 'GB', 'XGB', 'LGBM'],\n                                    [best_rf_clf, best_gbc_clf, best_xgb_clf, best_lgbmc_clf])):\n\n    ax = plt.subplot(2, 2, i + 1)\n    plot_feature_imp(clf, name, colors_fi[i], ax)\n    plt.ylabel('')\n\nplt.tight_layout();","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:58:22.992187Z","start_time":"2021-08-25T06:58:20.889149Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:51:11.941675Z","iopub.execute_input":"2021-08-25T07:51:11.942112Z","iopub.status.idle":"2021-08-25T07:51:13.099571Z","shell.execute_reply.started":"2021-08-25T07:51:11.942078Z","shell.execute_reply":"2021-08-25T07:51:13.098745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'Age' and 'NumOfProducts' seem like the most useful features for all classifiers, followed by 'IsActiveMember' and 'Balance'. On the other hand, 'CreditScore' is the least important feature with a small value close to zero for all estimators apart from LGBM. ","metadata":{}},{"cell_type":"markdown","source":"## Performance Comparison\n\nInitially, we can compare the performance of our classifiers in terms of four individual metrics (Accuracy, precision, recall, and area under the ROC curve or simply AUC):","metadata":{}},{"cell_type":"code","source":"comparison_matrix = {}\nfor key, value in comparison_dict.items():\n    comparison_matrix[str(key)] = value[0:4]\n\ncomparison_df = pd.DataFrame(comparison_matrix,\n                             index=['Accuracy', 'Precision', 'Recall', 'AUC']).T\ncomparison_df.style.highlight_max(color='indianred', axis=0)","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:58:23.140896Z","start_time":"2021-08-25T06:58:22.995162Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:51:13.100732Z","iopub.execute_input":"2021-08-25T07:51:13.101003Z","iopub.status.idle":"2021-08-25T07:51:13.215991Z","shell.execute_reply.started":"2021-08-25T07:51:13.100978Z","shell.execute_reply":"2021-08-25T07:51:13.215079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comparison_df.plot(kind='bar',\n                   figsize=(10, 5),\n                   fontsize=12,\n                   color=['#5081DE', '#A7AABD', '#D85870', '#424656'])\n\nplt.legend(loc='upper center',\n           fontsize=font_size - 6,\n           ncol=len(comparison_df.columns),\n           bbox_to_anchor=(0.5, 1.12))\nplt.xticks(rotation=0)\nplt.yticks([0, 0.4, 0.8])\n\nplt.axhline(y=0.70, color='red', linestyle='--')\nplt.text(x=-0.5, y=0.73, s='0.70', size=font_size + 2, color='red');","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:58:23.537735Z","start_time":"2021-08-25T06:58:23.144864Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:51:13.217342Z","iopub.execute_input":"2021-08-25T07:51:13.217627Z","iopub.status.idle":"2021-08-25T07:51:13.460058Z","shell.execute_reply.started":"2021-08-25T07:51:13.217596Z","shell.execute_reply":"2021-08-25T07:51:13.459085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All other classifiers have a recall higher than 70% (baseline performance). XGB is the model with the highest recall (78.5 %). However, the LGBM classifier has the best overall performance with the highest accuracy, precision, and AUC.\n\nUsing single metrics is not the only way of comparing the predictive performance of classification models. The ROC curve (Receiver Operating Characteristic curve) is a graph showing the performance of a classifier at different classification thresholds. It plots the true positive rate (another name for recall) against the false positive rate.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 5))\n\nfor index, key in enumerate(comparison_dict.keys()):\n    auc, fpr, tpr = comparison_dict[key][3], comparison_dict[key][4], comparison_dict[key][5]\n    ax.plot(fpr,\n            tpr,\n            color=colors_comp[index],\n            label='{}: {}'.format(key, np.round(auc, 3)))\n\nax.plot([0, 1], [0, 1], 'k--', label='Baseline')\n\nax.set_title('ROC Curve')\nax.set_xlabel('False Positive Rate')\nax.set_xticks([0, 0.25, 0.5, 0.75, 1])\nax.set_ylabel('False Positive Rate')\nax.set_yticks([0, 0.25, 0.5, 0.75, 1])\nax.autoscale(axis='both', tight=True)\nax.legend(fontsize=14);","metadata":{"ExecuteTime":{"end_time":"2021-08-25T06:58:24.129458Z","start_time":"2021-08-25T06:58:23.540673Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:51:13.461143Z","iopub.execute_input":"2021-08-25T07:51:13.46138Z","iopub.status.idle":"2021-08-25T07:51:13.752339Z","shell.execute_reply.started":"2021-08-25T07:51:13.461356Z","shell.execute_reply":"2021-08-25T07:51:13.751412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dashed diagonal line represents a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).\n\nIn our case, all classifiers, apart from Logistic Regression, perform similarly. It seems that LGBM performs marginally better, as evidenced by the slightly higher AUC (0.888).","metadata":{}},{"cell_type":"markdown","source":"Recently, I came across another tool for assessing the performance of a classifier model. Simply put, a Cumulative Gain shows the percentage of targets reached when considering a certain percentage of the population with the highest probability to be target according to the model (see [here](https://towardsdatascience.com/meaningful-metrics-cumulative-gains-and-lyft-charts-7aac02fc5c14) and [here](http://mlwiki.org/index.php/Cumulative_Gain_Chart)). The `scikitplot` library offers an easy way of plotting this chart:","metadata":{}},{"cell_type":"code","source":"print('Soft Voting:')\n\ny_pred = cross_val_predict(tuned_voting_soft,\n                           X_train,\n                           y_train,\n                           cv=5,\n                           method='predict_proba')\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nskplt.metrics.plot_cumulative_gain(y_train, y_pred, ax=ax)\n\nax.plot([0.5, 0.5], [0, 0.8], color='firebrick')\nax.plot([0.0, 0.5], [0.8, 0.8], color='firebrick')\n\nax.set_title('Cumulative Gains Curve', size=font_size)\nax.set_xlabel('Percentage of Sample', size=font_size)\nax.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\nax.set_xticklabels([0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=font_size - 2)\n\nax.set_ylabel('Gain', size=font_size)\nax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\nax.set_yticklabels([0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=font_size - 2)\n\nax.text(0.15, 0.81, '80%', size=font_size, color='firebrick')\nax.legend(fontsize=14);","metadata":{"ExecuteTime":{"end_time":"2021-08-25T07:00:27.892199Z","start_time":"2021-08-25T06:58:24.132398Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:51:13.75364Z","iopub.execute_input":"2021-08-25T07:51:13.7542Z","iopub.status.idle":"2021-08-25T07:52:42.552674Z","shell.execute_reply.started":"2021-08-25T07:51:13.75416Z","shell.execute_reply":"2021-08-25T07:52:42.551551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This chart shows that if we target 50% of the customers most likely to churn (according to the model), the model will pick 80% of customers who will actually churn, while the random pick would pick only 50% of the targets.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n# Evaluating the Test Set\n\nNow is the time to evaluate the models on unseen data. First, we need to perform the same preprocessing steps as the training set.","metadata":{}},{"cell_type":"code","source":"test_df = test_df.drop(features_drop, axis=1)\n\ntest_df['Gender'] = LabelEncoder().fit_transform(test_df['Gender'])\ntest_df['Geography'] = test_df['Geography'].map({\n    'Germany': 1,\n    'Spain': 0,\n    'France': 0\n})\n\ntest_df[scl_columns] = scaler.transform(test_df[scl_columns])  # not fit_transform, scaler has already been trained\n\ny_test = test_df['Exited']\nX_test = test_df.drop('Exited', 1)\n\nprint('‚úîÔ∏è Preprocessing Complete!')","metadata":{"ExecuteTime":{"end_time":"2021-08-25T07:00:27.924935Z","start_time":"2021-08-25T07:00:27.894642Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:52:42.554321Z","iopub.execute_input":"2021-08-25T07:52:42.554855Z","iopub.status.idle":"2021-08-25T07:52:42.575567Z","shell.execute_reply.started":"2021-08-25T07:52:42.554796Z","shell.execute_reply":"2021-08-25T07:52:42.57462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\nWe will use the same method for comparing our classifiers as we did in the training set.","metadata":{}},{"cell_type":"code","source":"tuned_voting_soft.fit(X_train, y_train)\n\nfig, ax = plt.subplots(7, 1, figsize=(5, 30))\n\nfor i, (name, clf) in enumerate(zip(['LR', 'SVC', 'RF', 'GB', 'XGB', 'LGBM', 'SVot'], \n                                    [best_lr_clf.best_estimator_, best_svc_clf.best_estimator_, best_rf_clf.best_estimator_, best_gbc_clf.best_estimator_, best_xgb_clf.best_estimator_, best_lgbmc_clf.best_estimator_, tuned_voting_soft])):\n    test_func(clf, name, ax=ax[i])\n\nplt.tight_layout();","metadata":{"ExecuteTime":{"end_time":"2021-08-25T07:01:09.801233Z","start_time":"2021-08-25T07:00:27.927879Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:52:42.576915Z","iopub.execute_input":"2021-08-25T07:52:42.577218Z","iopub.status.idle":"2021-08-25T07:53:14.723536Z","shell.execute_reply.started":"2021-08-25T07:52:42.577191Z","shell.execute_reply":"2021-08-25T07:53:14.722463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comparison_test_df = pd.DataFrame(comparison_test_dict,\n                                  index=['Accuracy', 'Precision', 'Recall']).T\ncomparison_test_df.style.highlight_max(color='indianred', axis=0)","metadata":{"ExecuteTime":{"end_time":"2021-08-25T07:01:09.839921Z","start_time":"2021-08-25T07:01:09.804705Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:53:14.725421Z","iopub.execute_input":"2021-08-25T07:53:14.725913Z","iopub.status.idle":"2021-08-25T07:53:14.746426Z","shell.execute_reply.started":"2021-08-25T07:53:14.725859Z","shell.execute_reply":"2021-08-25T07:53:14.745408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comparison_test_df.plot(kind='bar',\n                        figsize=(10, 5),\n                        fontsize=12,\n                        color=['#5081DE', '#A7AABD', '#D85870'])\n\nplt.legend(loc='upper center',\n           ncol=len(comparison_test_df.columns),\n           bbox_to_anchor=(0.5, 1.11))\nplt.xticks(rotation=0)\nplt.yticks([0, 0.4, 0.8])\n\nplt.axhline(y=0.70, color='red', linestyle='--')\nplt.text(x=-0.5, y=0.72, s='0.70', size=font_size + 2, color='red');","metadata":{"ExecuteTime":{"end_time":"2021-08-25T07:01:10.398911Z","start_time":"2021-08-25T07:01:09.843889Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-25T07:53:14.74785Z","iopub.execute_input":"2021-08-25T07:53:14.748322Z","iopub.status.idle":"2021-08-25T07:53:14.968193Z","shell.execute_reply.started":"2021-08-25T07:53:14.74828Z","shell.execute_reply":"2021-08-25T07:53:14.967196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The performance on the test set for all models is fairly similar to the training set, which proves that we do **not** overfit the training set. Therefore, we can predict customer churn with a recall approximately equal to **78%**.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n# Bibliography\n\nThe main resources I used are the following two books:\n\n[1] **[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), by Aur√©lien G√©ron (2019)**\n\n[2] **[Practical Statistics for Data Scientists, 2nd Edition](https://www.oreilly.com/library/view/practical-statistics-for/9781492072935/), by Peter Bruce, Andrew Bruce, and Peter Gedeck (2020)**\n\n<br>\n\nThe following resources (Retrieved on mid-December 2020) also helped me in my analysis:\n\n[3] [Bank Customer Churn](https://rstudio-pubs-static.s3.amazonaws.com/565148_6e82a5c320f14869bf63e23bcf59ce9b.html#compare-models-performance), by Zicheng Shi (same dataset but analysis in R)\n\n[4] [Metrics and scoring: quantifying the quality of predictions](https://scikit-learn.org/stable/modules/model_evaluation.html) on [scikit-learn.org](https://scikit-learn.org/stable/modules/model_evaluation.html)\n\n[5] [Easy Guide To Data Preprocessing In Python](https://www.kdnuggets.com/2020/07/easy-guide-data-preprocessing-python.html), by Ahmad Anis\n\n[6] [Meaningful Metrics: Cumulative Gains and Lyft Charts](https://towardsdatascience.com/meaningful-metrics-cumulative-gains-and-lyft-charts-7aac02fc5c14), by Raffi Sahakyan\n\n<br>\n\nIf you would like to read more about customer churn:\n\n[7] [Churn Rate](https://www.investopedia.com/terms/c/churnrate.asp), by Jake Frankenfield on [Investopedia](https://www.investopedia.com/)\n\n[8] [Customer attrition](https://en.wikipedia.org/wiki/Customer_attrition) on [Wikipedia](https://en.wikipedia.org/wiki/Main_Page)\n\n[9] [A Survey on Churn Prediction Techniques in Communication Sector](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.278.4171&rep=rep1&type=pdf), by N.Kamalraj and A.Malathi\n\n[10] [Customer Attrition](https://www.optimove.com/resources/learning-center/customer-attrition) on [optimove](https://www.optimove.com/resources/learning-center/customer-attrition)","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n# Future Development\n\n-  I tried combining existing features to produce more useful ones (Feature Engineering). However, this didn‚Äôt increase the predictive performance of my models. Feature engineering can prove quite important when done right, so it is worth exploring in a future version of this notebook.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n# Conclusions\n\nOur notebook just came to an end! Our final report to the bank should be based on two main points:\n\n- **EDA** can help us identify which features contribute to customer churn. Additionally, **feature importance** analysis can quantify the importance of each feature in predicting the likelihood of churn. Our results reveal that the most significant feature is **age** (older customers are more likely to churn), followed by the **number of products** (having more products increases a customer‚Äôs likelihood to churn). The bank could use our findings to adapt and improve its services in a way that increases satisfaction for those customers more likely to churn.\n\n- We can build several **machine learning models** with **recall** approximately equal to **78%**, meaning that they can successfully detect almost 80% of those customers more luckily to churn. Perhaps, adding more features or/and records could help us improve predictive performance. Therefore, the bank could benefit from investing in gathering more data.\n\n<br>\n\nPlease feel free to make any suggestions for improving my analysis. Also,  please consider <font size=+0 color=\"red\"><b>upvoting</b></font> if you found this notebook useful. Thank you! üòâ","metadata":{}}]}