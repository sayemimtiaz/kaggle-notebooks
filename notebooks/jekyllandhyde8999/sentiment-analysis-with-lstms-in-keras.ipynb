{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import layers, models, backend as K, optimizers\n\nfrom nltk.corpus import stopwords\n\neng_stopwords = stopwords.words('english')\neng_stopwords += [word.title() for word in eng_stopwords]\nno_punct_table = str.maketrans(dict.fromkeys(string.punctuation)) # translation table to remove punctuation\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(x):\n    x = x.replace(\"<br />\", '')\n    x = x.translate(no_punct_table)\n    x = ' '.join(list(filter(lambda y: y not in eng_stopwords, x.split(' '))))\n    x = re.sub(r'[A-Z][a-z]+', \"<name>\", x)\n    return x\n\n\ndef make_vocab(column):\n    vocab_ = set()\n    max_len_ = 0\n    for sent in column:\n        vocab_.update(sent.split(' '))\n        max_len_ = max(max_len_, len(sent.split(' ')))\n    \n    vocab_.update(['<PAD>', '<UNK>']) # <PAD> and <UNK> tokens for padding and unknown words found in sentences\n    \n    return sorted(vocab_), max_len_\n\n\ndef pad_index_closure(mapping, ref_len):\n    def inner(x):\n        new_x = [mapping['<PAD>']] * (ref_len - len(x.split(' ')))\n        \n        for word in x.split(' '):\n            try:\n                new_word = mapping[word]\n            except:\n                new_word = mapping['<UNK>']\n            \n            new_x.append(new_word)\n        \n        return new_x\n    \n    return inner\n\n\ndef fix_array(arr):\n    x_ = []\n    for row in arr:\n        x_.append(row)\n\n    return np.array(x_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the input dataset","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv', header=0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Relabelling sentiment column\nReplacing positive with 1 and negative with 0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sentiment = df.sentiment.apply(lambda x: 1 if x == 'positive' else 0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing\n\n* Remove `<br />` tokens that are present in the text.\n* Remove punctuation from the text.\n* Remove stopwords.\n* Substitute names of people with a special `<name>` token.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Before preprocessing:\\n\")\nprint(df.review[1])\nprint()\nprint(\"=\" * 140)\ndf[\"review_preprocessed\"] = df.review.apply(preprocess)\n\nprint(\"\\nAfter preprocessing:\\n\")\nprint(df.review[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building the vocabulary of the dataset.\nIn the below cell, `vocab` is just a set.<br>\n\nSo, I will make a dictionary that will map each unique token in vocab to an integer.<br>\n`vocab_itos` maps given integer to a word<br>\n`vocab_stoi` maps given word to an integer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab, max_len = make_vocab(df.review_preprocessed)\n\nvocab_itos = dict(enumerate(vocab))\nvocab_stoi = {val: key for key, val in vocab_itos.items()}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting the sequence of words to a sequence of integers\n* Padding each sentence to make all the training examples of same length.\n* Substituting each word for its corresponding integer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"review_indexed\"] = df.review_preprocessed.apply(pad_index_closure(vocab_stoi, max_len))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get input and output arrays for the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.review_indexed.values\ny = df.sentiment.values\n\n# turn x into numpy array\nx = fix_array(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build and train the model\n* 1D Convolution\n* LSTM layer\n* Dense network\n* Binary Cross Entropy loss with accuracy metric\n* Adam optimizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(input_length, embed_input_dim, embed_dim):\n    K.clear_session()\n    X_input =  layers.Input(shape=(input_length,))\n    \n    # An embedding layer that returns `embed_dim` sized vectors for each word\n    X = layers.Embedding(input_dim=embed_input_dim, output_dim=embed_dim, input_length=input_length)(X_input)\n    \n    # 1D Conv layer to compute local features for each word\n    X = layers.Conv1D(filters=10, kernel_size=3)(X)\n    \n    # LSTM layers to build a context vector of the input sentence\n    X = layers.LSTM(X.shape[-1] // 2, return_sequences=True)(X)\n    X = layers.LSTM(X.shape[-1] // 2)(X)\n    \n    # output Dense layer used for classification\n    X = layers.Dense(1, activation='sigmoid')(X)\n    \n    # make the Model instance and return it\n    return models.Model(inputs=X_input, outputs=X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make an instance of the model\nmodel = get_model(x.shape[-1], len(vocab), 25)\n\n# compile the model with Adam optimizer and Binary Cross Entropy loss with Accuracy metric\nmodel.compile(optimizer=optimizers.Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(x, y, epochs=12, batch_size=8, validation_split=0.2).history\n\n# Save the model in an h5 file\nmodel.save(\"imdb_analyse_sentiment.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting the results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows = 1\nncols = 2\nfig_size = plt.rcParams['figure.figsize']\n\nfig, ax = plt.subplots(nrows, ncols, figsize=(fig_size[0] * ncols, fig_size[1] * nrows))\n\n_ = ax[0].plot(range(len(history['loss'])), history['loss'], label='Training')\nif 'val_loss' in history.keys():\n    _ = ax[0].plot(range(len(history['val_loss'])), history['val_loss'], label='Validation')\n    _ = ax[0].legend()\n_ = ax[0].set_title('Loss Curve')\n\n_ = ax[1].plot(range(len(history['accuracy'])), history['accuracy'], label='Training')\nif 'val_accuracy' in history.keys():\n    _ = ax[1].plot(range(len(history['val_accuracy'])), history['val_accuracy'], label='Validation')\n    _ = ax[1].legend()\n_ = ax[1].set_title('Accuracy Curve')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing the model with randomly sampled examples from the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nNUM_EXAMPLES = int(0.1 * df.shape[0])\nsampled_indices = random.sample(range(df.shape[0]), NUM_EXAMPLES)\n\n# I have already preprocessed the text and converted them into sequences of integers and put them in a column called `review_indexed`.\n# So, I'll take the examples from there.\n\n# sampling the test examples and the ground truths\nsampled_examples = fix_array(df.loc[sampled_indices, \"review_indexed\"].values)\ny_true = df.loc[sampled_indices, \"sentiment\"].values\n\n# Feeding the test examples to the models\ny_pred = np.squeeze(model.predict(sampled_examples))\n\n\nshowing_indices = random.sample(range(NUM_EXAMPLES), 3)\nfor index in showing_indices:\n    print(f\"Review:\\n{df.loc[index, 'review']}\\n\")\n    print(f\"Ground Truth: {'positive' if y_true[index] else 'negative'}\")\n    print(f\"Predicted {'positive' if y_pred[index] >= 0.5 else 'negative'} with {(y_pred[index] if y_pred[index] >= 0.5 else 1 - y_pred[index]) * 100:.2f}% confidence\\n\")\n    print(\"=\" * 140)\n    print()\n\ny_pred = (y_pred >= 0.5).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluating the performance of the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom sklearn import metrics\n\naccuracy = sum(y_true == y_pred) / NUM_EXAMPLES\nconf_matrix = metrics.confusion_matrix(y_true, y_pred)\nprecision = metrics.precision_score(y_true, y_pred)\nrecall = metrics.recall_score(y_true, y_pred)\nf1_score = metrics.f1_score(y_true, y_pred)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1_score:.2f}\")\n\n_ = plt.figure(figsize=(fig_size[0] * 1.5, fig_size[1] * 1.5))\nax = sns.heatmap(conf_matrix, annot=True, fmt='d')\n_ = ax.set_xticklabels([\"negative\", \"positive\"], fontsize=13)\n_ = ax.set_yticklabels([\"negative\", \"positive\"], fontsize=13)\n_ = ax.set_xlabel(\"Predicted Labels\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Labels\", fontsize=15)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}