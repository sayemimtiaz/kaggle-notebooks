{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Context\nChocolate is one of the most popular candies in the world. Each year, residents of the United States collectively eat more than 2.8 billions pounds. However, not all chocolate bars are created equal! This dataset contains expert ratings of over 1,700 individual chocolate bars, along with information on their regional origin, percentage of cocoa, the variety of chocolate bean used and where the beans were grown.\n\n## Task\nThe purpose of this notebook is to create a model that will predict the quality of a bar. The target feature will be Rating. This will make the assumption that higher chocolate quality will translate to more sales; we will leave cost-benefit analysis to the manufacturing pipeline.\n\n## Features\n* Company: Name of company manufacturing the bar\n* Specific Bean Origin: The specific geographical region of origin for the bar\n* REF: A value linked to when the review was entered in the database. Higher = more recent.\n* ReviewDate: Date of publication for review\n* CocoaPercent: Cocoa percentage (darkness) of the chocolate bar being reviewed.\n* CompanyLocation: Manufacturer base country\n* Rating: Expert rating for the bar.\n* BeanType: The variety/breed of the bean(s) used, if provided\n* Broad BeanOrigin: The broad geographical region of origin for the bean\n\n## Flavors of Cacao Rating System\n(Quoted from dataset description)\n\n* 5= Elite (Transcending beyond the ordinary limits)\n* 4= Premium (Superior flavor development, character and style)\n* 3= Satisfactory(3.0) to praiseworthy(3.75) (well made with special qualities)\n* 2= Disappointing (Passable but contains at least one significant flaw)\n* 1= Unpleasant (mostly unpalatable)\n\nEach chocolate is evaluated from a combination of both objective qualities and subjective interpretation. A rating here only represents an experience with one bar from one batch. Batch numbers, vintages and review dates are included in the database when known.\n\nThe database is narrowly focused on plain dark chocolate with an aim of appreciating the flavors of the cacao when made into chocolate. The ratings do not reflect health benefits, social missions, or organic status.\n\nFlavor is the most important component of the Flavors of Cacao ratings. Diversity, balance, intensity and purity of flavors are all considered. It is possible for a straight forward single note chocolate to rate as high as a complex flavor profile that changes throughout. Genetics, terroir, post harvest techniques, processing and storage can all be discussed when considering the flavor component.\n\nTexture has a great impact on the overall experience and it is also possible for texture related issues to impact flavor. It is a good way to evaluate the makers vision, attention to detail and level of proficiency.\n\nAftermelt is the experience after the chocolate has melted. Higher quality chocolate will linger and be long lasting and enjoyable. Since the aftermelt is the last impression you get from the chocolate, it receives equal importance in the overall rating.\n\nOverall Opinion is really where the ratings reflect a subjective opinion. Ideally it is my evaluation of whether or not the components above worked together and an opinion on the flavor development, character and style. It is also here where each chocolate can usually be summarized by the most prominent impressions that you would remember about each chocolate.\n","metadata":{}},{"cell_type":"markdown","source":"### Analysis Plan for each feature\n* Company: This is categorical, and will need to be one hot encoded. Stored as a string.\n\n* Bean Origin/Bar name: This is categorical, and will need to be one hot encoded. This data may be suspect, as it seems to be conflating two different data. Stored as a string.\n\n* Review Year: This can be left as an integer. This might be interesting to reveal any review bias over time. Assume that any new chocolate we're gauging interest in came out in the present.\n\n* Cocoa Percent: This is currently stored as a string. This will need to be converted to a float.\n\n* Company Location: This is categorical, and will need to be one hot encoded. Stored as a string.\n\n* Rating: The feature we are trying to predict. It looks like this data is not discretized to whole numbers (i.e., some of the results show 3.75 stars), meaning we will have to feature engineer this into buckets, or make this a regression model.\n\n* Bean Type: This is categorical, and will need to be one hot encoded. We have some missing data. Stored as a string.\n\n* Broad Bean Origin: This is categorical, and will need to be one hot encoded. We have some missing data. Stored as a string.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#imports\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\npd.set_option('display.max_rows',100)\n\n%matplotlib inline\n\n# Load the data\ndata = pd.read_csv('/kaggle/input/chocolate-bar-ratings/flavors_of_cacao.csv')\n# Rename the columns\ndata.rename(columns = {'Company\\xa0\\n(Maker-if known)':'company', \n                       'Specific Bean Origin\\nor Bar Name':'bar_origin', \n                       'Review\\nDate':'review_year',\n                       'Cocoa\\nPercent':'cocoa_percent',\n                       'Company\\nLocation':'company_location',\n                       'Bean\\nType':'bean_type',\n                       'Broad Bean\\nOrigin':'bean_origin'},\n            inplace = True) \ndata.index.name='id'\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Functions I've written for various data cleaning tasks\n\ndef toPercent(string):\n    # This function will convert a percentage string into a decimal variable\n    # The purpose is to convert cocoa_percent into a numerical value\n    return(float(string.strip('%'))/100)\n\ndef spaceToNan(datastring):\n    # function to turn all cells with strings of one space character into a numpy null \n    # if the input is not a string, it will return the original input\n    # if the input is not a single space, it will return the input\n    # This is meant to clean the bean_type feature\n    if type(datastring)==str:\n        if datastring.strip() == '':\n            return np.nan\n        else:\n            return datastring.strip()\n    else:\n        return datastring\n    \ndef nanToUnknown(datastring):\n    #function to turn all cells with the pandas null value into a string reading 'unknown'\n    # as stated below, the purpose is to relabel null values of bean_type as unknown\n    # this will allow us to use that feature to see if it is significant\n    # This has to be run after spaceToNan above\n    if pd.isna(datastring):\n        return 'unknown'\n\n    else:\n        return datastring","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After importing and renaming all the libraries above, we map a converter function to turn all of the cocoa percentages to a float. By definition, they will also be normalized.","metadata":{}},{"cell_type":"code","source":"#convert the cocoa percentage to a decimal float (try block to pass it if it's already been converted)\ntry:\n    data['cocoa_percent']=data['cocoa_percent'].map(toPercent)\nexcept(AttributeError):\n    pass\n\n# Convert any cells that are blank spaces to NaN values (mostly for bean_type)\ndata=data.applymap(spaceToNan)\n\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have made sure that our string data at least lists missing points correctly, let's see the quality of our data. ","metadata":{}},{"cell_type":"code","source":"print('Missing data')\nprint(data.isnull().sum())\nprint('\\nUnique values')\nprint(data.nunique(dropna=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This matches the description given by our source (https://www.kaggle.com/rtatman/chocolate-bar-ratings); so far we haven't caught any mistakes they missed. 74 bars are missing the bean origin, about 4%. We will probably be able to drop those without too much issue. 888 data points are missing the bean type, however, which is too much to ignore. We could drop that column, but it seems a shame to lose that feature. If we assume that Bean Type and Bean origin represents what the critics knew (and not data displacement on the part of the aggregator), we can replace these missing data with \"unknown\", and then keep the data. This will let us see if knowledge of those features are affecting the review.","metadata":{}},{"cell_type":"code","source":"data_unknowns=data.applymap(nanToUnknown)\ndata_unknowns.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking the Cocoa Percent Distribution\nDespite having over a thousand data points, I don't think chocolate manufacturers are coming up with a new cocoa ratio every bar. I ran a quick list of the unique values in that column to make sure that there wouldn't be any weird gaps in the distribution if I ran a finer histogram. Satisfied, I plotted the data with 10 buckets, and confirmed that it looks relatively normal. Furthermore, it looks like this column is complete (no missing data).","metadata":{}},{"cell_type":"code","source":"print(np.sort(data_unknowns['cocoa_percent'].unique()))\nn_bins=10\ndata_unknowns['cocoa_percent'].hist(bins=n_bins)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking Categorical data\nNext, we look at the range of values for company location, bean origin, and bean type. Company location seems straightforward, but we see that the bean type lists blends as well. It may be best to fuse similar categories here. In addition, some of the bars seem to have their beans sourced from many different locations.","metadata":{}},{"cell_type":"code","source":"print(data_unknowns['company_location'].unique())\nprint(data_unknowns['bean_origin'].unique())\nprint(data_unknowns['bean_type'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking Bean types\nThe bean types clearly have some beans that are more common than others. Some show exactly which beans are in there, some show only there's a blend but not of which beans, and some even have percentages. Due to the varying amount of detail, this feature needs to be simplified. Again, this is the feature missing about 50% of its data.\n\nAfter doing some research, it looks like there are three main types of cacao beans: Criollo, Forastero, and Trinitario. There are also a fair number of blends of these three types. We recode into a smaller number of categories with the assumption that the subvarieties are less important discriminators.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\ndata_unknowns['bean_type'].value_counts().plot(kind='barh')\nplt.xticks(rotation=90);\nplt.title(\"Bean types\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collapse_bean_type(bean):\n    if bean == 'unknown':\n        label = 'unknown'\n    elif 'blend' in bean.lower() or ('forasetero' in bean.lower() and 'criollo' in bean.lower()) or ('forasetero' in bean.lower() and 'trinitario' in bean.lower()):\n        label = 'blend'\n    elif 'forasetero' in bean.lower():\n        label = 'forasetero'\n    elif 'criollo' in bean.lower():\n        label = 'criollo'\n    elif 'trinitario' in bean.lower():\n        label = 'trinitario'\n    else:\n        label = 'other'\n    return label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_unknowns['bean_type'] = data_unknowns['bean_type'].apply(collapse_bean_type)\ndata_unknowns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Bean Origin\nThe bean origin is heavily biased towards a few countries. Some of the entries with multiple countries could probably be fused with a more common country, or this could become a categorization vector that could show multiple values.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\ndata_unknowns['bean_origin'].value_counts().plot(kind='barh')\nplt.xticks(rotation=90);\nplt.title(\"Bean Origin\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_unknowns.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_unknowns.boxplot(by='bean_origin', column='Rating', figsize=(18, 10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I'm not sure what REF is, but here we can prove that it's not just a data index\ndata_unknowns.pivot_table(index=['REF'], aggfunc='size')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So we drop it\ndata_unknowns = data_unknowns.drop(columns=['REF'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model setup","metadata":{}},{"cell_type":"markdown","source":"Get a MSE baseline using the mean of various training sets to predict the test set.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data_unknowns[[col for col in data_unknowns.columns if col != 'Rating']]\ny = data_unknowns['Rating']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = KFold(n_splits=5)\nmses = []\nfor train_index, test_index in cv.split(X, y):\n    y_train, y_test = y.loc[train_index], y.loc[test_index]\n    y_mean = y_train.mean()\n    y_pred = y_test.copy()\n    y_pred.loc[:] = y_mean\n    mses.append(mean_squared_error(y_test, y_pred))\nprint(np.mean(mses))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_unknowns.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_unknowns_dumm = pd.get_dummies(data_unknowns)\ndata_unknowns_dumm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mses_rf = []\nfor train_index, test_index in cv.split(X, y):\n    X_train, X_test = X.loc[train_index], y.loc[test_index]\n    y_train, y_test = y.loc[train_index], y.loc[test_index]\n    rfr = RandomForestRegressor(n_estimators=100, max_depth=None)\n    rfr.fit(X_train, y_train)\n    y_pred = rfr.predict(X_test)\n    mses_rf.append(mean_squared_error(y_test, y_pred))\nprint(mses_rf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","metadata":{},"execution_count":null,"outputs":[]}]}