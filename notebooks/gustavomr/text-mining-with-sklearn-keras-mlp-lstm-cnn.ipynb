{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"556e7862-c59b-a1e7-c8d2-91bcd7dc5b8c"},"source":"Aim: Predict Rating from Review using basic and deep models"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d6a2d9e-f548-ceaf-9226-97d0ef00023c"},"outputs":[],"source":"import re\nimport nltk\n\nimport pandas as pd\nimport numpy as np\n\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nenglish_stemmer=nltk.stem.SnowballStemmer('english')\n\nfrom sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier, SGDRegressor\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport random\nimport itertools\n\nimport sys\nimport os\nimport argparse\nfrom sklearn.pipeline import Pipeline\nfrom scipy.sparse import csr_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport six\nfrom abc import ABCMeta\nfrom scipy import sparse\nfrom scipy.sparse import issparse\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils import check_X_y, check_array\nfrom sklearn.utils.extmath import safe_sparse_dot\nfrom sklearn.preprocessing import normalize, binarize, LabelBinarizer\nfrom sklearn.svm import LinearSVC\n\nfrom keras.preprocessing import sequence\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Lambda\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.recurrent import LSTM, SimpleRNN, GRU\nfrom keras.preprocessing.text import Tokenizer\nfrom collections import defaultdict\nfrom keras.layers.convolutional import Convolution1D\nfrom keras import backend as K\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline\nplt.style.use('ggplot')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3764b93f-8c69-a9c2-d40a-70f164032661"},"outputs":[],"source":"#https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones"},{"cell_type":"markdown","metadata":{"_cell_guid":"daf69378-7811-49c3-1b1c-b28aad12ad91"},"source":"## Preprocessing Function"},{"cell_type":"markdown","metadata":{"_cell_guid":"2688fc8f-ca7b-9bd7-bc6e-8ea18ebdb957"},"source":"Here is the process :\n* Remove the non Letters\n* Convert everything to lower case\n* Remove stop words\n* Stem the words"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"68c37e32-789d-1ecf-d88f-679cd79fe461"},"outputs":[],"source":"def review_to_wordlist( review, remove_stopwords=True):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words.  Returns a list of words.\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(review).get_text()\n\n    #\n    # 2. Remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n    #\n    # 3. Convert words to lower case and split them\n    words = review_text.lower().split()\n    #\n    # 4. Optionally remove stop words (True by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n\n    b=[]\n    stemmer = english_stemmer #PorterStemmer()\n    for word in words:\n        b.append(stemmer.stem(word))\n\n    # 5. Return a list of words\n    return(b)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a43527ec-5612-afeb-ffdd-3b823b676e72"},"source":"## Import Datas"},{"cell_type":"markdown","metadata":{"_cell_guid":"51e77b0c-7cd7-6ba0-53e7-c126d6ea2d6f"},"source":"We import only 20000 lines of our total data in order to run the notebook faster"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a741e1fc-0191-c6f4-f476-52d494d17698"},"outputs":[],"source":"data_file = '../input/Amazon_Unlocked_Mobile.csv'\n\nn = 413000  \ns = 20000 \nskip = sorted(random.sample(range(1,n),n-s))\n\n\ndata = pd.read_csv( data_file, delimiter = \",\", skiprows = skip)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5984c90f-8e97-8279-9cef-ee7735c75025"},"outputs":[],"source":"data.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd77f712-e27a-a5c3-2892-bd8b1c2a3106"},"outputs":[],"source":"data = data[data['Reviews'].isnull()==False]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a4b9503f-8820-40f2-31e3-a016d155bc2b"},"outputs":[],"source":"train, test = train_test_split(data, test_size = 0.3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d263bb83-4cf3-4f86-d30d-38902e140f30"},"source":"## Labels Exploration"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"075d9e7c-236b-e7bf-d374-971404c9bd18"},"outputs":[],"source":"sns.countplot(data['Rating'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"ba1cbbaf-2581-18c9-71bd-244c82d504b3"},"source":"Much More 5 than others ratings"},{"cell_type":"markdown","metadata":{"_cell_guid":"de5cb5a7-d4cd-9854-d479-4d1bb5ab7e71"},"source":"### Apply Preprocessing"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d7df42d-fc16-a8d0-2ac9-36ef7eca2a97"},"outputs":[],"source":"clean_train_reviews = []\nfor review in train['Reviews']:\n    clean_train_reviews.append( \" \".join(review_to_wordlist(review)))\n    \nclean_test_reviews = []\nfor review in test['Reviews']:\n    clean_test_reviews.append( \" \".join(review_to_wordlist(review)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"615da5fd-a050-83f4-41c7-179d2e6d9af6"},"source":"## TFidf transformation"},{"cell_type":"markdown","metadata":{"_cell_guid":"8b8b704d-3214-9422-13fd-275128b1155f"},"source":"### TFidf"},{"cell_type":"markdown","metadata":{"_cell_guid":"b6ed9c5d-ebd9-0a56-aeeb-3cab6ec2568a"},"source":"We will use tfidf transformation with ngrams between 1 and 4."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ec98d8dc-f48e-4c28-15d1-d37a7e84a1cc"},"outputs":[],"source":"vectorizer = TfidfVectorizer( min_df=2, max_df=0.95, max_features = 200000, ngram_range = ( 1, 4 ),\n                              sublinear_tf = True )\n\nvectorizer = vectorizer.fit(clean_train_reviews)\ntrain_features = vectorizer.transform(clean_train_reviews)\n\ntest_features = vectorizer.transform(clean_test_reviews)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b4cebdfe-17c7-ffb7-969b-8048424a885a"},"source":"### Select Best Features"},{"cell_type":"markdown","metadata":{"_cell_guid":"a68058f9-c22e-70b1-8936-70229e52a513"},"source":"We only select the best features for our prediction"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0db6ebf3-7d61-17a0-a6d1-562c50166506"},"outputs":[],"source":"fselect = SelectKBest(chi2 , k=10000)\ntrain_features = fselect.fit_transform(train_features, train[\"Rating\"])\ntest_features = fselect.transform(test_features)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d968b546-51c6-41c7-0680-3043d1ba6660"},"source":"## Modelling"},{"cell_type":"markdown","metadata":{"_cell_guid":"cc3fb1cc-b41f-45fc-8855-a976bc41f8cd"},"source":"### NBayes, RF, GBM, SGDClassifier ..."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0f8fc18-ac27-ca7a-259a-a2eb26c0bc78"},"outputs":[],"source":"model1 = MultinomialNB(alpha=0.001)\nmodel1.fit( train_features, train[\"Rating\"] )\n\nmodel2 = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\nmodel2.fit( train_features, train[\"Rating\"] )\n\nmodel3 = RandomForestClassifier()\nmodel3.fit( train_features, train[\"Rating\"] )\n\nmodel4 = GradientBoostingClassifier()\nmodel4.fit( train_features, train[\"Rating\"] )\n\npred_1 = model1.predict( test_features.toarray() )\npred_2 = model2.predict( test_features.toarray() )\npred_3 = model3.predict( test_features.toarray() )\npred_4 = model4.predict( test_features.toarray() )"},{"cell_type":"markdown","metadata":{"_cell_guid":"72c6581b-7ac8-7d6c-f1b4-e3415c7aad03"},"source":"### NBSVM"},{"cell_type":"markdown","metadata":{"_cell_guid":"7d5aab86-1504-c7c4-eff7-5f26065e7135"},"source":"taken from https://github.com/lrei/nbsvm/blob/master/nbsvm2.py"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ea4e8d4f-74b8-87b2-e044-03de8a99de4d"},"outputs":[],"source":"class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n\n    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n        self.alpha = alpha\n        self.max_iter = max_iter\n        self.C = C\n        self.svm_ = [] # fuggly\n\n    def fit(self, X, y):\n        X, y = check_X_y(X, y, 'csr')\n        _, n_features = X.shape\n\n        labelbin = LabelBinarizer()\n        Y = labelbin.fit_transform(y)\n        self.classes_ = labelbin.classes_\n        if Y.shape[1] == 1:\n            Y = np.concatenate((1 - Y, Y), axis=1)\n\n        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n        # so we don't have to cast X to floating point\n        Y = Y.astype(np.float64)\n\n        # Count raw events from data\n        n_effective_classes = Y.shape[1]\n        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n                                 dtype=np.float64)\n        self._compute_ratios(X, Y)\n\n        # flugglyness\n        for i in range(n_effective_classes):\n            X_i = X.multiply(self.ratios_[i])\n            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n            Y_i = Y[:,i]\n            svm.fit(X_i, Y_i)\n            self.svm_.append(svm) \n\n        return self\n\n    def predict(self, X):\n        n_effective_classes = self.class_count_.shape[0]\n        n_examples = X.shape[0]\n\n        D = np.zeros((n_effective_classes, n_examples))\n\n        for i in range(n_effective_classes):\n            X_i = X.multiply(self.ratios_[i])\n            D[i] = self.svm_[i].decision_function(X_i)\n        \n        return self.classes_[np.argmax(D, axis=0)]\n        \n    def _compute_ratios(self, X, Y):\n        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n        if np.any((X.data if issparse(X) else X) < 0):\n            raise ValueError(\"Input X must be non-negative\")\n\n        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n        check_array(self.ratios_)\n        self.ratios_ = sparse.csr_matrix(self.ratios_)\n\n        #p_c /= np.linalg.norm(p_c, ord=1)\n        #ratios[c] = np.log(p_c / (1 - p_c))\n\n\ndef f1_class(pred, truth, class_val):\n    n = len(truth)\n\n    truth_class = 0\n    pred_class = 0\n    tp = 0\n\n    for ii in range(0, n):\n        if truth[ii] == class_val:\n            truth_class += 1\n            if truth[ii] == pred[ii]:\n                tp += 1\n                pred_class += 1\n                continue;\n        if pred[ii] == class_val:\n            pred_class += 1\n\n    precision = tp / float(pred_class)\n    recall = tp / float(truth_class)\n\n    return (2.0 * precision * recall) / (precision + recall)\n\n\ndef semeval_senti_f1(pred, truth, pos=2, neg=0): \n\n    f1_pos = f1_class(pred, truth, pos)\n    f1_neg = f1_class(pred, truth, neg)\n\n    return (f1_pos + f1_neg) / 2.0;\n\n\ndef main(train_file, test_file, ngram=(1, 3)):\n    print('loading...')\n    train = pd.read_csv(train_file, delimiter='\\t', encoding='utf-8', header=0,\n                        names=['text', 'label'])\n\n    # to shuffle:\n    #train.iloc[np.random.permutation(len(df))]\n\n    test = pd.read_csv(test_file, delimiter='\\t', encoding='utf-8', header=0,\n                        names=['text', 'label'])\n\n    print('vectorizing...')\n    vect = CountVectorizer()\n    classifier = NBSVM()\n\n    # create pipeline\n    clf = Pipeline([('vect', vect), ('nbsvm', classifier)])\n    params = {\n        'vect__token_pattern': r\"\\S+\",\n        'vect__ngram_range': ngram, \n        'vect__binary': True\n    }\n    clf.set_params(**params)\n\n    #X_train = vect.fit_transform(train['text'])\n    #X_test = vect.transform(test['text'])\n\n    print('fitting...')\n    clf.fit(train['text'], train['label'])\n\n    print('classifying...')\n    pred = clf.predict(test['text'])\n   \n    print('testing...')\n    acc = accuracy_score(test['label'], pred)\n    f1 = semeval_senti_f1(pred, test['label'])\n    print('NBSVM: acc=%f, f1=%f' % (acc, f1))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd69a67b-3abb-6417-21d1-59ff3fc44344"},"outputs":[],"source":"model5 = NBSVM(C=0.01)\nmodel5.fit( train_features, train[\"Rating\"] )\n\npred_5 = model5.predict( test_features )"},{"cell_type":"markdown","metadata":{"_cell_guid":"2f27a88e-53e3-4c82-319c-4bfe7272b995"},"source":"## Visualize Errors"},{"cell_type":"markdown","metadata":{"_cell_guid":"621a7de7-eb20-6331-80a5-e7a8c252b595"},"source":"### Classification Report"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0bf3a12a-93c8-dfba-e459-4174d05a5098"},"outputs":[],"source":"print(classification_report(test['Rating'], pred_2, target_names=['1','2','3','4','5']))"},{"cell_type":"markdown","metadata":{"_cell_guid":"7bc7ae4e-c670-df97-1d1c-484c64efbbfa"},"source":"### Confusion Matrix"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb00617b-cd4f-79c7-f670-b973e46978d8"},"outputs":[],"source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(test['Rating'], pred_5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7cffefa2-15b6-3073-8bf5-af83ca01273f"},"outputs":[],"source":"plot_confusion_matrix(cnf_matrix, classes=['1','2','3','4','5'],\n                      title='Confusion matrix, without normalization')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6a4d510-e8c2-305b-8905-8956701abaa3"},"outputs":[],"source":"print('prediction 1 accuracy: ', accuracy_score(test['Rating'], pred_1))\nprint('prediction 2 accuracy: ', accuracy_score(test['Rating'], pred_2))\nprint('prediction 3 accuracy: ', accuracy_score(test['Rating'], pred_3))\nprint('prediction 4 accuracy: ', accuracy_score(test['Rating'], pred_4))\nprint('prediction 5 accuracy: ', accuracy_score(test['Rating'], pred_5))"},{"cell_type":"markdown","metadata":{"_cell_guid":"f0704e43-9ab0-25a4-0a8e-68d518d7887e"},"source":"## Deep Learning"},{"cell_type":"markdown","metadata":{"_cell_guid":"688a6e71-22c2-1f0e-18a1-35da16e90636"},"source":"### MLP"},{"cell_type":"markdown","metadata":{"_cell_guid":"6af800f5-ded6-6772-47cb-2f421477feb6"},"source":"A Multilayer Perceptron (MLP) is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs. An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a nonlinear activation function. (source Wikipedia)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6bc1bb7a-1ee9-bf95-c248-86b3f81ac314"},"source":"They can be usefull but are not the best deep models to use with unstructured data as text."},{"cell_type":"markdown","metadata":{"_cell_guid":"d429144e-39ed-7c73-6a15-d6940aba8075"},"source":"We will apply the MLP on the Tfidf Matrix"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99ab74c0-2e18-6326-8533-c24a0d932968"},"outputs":[],"source":"batch_size = 32\nnb_classes = 5"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"65baef9c-1531-33bf-be3d-0dc1984b26ed"},"outputs":[],"source":"vectorizer = TfidfVectorizer( min_df=2, max_df=0.95, max_features = 1000, ngram_range = ( 1, 3 ),\n                              sublinear_tf = True )\n\nvectorizer = vectorizer.fit(clean_train_reviews)\ntrain_features = vectorizer.transform(clean_train_reviews)\n\ntest_features = vectorizer.transform(clean_test_reviews)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18670f42-9924-c06a-af8a-ee09ce2d8950"},"outputs":[],"source":"X_train = train_features.toarray()\nX_test = test_features.toarray()\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)\ny_train = np.array(train['Rating']-1)\ny_test = np.array(test['Rating']-1)\n\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\n\n# pre-processing: divide by max and substract mean\nscale = np.max(X_train)\nX_train /= scale\nX_test /= scale\n\nmean = np.mean(X_train)\nX_train -= mean\nX_test -= mean\n\ninput_dim = X_train.shape[1]\n\n# Here's a Deep Dumb MLP (DDMLP)\nmodel = Sequential()\nmodel.add(Dense(256, input_dim=input_dim))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n\n# we'll use categorical xent for the loss, and RMSprop as the optimizer\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\nprint(\"Training...\")\nmodel.fit(X_train, Y_train, nb_epoch=5, batch_size=16, validation_split=0.1, show_accuracy=True)\n\nprint(\"Generating test predictions...\")\npreds = model.predict_classes(X_test, verbose=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20a7e1d9-4b7a-f03c-82a6-5974504ba15d"},"outputs":[],"source":"print('prediction 6 accuracy: ', accuracy_score(test['Rating'], preds+1))"},{"cell_type":"markdown","metadata":{"_cell_guid":"6711f545-9784-2e77-4a7a-58af16519096"},"source":"### LSTM"},{"cell_type":"markdown","metadata":{"_cell_guid":"dbb07555-21c1-aa73-822e-04b7ad761b94"},"source":"The forward pass of a RNN is the same as the one of a MLP except that outputs from hidden layers are also used as inputs from the same layer. That means that the input from the hidden layer is both the outputs from the hidden layer one step back in time and the external input. So we have the equation:\n\n\n\\begin{equation} \na_{h,t} = \\sum_{i}{w_{i,h}*x_{i,t}} + \\sum_{h'}{w_{h',h}*b_{h',t-1}} \n\\end{equation} \n\n\n\\begin{equation} \nb_{h,t} = \\phi_{h}(a_{h,t}) \n\\end{equation} \n\n\nwhere: \n* $x_{t,i} =$ value of input i at time t \n* $a_{t,j} =$ network input to unit j at time t \n* $b_{t,j}=$ output of activation of unit j at time t\n* $w_{i,h} =$  weights of the network"},{"cell_type":"markdown","metadata":{"_cell_guid":"b00f06a3-eae9-b00c-df17-6c84b7990105"},"source":"Long Short Term Memory networks – usually just called LSTM – are a special kind of RNN, capable of learning long-term dependencies. (The idea behind those is to counter the vanishing problem of some basic RNN.)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a45e658b-9a42-9ac9-20ca-d568988e2df4"},"source":"Thus, LSTM can be very usefull in text mining problems since it involves dependencies in the sentences which can be caught in the \"memory\" of the LSTM."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"09ec7a73-b091-92e3-a24f-77a28c18dd31"},"outputs":[],"source":"max_features = 20000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2\nmaxlen = 80\nbatch_size = 32\nnb_classes = 5"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f28a2622-6859-41e9-f98d-aec9098962e2"},"outputs":[],"source":"# vectorize the text samples into a 2D integer tensor\ntokenizer = Tokenizer(nb_words=max_features)\ntokenizer.fit_on_texts(train['Reviews'])\nsequences_train = tokenizer.texts_to_sequences(train['Reviews'])\nsequences_test = tokenizer.texts_to_sequences(test['Reviews'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c9405324-f068-a1b2-fc63-dd98a4d3a82f"},"outputs":[],"source":"print('Pad sequences (samples x time)')\nX_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\nX_test = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5e908847-9bcb-cf94-9805-0c25bb3f5b08"},"source":"The embedding layer in text mining is really important. Indeed, it is a way to map our text input into a space (a dictionary of dimension here 128). The layer is trained through iterations (epochs) to have a better weights for the dictionary that allow to minimize the global error of the network.\n\nSkip-gram, CBOW, and GloVe (or any other word2vec variant) are pre-trained word embeddings which can be set as the weight of an embedding layer. If the weight of this layer (generally the first layer of the network) is not initialized by these pre-trained vectors, the model/network itself would assign random weights and will learn the embeddings (i.e. weights) on the fly.\n\nExplanation summarized from :\nhttps://github.com/fchollet/keras/issues/3110"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36d3d256-4fa8-5664-a716-61e7af263220"},"outputs":[],"source":"print('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128, dropout=0.2))\nmodel.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nprint('Train...')\nmodel.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1,\n          validation_data=(X_test, Y_test))\nscore, acc = model.evaluate(X_test, Y_test,\n                            batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n\n\nprint(\"Generating test predictions...\")\npreds = model.predict_classes(X_test, verbose=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5d0507c-d01c-11e3-70b0-ae5a790bea26"},"outputs":[],"source":"print('prediction 7 accuracy: ', accuracy_score(test['Rating'], preds+1))"},{"cell_type":"markdown","metadata":{"_cell_guid":"5468eeeb-e44b-925e-c70b-0f29e9d70f6b"},"source":"add epochs and reviews to be more accurate with LSTM"},{"cell_type":"markdown","metadata":{"_cell_guid":"aa310c6a-414d-3781-90a5-6cb83e4da82a"},"source":"### CNN"},{"cell_type":"markdown","metadata":{"_cell_guid":"f26ea171-e228-5eb3-2f74-21754a10eead"},"source":"Explanation from: https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/"},{"cell_type":"markdown","metadata":{"_cell_guid":"33881dc1-063c-97c1-7e60-8bc57c45167d"},"source":"\nThere are four main operations in the ConvNet :\n\nConvolution\nNon Linearity (ReLU)\nPooling or Sub Sampling\nClassification (Fully Connected Layer)\n\nConvolution consists in applying different filters (here we chose to use 250 filters) on our input (here text). The main goal of the convolution step is to extract features from the input.\n\nThe different filters allow to apply different convolutions (mathematical convolutions) on our input. As a result we obtain N (N: number of filters) convolved input after the application of our convolved layer.\n\nIn order to add non linearity , we use Relu after the convolution layer. \n\nAfter we have pooling. Spatial Pooling reduces the dimensionality of each feature map but retains the most important information. Spatial Pooling can be of different types: Max, Average, Sum etc.\n\nThe last step is the same as the MLP"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4f60df0-909e-6bc4-e2c1-210902655155"},"outputs":[],"source":"nb_filter = 250\nfilter_length = 3\nhidden_dims = 250\nnb_epoch = 2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8f524ba-611f-67b9-5973-e1d3d7b3ee08"},"outputs":[],"source":"print('Build model...')\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128, dropout=0.2))\n# we add a Convolution1D, which will learn nb_filter\n# word group filters of size filter_length:\nmodel.add(Convolution1D(nb_filter=nb_filter,\n                        filter_length=filter_length,\n                        border_mode='valid',\n                        activation='relu',\n                        subsample_length=1))\n\ndef max_1d(X):\n    return K.max(X, axis=1)\n\nmodel.add(Lambda(max_1d, output_shape=(nb_filter,)))\nmodel.add(Dense(hidden_dims)) \nmodel.add(Dropout(0.2)) \nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"109858be-7b08-d3b6-4930-d4db0f124d7d"},"outputs":[],"source":"print('Train...')\nmodel.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1,\n          validation_data=(X_test, Y_test))\nscore, acc = model.evaluate(X_test, Y_test,\n                            batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n\n\nprint(\"Generating test predictions...\")\npreds = model.predict_classes(X_test, verbose=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d0bc468-cef3-bb40-adbb-846490cc273d"},"outputs":[],"source":"print('prediction 8 accuracy: ', accuracy_score(test['Rating'], preds+1))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cedd0e04-c7ce-b276-6ce2-724752c8d5cf"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}