{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.mode.chained_assignment = None","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Purpose\n\nIn a [previous tutorial](https://www.kaggle.com/mdabbert/tutorial-2-train-a-model-to-make-bet-predictions) I showed how to create a model that could predict profitable UFC bets by using `predict_proba()` and gambling odds for the fight.  Now I will show a way to evaluate how this model is performing.  It isn't as simple as seeing how many winning fighters it predicts.  It comes down to how much profit it sees from its predicted bets.\n\nYou may find you train models that predict a lot of favorites to win that has a much higher accuracy than models that predict a lot of underdogs to win.  But it could still be possible that the less accurate model is the more profitable one.  I will walk through how to set up an evaluation that will give a model a score based off it its profit.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Prep the Train and Test Set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load the matches that have already occurred \ndf = pd.read_csv(\"/kaggle/input/ultimate-ufc-dataset/ufc-master.csv\")\n\n#Let's put all the labels in a dataframe\ndf['label'] = ''\n#If the winner is not Red or Blue we can remove it.\nmask = df['Winner'] == 'Red'\ndf['label'][mask] = 0\nmask = df['Winner'] == 'Blue'\ndf['label'][mask] = 1\n\n#df[\"Winner\"] = df[\"Winner\"].astype('category')\ndf = df[(df['Winner'] == 'Blue') | (df['Winner'] == 'Red')]\n\n\n#Make sure lable is numeric\ndf['label'] = pd.to_numeric(df['label'], errors='coerce')\n\n#Let's fix the date\ndf['date'] = pd.to_datetime(df['date'])\n\n#Create a label df:\nlabel_df = df['label']\n\n#Let's create an odds df too:\nodds_df = df[['R_odds', 'B_odds']]\n\n#Split the test set.  We are always(?) going to use the last 200 matches as the test set, so we don't want those around\n#as we pick models\n\ndf_train = df[200:]\nodds_train = odds_df[200:]\nlabel_train = label_df[200:]\n\ndf_test = df[:200]\nodds_test = odds_df[:200]\nlabel_test = label_df[:200]\n\nprint(len(df_test))\nprint(len(odds_test))\nprint(len(label_test))\n\nprint(len(df_train))\nprint(len(odds_train))\nprint(len(label_train))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Pick a Model and Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n#Pick a model\nmy_model = DecisionTreeClassifier(max_depth=5)\n\n#Pick some features\n#I would not recommend placing bets based off of these features...\nmy_features = ['R_odds', 'B_Stance']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Create Some Helper Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Input: American Odds, and Probability of a Winning Bet\n#Output: Bet EV based on a $100 bet\ndef get_bet_ev(odds, prob):\n    if odds>0:\n        return ((odds * prob) - (100 * (1-prob)) )\n    else:\n        return ((100 / abs(odds))*100*prob - (100 * (1-prob)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Input: American Odds\n#Output: Profit on a successful bet\ndef get_bet_return(odds):\n    if odds>0:\n        return odds\n    else:\n        return (100 / abs(odds))*100\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Takes a prepared df and returns stats about how the model did.\n#This df must be prepared using the evaluate_model() function\n\ndef get_ev_from_df(ev_df, print_stats = False):\n    get_total = True #if this is False we would return profit per bet\n    min_ev = 0 #If we wanted to have a minimum ev to make a bet other than 0...\n    num_matches = 0\n    num_bets = 0\n    num_wins = 0\n    num_losses= 0\n    num_under= 0\n    num_under_losses = 0\n    num_under_wins = 0\n    num_even = 0\n    num_even_losses = 0\n    num_even_wins = 0\n    num_fav = 0\n    num_fav_wins = 0\n    num_fav_losses = 0\n    profit = 0\n    profit_per_bet = 0\n    profit_per_match = 0    \n\n    for index, row in ev_df.iterrows():\n        num_matches = num_matches+1\n        t1_bet_ev = get_bet_ev(row['t1_odds'], row['t1_prob'])\n        #print(f\"ODDS:{row['t1_odds']} PROB: {row['t1_prob']} EV: {t1_bet_ev}\")\n        t2_bet_ev = get_bet_ev(row['t2_odds'], row['t2_prob'])\n        #print(f\"ODDS:{row['t2_odds']} PROB: {row['t2_prob']} EV: {t2_bet_ev}\")\n        #print()\n        \n        t1_bet_return = get_bet_return(row['t1_odds'])\n        t2_bet_return = get_bet_return(row['t2_odds'])\n        \n        \n        if (t1_bet_ev > min_ev or t2_bet_ev > min_ev):\n            num_bets = num_bets+1\n\n            \n        if t1_bet_ev > min_ev:\n            if row['winner'] == 0:\n                num_wins += 1\n                profit = profit + t1_bet_return\n                #print(t1_bet_return)\n            elif row['winner'] == 1:\n                num_losses += 1\n                profit = profit - 100\n            if (t1_bet_return > t2_bet_return):\n                num_under += 1\n                if row['winner'] == 0:\n                    num_under_wins += 1\n                elif row['winner'] == 1:\n                    num_under_losses += 1\n            elif (t1_bet_return < t2_bet_return):\n                num_fav += 1\n                if row['winner'] == 0:\n                    num_fav_wins += 1\n                elif row['winner'] == 1:\n                    num_fav_losses += 1\n            else:\n                num_even += 1\n                if row['winner'] == 0:\n                    num_even_wins += 1\n                elif row['winner'] == 1:\n                    num_even_losses += 1\n\n        if t2_bet_ev > min_ev:\n            if row['winner'] == 1:\n                num_wins += 1                    \n                profit = profit + t2_bet_return\n            elif row['winner'] == 0:\n                num_losses += 1\n                profit = profit - 100\n            if (t2_bet_return > t1_bet_return):\n                num_under += 1\n                if row['winner'] == 1:\n                    num_under_wins += 1\n                elif row['winner'] == 0:\n                    num_under_losses += 1\n            elif (t2_bet_return < t1_bet_return):\n                num_fav += 1\n                if row['winner'] == 1:\n                    num_fav_wins += 1\n                elif row['winner'] == 0:\n                    num_fav_losses += 1\n            else:\n                num_even += 1\n                if row['winner'] == 1:\n                    num_even_wins += 1\n                elif row['winner'] == 0:\n                    num_even_losses += 1\n            \n    if num_bets > 0:\n        profit_per_bet = profit / num_bets\n    else:\n        profit_per_bet = 0\n    if num_matches > 0:\n        profit_per_match = profit / num_matches\n    else:\n        profit_per_match = 0\n        \n    if print_stats:\n        print(f\"\"\"\n          Number of matches: {num_matches}\n          Number of bets: {num_bets}\n          Number of winning bets: {num_wins}\n          Number of losing bets: {num_losses}\n          Number of underdog bets: {num_under}\n          Number of underdog wins: {num_under_wins}\n          Number of underdog losses: {num_under_losses}\n          Number of Favorite bets: {num_fav}\n          Number of favorite wins: {num_fav_wins}\n          Number of favorite losses: {num_fav_losses}\n          Number of even bets: {num_even}\n          Number of even wins: {num_even_wins}\n          Number of even losses: {num_even_losses}\n          Profit: {profit}\n          Profit per bet: {profit_per_bet}\n          Profit per match: {profit_per_match}\n          \n          \"\"\")\n    if (get_total):\n        #print(f\"# Matches: {num_matches}, # Bets: {num_bets} # Wins: {num_wins}\")\n        return(profit)\n    else:\n        return (profit_per_bet)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This function will give us a score\n#input_model: the model we want to train and use.  It must be able to call predict_proba()\n#input_features: the list of features we want to use.\n#train_df: The training set. Created above\n#train_labels: the winners of the training set.  Created above\n#train_odds: A list of red fighter and blue fighter odds.  Created above\n#test_df: The test set. Created above\n#test_labels: The winners of the test set. Created above\n#test_odds: The odds of the test set fights.  Created above\n#verbose: if verbose is true we print a bunch of stats.\ndef evaluate_model(input_model, input_features, train_df, train_labels, train_odds, test_df, test_labels,\n                  test_odds, verbose=True):\n    \n    model_score = 0\n    \n    #Tutorial 1 will explain what is going on here.....\n    df_train = train_df[input_features].copy()\n    df_test = test_df[input_features].copy()\n    df_train = df_train.dropna()\n    df_test = df_test.dropna()\n        \n    df_train = pd.get_dummies(df_train)\n    df_test = pd.get_dummies(df_test)\n    df_train, df_test = df_train.align(df_test, join='left', axis=1)    #Ensures both sets are dummified the same\n    df_test = df_test.fillna(0)\n    \n    labels_train = train_labels[train_labels.index.isin(df_train.index)]\n    odds_train = train_odds[train_odds.index.isin(df_train.index)] \n    labels_test = test_labels[test_labels.index.isin(df_test.index)]\n    odds_test = test_odds[test_odds.index.isin(df_test.index)]     \n\n    #Quick shape check\n    #display(df_train.shape)\n    #display(labels_train.shape)\n    #display(odds_train.shape)\n    #display(df_test.shape)\n    #display(labels_test.shape)\n    #display(odds_test.shape)    \n\n    input_model.fit(df_train, labels_train)\n\n    \n    \n    probs = input_model.predict_proba(df_test)\n\n    \n    odds_test = np.array(odds_test)    \n    \n    prepped_test = list(zip(odds_test[:, -2], odds_test[:, -1], probs[:, 0], probs[:, 1], labels_test))\n    #Prepped test now is a list of [Red Odds, Blue Odds, Red Prob of winning, Blue prob of winning, winner label]\n    ev_prepped_df = pd.DataFrame(prepped_test, columns=['t1_odds', 't2_odds', 't1_prob', 't2_prob', 'winner'])\n    display(ev_prepped_df)\n    model_score = get_ev_from_df(ev_prepped_df, print_stats = True)\n\n    return(model_score)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Run it and see how the model and features perform","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score = evaluate_model(my_model, my_features, df_train, label_train, odds_train, df_test, label_test,\n                         odds_test, verbose = True)\n\nprint(f\"Model: {my_model}\")\nprint(f\"Features: {my_features}\")\nprint(f\"The score of this model feature combo is: {score}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's try another one quick to see the difference.\nfrom sklearn.linear_model import LogisticRegression\nmy_model_2 = LogisticRegression()\nmy_features_2 = ['B_ev', 'country']\nscore = evaluate_model(my_model_2, my_features_2, df_train, label_train, odds_train, df_test, label_test,\n                         odds_test, verbose = True)\n\nprint(f\"Model: {my_model_2}\")\nprint(f\"Features: {my_features_2}\")\nprint(f\"The score of this model feature combo is: {score}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Discussion of Performance:\n\nThis naive model that is simply a decision tree that takes into consideration the red fighter's odds and the blue fighter's stance actually does pretty well.  Out of 200 fights it sees 142 of them as having a positive expected value.  It wins 81 of these 142 bets.  It wins 38 out of 84 underdog bets.  A good number.  Anything close to 50% will be profitable for that part of it.  For favorites it wins 39/52 bets.  For even fights it is 4/6.  \n\nFor 142 bets of 100 units each this model sees a total return of 1162.21 units.  A profit of 8.18 per bet or 5.81 per match.\n\nI just chose these features and model type at random, but it would not be a bad starting point to develop an actual model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Alternate Scoring Ideas\n\nI have used total profit as how I tally the score but there are other possibilities that have their pros and cons.\n\n* Profit per bet: \n  - Pros: With limited funds profit per bet could be a better metric.\n  - Cons: Could skew towards models with only a few outlier wins.\n  \n  \n* (Profit) / (Total Possible Profit):\n  - Pros: A number no greater than 1 that could be used to compare models that were evaluated at different times.  Possibly more useful for visualization\n  - Cons: Models that were evaluated at different times still isn't an apples-to-apples comparison","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}