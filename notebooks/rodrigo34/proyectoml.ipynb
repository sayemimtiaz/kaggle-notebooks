{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Image Segmentation en deteccion de areas de aterrizaje para drones**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Rodrigo MontaÃ±o Villarroel**","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install keras-segmentation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport keras\nfrom keras.models import *\nfrom keras.layers import *\n\nfrom types import MethodType\nimport random\nimport six\nimport json\nfrom tqdm import tqdm\nimport cv2\nimport numpy as np\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nprint(sys.version)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_ORDERING_CHANNELS_FIRST = \"channels_first\"\nIMAGE_ORDERING_CHANNELS_LAST = \"channels_last\"\n# Default IMAGE_ORDERING = channels_last\nIMAGE_ORDERING = IMAGE_ORDERING_CHANNELS_LAST\n\nif IMAGE_ORDERING == 'channels_first':\n    MERGE_AXIS = 1\nelif IMAGE_ORDERING == 'channels_last':\n    MERGE_AXIS = -1\n    \nif IMAGE_ORDERING == 'channels_first':\n    pretrained_url = \"https://github.com/fchollet/deep-learning-models/\" \\\n                     \"releases/download/v0.1/\" \\\n                     \"vgg16_weights_th_dim_ordering_th_kernels_notop.h5\"\nelif IMAGE_ORDERING == 'channels_last':\n    pretrained_url = \"https://github.com/fchollet/deep-learning-models/\" \\\n                     \"releases/download/v0.1/\" \\\n                     \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n    \nclass_colors = [(random.randint(0, 255), random.randint(\n    0, 255), random.randint(0, 255)) for _ in range(5000)]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_colored_segmentation_image( seg_arr  , n_classes , colors=class_colors ):\n    output_height = seg_arr.shape[0]\n    output_width = seg_arr.shape[1]\n\n    seg_img = np.zeros((output_height, output_width, 3))\n\n    for c in range(n_classes):\n        seg_img[:, :, 0] += ((seg_arr[:, :] == c)*(colors[c][0])).astype('uint8')\n        seg_img[:, :, 1] += ((seg_arr[:, :] == c)*(colors[c][1])).astype('uint8')\n        seg_img[:, :, 2] += ((seg_arr[:, :] == c)*(colors[c][2])).astype('uint8')\n\n    return seg_img \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_segmentation( seg_arr , inp_img=None  , n_classes=None , \n    colors=class_colors , class_names=None , overlay_img=False , show_legends=False , \n    prediction_width=None , prediction_height=None  ):\n    \n\n    if n_classes is None:\n        n_classes = np.max(seg_arr)\n\n    seg_img = get_colored_segmentation_image( seg_arr  , n_classes , colors=colors )\n\n    if not inp_img is None:\n        orininal_h = inp_img.shape[0]\n        orininal_w = inp_img.shape[1]\n        seg_img = cv2.resize(seg_img, (orininal_w, orininal_h))\n\n\n    if (not prediction_height is None) and  (not prediction_width is None):\n        seg_img = cv2.resize(seg_img, (prediction_width, prediction_height ))\n        if not inp_img is None:\n            inp_img = cv2.resize(inp_img, (prediction_width, prediction_height ))\n\n\n    if overlay_img:\n        assert not inp_img is None\n        seg_img = overlay_seg_image( inp_img , seg_img  )\n\n\n    if show_legends:\n        assert not class_names is None\n        legend_img = get_legends(class_names , colors=colors )\n\n        seg_img = concat_lenends( seg_img , legend_img )\n\n\n    return seg_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_image_array(image_input, width, height, imgNorm=\"sub_mean\",\n                  ordering='channels_first'):\n    \"\"\" Load image array from input \"\"\"\n\n    if type(image_input) is np.ndarray:\n        # It is already an array, use it as it is\n        img = image_input\n    elif  isinstance(image_input, six.string_types)  :\n        if not os.path.isfile(image_input):\n            raise DataLoaderError(\"get_image_array: path {0} doesn't exist\".format(image_input))\n        img = cv2.imread(image_input, 1)\n    else:\n        raise DataLoaderError(\"get_image_array: Can't process input type {0}\".format(str(type(image_input))))\n\n    if imgNorm == \"sub_and_divide\":\n        img = np.float32(cv2.resize(img, (width, height))) / 127.5 - 1\n    elif imgNorm == \"sub_mean\":\n        img = cv2.resize(img, (width, height))\n        img = img.astype(np.float32)\n        img[:, :, 0] -= 103.939\n        img[:, :, 1] -= 116.779\n        img[:, :, 2] -= 123.68\n        img = img[:, :, ::-1]\n    elif imgNorm == \"divide\":\n        img = cv2.resize(img, (width, height))\n        img = img.astype(np.float32)\n        img = img/255.0\n\n    if ordering == 'channels_first':\n        img = np.rollaxis(img, 2, 0)\n    return img\n\ndef get_image_arr( path , width , height , imgNorm=\"sub_mean\" , odering='channels_first' ):\n\n\tif type( path ) is np.ndarray:\n\t\timg = path\n\telse:\n\t\timg = cv2.imread(path, 1)\n\n\tif imgNorm == \"sub_and_divide\":\n\t\timg = np.float32(cv2.resize(img, ( width , height ))) / 127.5 - 1\n\telif imgNorm == \"sub_mean\":\n\t\timg = cv2.resize(img, ( width , height ))\n\t\timg = img.astype(np.float32)\n\t\timg[:,:,0] -= 103.939\n\t\timg[:,:,1] -= 116.779\n\t\timg[:,:,2] -= 123.68\n\t\timg = img[ : , : , ::-1 ]\n\telif imgNorm == \"divide\":\n\t\timg = cv2.resize(img, ( width , height ))\n\t\timg = img.astype(np.float32)\n\t\timg = img/255.0\n\n\tif odering == 'channels_first':\n\t\timg = np.rollaxis(img, 2, 0)\n\treturn img\n\n\ndef get_segmentation_array(image_input, nClasses, width, height, no_reshape=False):\n    \"\"\" Load segmentation array from input \"\"\"\n\n    seg_labels = np.zeros((height, width, nClasses))\n\n    if type(image_input) is np.ndarray:\n        # It is already an array, use it as it is\n        img = image_input\n    elif isinstance(image_input, six.string_types) :\n        if not os.path.isfile(image_input):\n            raise DataLoaderError(\"get_segmentation_array: path {0} doesn't exist\".format(image_input))\n        img = cv2.imread(image_input, 1)\n    else:\n        raise DataLoaderError(\"get_segmentation_array: Can't process input type {0}\".format(str(type(image_input))))\n\n    img = cv2.resize(img, (width, height), interpolation=cv2.INTER_NEAREST)\n    img = img[:, :, 0]\n\n    for c in range(nClasses):\n        seg_labels[:, :, c] = (img == c).astype(int)\n\n    if not no_reshape:\n        seg_labels = np.reshape(seg_labels, (width*height, nClasses))\n\n    return seg_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def image_segmentation_generator(images_path, segs_path, batch_size,\n                                 n_classes, input_height, input_width,\n                                 output_height, output_width,\n                                 do_augment=False ,augmentation_name=\"aug_all\" ):\n\n    img_seg_pairs = get_pairs_from_paths(images_path, segs_path)\n    random.shuffle(img_seg_pairs)\n    zipped = itertools.cycle(img_seg_pairs)\n\n    while True:\n        X = []\n        Y = []\n        for _ in range(batch_size):\n            im, seg = next(zipped)\n\n            im = cv2.imread(im, 1)\n            seg = cv2.imread(seg, 1)\n\n            if do_augment:\n                im, seg[:, :, 0] = augment_seg(im, seg[:, :, 0] , augmentation_name=augmentation_name )\n\n            X.append(get_image_array(im, input_width,\n                                   input_height, ordering=IMAGE_ORDERING))\n            Y.append(get_segmentation_array(\n                seg, n_classes, output_width, output_height))\n\n        yield np.array(X), np.array(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pairs_from_paths(images_path, segs_path, ignore_non_matching=False):\n    \"\"\" Find all the images from the images_path directory and\n        the segmentation images from the segs_path directory\n        while checking integrity of data \"\"\"\n\n    ACCEPTABLE_IMAGE_FORMATS = [\".jpg\", \".jpeg\", \".png\" , \".bmp\"]\n    ACCEPTABLE_SEGMENTATION_FORMATS = [\".png\", \".bmp\"]\n\n    image_files = []\n    segmentation_files = {}\n\n    for dir_entry in os.listdir(images_path):\n        if os.path.isfile(os.path.join(images_path, dir_entry)) and \\\n                os.path.splitext(dir_entry)[1] in ACCEPTABLE_IMAGE_FORMATS:\n            file_name, file_extension = os.path.splitext(dir_entry)\n            image_files.append((file_name, file_extension, os.path.join(images_path, dir_entry)))\n\n    for dir_entry in os.listdir(segs_path):\n        if os.path.isfile(os.path.join(segs_path, dir_entry)) and \\\n                os.path.splitext(dir_entry)[1] in ACCEPTABLE_SEGMENTATION_FORMATS:\n            file_name, file_extension = os.path.splitext(dir_entry)\n            if file_name in segmentation_files:\n                raise DataLoaderError(\"Segmentation file with filename {0} already exists and is ambiguous to resolve with path {1}. Please remove or rename the latter.\".format(file_name, os.path.join(segs_path, dir_entry)))\n            segmentation_files[file_name] = (file_extension, os.path.join(segs_path, dir_entry))\n\n    return_value = []\n    # Match the images and segmentations\n    for image_file, _, image_full_path in image_files:\n        if image_file in segmentation_files:\n            return_value.append((image_full_path, segmentation_files[image_file][1]))\n        elif ignore_non_matching:\n            continue\n        else:\n            # Error out\n            raise DataLoaderError(\"No corresponding segmentation found for image {0}.\".format(image_full_path))\n\n    return return_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def verify_segmentation_dataset(images_path, segs_path, n_classes, show_all_errors=False):\n    try:\n        img_seg_pairs = get_pairs_from_paths(images_path, segs_path)\n        if not len(img_seg_pairs):\n            print(\"Couldn't load any data from images_path: {0} and segmentations path: {1}\".format(images_path, segs_path))\n            return False\n\n        return_value = True\n        for im_fn, seg_fn in tqdm(img_seg_pairs):\n            img = cv2.imread(im_fn)\n            seg = cv2.imread(seg_fn)\n            # Check dimensions match\n            if not img.shape == seg.shape:\n                return_value = False\n                print(\"The size of image {0} and its segmentation {1} doesn't match (possibly the files are corrupt).\".format(im_fn, seg_fn))\n                if not show_all_errors:\n                    break\n            else:\n                max_pixel_value = np.max(seg[:, :, 0])\n                if max_pixel_value >= n_classes:\n                    return_value = False\n                    print(\"The pixel values of the segmentation image {0} violating range [0, {1}]. Found maximum pixel value {2}\".format(seg_fn, str(n_classes - 1), max_pixel_value))\n                    if not show_all_errors:\n                        break\n        if return_value:\n            print(\"Dataset verified! \")\n        else:\n            print(\"Dataset not verified!\")\n        return return_value\n    except Exception as e:\n        print(\"Found error during data loading\\n{0}\".format(str(e)))\n        return False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate( model=None , inp_images=None , annotations=None,inp_images_dir=None ,annotations_dir=None , checkpoints_path=None ):\n    \n    if model is None:\n        assert (checkpoints_path is not None) , \"Please provide the model or the checkpoints_path\"\n        model = model_from_checkpoint_path(checkpoints_path)\n        \n    if inp_images is None:\n        assert (inp_images_dir is not None) , \"Please privide inp_images or inp_images_dir\"\n        assert (annotations_dir is not None) , \"Please privide inp_images or inp_images_dir\"\n        \n        paths = get_pairs_from_paths(inp_images_dir , annotations_dir )\n        paths = list(zip(*paths))\n        inp_images = list(paths[0])\n        annotations = list(paths[1])\n        \n    assert type(inp_images) is list\n    assert type(annotations) is list\n        \n    tp = np.zeros( model.n_classes  )\n    fp = np.zeros( model.n_classes  )\n    fn = np.zeros( model.n_classes  )\n    n_pixels = np.zeros( model.n_classes  )\n    \n    for inp , ann   in tqdm( zip( inp_images , annotations )):\n        pr = predict(model , inp )\n        gt = get_segmentation_array( ann , model.n_classes ,  model.output_width , model.output_height , no_reshape=True  )\n        gt = gt.argmax(-1)\n        pr = pr.flatten()\n        gt = gt.flatten()\n                \n        for cl_i in range(model.n_classes ):\n            \n            tp[ cl_i ] += np.sum( (pr == cl_i) * (gt == cl_i) )\n            fp[ cl_i ] += np.sum( (pr == cl_i) * ((gt != cl_i)) )\n            fn[ cl_i ] += np.sum( (pr != cl_i) * ((gt == cl_i)) )\n            n_pixels[ cl_i ] += np.sum( gt == cl_i  )\n            \n    cl_wise_score = tp / ( tp + fp + fn + 0.000000000001 )\n    n_pixels_norm = n_pixels /  np.sum(n_pixels)\n    frequency_weighted_IU = np.sum(cl_wise_score*n_pixels_norm)\n    mean_IU = np.mean(cl_wise_score)\n    return {\"frequency_weighted_IU\":frequency_weighted_IU , \"mean_IU\":mean_IU , \"class_wise_IU\":cl_wise_score }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_multiple(model=None, inps=None, inp_dir=None, out_dir=None,\n                     checkpoints_path=None ,overlay_img=False ,\n    class_names=None , show_legends=False , colors=class_colors , prediction_width=None , prediction_height=None  ):\n\n    if model is None and (checkpoints_path is not None):\n        model = model_from_checkpoint_path(checkpoints_path)\n\n    if inps is None and (inp_dir is not None):\n        inps = glob.glob(os.path.join(inp_dir, \"*.jpg\")) + glob.glob(\n            os.path.join(inp_dir, \"*.png\")) + \\\n            glob.glob(os.path.join(inp_dir, \"*.jpeg\"))\n\n    assert type(inps) is list\n\n    all_prs = []\n\n    for i, inp in enumerate(tqdm(inps)):\n        if out_dir is None:\n            out_fname = None\n        else:\n            if isinstance(inp, six.string_types):\n                out_fname = os.path.join(out_dir, os.path.basename(inp))\n            else:\n                out_fname = os.path.join(out_dir, str(i) + \".jpg\")\n\n        pr = predict( model, inp, out_fname ,\n            overlay_img=overlay_img,class_names=class_names ,show_legends=show_legends , \n            colors=colors , prediction_width=prediction_width , prediction_height=prediction_height  )\n\n        all_prs.append(pr)\n\n    return all_prs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model=None, inp=None, out_fname=None, checkpoints_path=None,overlay_img=False ,\n    class_names=None , show_legends=False , colors=class_colors , prediction_width=None , prediction_height=None  ):\n\n    if model is None and (checkpoints_path is not None):\n        model = model_from_checkpoint_path(checkpoints_path)\n\n    assert (inp is not None)\n    assert((type(inp) is np.ndarray) or isinstance(inp, six.string_types)\n           ), \"Inupt should be the CV image or the input file name\"\n\n    if isinstance(inp, six.string_types):\n        inp = cv2.imread(inp)\n\n    assert len(inp.shape) == 3, \"Image should be h,w,3 \"\n    orininal_h = inp.shape[0]\n    orininal_w = inp.shape[1]\n\n    output_width = model.output_width\n    output_height = model.output_height\n    input_width = model.input_width\n    input_height = model.input_height\n    n_classes = model.n_classes\n\n    x = get_image_array(inp, input_width, input_height, ordering=IMAGE_ORDERING)\n    pr = model.predict(np.array([x]))[0]\n    pr = pr.reshape((output_height,  output_width, n_classes)).argmax(axis=2)\n\n    seg_img = visualize_segmentation( pr , inp ,n_classes=n_classes , colors=colors\n        , overlay_img=overlay_img ,show_legends=show_legends ,class_names=class_names ,prediction_width=prediction_width , prediction_height=prediction_height   )\n\n    if out_fname is not None:\n        cv2.imwrite(out_fname, seg_img)\n\n    return pr\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model,\n          train_images,\n          train_annotations,\n          input_height=None,\n          input_width=None,\n          n_classes=None,\n          verify_dataset=True,\n          checkpoints_path=None,\n          epochs=5,\n          batch_size=2,\n          validate=False,\n          val_images=None,\n          val_annotations=None,\n          val_batch_size=2,\n          auto_resume_checkpoint=False,\n          load_weights=None,\n          steps_per_epoch=512,\n          val_steps_per_epoch=512,\n          gen_use_multiprocessing=False,\n          ignore_zero_class=False , \n          optimizer_name='adadelta' , do_augment=False , augmentation_name=\"aug_all\"\n          ):\n\n    \n    # check if user gives model name instead of the model object\n    if isinstance(model, six.string_types):\n        # create the model from the name\n        assert (n_classes is not None), \"Please provide the n_classes\"\n        if (input_height is not None) and (input_width is not None):\n            model = model_from_name[model](\n                n_classes, input_height=input_height, input_width=input_width)\n        else:\n            model = model_from_name[model](n_classes)\n\n    n_classes = model.n_classes\n    input_height = model.input_height\n    input_width = model.input_width\n    output_height = model.output_height\n    output_width = model.output_width\n\n    if validate:\n        assert val_images is not None\n        assert val_annotations is not None\n\n    if optimizer_name is not None:\n\n        if ignore_zero_class:\n            loss_k = masked_categorical_crossentropy\n        else:\n            loss_k = 'categorical_crossentropy'\n\n        model.compile(loss= loss_k ,\n                      optimizer=optimizer_name,\n                      metrics=['accuracy'])\n\n    if checkpoints_path is not None:\n        with open(checkpoints_path+\"_config.json\", \"w\") as f:\n            json.dump({\n                \"model_class\": model.model_name,\n                \"n_classes\": n_classes,\n                \"input_height\": input_height,\n                \"input_width\": input_width,\n                \"output_height\": output_height,\n                \"output_width\": output_width\n            }, f)\n\n    if load_weights is not None and len(load_weights) > 0:\n        print(\"Loading weights from \", load_weights)\n        model.load_weights(load_weights)\n\n    if auto_resume_checkpoint and (checkpoints_path is not None):\n        latest_checkpoint = find_latest_checkpoint(checkpoints_path)\n        if latest_checkpoint is not None:\n            print(\"Loading the weights from latest checkpoint \",\n                  latest_checkpoint)\n            model.load_weights(latest_checkpoint)\n\n    if verify_dataset:\n        print(\"Verifying training dataset\")\n        verified = verify_segmentation_dataset(train_images, train_annotations, n_classes)\n        assert verified\n        if validate:\n            print(\"Verifying validation dataset\")\n            verified = verify_segmentation_dataset(val_images, val_annotations, n_classes)\n            assert verified\n\n    train_gen = image_segmentation_generator(\n        train_images, train_annotations,  batch_size,  n_classes,\n        input_height, input_width, output_height, output_width , do_augment=do_augment ,augmentation_name=augmentation_name )\n\n    if validate:\n        val_gen = image_segmentation_generator(\n            val_images, val_annotations,  val_batch_size,\n            n_classes, input_height, input_width, output_height, output_width)\n\n    if not validate:\n        for ep in range(epochs):\n            print(\"Starting Epoch \", ep)\n            model.fit_generator(train_gen, steps_per_epoch, epochs=1, use_multiprocessing=True)\n            if checkpoints_path is not None:\n                model.save_weights(checkpoints_path + \".\" + str(ep))\n                print(\"saved \", checkpoints_path + \".model.\" + str(ep))\n            print(\"Finished Epoch\", ep)\n    else:\n        for ep in range(epochs):\n            print(\"Starting Epoch \", ep)\n            model.fit_generator(train_gen, steps_per_epoch,\n                                validation_data=val_gen,\n                                validation_steps=val_steps_per_epoch,  epochs=1 , use_multiprocessing=gen_use_multiprocessing)\n            if checkpoints_path is not None:\n                model.save_weights(checkpoints_path + \".\" + str(ep))\n                print(\"saved \", checkpoints_path + \".model.\" + str(ep))\n            print(\"Finished Epoch\", ep)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_segmentation_model(input, output):\n\n    img_input = input\n    o = output\n\n    o_shape = Model(img_input, o).output_shape\n    i_shape = Model(img_input, o).input_shape\n\n    if IMAGE_ORDERING == 'channels_first':\n        output_height = o_shape[2]\n        output_width = o_shape[3]\n        input_height = i_shape[2]\n        input_width = i_shape[3]\n        n_classes = o_shape[1]\n        o = (Reshape((-1, output_height*output_width)))(o)\n        o = (Permute((2, 1)))(o)\n    elif IMAGE_ORDERING == 'channels_last':\n        output_height = o_shape[1]\n        output_width = o_shape[2]\n        input_height = i_shape[1]\n        input_width = i_shape[2]\n        n_classes = o_shape[3]\n        o = (Reshape((output_height*output_width, -1)))(o)\n\n    o = (Activation('softmax'))(o)\n    model = Model(img_input, o)\n    model.output_width = output_width\n    model.output_height = output_height\n    model.n_classes = n_classes\n    model.input_height = input_height\n    model.input_width = input_width\n    model.model_name = \"\"\n\n    model.train = MethodType(train, model)\n    model.predict_segmentation = MethodType(predict, model)\n    model.predict_multiple = MethodType(predict_multiple, model)\n    model.evaluate_segmentation = MethodType(evaluate, model)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_vgg_encoder(input_height=224,  input_width=224, pretrained='imagenet'):\n\n    assert input_height % 32 == 0\n    assert input_width % 32 == 0\n\n    if IMAGE_ORDERING == 'channels_first':\n        img_input = Input(shape=(3, input_height, input_width))\n    elif IMAGE_ORDERING == 'channels_last':\n        img_input = Input(shape=(input_height, input_width, 3))\n\n    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n               name='block1_conv1', data_format=IMAGE_ORDERING)(img_input)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n               name='block1_conv2', data_format=IMAGE_ORDERING)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool',\n                     data_format=IMAGE_ORDERING)(x)\n    f1 = x\n    # Block 2\n    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n               name='block2_conv1', data_format=IMAGE_ORDERING)(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n               name='block2_conv2', data_format=IMAGE_ORDERING)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool',\n                     data_format=IMAGE_ORDERING)(x)\n    f2 = x\n\n    # Block 3\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='block3_conv1', data_format=IMAGE_ORDERING)(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='block3_conv2', data_format=IMAGE_ORDERING)(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='block3_conv3', data_format=IMAGE_ORDERING)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool',\n                     data_format=IMAGE_ORDERING)(x)\n    f3 = x\n\n    # Block 4\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block4_conv1', data_format=IMAGE_ORDERING)(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block4_conv2', data_format=IMAGE_ORDERING)(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block4_conv3', data_format=IMAGE_ORDERING)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool',\n                     data_format=IMAGE_ORDERING)(x)\n    f4 = x\n\n    # Block 5\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block5_conv1', data_format=IMAGE_ORDERING)(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block5_conv2', data_format=IMAGE_ORDERING)(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block5_conv3', data_format=IMAGE_ORDERING)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool',\n                     data_format=IMAGE_ORDERING)(x)\n    f5 = x\n\n    if pretrained == 'imagenet':\n        VGG_Weights_path = keras.utils.get_file(pretrained_url.split(\"/\")[-1], pretrained_url)\n        Model(img_input, x).load_weights(VGG_Weights_path)\n\n    return img_input, [f1, f2, f3, f4, f5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _unet(n_classes, encoder, l1_skip_conn=True, input_height=416,\n          input_width=608):\n\n    img_input, levels = encoder(\n        input_height=input_height, input_width=input_width)\n    [f1, f2, f3, f4, f5] = levels\n\n    o = f4\n\n    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n    o = (Conv2D(512, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n    o = (BatchNormalization())(o)\n\n    o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n    o = (concatenate([o, f3], axis=MERGE_AXIS))\n    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n    o = (Conv2D(256, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n    o = (BatchNormalization())(o)\n\n    o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n    o = (concatenate([o, f2], axis=MERGE_AXIS))\n    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n    o = (Conv2D(128, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n    o = (BatchNormalization())(o)\n\n    o = (UpSampling2D((2, 2), data_format=IMAGE_ORDERING))(o)\n\n    if l1_skip_conn:\n        o = (concatenate([o, f1], axis=MERGE_AXIS))\n\n    o = (ZeroPadding2D((1, 1), data_format=IMAGE_ORDERING))(o)\n    o = (Conv2D(64, (3, 3), padding='valid', data_format=IMAGE_ORDERING))(o)\n    o = (BatchNormalization())(o)\n\n    o = Conv2D(n_classes, (3, 3), padding='same',data_format=IMAGE_ORDERING)(o)\n\n    model = get_segmentation_model(img_input, o)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def vgg_unet(n_classes, input_height=416, input_width=608, encoder_level=3):\n\n    model = _unet(n_classes, get_vgg_encoder,input_height=input_height, input_width=input_width)\n    model.model_name = \"vgg_unet\"\n    return model\n\nn_classes = 23 # Aerial Semantic Segmentation Drone Dataset tree, gras, other vegetation, dirt, gravel, rocks, water, paved area, pool, person, dog, car, bicycle, roof, wall, fence, fence-pole, window, door, obstacle\n\nmodel = vgg_unet(n_classes=n_classes,  input_height=416, input_width=608)\nmodel_from_name = {}\nmodel_from_name[\"vgg_unet\"] = vgg_unet\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ENTRENAMIENTO VGG UNET**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kaggle_commit = True\n\nepochs = 20\nif kaggle_commit:\n    epochs = 5\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train( \n    train_images =  \"/kaggle/input/semantic-drone-dataset/semantic_drone_dataset/original_images/\",\n    train_annotations = \"/kaggle/input/semantic-drone-dataset/semantic_drone_dataset/label_images_semantic/\",\n    checkpoints_path = \"vgg_unet\" , epochs=epochs\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport time\n%matplotlib inline\n\nstart = time.time()\n\ninput_image = \"/kaggle/input/semantic-drone-dataset/semantic_drone_dataset/original_images/050.jpg\"\nout = model.predict_segmentation(\n    inp=input_image,\n    out_fname=\"out.png\"\n)\n\nfig, axs = plt.subplots(1, 3, figsize=(20, 20), constrained_layout=True)\n\nimg_orig = Image.open(input_image)\naxs[0].imshow(img_orig)\naxs[0].set_title('original image-002.jpg')\naxs[0].grid(False)\n\naxs[1].imshow(out)\naxs[1].set_title('prediction image-out.png')\naxs[1].grid(False)\n\nvalidation_image = \"/kaggle/input/semantic-drone-dataset/semantic_drone_dataset/label_images_semantic/050.png\"\naxs[2].imshow( Image.open(validation_image))\naxs[2].set_title('true label image-002.png')\naxs[2].grid(False)\n\n\ndone = time.time()\nelapsed = done - start","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(elapsed)\nprint(out)\nprint(out.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Entrenamiento FCN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_segmentation.models.fcn import fcn_32_vgg\n\nn_classes = 23 # Aerial Semantic Segmentation Drone Dataset tree, gras, other vegetation, dirt, gravel, rocks, water, paved area, pool, person, dog, car, bicycle, roof, wall, fence, fence-pole, window, door, obstacle\nmodel = fcn_32_vgg(n_classes=n_classes ,  input_height=416, input_width=608  )\n\nmodel.train( \n    train_images =  \"/kaggle/input/semantic-drone-dataset/semantic_drone_dataset/original_images/\",\n    train_annotations = \"/kaggle/input/semantic-drone-dataset/semantic_drone_dataset/label_images_semantic/\",\n    checkpoints_path = \"fcn_32_vgg\" , epochs=epochs\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Entrenamiento Segnet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_segmentation.models.segnet import vgg_segnet\n\nn_classes = 23 # Aerial Semantic Segmentation Drone Dataset tree, gras, other vegetation, dirt, gravel, rocks, water, paved area, pool, person, dog, car, bicycle, roof, wall, fence, fence-pole, window, door, obstacle\nmodel = vgg_segnet(n_classes=n_classes ,  input_height=416, input_width=608  )\n\nmodel.train( \n    train_images =  \"/kaggle/input/semantic-drone-dataset/semantic_drone_dataset/original_images/\",\n    train_annotations = \"/kaggle/input/semantic-drone-dataset/semantic_drone_dataset/label_images_semantic/\",\n    checkpoints_path = \"vgg_segnet\" , epochs=epochs\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediccion","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport time\n%matplotlib inline\n\nstart = time.time()\n\ninput_image = \"/kaggle/input/semantic-drone-dataset/semantic_drone_dataset/original_images/050.jpg\"\nout = model.predict_segmentation(\n    inp=input_image,\n    out_fname=\"out.png\"\n)\n\nfig, axs = plt.subplots(1, 3, figsize=(20, 20), constrained_layout=True)\n\nimg_orig = Image.open(input_image)\naxs[0].imshow(img_orig)\naxs[0].set_title('original image-002.jpg')\naxs[0].grid(False)\n\naxs[1].imshow(out)\naxs[1].set_title('prediction image-out.png')\naxs[1].grid(False)\n\nvalidation_image = \"/kaggle/input/semantic-drone-dataset/semantic_drone_dataset/label_images_semantic/050.png\"\naxs[2].imshow( Image.open(validation_image))\naxs[2].set_title('true label image-002.png')\naxs[2].grid(False)\n\n\ndone = time.time()\nelapsed = done - start\n\nprint(elapsed)\nprint(out)\nprint(out.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}