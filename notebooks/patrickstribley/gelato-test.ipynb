{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gelato Exercise - Predict Test Scores of students.  \n  \n## Predicting the posttest scores of students  \n  \n### 1) Import packages and dataset","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as lines\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\ndf = pd.read_csv('../input/predict-test-scores-of-students/test_scores.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:15:32.775988Z","iopub.execute_input":"2021-09-10T10:15:32.77645Z","iopub.status.idle":"2021-09-10T10:15:34.116946Z","shell.execute_reply.started":"2021-09-10T10:15:32.776347Z","shell.execute_reply":"2021-09-10T10:15:34.115677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2) Overview of data","metadata":{}},{"cell_type":"code","source":"#Return summaries of data and top rows\nprint(df.info())\nprint()\nprint(df.describe())\nprint()\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:15:34.118752Z","iopub.execute_input":"2021-09-10T10:15:34.119095Z","iopub.status.idle":"2021-09-10T10:15:34.172454Z","shell.execute_reply.started":"2021-09-10T10:15:34.119053Z","shell.execute_reply":"2021-09-10T10:15:34.171376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Return unique values of object (categorical) variables\nfor col in df:\n    if df[col].dtype == \"object\":\n        print(col + \" - \" + str(len(np.unique(df[col]))))\n        print(str(np.unique(df[col])))","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:15:34.17458Z","iopub.execute_input":"2021-09-10T10:15:34.175183Z","iopub.status.idle":"2021-09-10T10:15:34.212397Z","shell.execute_reply.started":"2021-09-10T10:15:34.175137Z","shell.execute_reply":"2021-09-10T10:15:34.211147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summary  \nData loaded successfully.  \n8 category variables including student_id.  \n3 continuous variables including the target posttest.  \nNo evidence of any zeros or missing values in the continuous variables. \nNo unexpected values in any categorical variables.  \nCorrect number of unique student_id values.  \n  \nBased on this investigation we will assume that the data is correct, with no missing values and there will be no need for any further cleaning.","metadata":{}},{"cell_type":"markdown","source":"### 2) Initial Data Investigation\nWe will now produce some basic visualisations of the data.\nThis will provide a second check that everything looks in order.\nIt will also give information that could be useful for the model building stage.","metadata":{}},{"cell_type":"code","source":"plot1 = sns.PairGrid(df)\nplot1.map_diag(sns.histplot)\nplot1.map_offdiag(sns.scatterplot)","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:15:34.214625Z","iopub.execute_input":"2021-09-10T10:15:34.215037Z","iopub.status.idle":"2021-09-10T10:15:36.14591Z","shell.execute_reply.started":"2021-09-10T10:15:34.214992Z","shell.execute_reply":"2021-09-10T10:15:36.14492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a strong relationship between pretest and posttest.","metadata":{}},{"cell_type":"code","source":"plot2 = sns.FacetGrid(df, col=\"school\", col_wrap=6)\nplot2.map(sns.histplot, \"posttest\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:15:36.147485Z","iopub.execute_input":"2021-09-10T10:15:36.148087Z","iopub.status.idle":"2021-09-10T10:15:40.907607Z","shell.execute_reply.started":"2021-09-10T10:15:36.148023Z","shell.execute_reply":"2021-09-10T10:15:40.906908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is strong evidence that test results (posttest) is affected by school.\nThe highest performing student in some schools is beaten by the lowest performer in another.","metadata":{}},{"cell_type":"code","source":"plot3 = sns.FacetGrid(df, col=\"school_setting\")\nplot3.map(sns.histplot, \"posttest\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:15:40.908677Z","iopub.execute_input":"2021-09-10T10:15:40.909053Z","iopub.status.idle":"2021-09-10T10:15:41.545695Z","shell.execute_reply.started":"2021-09-10T10:15:40.909023Z","shell.execute_reply":"2021-09-10T10:15:41.544917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot4 = sns.FacetGrid(df, col=\"school_type\")\nplot4.map(sns.histplot, \"posttest\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:15:41.546802Z","iopub.execute_input":"2021-09-10T10:15:41.547258Z","iopub.status.idle":"2021-09-10T10:15:42.034007Z","shell.execute_reply.started":"2021-09-10T10:15:41.547212Z","shell.execute_reply":"2021-09-10T10:15:42.033299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot5 = sns.FacetGrid(df, col=\"classroom\", col_wrap = 12)\nplot5.map(sns.histplot, \"posttest\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:15:42.036414Z","iopub.execute_input":"2021-09-10T10:15:42.036848Z","iopub.status.idle":"2021-09-10T10:16:11.120088Z","shell.execute_reply.started":"2021-09-10T10:15:42.036808Z","shell.execute_reply":"2021-09-10T10:16:11.119262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Classroom has a large number of values.  \nHowever some classrooms do appear to perform significantly better than others.  \n\nI will attempt to confirm later whether the classroom feature is a large number of general types, or the ID of a particular room.","metadata":{}},{"cell_type":"code","source":"plot6 = sns.FacetGrid(df, col=\"teaching_method\")\nplot6.map(sns.histplot, \"posttest\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:16:11.121734Z","iopub.execute_input":"2021-09-10T10:16:11.122169Z","iopub.status.idle":"2021-09-10T10:16:11.653114Z","shell.execute_reply.started":"2021-09-10T10:16:11.122117Z","shell.execute_reply":"2021-09-10T10:16:11.652065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot7 = sns.FacetGrid(df, col=\"gender\")\nplot7.map(sns.histplot, \"posttest\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:16:11.65461Z","iopub.execute_input":"2021-09-10T10:16:11.654895Z","iopub.status.idle":"2021-09-10T10:16:12.167029Z","shell.execute_reply.started":"2021-09-10T10:16:11.654866Z","shell.execute_reply":"2021-09-10T10:16:12.1652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot8 = sns.FacetGrid(df, col=\"lunch\")\nplot8.map(sns.histplot, \"posttest\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:16:12.168523Z","iopub.execute_input":"2021-09-10T10:16:12.1688Z","iopub.status.idle":"2021-09-10T10:16:12.703123Z","shell.execute_reply.started":"2021-09-10T10:16:12.168772Z","shell.execute_reply":"2021-09-10T10:16:12.70217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#crosstab of classroom and school to see which classrooms are in each school\ndf_count = pd.crosstab(df.classroom, df.school)\nprint(\"number of classrooms in each school\")  \n#count the number of classrooms that exist in each school\nprint (df_count[df_count > 1.0].count())\nprint(\"\")\n\n#similar crosstab to see how many schools have each classroom\ndf_count2 = pd.crosstab( df.school,df.classroom,)\nprint(\"max no of schools with each classroom - \" + str(df_count2[df_count2 > 1.0].count().max()))","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:16:12.70495Z","iopub.execute_input":"2021-09-10T10:16:12.705229Z","iopub.status.idle":"2021-09-10T10:16:12.801733Z","shell.execute_reply.started":"2021-09-10T10:16:12.705203Z","shell.execute_reply":"2021-09-10T10:16:12.80066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This confirms that each school has several classrooms, but each classroom only exists within one school.  \nI will assume that classroom is an ID for a unique classroom, not a category of classrooms.  ","metadata":{}},{"cell_type":"markdown","source":"### Summary\nSeveral of the variables appear to have strong relationships with the target posttest.  \nThere are no significant concerns about any of the variable distributions or weightings.  \n","metadata":{}},{"cell_type":"markdown","source":"### 3) Data Preparation\nNormalizing, encoding and splitting out the target variable","metadata":{}},{"cell_type":"code","source":"#Converting student_id to an index\ndf.set_index('student_id', inplace=True)\n\n#Splitting data into X and y\nX_pre = df.drop('posttest', axis=1)\ny_pre = df[['posttest']]\n\n#List of category variables\ncat_list = ['school', 'school_setting', 'school_type', 'classroom', 'teaching_method', 'gender', 'lunch']\n   \n#Split into category and continuous variables\nX_cont = X_pre[X_pre.columns[~X_pre.columns.isin(cat_list)]]\nX_cat = X_pre[cat_list]\n\n#Apply StandardScaler\nX_cont_col = list(X_cont.columns)\nX_scaler = StandardScaler().fit(X_cont)\nX_scale_cont = pd.DataFrame(X_scaler.transform(X_cont))\nX_scale_cont.columns = X_cont_col\n#test = pd.DataFrame(scaler.inverse_transform(X_scale_cont))\n\n#Apply StandardScaler to target variable\ny_scaler = StandardScaler().fit(y_pre)\ny_df = pd.DataFrame(y_scaler.transform(y_pre))\n#Retrieve Series from DataFrame\ny = y_df.iloc[:,0]\ny.columns = ['posttest']\n\n#Apply OneHotEncoder to convert categories into dummies, drop one value when binary to remove redundant columns\nohe = OneHotEncoder(drop = 'if_binary', sparse=False)\nX_encoded_cat = pd.DataFrame(ohe.fit_transform(X_cat), columns=ohe.get_feature_names(X_cat.columns))\n\n#Combine category and continuous feature dataframes\nX = pd.concat([X_scale_cont, X_encoded_cat], axis=1)\n\n#Split into test and train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22, shuffle=True)\nResults  = {}","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:16:12.803415Z","iopub.execute_input":"2021-09-10T10:16:12.803843Z","iopub.status.idle":"2021-09-10T10:16:12.857225Z","shell.execute_reply.started":"2021-09-10T10:16:12.803805Z","shell.execute_reply":"2021-09-10T10:16:12.856114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4)Model Building\n### 4a) Regularised Regression\nWe will use Lasso and Ridge cross-validated regularised regression to predict posttest","metadata":{}},{"cell_type":"markdown","source":"### Lasso Regression","metadata":{}},{"cell_type":"code","source":"#Create the range of possible values for alpha\nlasso_space = np.logspace(-10, 0, 30)\n#Set parameter space for GridSearchCV\nparam_grid = {\"alpha\": lasso_space}\n\n#Instantiate Model\nlassoreg = Lasso(normalize = True, tol = 0.01)\n\n#Create GridSearch Cross Validation with 10 folds\nlassoreg_cv = GridSearchCV(lassoreg, param_grid, cv = 10)\n#Fit to training data\nlassoreg_cv.fit(X_train,y_train)\n\nprint('Best Parameter Set: '+format(lassoreg_cv.best_params_))\nprint('Best Estimator Training Score: '+format(lassoreg_cv.best_score_))\n\n#Best model, test set prediction and scores\nlasso_best = lassoreg_cv.best_estimator_\nlasso_best_pred = lasso_best.predict(X_test)\nlasso_test_score = lasso_best.score(X_test, y_test)\nlasso_best_RMSE = MSE(y_test,lasso_best_pred)**(1/2)\nResults['Lasso'] = [lasso_test_score, lasso_best_RMSE]\n\nprint('Test set RSquared: ' + str(lasso_test_score))\nprint('Test set RMSE: ' + str(lasso_best_RMSE))\nprint('')\n#Check Coefficient List\nlasso_coeffs = pd.DataFrame(lasso_best.coef_, X_train.columns, columns=['Coefficients'])\nprint(lasso_coeffs[(lasso_coeffs.Coefficients != 0)].sort_values(by=['Coefficients'], ascending=False))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:16:12.858766Z","iopub.execute_input":"2021-09-10T10:16:12.859187Z","iopub.status.idle":"2021-09-10T10:16:25.650816Z","shell.execute_reply.started":"2021-09-10T10:16:12.859146Z","shell.execute_reply":"2021-09-10T10:16:25.649598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Simple scatter of performance\nplt.scatter(y_test, lasso_best_pred)\nplt.plot(plt.xlim(), plt.xlim(), linestyle='--', color='k', lw=3, scalex=False, scaley=False)\nplt.xlabel('Actual Score')\nplt.ylabel('Predicted score')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:16:25.656349Z","iopub.execute_input":"2021-09-10T10:16:25.659197Z","iopub.status.idle":"2021-09-10T10:16:25.848488Z","shell.execute_reply.started":"2021-09-10T10:16:25.659138Z","shell.execute_reply":"2021-09-10T10:16:25.847152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lasso Regression Results\nRSquared of 0.958  \nRMSE of 0.204  \nNo evidence of Overfitting.  \nNote that the model has chiefly assigned values to classrooms and some schools, with individual student performance captured through the pretest feature. \nThere is no evidence of non-linear relationships and no major outliers.  \nNo obvious trends or heteroscedacity in the residuals.  \nNo additional transformation of features will be required.","metadata":{}},{"cell_type":"markdown","source":"### Ridge Regression","metadata":{}},{"cell_type":"code","source":"#Create the range of possible values for alpha\nridge_space = np.logspace(-10, 0, 30)\nparam_grid2 = {\"alpha\": ridge_space}\n\n#Instantiate Model\nridgereg = Ridge(normalize = True)\n\n#Create GridSearch Cross Validation with 10 folds\nridgereg_cv = GridSearchCV(ridgereg, param_grid2, cv = 10)\n#Fit to training data\nridgereg_cv.fit(X_train,y_train)\n\n\nprint('Best Parameter Set: '+format(ridgereg_cv.best_params_))\nprint('Best Estimator Training Score: '+format(ridgereg_cv.best_score_))\n\n#Best model, test set prediction and scores\nridge_best = ridgereg_cv.best_estimator_\nridge_best_pred = ridge_best.predict(X_test)\nridge_test_score = ridge_best.score(X_test, y_test)\nridge_best_RMSE = MSE(y_test,ridge_best_pred)**(1/2)\nResults['Ridge'] = [ridge_test_score, ridge_best_RMSE]\n\nprint('Test set RSquared: ' + str(ridge_test_score))\nprint('Test set RMSE: ' + str(ridge_best_RMSE))\n\n#Check Coefficient List\nridge_coeffs = pd.DataFrame(ridge_best.coef_, X_train.columns, columns=['Coefficients'])\nprint(ridge_coeffs[(ridge_coeffs.Coefficients != 0)].sort_values(by=['Coefficients'], ascending=False))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:16:25.849949Z","iopub.execute_input":"2021-09-10T10:16:25.850357Z","iopub.status.idle":"2021-09-10T10:16:33.111484Z","shell.execute_reply.started":"2021-09-10T10:16:25.850312Z","shell.execute_reply":"2021-09-10T10:16:33.110392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(y_test, ridge_best_pred)\nplt.plot(plt.xlim(), plt.xlim(), linestyle='--', color='k', lw=3, scalex=False, scaley=False)\nplt.xlabel('Actual Score')\nplt.ylabel('Predicted score')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:16:33.116462Z","iopub.execute_input":"2021-09-10T10:16:33.119409Z","iopub.status.idle":"2021-09-10T10:16:33.301345Z","shell.execute_reply.started":"2021-09-10T10:16:33.119352Z","shell.execute_reply":"2021-09-10T10:16:33.300404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ridge Regression Results\nRSquared of 0.958  \nRMSE of 0.204  \nNo evidence of Overfitting.  \nPerformance was very slightly better than Lasso.","metadata":{}},{"cell_type":"markdown","source":"### Summary\nLasso and Ridge regularised regression modelled the problem well and produced good accuracy, with no obvious problems.","metadata":{}},{"cell_type":"markdown","source":"### 4b) Tree based regressors\nWe will now test a tree based approach, including ensemble models, to see if they are able to improve on this accuracy.","metadata":{}},{"cell_type":"markdown","source":"### Decision Tree","metadata":{}},{"cell_type":"code","source":"#Create parameter space for Decision Tree Regressor\nparam_grid3 = {'criterion': ['mse'],\n               'min_samples_leaf':[ 0.003, 0.005, 0.01, 0.03],\n               'max_depth':[6,8,10,12,14]}\n\n#Instantiate Regressor\ntreereg = DecisionTreeRegressor(random_state = 22)\n\n#Create Grid Search and fit to training data\ntreereg_cv = GridSearchCV(treereg, param_grid3, cv = 10)\ntreereg_cv.fit(X_train,y_train)\n\nprint('Best Parameter Set: '+format(treereg_cv.best_params_))\nprint('Best Estimator Training Score: '+format(treereg_cv.best_score_))\n\n#Best model, test set prediction and scores\ntree_best = treereg_cv.best_estimator_\ntree_best_pred = tree_best.predict(X_test)\ntree_test_score = tree_best.score(X_test, y_test)\ntree_best_RMSE = MSE(y_test,tree_best_pred)**(1/2)\nResults['Decision Tree'] = [tree_test_score, tree_best_RMSE]\n\nprint('Test set RSquared: ' + str(tree_test_score))\nprint('Test set RMSE: ' + str(tree_best_RMSE))\n\n#Check Variable Importances\ntree_importances = pd.DataFrame(tree_best.feature_importances_, X_train.columns, columns=['Importance'])\nprint(tree_importances[(tree_importances.Importance != 0)].sort_values(by=['Importance'], ascending=False))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:16:33.302789Z","iopub.execute_input":"2021-09-10T10:16:33.303169Z","iopub.status.idle":"2021-09-10T10:16:35.798415Z","shell.execute_reply.started":"2021-09-10T10:16:33.303137Z","shell.execute_reply":"2021-09-10T10:16:35.797381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(y_test, tree_best_pred)\nplt.plot(plt.xlim(), plt.xlim(), linestyle='--', color='k', lw=3, scalex=False, scaley=False)\nplt.xlabel('Actual Score')\nplt.ylabel('Predicted score')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:16:35.799769Z","iopub.execute_input":"2021-09-10T10:16:35.800099Z","iopub.status.idle":"2021-09-10T10:16:35.953284Z","shell.execute_reply.started":"2021-09-10T10:16:35.800069Z","shell.execute_reply":"2021-09-10T10:16:35.952202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree Results  \nRSquared of 0.946  \nRMSE of 0.232  \nSlightly worse than the two Regression models  \nNo trend in the residuals, although the model shows some strata in the predictions due to using a single tree.   \nWe will now try some ensemble techniques to reduce the error.","metadata":{}},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"#Create Parameter Space for Random Forest\nparam_grid4 = {'criterion': ['mse'],\n               'n_estimators':[500, 1000, 1500],\n               'min_samples_leaf':[0.001, 0.003, 0.005],\n               'max_depth':[8,10,12,14],\n               'max_features':['sqrt']}\n\n#Instantiate Forest\nforestreg = RandomForestRegressor(random_state = 22)\n\n#Create Gridsearch for Forest and fit to training data\nforestreg_cv = GridSearchCV(forestreg, param_grid = param_grid4, cv = 3, n_jobs = 1)\nforestreg_cv.fit(X_train,y_train)\n\nprint('Best Parameter Set: '+format(forestreg_cv.best_params_))\nprint('Best Estimator Training Score: '+format(forestreg_cv.best_score_))\n\n#Best model, test set prediction and scores\nforest_best = forestreg_cv.best_estimator_\nforest_best_pred = forest_best.predict(X_test)\nforest_test_score = forest_best.score(X_test, y_test)\nforest_best_RMSE = MSE(y_test,forest_best_pred)**(1/2)\nResults['Random Forest'] = [forest_test_score, forest_best_RMSE]\n\nprint('Test set RSquared: ' + str(forest_test_score))\nprint('Test set RMSE: ' + str(forest_best_RMSE))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:16:35.95465Z","iopub.execute_input":"2021-09-10T10:16:35.954947Z","iopub.status.idle":"2021-09-10T10:20:58.549509Z","shell.execute_reply.started":"2021-09-10T10:16:35.954916Z","shell.execute_reply":"2021-09-10T10:20:58.547906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forest_importances = pd.DataFrame(forest_best.feature_importances_, X_train.columns, columns=['Coefficients'])\nprint(forest_importances[(forest_importances.Coefficients != 0)].sort_values(by=['Coefficients'], ascending=False))\nplt.scatter(y_test, forest_best_pred)\nplt.plot(plt.xlim(), plt.xlim(), linestyle='--', color='k', lw=3, scalex=False, scaley=False)\nplt.xlabel('Actual Score')\nplt.ylabel('Predicted score')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:20:58.551161Z","iopub.execute_input":"2021-09-10T10:20:58.551576Z","iopub.status.idle":"2021-09-10T10:20:58.849973Z","shell.execute_reply.started":"2021-09-10T10:20:58.55153Z","shell.execute_reply":"2021-09-10T10:20:58.849259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest Results  \nRSquared of 0.952  \nRMSE of 0.220  \nThe Random Forest optimises to a high maximum depth, small leaves and a large number of estimators.  \nAlthough performance has improved versus the single tree this is very resource intensive, and is still not as accurate as the Regression models.\n","metadata":{}},{"cell_type":"markdown","source":"### AdaBoost","metadata":{}},{"cell_type":"code","source":"#Create parameter space\nparam_grid5 = {'n_estimators': [500,750],\n                 'learning_rate' : [ 0.1, 0.25, 0.5, 0.75 ],\n                 'loss' : ['square', 'exponential'],\n                 'base_estimator__max_depth' : [6,8,10]\n                }\n#Instantiate decision tree estimator\ntreereg2 = DecisionTreeRegressor( random_state = 22)\n\n#Instantiate AdaBoost\nadareg = AdaBoostRegressor(treereg2)\n\n#Create GridSearch and fit to training data\nadareg_cv = GridSearchCV(adareg, param_grid = param_grid5, cv = 3, n_jobs = 1)\nadareg_cv.fit(X_train,y_train)\n\nprint('Best Parameter Set: '+format(adareg_cv.best_params_))\nprint('Best Estimator Score: '+format(adareg_cv.best_score_))\n\n#Best model, test set prediction and scores\nada_best = adareg_cv.best_estimator_\nada_best_pred = ada_best.predict(X_test)\nada_test_score = ada_best.score(X_test, y_test)\nada_best_RMSE = MSE(y_test,ada_best_pred)**(1/2)\nResults['AdaBoost'] = [ada_test_score, ada_best_RMSE]\n\nprint('Test set RSquared: ' + str(ada_test_score))\nprint('Test set RMSE: ' + str(ada_best_RMSE))","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:20:58.850947Z","iopub.execute_input":"2021-09-10T10:20:58.851251Z","iopub.status.idle":"2021-09-10T10:28:37.884034Z","shell.execute_reply.started":"2021-09-10T10:20:58.851223Z","shell.execute_reply":"2021-09-10T10:28:37.882883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importance of features and plot of performance\nada_importances = pd.DataFrame(ada_best.feature_importances_, X_train.columns, columns=['Importance'])\nprint(ada_importances[(ada_importances.Importance != 0)].sort_values(by=['Importance'], ascending=False))\ny_pred_ada = ada_best.predict(X_test)\nplt.scatter(y_test, y_pred_ada)\nplt.plot(plt.xlim(), plt.xlim(), linestyle='--', color='k', lw=3, scalex=False, scaley=False)\nplt.xlabel('Actual Score')\nplt.ylabel('Predicted score')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:28:37.887156Z","iopub.execute_input":"2021-09-10T10:28:37.887466Z","iopub.status.idle":"2021-09-10T10:28:38.211101Z","shell.execute_reply.started":"2021-09-10T10:28:37.887438Z","shell.execute_reply":"2021-09-10T10:28:38.210131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AdaBoost Results  \nRSquared of 0.949  \nRMSE of 0.227  \nOnce again this ensemble approach was very resource intensive, taking a long time to run, but did not produce better results than previous models.  \n ","metadata":{}},{"cell_type":"markdown","source":"### Stochastic Gradient Boost","metadata":{}},{"cell_type":"code","source":"#Create parameter space\nparam_grid6 = {'n_estimators': [250,500],\n                 'subsample' : [ 0.7, 0.8, 0.9, 1 ],\n                 'max_features' : [0.7,0.8,0.9, 1]\n                }\n\n#Instantiate Regressor\ngbreg = GradientBoostingRegressor( random_state = 22)\n\n#create GridSearch and fit to training data\ngbreg_cv = GridSearchCV(gbreg, param_grid = param_grid6, cv = 3, n_jobs = 1)\ngbreg_cv.fit(X_train,y_train)\n\nprint('Best Parameter Set: '+format(gbreg_cv.best_params_))\nprint('Best Estimator Score: '+format(gbreg_cv.best_score_))\n\n#Best model, test set prediction and scores\ngb_best = gbreg_cv.best_estimator_\ngb_best_pred = gb_best.predict(X_test)\ngb_test_score = gb_best.score(X_test, y_test)\ngb_best_RMSE = MSE(y_test,gb_best_pred)**(1/2)\nResults['Gradient Boost'] = [gb_test_score, gb_best_RMSE]\n\nprint('Test set RSquared: ' + str(gb_test_score))\nprint('Test set RMSE: ' + str(gb_best_RMSE))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:28:38.212454Z","iopub.execute_input":"2021-09-10T10:28:38.212732Z","iopub.status.idle":"2021-09-10T10:29:31.102613Z","shell.execute_reply.started":"2021-09-10T10:28:38.212704Z","shell.execute_reply":"2021-09-10T10:29:31.101426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb_importances = pd.DataFrame(gb_best.feature_importances_, X_train.columns, columns=['Importance'])\nprint(gb_importances[(gb_importances.Importance != 0)].sort_values(by=['Importance'], ascending=False))\ny_pred_gb = gb_best.predict(X_test)\nplt.scatter(y_test, y_pred_gb)\nplt.plot(plt.xlim(), plt.xlim(), linestyle='--', color='k', lw=3, scalex=False, scaley=False)\nplt.xlabel('Actual Score')\nplt.ylabel('Predicted score')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:29:31.104286Z","iopub.execute_input":"2021-09-10T10:29:31.104615Z","iopub.status.idle":"2021-09-10T10:29:31.264745Z","shell.execute_reply.started":"2021-09-10T10:29:31.104585Z","shell.execute_reply":"2021-09-10T10:29:31.263557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stochastic Gradient Boost Results  \nRSquared of 0.952  \nRMSE of 0.219  \nStochastic Gradient Boost was one of the stronger ensemble methods on this data and took less time than some of the others, but still did not outperform the linear regression models.  \nSeveral other Boosting Algorithmns are available, but the performance of these first models doesn't suggest that an exhaustive search would be worthwhile.   ","metadata":{}},{"cell_type":"markdown","source":"### 5) Results  \nRegularised Linear Regression appears to be the best predictor on this dataset, of the methods that I have explored.  \nIt is also simpler and less expensive in time and resource than the ensemble approaches.  \nThere is little to choose between the two regression approaches, but Ridge performed slightly better in this case.\n","metadata":{}},{"cell_type":"code","source":"Models = pd.Series(list(Results.keys()))\nScores = pd.DataFrame(list(Results.values()))\n\nFinal_Results = pd.concat([Models, Scores], axis = 1)\nFinal_Results.columns = ['Model', 'RSquared', 'RMSE']\nprint(Final_Results)\nsns.catplot(data=Final_Results, kind=\"bar\", x=\"RSquared\", y=\"Model\")\nplt.xlim(0.9, 1)\nsns.catplot(data=Final_Results, kind=\"bar\", x=\"RMSE\", y=\"Model\")","metadata":{"execution":{"iopub.status.busy":"2021-09-10T10:29:31.26808Z","iopub.execute_input":"2021-09-10T10:29:31.268397Z","iopub.status.idle":"2021-09-10T10:29:31.821409Z","shell.execute_reply.started":"2021-09-10T10:29:31.268366Z","shell.execute_reply":"2021-09-10T10:29:31.820223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model considerations\nAll the models that I looked at predicted the posttest score with a good level of accuracy.  \nHowever if we were building this model as a project there are other considerations that would need to be included, specifically what decisions the model is intended to inform. \n  \nFor example if you wanted to achieve the best outcome for a pupil my most accurate model would suggest putting them into the best classroom. But if you wanted to improve the outcomes for all of the pupils you couldn't put all two thousand pupils in that classroom.\n  \nSimilarly using the pretest score to predict the posttest score is a good way to get an accurate prediction. But it doesn't have any information on why the scores are high. That feature only tells you that students who got high scores before will likely get high scores again in the future.\n  \nDepending on the specific application it might be necessary to get more data relating to the underlying causes of performance, or possibly accept a lower level of accuracy.\n","metadata":{}},{"cell_type":"markdown","source":"### Next Steps and Builds  \nIf I were to spend more time on this project I think that it could be worthwhile to build my pre-processing as a pipeline, so that each model could be set up independently and could run new data more easily.  \nAs the pre-processing was the same for all the models and there is no additional data, it is less impactful in this case.  \nSimilarly although it is only a few lines of code, the RMSE, scoring and plotting are reused repeatedly and I could wrap them in one or more functions. ","metadata":{}}]}