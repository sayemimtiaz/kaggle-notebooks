{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](http://upload.wikimedia.org/wikipedia/commons/7/7d/American_Sign_Language_ASL.svg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Introduction\n\nThis notebook is aimed at beginners who want a fully explained example of how to build and optimize an image recognition model using TensorFlow.\nIn this notebook I will outline how to process, create and implement a convolutional NN for the MNIST sign language dataset using TensorFlow with Keras. \nI will also include some EDA and provide some code for making nice image visualisations which could be used in a wide range of other image recognition tasks. \nI hope you find this book helpful!\n\n1. Introduction\n2. Data Loading and Preprocessing\n3. EDA and Image Visualisation\n4. Augmentations\n5. Creating A Convolutional Neural Network with TF \n6. Training The Model\n7. Plotting Metrics\n8. Optimizing The Model\n9. Optimized Model Metrics\n10. Model Evaluation\n11. Discussing The Results\n12. Conclusions","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import csv\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Loading and Preprocessing\n\nFirst we need to load the training and testing data from the csv files.\nLets take a look at our column labels to see what data we have:\n\nBoth the training and testing data are stored as 785 columns where the first column 'label' contains numbers 0 - 25 representing letters of the alphabet and the remaining 784 columns contain values (0 - 255) which correspond to pixel intensities for the 28x28 pixel images of each sign language hand gesture.\n\nWe need to seperate the labels (Y) column from the pixel columns (X) for training and evaluation. The 784 pixel values for each example also need to be reshaped into a 28x28 array to construct the images.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Paths to training and testing csv files\nROOT = \"../input/sign-language-mnist/\"\ntest_path = ROOT + \"sign_mnist_test/sign_mnist_test.csv\"\ntrain_path = ROOT + \"sign_mnist_train/sign_mnist_train.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Visulaising the training data column headings\nwith open(train_path) as file:\n    data = csv.reader(file)\n    for i, row in enumerate(data):\n        if i == 0:\n            print(row)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a function to extract all the data we need:\nBoth the train and test csv files can be passed into this to return the labels seperately from the 28 x 28 image arrays.\n\nWe also need to split off part of the training set (20 % of the data) for model validation. This is data that we wont show the model during training to allow us to check the models performing after each training epoch on data it hasnt been trained on. If we used the test set for validation during model optimization, there is a chance we could overfit our model to the test data so we will save the test set for only a final model check after optimization.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracts data from csv files\ndef get_data(file_path):\n    with open(file_path) as file:\n        data = csv.reader(file)\n        labels = []\n        images = []\n        for i, row in enumerate(data):\n            if i == 0:\n                continue\n            labels.append(row[0])\n            image = row[1:]\n            image_array = np.array_split(image, 28)\n            images.append(image_array)\n    return np.array(images).astype(float), np.array(labels).astype(float)\n\n# Extract data from train and test csv files\ntrain_images, train_labels = get_data(train_path)\ntest_images, test_labels = get_data(test_path)\n\n# Create validation data set\nsplit = 0.8\ntrain_split = int(split*len(train_labels))\nvalidation_images = train_images[train_split:]\nvalidation_labels = train_labels[train_split:]\ntrain_images = train_images[:train_split]\ntrain_labels = train_labels[:train_split]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are {} training images of shape {} by {} with {} labels. There are {} validation images of shape {} by {} with {} labels. There are {} test images of shape {} by {} with {} labels\".format(train_images.shape[0], train_images.shape[1], train_images.shape[2], train_labels.shape[0], validation_images.shape[0], validation_images.shape[1], validation_images.shape[2], validation_labels.shape[0], test_images.shape[0], test_images.shape[1], test_images.shape[2], test_labels.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. EDA and Image Visualisation\n\nNow that we have finished processing our data, lets explore it.\n\nFirstly how is the data spread around the different classes i.e. the different hand signs?\nThe bar chart below shows that there isn't the exact same number of images for each class in the train set, but there is still a good number of each. This variation is also reflected in the test set.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Returns a list of unique labels and number of occurances for each\ndef counter(labels):\n    unique, counts = np.unique(labels, return_counts=True)\n    return list(unique.astype(int)), list(counts)\n\nunique_train, counts_train = counter(train_labels)\nunique_test, counts_test = counter(test_labels)\n\n# Plotting bar chart of the counts for training and testing labels\n# bar locations and width\nx = np.arange(len(unique_train))  \nwidth = 0.35  \n\nfig, ax = plt.subplots(figsize=(12,6))\ntest = ax.bar(x - width/2, counts_train, width, label='Train')\ntrain = ax.bar(x + width/2, counts_test, width, label='Test')\n\n# Add text for labels, title and x-axis tick labels\nax.set_ylabel('Data Count', fontsize=12)\nax.set_xlabel('Sign', fontsize=12)\nax.set_title('Quantity of Classes In The Test And Train Sets', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(unique_train)\nax.legend()\n\nfig.tight_layout()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets take a look at the actual training images.\n\nLater we will be using augmentations during training to help artificially increase the training dataset. These are small changes to the exisiting images which help the model to see a greater data range. Visualising the training images will help us decide how to apporach this augmentation process.\nThe figure below shows a selection of training images which are 28x28 arrays of hand gestures. \n\nGenerally the hands are centered in the middle of the image though some do show a small amount of displacement from the very center. This means that adding augmentations like image shift may help to artificially increase our dataset by simulating these shifts. \n\nIf you compare the 3rd and 4th images in the 1st row you can see that they are both labelled '2.0' meaning they are the same hand gesture. However, the hands are rotated at different angles. This means that adding a rotation augmentation might be a good idea.\n\nSome of the hands seem bigger than others, using zoom augmentations may also help with training. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(4, 4, figsize=(16, 16))\n[axi.set_axis_off() for axi in axs.ravel()]\nfor i, image in enumerate(train_images[:16]):\n    a = fig.add_subplot(4, 4, i + 1)\n    plt.imshow(image)\n    a.set_title(train_labels[i])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Augmentations\n\nTo apply augmentations to our data we can use ImageDataGenerator. We will flow our training images through this generator and into the model allowing the chosen augmentations to be randomly performed on the fly. The image augmentations are only done on the training set. For the validation and test sets we will still flow the images through ImageDataGenerator before thay are passed into the model for evaluation but pixel value normalization (from 0 - 255 to 0 - 1) will be the only augmentation.\n\nBelow you can take a look at the augmentations which are achieved by passing just 1 image through the training ImageDataGenerator. The image is flipped, rotated, zoomed and sheared to make it seem like a new, but realistic image each time!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting image dimensions to (X, 28, 28, 1)\ntrain_images = np.expand_dims(train_images, axis=3)\nvalidation_images = np.expand_dims(validation_images, axis=3)\ntest_images = np.expand_dims(test_images, axis=3)\n\n# Training ImageGenerator\ntrain_datagen = ImageDataGenerator(\n    rescale=1/255.,\n    rotation_range=40,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.2,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    fill_mode='nearest')\n\ntrain_gen = train_datagen.flow(train_images, train_labels, batch_size=32)\n\n# Validation ImageGenerator\nvalid_datagen = ImageDataGenerator(\n    rescale=1/255.)\n\nvalid_gen = valid_datagen.flow(validation_images, validation_labels, batch_size=32)\n\n# Testing Image Generator\ntest_datagen = ImageDataGenerator(\n    rescale=1/255.)\n\ntest_gen = test_datagen.flow(test_images, test_labels, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choosing an example image for augmentation\nexample_image = train_images[7:8]\nexample_label = train_labels[7:8]\nexample_gen = train_datagen.flow(example_image, example_label, batch_size=1)\n\n# Plotting a grid of the example image after augmentation\nfig, axs = plt.subplots(4, 4, figsize=(16, 16))\n[axi.set_axis_off() for axi in axs.ravel()]\nfor i in range (16):\n    x_batch, y_batch = next(example_gen)\n    a = fig.add_subplot(4, 4, i + 1)\n    image = x_batch[0]\n    plt.imshow(image[:, :, -1])\n    a.set_title(y_batch[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Creating A Convolutional Neural Network with TF \n\nNow that the data has been sorted into training, validation and testing sets and passed into the ImageDataGenerators we are ready to make a model.\n\nHere I create a convolutional neural network by using keras to define each layer sequentially; you can see more information about sequential models https://keras.io/api/models/sequential/ and different types of layer https://keras.io/api/layers/ here.\n\nIn the first layer it is important to specify the shape of the data being passed in. For our 28x28 greyscale images, this shape is (28, 28, 1).\nWe will start with a fairly simple model: \nI am using just 2 convolutional layers with 64 convolutional filters of dimensions 3x3 and a relu activation each time, though all of these parameters could be tuned. These layers extract image features from the pixel arrays.\nAfter each convolution the data is downsampled using a maxpooling layer with a 2x2 poolsize.\nAfter these convolutional layers the matrix output of the convolutions is flattened into a single array which is passed into densely connected layers which terminate in a 24 node softmax output layer which chooses the most probable label.\n\nTo complete the model it is compiled using Adam as the optimiser and sparse categorical crossentropy and accuracy as the metrics for evaluation. \n\nHere is the final code and a summary of the models structure:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the CNN \nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(26, activation='softmax')\n])\n\n# Compiling the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n# Printing a summary of the model structure\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Training The Model\n\nAnd thats it! We have prepared our data and created the model and now it needs to be trained. \nThe training images are passed into the model and augmented using the train generator and the validation data is passed in also. \nFor this first attempt I am training for 10 epochs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model to the training data\nhistory = model.fit_generator(\n    train_gen,\n    steps_per_epoch=len(train_images)/32,\n    epochs=10, validation_data=valid_gen,\n    validation_steps=len(validation_images)/32\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Plotting Metrics\n\nWe can plot the metrics (loss and accuracy) for both the training and validation set to see how our model progressed during training. \n\nEven on this first attempt, the model did pretty well: both the training and validation accuracy increased well over the sequential epochs.\nNormally we would expect the validation loss to be higher than for the training set, but here it is lower. This means that the model is better at predicting labels for the validation data which it hasn't seen before than it is at predicting labels for the training set which it has seen before. This is most likely caused by the augmentations which are only performed on the training data.\n\nFor both datasets the accuracy curve flattens to show reduced learning improvement near the end of training. After these 10 epochs we ended with about 94% and 99% accuracy for the training and validation sets which is a good first attempt. But could it be improved more?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Metrics\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\n# Plotting accuracy\nplt.figure(figsize=(18, 6))\nplt.subplot(1, 2, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy', fontsize=12)\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch', fontsize=12)\n\n# Plotting loss\nplt.subplot(1, 2, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Loss', fontsize=12)\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Optimizing The Model\n\nHow can we make the model even better?\nNow that we have the full model framework and evaluation process set out, it should be relatively easy for us to try to improve our model. There are quite a few different things that we can try:\n\n1. Adding more convolutional layers to the model - this may allow the model to extract more important image features.\n2. Changing the number of convolutional filters applied per convolutional layer\n3. Adding different model layers like drop out and batch normalisation to help stabilize the learning proccess. \n4. Adding a learning rate optimiser which reduces learning rate as the model converges to help direct it to the loss minimum\n5. Trying different augmentations\n6. Training for longer \n\nIn practice different ideas should be tested in a systematic way over several iterations, but for demonstration purposes I will make a few of these changes at once to the model so we can see how it affects the accuracy and loss. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modified Augmentations\ntrain_datagen2 = ImageDataGenerator(\n    rescale=1/255.,\n    rotation_range=30, #Reduced the rotation range\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.2,\n    zoom_range=0.1,\n    # Removed Flip which might change the meaning of a sign\n    fill_mode='nearest')\n\ntrain_gen2 = train_datagen2.flow(train_images, train_labels, batch_size=64) #Increased batch size\n\n# Modified Model\nmodel2 = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n    tf.keras.layers.BatchNormalization(), # New layer\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.BatchNormalization(),  # New layer\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),  # New layer\n    tf.keras.layers.BatchNormalization(),# New layer\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.1),  # New layer\n    tf.keras.layers.Dense(26, activation='softmax')\n])\n\n# Adding in a learning rate reducer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n\n# Compiling and printing the model\nmodel2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nmodel2.summary()\n\n# Training\nhistory2 = model2.fit_generator(\n    train_gen2, \n    steps_per_epoch=len(train_images)/64,\n    epochs=15, # Training for longer \n    validation_data=valid_gen,\n    validation_steps=len(validation_images)/32, \n    callbacks=[learning_rate_reduction] \n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Optimized Model Metrics\n\nThe accuracy and loss plots for Model2 are very different to the first model. \nThe curves are not as smooth near the start of training but eventually converge to better metric values with > 99% accuracy for both data sets. \nThere is always more optimization and hyperparameter tuning that could be performed but this result is still really good so I will now check this final model by seeing how it performs on the test set which it hasn't seen before.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history2.history['accuracy']\nval_acc = history2.history['val_accuracy']\nloss = history2.history['loss']\nval_loss = history2.history['val_loss']\n\n# Plotting accuracy\nplt.figure(figsize=(18, 6))\nplt.subplot(1, 2, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy', fontsize=12)\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch', fontsize=12)\n\n# Plotting loss\nplt.subplot(1, 2, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Loss', fontsize=12)\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 10. Model Evaluation\nNow that we have optimized our model lets evaluate its performance on the test set. We already created the ImageDataGenerator for this data set so we just need to pass it into the model in evaluation mode:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Evaluate on test data:\")\nresults = model2.evaluate(test_gen, batch_size=len(test_images)/32)\nprint(\"test loss, test acc:\", results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 11. Discussing The Results\n\nThe final model achieved 100% accuracy and only a loss of only 0.0007 on the test set after only one round of model optimization which is a really great result :) \n\nTo explore how this result was achieved we can plot the intermediate representations of the images calculated by each layer of the model. \nThe image below shows how an example image is convoluted to make increasingly abstract representations as it moves through the model layers. This demonstrates how the convolutional model learns image features to identify the object in the image. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a new model that takes an image as input and outputs intermediate representations layers Model2\nsuccessive_outputs = [layer.output for layer in model2.layers]\nvisualization_model = tf.keras.models.Model(inputs = model2.input, outputs = successive_outputs)\n# Pass the example image into the model\nsuccessive_feature_maps = visualization_model.predict(example_gen)\n\n# Plotting the intermdiate image representations\n# Layer names in plot\nlayer_names = [layer.name for layer in model2.layers]\n# Plotting only conv / maxpool/ batchnorm. layers, not Dense layers\nnp.seterr(divide='ignore', invalid='ignore')\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n  if len(feature_map.shape) == 4:\n    # number of features in feature map\n    n_features = feature_map.shape[-1] \n    # Feature map has shape (1, size, size, n_features)\n    size = feature_map.shape[1]\n    display_grid = np.zeros((size, size * n_features))\n    for i in range(n_features):\n      # Visualisations\n      x = feature_map[0, :, :, i]\n      x -= x.mean()\n      x /= x.std()\n      x *= 64\n      x += 128\n      x = np.clip(x, 0, 255).astype('uint8')\n      display_grid[:, i * size : (i + 1) * size] = x\n    # Display the grid\n    scale = 20. / n_features\n    plt.figure(figsize=(scale * n_features, scale))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 12. Conclusions\n\n* In this notebook the MNIST dataset of sign language images was pre-processed and augmented using ImageDataGenerator. \n\n* Data EDA and visualisation was performed.\n\n* A simple CNN was made using tensorflow and trained to acheieve 94% training accuracy and 99% validation accuracy on the MNIST images.\n\n* Methods for improving the model were discussed and some of these were tried to achieve a new model with > 99% accuracy for both the train and validation datasets.\n\n* The models performance on the test set was evaluated to show 100% accuracy - a really great result. \n\nI hope you have enjoyed reading this notebook and will find it helpful in your own work :) \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}