{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=5>**Classification**</font>\n\nIn this notebook I split the given data into train and test set and then used the following classification models and compared their accuracy (from confusion metrix) in predicting the test set:\n<ul>\n<li>Logistic Regression \n<li>Support Vector Machine\n<li>K-Neighbor Classifier\n<li>Decision Tree Classifier\n<li>Random Forest classifier\n<li>Gradient Boost classifier\n<li>Gaussian Naive Bayes\n<li>Ada Boost Classifier\n    </ul>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\n%matplotlib inline\n\ndataset = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ploting distplot for all columns(for checking the distribution of data)\nfig, axs = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in dataset.iloc[:,0:8].items():\n    sns.distplot(v, ax=axs[index])\n    index += 1\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can draw some major conclusion from the above plots and domain knowledge:\n<br>Glucose, BloodPressure, SkinThickness, Insulin, BMI cannot be zero for any person.\n<br>These are missing values replaced with zeros as mentioned in section 3.7 of this paper:**<br>https://www.sciencedirect.com/science/article/pii/S2352914816300016?via%3Dihub#s0050"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing the missing values\ndataset[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n         'BMI']] = dataset[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.NaN)\ndataset.dropna(inplace=True)\n\n# Spliting dataset into features(X) and target(y)\nX = dataset.iloc[:,0:8].values\ny = dataset[['Outcome']].values\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nscalerX = StandardScaler().fit(X)\nX = scalerX.transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size = 0.2, random_state = 89)\nlogreg = LogisticRegression()\nlogreg.fit(X_train1, y_train1.ravel())\ny_pred1 = logreg.predict(X_test1)\ncm1 = confusion_matrix(y_test1, y_pred1)\nacc1 = (cm1[0,0]+cm1[1,1])/79\nprint(\"confusion matrix:\\n\", cm1)\nprint(classification_report(y_test1, y_pred1))\nprint(\"Accuracy:\", acc1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=89)\nsvm = SVC(C=5)\nsvm.fit(X_train2, y_train2.ravel())\ny_pred2 = svm.predict(X_test2)\ncm2 = confusion_matrix(y_test2, y_pred2)\nacc2 = (cm2[0,0]+cm2[1,1])/79\nprint(\"confusion matrix:\\n\", cm2)\nprint(classification_report(y_test2, y_pred2))\nprint(\"Accuracy:\", acc2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding kvalue\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.2, random_state=89)\ntraining_accuracy = []\ntest_accuracy = []\nneighbors_settings = range(1, 11)\nfor n_neighbors in neighbors_settings:\n    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn.fit(X_train3, y_train3.ravel())\n    training_accuracy.append(knn.score(X_train3, y_train3))\n    test_accuracy.append(knn.score(X_test3, y_test3))\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=9) #Kvalue 9 from above graph\nknn.fit(X_train3, y_train3.ravel())\ny_pred3 = knn.predict(X_test3)\ncm3 = confusion_matrix(y_test3, y_pred3)\nacc3 = (cm3[0,0]+cm3[1,1])/79\nprint(\"confusion matrix:\\n\", cm3)\nprint(classification_report(y_test3, y_pred3))\nprint(\"Accuracy:\", acc3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=0.2, random_state=89)\ntree = DecisionTreeClassifier(max_depth=1)\ntree.fit(X_train4, y_train4.ravel())\ny_pred4 = tree.predict(X_test4)\ncm4 = confusion_matrix(y_test4, y_pred4)\nacc4 = (cm4[0,0]+cm4[1,1])/79\nprint(\"confusion matrix:\\n\", cm4)\nprint(classification_report(y_test4, y_pred4))\nprint(\"Accuracy:\", acc4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=0.2, random_state=89)\nrf = RandomForestClassifier(max_depth=3, random_state=15)\nrf.fit(X_train5, y_train5.ravel())\ny_pred5 = rf.predict(X_test5)\ncm5 = confusion_matrix(y_test5, y_pred5)\nacc5 = (cm5[0,0]+cm5[1,1])/79\nprint(\"confusion matrix:\\n\", cm5)\nprint(classification_report(y_test5, y_pred5))\nprint(\"Accuracy:\", acc5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=0.2, random_state=89)\ngb = GradientBoostingClassifier(learning_rate=0.01, max_depth=4)\ngb.fit(X_train6, y_train6.ravel())\ny_pred6 = gb.predict(X_test6)\ncm6 = confusion_matrix(y_test6, y_pred6)\nacc6 = (cm6[0,0]+cm6[1,1])/79\nprint(\"confusion matrix:\\n\", cm6)\nprint(classification_report(y_test6, y_pred6))\nprint(\"Accuracy:\", acc6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=0.2, random_state=89)\ngnb = GaussianNB()\ngnb.fit(X_train7, y_train7.ravel())\ny_pred7 = gnb.predict(X_test7)\ncm7 = confusion_matrix(y_test7, y_pred7)\nacc7 = (cm7[0,0]+cm7[1,1])/79\nprint(\"confusion matrix:\\n\", cm7)\nprint(classification_report(y_test7, y_pred7))\nprint(\"Accuracy:\", acc7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=0.2, random_state=89)\nada = AdaBoostClassifier(learning_rate=0.01, n_estimators=30)\nada.fit(X_train8, y_train8.ravel())\ny_pred8 = ada.predict(X_test8)\ncm8 = confusion_matrix(y_test8, y_pred8)\nacc8 = (cm8[0,0]+cm8[1,1])/79\nprint(\"confusion matrix:\\n\", cm8)\nprint(classification_report(y_test8, y_pred8))\nprint(\"Accuracy:\", acc8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot of accuracy of each type of classifier\nacc=np.array([acc1, acc2, acc3, acc4, acc5, acc6, acc7, acc8])\nx=np.array(['Logistic Regression', 'Support Vector Machine', 'K-Neighbor Classifier', 'Decision Tree Classifier',\n            'Random Forest classifier', 'Gradient Boost classifier', 'Gaussian Naive Bayes', 'Ada Boost Classifier'])\nplt.scatter(x,acc,s=200,color=['cyan','green','red','yellow','black','magenta','blue','orange'])\nplt.xticks(rotation=90)\nplt.ylabel('Accuracy')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}