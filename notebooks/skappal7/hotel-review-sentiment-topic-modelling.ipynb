{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.express as px\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\nfrom wordcloud import WordCloud,STOPWORDS\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-21T17:17:02.856209Z","iopub.execute_input":"2021-07-21T17:17:02.856634Z","iopub.status.idle":"2021-07-21T17:17:06.36736Z","shell.execute_reply.started":"2021-07-21T17:17:02.856596Z","shell.execute_reply":"2021-07-21T17:17:06.366213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Objective of the Analysis**","metadata":{}},{"cell_type":"markdown","source":"This end to end sentiment analysis and prediction routine is based on TripAdvisor's hotel reviews, this data set has two columns Review and Ratings. The objective of this analysis cum prediction routine is to identify sentiments for the reviews posted by various customers using the NLTK Vader Sentiment Analyser and later create a model to predict the sentiment scores based on the input text. \n\nWe will also use a low code library named PyCaret to perform topic modeling to better understand the leading topic models within this corpus.\n\nSo let's get started!","metadata":{}},{"cell_type":"markdown","source":"# **Let's Understand What is Sentiment Analysis?**","metadata":{}},{"cell_type":"markdown","source":"Sentiment analysis is a method of identifying sentiment from a piece of text, it entails the process of text classification into a positive, negative or a neutral emotions leveragin various analytical techniques.","metadata":{}},{"cell_type":"markdown","source":"# **Why it is so important for academicians and organizations to perform sentiment analysis?**","metadata":{}},{"cell_type":"markdown","source":"Sentiment analysis is essential for businesses and academai alike to better understand customers emotions. \n\n**Importance of SA from a business perspective:** You have just launched a new range of products and you want to identify the areas of opportunities to further enhance the product and to do that Sentiment Analysis can come in quite handy to identify those granular level of details to understand the product improvement opportunities relative to the sentiment of the customer. \n\n**From the perspective of academia:** Analysing students' feedback using sentiment analysis techniques can identify the students' positive or negative feelings, or even more reï¬ned emotions, that students have towards the current teaching.\n\nThere are otherways to slice and dice the data to get down to finer insights by utilizing demography, goegraphy and timestamp data and make valuable business decisions. ","metadata":{}},{"cell_type":"markdown","source":"**SENTIMENT ANALYSIS PROCESS FLOW** *(SOURCE: DATACAMP)*\n\n![](https://cdn-images-1.medium.com/max/361/0*ga5rNPmVYBsCm-lz.)","metadata":{}},{"cell_type":"markdown","source":"**Step 1: Let's read the data**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/trip-advisor-hotel-reviews/tripadvisor_hotel_reviews.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:17:06.369232Z","iopub.execute_input":"2021-07-21T17:17:06.369718Z","iopub.status.idle":"2021-07-21T17:17:06.792871Z","shell.execute_reply.started":"2021-07-21T17:17:06.369673Z","shell.execute_reply":"2021-07-21T17:17:06.791664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Step 2. EDA**","metadata":{}},{"cell_type":"markdown","source":"Now, we will take a look at the variable â€œRatingâ€ to see if majority of the customer ratings are positive or negative.","metadata":{}},{"cell_type":"code","source":"sns.countplot(x= df['Rating'])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:17:06.795467Z","iopub.execute_input":"2021-07-21T17:17:06.795902Z","iopub.status.idle":"2021-07-21T17:17:06.980916Z","shell.execute_reply.started":"2021-07-21T17:17:06.795858Z","shell.execute_reply":"2021-07-21T17:17:06.979717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above graph, we can see that most of the customer rating is within the positive zone (high = 4-5). This leads us to believe that most reviews will be pretty positive too, which will be analyzed in a while.Now, we can create some wordclouds to see the most frequently used words in the reviews.","metadata":{}},{"cell_type":"markdown","source":"# **Sentiment Analysis**","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:17:06.983031Z","iopub.execute_input":"2021-07-21T17:17:06.983392Z","iopub.status.idle":"2021-07-21T17:17:07.151561Z","shell.execute_reply.started":"2021-07-21T17:17:06.983355Z","shell.execute_reply":"2021-07-21T17:17:07.150046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Generating Sentiment Scores using Vader Sentiment Analyzer**","metadata":{}},{"cell_type":"code","source":"df['scores'] = df['Review'].apply(lambda hotel_overview: sid.polarity_scores(str(hotel_overview)))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:17:07.153721Z","iopub.execute_input":"2021-07-21T17:17:07.154181Z","iopub.status.idle":"2021-07-21T17:17:42.152491Z","shell.execute_reply.started":"2021-07-21T17:17:07.154134Z","shell.execute_reply":"2021-07-21T17:17:42.151758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating Compound Scores along with the Sentiment Description for further analysis**","metadata":{}},{"cell_type":"markdown","source":"Let's take a step back to better understand how the compound scores are calculated in Vader\n\nWhat does **VADER** stands for?\n\n**V**alence **A**ware **D**ictionary and s**E**ntiment **R**easoner\n\n**What is a compound score and how it is calculated?**\n\nThe compound score is the sum of positive, negative & neutral scores which is then normalized between **-1(most extreme negative)** and **+1 (most extreme positive)**.\n**The more Compound score closer to +1**, the higher the positivity of the text. These scores are calculated based on the Valence scores for the words in a sentence.\n\n**What is a Valence Score?**\n\nIt is a score assigned to the word under consideration by means of observation and experiences rather than pure logic.\n\nConsider the words 'terrible' , 'hopeless', 'miserable'. Any self-aware Human would easily gauge the sentiment of these words as Negative.\n\nWhile on the other side, words like 'marvellous', 'worthy', 'adequate' are signifying positive sentiment.\n\nAccording to the academic paper on VADER, the Valence score is measured on a scale from -4 to +4, where -4 stands for the most â€˜Negativeâ€™ sentiment and +4 for the most â€˜Positiveâ€™ sentiment. Intuitively one can guess that midpoint 0 represents â€˜Neutralâ€™ Sentiment, and this is how it is defined actually too.","metadata":{}},{"cell_type":"code","source":"df['compound'] = df['scores'].apply(lambda score_dict: score_dict['compound'])\ndf['sentiment_type']=''\ndf.loc[df.compound>0,'sentiment_type']='POSITIVE'\ndf.loc[df.compound==0,'sentiment_type']='NEUTRAL'\ndf.loc[df.compound<0,'sentiment_type']='NEGATIVE'\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:17:42.153541Z","iopub.execute_input":"2021-07-21T17:17:42.153945Z","iopub.status.idle":"2021-07-21T17:17:42.207625Z","shell.execute_reply.started":"2021-07-21T17:17:42.153916Z","shell.execute_reply":"2021-07-21T17:17:42.206562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sentiment_type.value_counts().plot(kind='bar',title=\"sentiment analysis\")","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:17:42.211079Z","iopub.execute_input":"2021-07-21T17:17:42.211405Z","iopub.status.idle":"2021-07-21T17:17:42.392157Z","shell.execute_reply.started":"2021-07-21T17:17:42.211371Z","shell.execute_reply":"2021-07-21T17:17:42.39122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above graph we can clearl see that most of the hotel reviews have a positive sentiments compared to negative and neurtal sentiments are quite low,hence,it will be agood idea to focus only on positive and negative set of reviews for further analysis. \n\nI will also reduce the data set by including only the review, sentiment type and compound score cols. ","metadata":{}},{"cell_type":"code","source":"data = df[['Review','sentiment_type','compound', 'Rating']]\ndata","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:17:42.394659Z","iopub.execute_input":"2021-07-21T17:17:42.394982Z","iopub.status.idle":"2021-07-21T17:17:42.417819Z","shell.execute_reply.started":"2021-07-21T17:17:42.39495Z","shell.execute_reply":"2021-07-21T17:17:42.416825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nNow let's split the data into training and a testing sets. The test set is the 10% of the original dataset. For this particular analysis I dropped reviews with neutral sentiment, as the reviews for neutral are almost none and may not have a great impact on the over all dataset. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split # function for splitting data to train and test sets\ntrain, test = train_test_split(data,test_size = 0.1)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:17:42.419541Z","iopub.execute_input":"2021-07-21T17:17:42.419837Z","iopub.status.idle":"2021-07-21T17:17:42.430271Z","shell.execute_reply.started":"2021-07-21T17:17:42.419808Z","shell.execute_reply":"2021-07-21T17:17:42.429328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing neutral sentiments\ntrain = train[train.sentiment_type != \"NEUTRAL\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:17:42.431869Z","iopub.execute_input":"2021-07-21T17:17:42.432381Z","iopub.status.idle":"2021-07-21T17:17:42.446754Z","shell.execute_reply.started":"2021-07-21T17:17:42.432334Z","shell.execute_reply":"2021-07-21T17:17:42.445534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a next step I separated the Positive and Negative tweets of the training set in order to easily visualize their contained words. After that I cleaned the text. Now they were ready for a WordCloud visualization which shows only the most emphatic words of the Positive and Negative tweets.","metadata":{}},{"cell_type":"code","source":"train_pos = train[ train['sentiment_type'] == 'POSITIVE']\ntrain_pos = train_pos['Review']\ntrain_neg = train[ train['sentiment_type'] == 'NEGATIVE']\ntrain_neg = train_neg['Review']\n\ndef wordcloud_draw(data, color = 'black'):\n    words = ' '.join(data)\n    cleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and not word.startswith('#')\n                                and word != 'RT'\n                            ])\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color=color,\n                      width=2500,\n                      height=2000\n                     ).generate(cleaned_word)\n    plt.figure(1,figsize=(13, 13))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n    \nprint(\"Positive words\")\nwordcloud_draw(train_pos,'white')\nprint(\"Negative words\")\nwordcloud_draw(train_neg)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:17:42.448475Z","iopub.execute_input":"2021-07-21T17:17:42.448883Z","iopub.status.idle":"2021-07-21T17:18:21.423971Z","shell.execute_reply.started":"2021-07-21T17:17:42.448838Z","shell.execute_reply":"2021-07-21T17:18:21.422707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interesting to notice the following words and expressions in the positive word set: good, room, people, lovely, nice, restaurant\n\nMy interpretation relted to these words are, in general customers had a positive experience as their rooms were well appointed and taken care of by good people.\n\nAt the same time, negative reviews contains words like: bad, clean, toilets, bathroom, reservation, paid, disappointed, problem\n\nMy interpertation about these words are that overall the experience was good (that's why high positive sentiment), however, cleanliness of toilets and washrooms could have caused bad experience and at the same time problematic reservation experience may have disappointed the customers.\n\nStop Word: Stop Words are words which do not contain important significance to be used in Search Queries. Usually these words are filtered out from search queries because they return vast amount of unnecessary information. ( the, for, this etc. )","metadata":{}},{"cell_type":"markdown","source":"Now that we have performed some basic EDA on the data, we are ready for the next step that is Topic Modelling. From this point onwards I will be using the original data set for the remainder of the predictive analysis cycle.","metadata":{}},{"cell_type":"markdown","source":"# **Get the Data Ready**","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:18:21.425233Z","iopub.execute_input":"2021-07-21T17:18:21.425556Z","iopub.status.idle":"2021-07-21T17:18:21.438642Z","shell.execute_reply.started":"2021-07-21T17:18:21.425524Z","shell.execute_reply":"2021-07-21T17:18:21.437588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **PyCaret the ML Workhorse ðŸ´**","metadata":{}},{"cell_type":"markdown","source":"For this phase of the analysis I will be utilizing PyCaret's capabilities to perform NLP routine!","metadata":{}},{"cell_type":"code","source":"pip install --upgrade pycaret-nightly","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:18:21.440208Z","iopub.execute_input":"2021-07-21T17:18:21.440566Z","iopub.status.idle":"2021-07-21T17:19:15.588275Z","shell.execute_reply.started":"2021-07-21T17:18:21.440535Z","shell.execute_reply":"2021-07-21T17:19:15.586657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's Setup the Environment\n\nSetting up the nlp environment entails the following actions, automagically performed!\n\n**Removing Numeric Characters:** All numeric characters are removed from the text. They are replaced with blanks.\n\n**Removing Special Characters:** All non-alphanumeric special characters are removed from the text. They are also replaced with blanks.\n\n**Word Tokenization:** Word tokenization is the process of splitting a large sample of text into words.\n\n**Stopword Removal:** A stop word (or stopword) is a word that is often removed from text because it is common and provides little value for information retrieval, even though it might be linguistically meaningful. Example of such words in english language are: \"the\", \"a\", \"an\", \"in\" etc.\n\n**Bigram Extraction:** A bigram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words.\n\n**Trigram Extraction:** Similar to bigram extraction, trigram is a sequence of three adjacent elements from a string of tokens.\n\n**Lemmatizing:** Lemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single word, identified by the word's lemma, or dictionary form. In English language, word appears in several inflected forms. For example the verb 'to walk' may appear as 'walk', 'walked', 'walks', 'walking'. The base form, 'walk', that one might look up in a dictionary, is called the lemma for the word.\n\n**Custom Stopwords:** We didn't use it but this option lets the user define specific words that they want to exclude from the text.","metadata":{}},{"cell_type":"code","source":"from pycaret.nlp import *\nexp_name = setup(data = data, target = 'Review')","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:19:15.590346Z","iopub.execute_input":"2021-07-21T17:19:15.590703Z","iopub.status.idle":"2021-07-21T17:23:11.063789Z","shell.execute_reply.started":"2021-07-21T17:19:15.590666Z","shell.execute_reply":"2021-07-21T17:23:11.062895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda = create_model('lda', num_topics = 6, multi_core = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:23:11.064887Z","iopub.execute_input":"2021-07-21T17:23:11.065307Z","iopub.status.idle":"2021-07-21T17:25:30.471756Z","shell.execute_reply.started":"2021-07-21T17:23:11.065269Z","shell.execute_reply":"2021-07-21T17:25:30.470245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_top = assign_model(lda)\nlda_top.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:25:30.473922Z","iopub.execute_input":"2021-07-21T17:25:30.47442Z","iopub.status.idle":"2021-07-21T17:25:59.668364Z","shell.execute_reply.started":"2021-07-21T17:25:30.474358Z","shell.execute_reply":"2021-07-21T17:25:59.66729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(lda)","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:25:59.669856Z","iopub.execute_input":"2021-07-21T17:25:59.670175Z","iopub.status.idle":"2021-07-21T17:25:59.724236Z","shell.execute_reply.started":"2021-07-21T17:25:59.670144Z","shell.execute_reply":"2021-07-21T17:25:59.723274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(lda,'topic_model')","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:25:59.725612Z","iopub.execute_input":"2021-07-21T17:25:59.725908Z","iopub.status.idle":"2021-07-21T17:26:19.53726Z","shell.execute_reply.started":"2021-07-21T17:25:59.725878Z","shell.execute_reply":"2021-07-21T17:26:19.536035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LDAvis** = **L**atent **D**irichlet **A**llocation **Vis**ualization ðŸ‘†\n\n**LDAvis** tool helps to create an interactive web-based visualization of a topic model that has been fit to a corpus of text data using Latent Dirichlet Allocation (LDA). \n\nGiven the estimated parameters of the topic model, it computes various summary statistics as input to an interactive visualization built with D3.js that is accessed via a browser. The goal is to help users interpret the topics in their LDA topic model.","metadata":{}},{"cell_type":"markdown","source":"# **Saving the Model ðŸ’¾**","metadata":{}},{"cell_type":"code","source":"save_model(lda,'Final LDA Model 07212020')","metadata":{"execution":{"iopub.status.busy":"2021-07-21T17:26:19.539093Z","iopub.execute_input":"2021-07-21T17:26:19.539466Z","iopub.status.idle":"2021-07-21T17:26:20.584278Z","shell.execute_reply.started":"2021-07-21T17:26:19.539412Z","shell.execute_reply":"2021-07-21T17:26:20.582812Z"},"trusted":true},"execution_count":null,"outputs":[]}]}