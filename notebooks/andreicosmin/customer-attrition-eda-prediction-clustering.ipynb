{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(url = 'https://atrium.ai/wp-content/uploads/elementor/thumbs/real_cost_retention-ooi87zuk2wbz6qkh6nglnfwpm2vkbw4t3idvdyf3bc.jpg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Summary</h3> Around 20% of the bank's customers in this dataset have exited. Methods for predicting customer attrition are devised, achieving a 0.702, 0.77 roc area under curve score respectively. Age is the most important feature, being positively correlated to the probability that a customer will exit. Three customer groups are identified, one of which is 14% more likely to exit than the mean. This document provides a framework for classifying the customers. Further data has the potential to significantly improve the efficiency of the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/predicting-churn-for-bank-customers/Churn_Modelling.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The dataset contains 10,000 bank customers, with 13 independent variables and 1 dependent variable: whether they exited the bank or not. In this document, predictive analysis is leveraged in order to predict future customer behaviour and inform decisions to arrest the attrition of bank clients."},{"metadata":{},"cell_type":"markdown","source":"## 1. Data cleaning"},{"metadata":{},"cell_type":"markdown","source":"Before performing predictive analysis, data must be cleaned and formatted. 'RowNumber','CustomerId','Surname' are not relevant in predictive analysis, and are therefore dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the first three columns, as they are not relevant in predictive analysis:\ndf.drop(labels=['RowNumber','CustomerId','Surname'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some variables that should be categorical objects are integers: HasCrCard, IsActiveMember and Exited. \nTenure and NumOfProducts are ordinal variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['HasCrCard', 'IsActiveMember']: \n    df[col] = df[col].astype('object')\nprint(df.dtypes) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Exploratory Data Analysis (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.countplot(df.Exited,palette=['#D7263D','#27FB6B'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Ratio of customers who exited: {(df.Exited==1).sum()/len(df):.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_data=['Geography', 'Gender', 'Tenure','NumOfProducts', 'HasCrCard', \n                  'IsActiveMember']\nq=1\nplt.figure(figsize=(16,12))\n# Plot a grid with count plots of all categorical variables\nfor j in fig_data:\n    plt.subplot(2,3,q)\n    ax=sns.countplot(df[j],hue=df.Exited, palette=['#D7263D','#27FB6B'])\n    plt.xlabel(j)\n    q+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"German citizens have the highest rates of attrition among the 3 countries. Although there are more males than females in the dataset, the latter were responsible for more exits.\n\nPeople who purchased 2 products were less likely to exit than those who bought only one. Interestingly, the rates of churn of people with 3+ products are very high.\n\nNon-active members were much more likely to exit than active ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nA heat map showing the correlation between variables\n'''\n\n# Get dummies of the non-binary categorical variables\ngender_dummies = pd.get_dummies(df.Gender,drop_first=True,dtype='int32')\ngeography_dummies = pd.get_dummies(df.Geography,drop_first=True,dtype='int32')\n\n# Initialize a new data frame\ndf_new=pd.DataFrame()\n# Loop through all columns\nfor col in df.columns:\n    # If data type is not float, add the categorical variable\n    if df[col].dtype!='float64':\n        # If non-binary, add the dummies columns\n        if col == 'Gender':\n            df_new['Male'] = gender_dummies\n        elif col =='Geography':\n            df_new=pd.concat([df_new,geography_dummies],axis=1)\n        # If binary category, add the column as an integer data type  \n        else:\n            df_new[col] = df[col].astype('int32')\n    # If data type is float, simply add it to the new data frame\n    else:\n        df_new[col] = df[col]\n\n# Get the correlation matrix and plot it\ncorr = df_new.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age is the feature most positively correlated with churn status (~0.25).\n\nGermany customers appear to be significantly positiely correlated with a higher balance (~0.5)."},{"metadata":{},"cell_type":"markdown","source":"## 3. Predictive Analysis on all Customers"},{"metadata":{},"cell_type":"markdown","source":"Categorical and ordinal data is encoded, and continous data standardised."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n\n# Column Transformer to scale numerical data, and encode categorical non-binary columns\nct = ColumnTransformer([\n     (\"scaling\", StandardScaler(), ['CreditScore', 'Age','Balance','EstimatedSalary',\n                                   'Tenure','NumOfProducts']),\n     (\"onehot\", OneHotEncoder(sparse=False,drop='if_binary'), ['Gender', 'Geography'])\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save a series of the target variable\ndata_features = df.drop('Exited', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is split into train and test sets, using the default shuffling and 3:1 ratio."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split data into training and test datasets\nX_train, X_test, y_train, y_test = train_test_split(data_features, df.Exited)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, GridSearchCV is performed on Random Forest Classifier, in order to find the most optimal hyper parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nPerform grid search to find best the parameters for\nRandomForestClassifier\n'''\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\n# Define the parameter grid\nparam_grid = {\n   'clf__max_features': ['sqrt', 'log2'],\n    'clf__n_estimators': [100, 200],\n    'clf__max_depth': [3, 5, None], \n}\n\n# Set up the pipeline\npipe = Pipeline([\n    ('preprocess', ct),\n    ('clf',RandomForestClassifier())\n])\n\n# Grid search, using recall as the score to maximise\ngrid=GridSearchCV(estimator=pipe, param_grid=param_grid,cv=10,\n                  scoring='recall_macro',return_train_score=True,\n                  verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the grid search on the training data\ngrid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best model has no maximum depth, uses sqrt(n_features) when looking for best split, and has a maximum of 200 estimators (Decision Trees). For more info on the RFC: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The recall of the best model is 70.2%"},{"metadata":{},"cell_type":"markdown","source":"The next step is to assess the model's performance on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the model on the test data\ntest_predictions = grid.best_estimator_.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\ntest_accuracy = accuracy_score(y_test, test_predictions)\ntest_precision = precision_score(y_test, test_predictions)\ntest_recall = recall_score(y_test, test_predictions)\ntest_f1_score = f1_score(y_test, test_predictions)\ntest_roc_auc_score = roc_auc_score(y_test, test_predictions)\n\nprint(\"Accuracy on test data: {:.3f}%\".format(test_accuracy*100))\nprint(\"Precision on test data: {:.3f}%\".format(test_precision*100))\nprint(\"Recall on test data: {:.3f}%\".format(test_recall*100))\nprint(\"F1 Score on test data: {:.3f}%\".format(test_f1_score*100))\nprint(\"AUC Score on test data: {:.3f}\".format(test_roc_auc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even though the accuracy is relatively high (84.16%), recall/sensitivity is fairly low, meaning that out of the total number of customers who exited, only 41.602% are identified."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ndef get_conf_matrix(y_test, y_pred):    \n    # Get confusion matrix\n    data = confusion_matrix(y_test, y_pred) \n    # Build the confusion matrix as a dataframe table\n    cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test)) \n    cm.index.name = 'Observed'\n    cm.columns.name = 'Predicted'\n    plt.figure(figsize = (10,7))\n    # Plot a heatmap\n    sns.heatmap(cm, annot=True, fmt=\"d\", annot_kws={\"size\": 12}) \n    plt.title(\"Confusion Matrix\")\n    plt.show()\nget_conf_matrix(y_test, test_predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Feature Importance"},{"metadata":{},"cell_type":"markdown","source":"Let's look at the feature importance, which variables are the best independent predictors:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.inspection import permutation_importance\n# Obtain feature importance\nr = permutation_importance(grid.best_estimator_, df, df.Exited.astype('int32'), \n                           n_repeats=10, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the mean importance and the margin of error, for each variable\nfor i in r.importances_mean.argsort()[::-1]:\n    print(f\"{df.columns[i]:<28}\"\n          f\"{r.importances_mean[i]:.3f}\"\n          f\" +/- {r.importances_std[i]:.3f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age is the best predictor, followed by Number of Products."},{"metadata":{},"cell_type":"markdown","source":"## 5. Customer Clusters"},{"metadata":{},"cell_type":"markdown","source":"In this section, unsupervised machine learning is leveraged in order to cluster the customers into different categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform the train data using the previously defined metrics\nct.fit(X_train)\nX_train_trans = ct.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting a linkage matrix of the transformed data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import linkage\n\nlinkage_matrix = linkage(X_train_trans, method='complete', metric='euclidean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hierarchical Clustering Dendogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import dendrogram, set_link_color_palette\n\n'''\n'''\ndef fancy_dendrogram(*args, **kwargs):\n    max_d = kwargs.pop('max_d', None)\n    if max_d and 'color_threshold' not in kwargs:\n        kwargs['color_threshold'] = max_d\n    annotate_above = kwargs.pop('annotate_above', 0)\n\n    ddata = dendrogram(*args, **kwargs)\n\n    if not kwargs.get('no_plot', False):\n        plt.title('Hierarchical Clustering Dendrogram (truncated)')\n        plt.xlabel('sample index or (cluster size)')\n        plt.ylabel('distance')\n        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n            x = 0.5 * sum(i[1:3])\n            y = d[1]\n            if y > annotate_above:\n                plt.plot(x, y, 'o', c=c)\n                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n                             textcoords='offset points',\n                             va='top', ha='center')\n        if max_d:\n            plt.axhline(y=max_d, c='k')\n    return ddata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reference: [SciPy Hierarchical Clustering and Dendrogram Tutorial](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,7))\nfancy_dendrogram(\n    linkage_matrix,\n    truncate_mode='lastp',\n    p=8,\n    leaf_rotation=90.,\n    leaf_font_size=12.,\n    show_contracted=True,\n    annotate_above=6,\n)\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph above shows hierarchically clustered groups of customers, based on the distance between them. Each black dot is an individual person."},{"metadata":{},"cell_type":"markdown","source":"Elbow Plot is drawn to determine the optimal number of clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"last = linkage_matrix[-10:, 2]\nlast_rev = last[::-1]\nidxs = np.arange(1, len(last) + 1)\nplt.plot(idxs, last_rev)\n\n# 2nd derivative of the distances\nacceleration = np.diff(last, 2)  \nacceleration_rev = acceleration[::-1]\nplt.plot(idxs[:-2] + 1, acceleration_rev)\nplt.show()\n# If idx 0 is the max of this we want 2 clusters\nk = acceleration_rev.argmax() + 2  \nprint (\"clusters:\", k)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reference: [SciPy Hierarchical Clustering and Dendrogram Tutorial](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)"},{"metadata":{},"cell_type":"markdown","source":"Creating 4 flat clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import fcluster\nclusters_smaller  = fcluster(linkage_matrix, 4, criterion='maxclust')\n# Show the counts of each cluster\nnp.unique(clusters_smaller, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Customer Profiles Analysis"},{"metadata":{},"cell_type":"markdown","source":"Add back the dependent variable, and separate the three customer profiles"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clusters=pd.concat([X_train,y_train],axis=1)\n\nGroup1 = df_clusters[clusters_smaller == 1]\nGroup2 = df_clusters[clusters_smaller == 2]\nGroup3 = df_clusters[clusters_smaller == 3]\nGroup4 = df_clusters[clusters_smaller == 4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Group2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count Plots of customers who exited, for each group\nf, axes = plt.subplots(1, 4, figsize=(9, 6), sharey = True)\nax1=sns.countplot(Group1.Exited,ax=axes[0], palette=['#D7263D','#27FB6B'])\nax2=sns.countplot(Group2.Exited,ax=axes[1], palette=['#D7263D','#27FB6B'])\nax3=sns.countplot(Group3.Exited,ax=axes[2], palette=['#D7263D','#27FB6B'])\nax4=sns.countplot(Group4.Exited,ax=axes[3], palette=['#D7263D','#27FB6B'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print percentages of exited customers, for each group\nfor i,group in enumerate([Group1, Group2, Group3, Group4]):\n    print(f'Group {i+1} customer attrition rate :',f'{group.Exited.sum()/len(group)*100:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clustering has identified a particular group of 150 customers who are 90.67% likely to leave. Aggregating this model on top of the supervised learning model may improve its performance and provide additional insights into what makes clients exit, and how churn rates can be decreased. "},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_data=['Geography', 'Gender', 'Tenure','NumOfProducts', 'HasCrCard', \n                  'IsActiveMember']\nq=1\nplt.figure(figsize=(20,20))\nfor j in categorical_data:\n    plt.subplot(3,3,q)\n    ax=sns.countplot(X_train[j],hue=clusters_smaller)\n    plt.xlabel(j)\n    q+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of products seems to be the strongest determinant for categorising customers into the second group. However, for clients with 3 products, there is an unclear distinction being made between the second and third clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_data=['Age','CreditScore','Balance','EstimatedSalary']\n\nq=1\nplt.figure(figsize=(14,14))\n\nfor col in numerical_data:\n    plt.subplot(2,2,q)\n    ax=sns.boxplot(y=X_train[col], x=pd.Series(clusters_smaller), hue = clusters_smaller)\n    plt.xlabel('Group')\n    q+=1\nplt.show()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The four groups are similar in terms of the numerical variables "},{"metadata":{},"cell_type":"markdown","source":"### Alternative Model"},{"metadata":{},"cell_type":"markdown","source":"Because the recall of the first model was so low, an alternative model is plotted, which aims to improve recall by reducing the number of customers who did not exit. Levelling the categories of the target variable should decrease the model's incentive to make negative prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"exted = df[df.Exited==1]\nothers = df[df.Exited!=1][:2000]\nnew_df = pd.concat([exted, others],axis=0)\n\nX = new_df.drop('Exited',axis=1)\n\ngrid=GridSearchCV(estimator=pipe, param_grid=param_grid,cv=5,\n                  scoring='f1',return_train_score=True,\n                  verbose=0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, new_df.Exited)\n\ngrid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = grid.best_estimator_.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_accuracy = accuracy_score(y_test, test_predictions)\ntest_precision = precision_score(y_test, test_predictions)\ntest_recall = recall_score(y_test, test_predictions)\ntest_f1_score = f1_score(y_test, test_predictions)\ntest_roc_auc_score = roc_auc_score(y_test, test_predictions)\n\nprint(\"Accuracy on test data: {:.3f}%\".format(test_accuracy*100))\nprint(\"Precision on test data: {:.3f}%\".format(test_precision*100))\nprint(\"Recall on test data: {:.3f}%\".format(test_recall*100))\nprint(\"F1 Score on test data: {:.3f}%\".format(test_f1_score*100))\nprint(\"AUC Score on test data: {:.3f}\".format(test_roc_auc_score))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def get_conf_matrix(y_test, y_pred):    \n    # Get confusion matrix\n    data = confusion_matrix(y_test, y_pred) \n    # Build the confusion matrix as a dataframe table\n    cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test)) \n    cm.index.name = 'Observed'\n    cm.columns.name = 'Predicted'\n    plt.figure(figsize = (10,7))\n    # Plot a heatmap\n    sns.heatmap(cm, annot=True, fmt=\"d\", annot_kws={\"size\": 12}) \n    plt.title(\"Confusion Matrix\")\n    plt.show()\nget_conf_matrix(y_test, test_predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model achieves smaller accuracy, but larger recall, f1 and AUC score. While following it may lead to targeting some customers at small risk of exiting, it misses significantly less of the customers at risk of leaving, and can therefore be a better option."},{"metadata":{},"cell_type":"markdown","source":"<h2>7. Conclusion</h2>\nTwo models are designed to predict whether a customer will exit or not, depending on whether accuracy or recall are the main focus. Customers were agglomeratively clustered into 4 groups, out of which one is of particular interest. While these models could be leveraged to identifiy customers at risk of leaving, additional data could significantly improve the performance of the models."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}