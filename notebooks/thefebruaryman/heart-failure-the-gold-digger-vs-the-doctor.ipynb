{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart Failure: The gold digger V the Doctor, Precision V Recall\n\nIn this notebook I am looking at heart failure data, a dataset that takes a number of measureable health related features that correspond to a person's mortality from heart failure.\n\nTo keep this interesting I wanted to view this from two perspectives, one being that of a gold digger and the other a Doctor. The reason for this is that they both will want something very different out of a model that predictes mortality.\n\nThe sinister gold digger will not want to waste their time hooking up with someone that has been falsely predicted to die as the result of heart failure. They will want to know with high precision that their betrothed will die.\n\nOur kind Doctor on the other hand is there to save as many lifes as they can. To them they want to find all the people that are at risk of heart failure so they can provide the best action as soon as possible, be it a healthier lifestyle, medication or surgical intervention. It doesn't matter as much if they wrongly predict a person to be positive when they arn't as they know a number of follow up tests will be carried out and healthy lifestyle changes will be good for a person even if they are not at present risk. However letting someone slip through the net as it were could be aweful. For them they want high recall.\n\n\n## Steps Needed\n\nNow that we have some motives and we have some data. Next is how I plan to tackle this.\n\n1.   Load and analyse the data. For this I will be using pandas and pandas-profiling. For this small amount of data these are perfect tools to load and check what we have with very little effort.\n2.   Clean the data.\n3.   Create train and test sets.\n4.   Preprocess the data.\n5.   Train our model.\n6.   Validate our model.\n7.   Prediction Probablilities and Precision/Recall Trade Off","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n#Suppressing all warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report, plot_roc_curve\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom pandas_profiling import ProfileReport","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and Analyse the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadData():\n    return pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\n\ndata = loadData()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report = ProfileReport(data,progress_bar=False)\nreport.to_notebook_iframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The pandas profiling report is great. There are no missing values and all the data types are as they should be. I do however want to have a closer look at the correlations as I woulld like to know which features are of importance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_corr = data.corr(method='pearson')\n\ndata_corr_columns = data_corr.columns\ndata_corr_index = data_corr.index\n\ndata_corr = data_corr.to_numpy()\n\nnp.fill_diagonal(data_corr, np.nan)\n\ndata_corr = pd.DataFrame(data_corr, columns=data_corr_columns, index=data_corr_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,10))\nsns.heatmap(data=data_corr, cmap='YlGnBu', annot=True).set_title('Correlation Heat Map');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks as if age and serum_creatinine have the largest positive correlations to the death event. The age factor makes common sense as we expect the older person to be more at risk of heart failure. This is also something that is also quite common when we thing about famous gold diggers, those that marry people much older than themselves.\n\nThe serum creatinine is an interesting one as this has to do with how well the kidneys are working, a high levels shows poor kidney function.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Clean the Data\n\nAfter looking through the report there doesn't seem much to do with the data and so I am going to model it as it is.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Create Train and Test Sets\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping target value from featues\nfeatures = data.drop(['DEATH_EVENT'],axis=1)\n# extracting DEATH_EVENT as an array\ntarget = data['DEATH_EVENT']\n\n# Spliting the data into train test sets\nX_train, X_test, y_train, y_test = train_test_split(features,\n                                                    target,\n                                                    test_size=0.2,\n                                                    random_state=30,\n                                                    stratify=target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocess the Data\n\nTo process the data I am going to use the Pipeline and ColumnTransform functions from sklearn.\nThese make life a lot easier although their use here is a bit overkill.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lists of columns that will go through the pipeline\nscaled_cols = ['age',\n               'creatinine_phosphokinase',\n               'ejection_fraction',\n               'platelets',\n               'serum_creatinine',\n               'serum_sodium']\nnothing_cols = ['anaemia',\n                'diabetes',\n                'high_blood_pressure',\n                'sex',\n                'smoking',\n                'time']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\nnumeric_transform = Pipeline(steps=[\n    ('scaler', StandardScaler())])\n\nnothing_transform = Pipeline(steps=[\n    ('nothing', None)])\n\n\npreprocess = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transform, scaled_cols),\n        ('nothing', nothing_transform, nothing_cols)\n    ])\n\n\nrf = Pipeline(steps=[('preprocessor', preprocess),\n#                     ('classifier', RandomForestClassifier())\n                    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transforming the training and testing data\ntrans_train_data = rf.fit_transform(X_train)\n\ntrans_test_data = rf.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train our Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# using the Random Forest Classifier with it default parameters\nrfc = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training the model with our transformed data\nrfc.fit(trans_train_data, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validate our Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting predictions to our test data\npredictions = rfc.predict(trans_test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model has done well we have a accuracy of 88%. However this isn't perfect for either our Doctor or our gold digger.\nLets have a look at a confusion matrix to get a better idea of what is going on.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, predictions)\n\ncm = pd.DataFrame(cm,\n                  columns=['Predicted Negative',\n                           'Predicted Positive'],\n                  index=['Actual Negative',\n                         'Actual Positive'])\n\ncm.style.background_gradient(cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above confusion matrix we can clearly see the issue for both our Doctor and our gold digger. The Predicted column shows that 2 out of our 16 positive predictions were actually negatives. This is bad news for our gold digger as they wouldn't want to accidently marry one of those two people as they will have to wait a long time if at all before getting their money. The Actual Positive row shows the issues for the Doctor because 4 out of the total 19 positive cases failed to be positively predicted.\n\nOur Precision is 0.88 and our Recall is 0.74\n\nThis isn't the end of the story though as our model also gives us access to the probabilities for each prediction and as both of our characters are willing to trade off precision for recall or vice verse we can do more with this data.\n\nI am next going to get these probabilities, the predictions and the actual outcomes and concatenate them into a dataframe so we can get a better understanding of it.\n\nTo do this Random Forest Classifier in sklearn comes with the <code>.predict_proba()</code> method. Other classifiers may use <code>.decision_function()</code>. This allows us to get the probabilities that we want.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Prediction Probablilities and Precision/Recall Trade Off","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets first get the probability given to each prediction\ny_pred_proba = rfc.predict_proba(X_test)\n\n# next make a dataframe of the outcomes\ntrain_df = pd.DataFrame(pd.Series(y_test))\n\n# reset the index so we can concatenate the right rows together\ntrain_df.reset_index(inplace=True)\n\n# crate a dataframe of the predictions\npredictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n# from the proba we are taking the second column and making it a dataframe\nproba_df = pd.DataFrame(y_pred_proba[:,1], columns=['proba'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenating all the dataframes together\nprecision_recall = pd.concat([train_df, predictions_df, proba_df], axis=1, ignore_index=False).set_index('index')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To see how these probablilties relate to the outcomes and predictions I am going to take a random sample from the precision_recall dataframe them sort in ascending order by the proba column and lastly transpose the dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# increasing max columns displayed by pandad\npd.options.display.max_columns = 30\n\nprecision_recall.sample(30).sort_values('proba').T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this we can see that as the proba gets higher than 0.7 all the predictions are positive and so are the the outcomes. This is exactly what our gold digger wants. If we were to filter the data by only classing proba 0.7 and above as positive we would have a higher Precision however we would miss out of predicting some of the positive outcomes lowering Recall. The oposite can be done by lowering the wanted proba to say 0.4, this would mean that more of the positive outcomes would be classed as positive how we would lower the Precission.\n\nLets do this for both our Doctor and gold digger and see however the precission and recall are affected.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gold_digger_df = precision_recall.copy()\ndoctor_df = precision_recall.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gold_digger_df.loc[(gold_digger_df.proba >= 0.7), ('Predictions')] = 1\ngold_digger_df.loc[(gold_digger_df.proba < 0.7), ('Predictions')] = 0\n\ndoctor_df.loc[(doctor_df.proba >= 0.4), ('Predictions')] = 1\ndoctor_df.loc[(doctor_df.proba < 0.4), ('Predictions')] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(gold_digger_df.DEATH_EVENT, gold_digger_df.Predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_cm = confusion_matrix(gold_digger_df.DEATH_EVENT, gold_digger_df.Predictions)\ngb_cm = pd.DataFrame(gb_cm, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])\ngb_cm.style.background_gradient(cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In our gold digger set we can now see that of all the instances we predicted to be postive all of them were thus giving us 100% precision. However we did miss out on an number of actual postive outcomes. Which has led to a lower recall.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(doctor_df.DEATH_EVENT, doctor_df.Predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dr_cm = confusion_matrix(doctor_df.DEATH_EVENT, doctor_df.Predictions)\ndr_cm = pd.DataFrame(dr_cm, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])\ndr_cm.style.background_gradient(cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the Doctor set most of the actual positives have now been predicted as positive giving us a much higher Recall. However we have now got a lot more False Positives lowering the Precision.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To see this trade of we can plot the the True Positive Rate against the False Positive Rate as below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(rfc, trans_test_data, y_test);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}