{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier # required for multiclass classification\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn import tree\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#   for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/fetal-health-classification/fetal_health.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if there is any null value\nfor i in train_df.columns:\n    if (train_df[i].isnull().any()):\n        print (\"Column: {}\".format(i))\n    else:\n        print(\"No null in column {}\".format(i))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# studying the data carefully\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of unique classes\nclasses = train_df['fetal_health'].unique()\nprint(classes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# percentage of each class in the dataset\ncount_1 = 0\ncount_2 = 0\ncount_3 = 0\nfor i in range(len(train_df)):\n    if train_df[\"fetal_health\"].iloc[i] == 1:\n        count_1  = count_1 + 1\n    elif train_df[\"fetal_health\"].iloc[i] == 2:\n        count_2  = count_2 + 1\n    elif train_df[\"fetal_health\"].iloc[i] == 3:\n        count_3  = count_3 + 1\npercent1 = (count_1/len(train_df))* 100\npercent2 = (count_2/len(train_df))* 100\npercent3= (count_3/len(train_df))* 100\nprint(\"Class 1 {}%\".format(percent1))\nprint(\"Class 2 {}%\".format(percent2))\nprint(\"Class 3 {}%\".format(percent3))            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the correlation between different columns in the \ncorr = train_df.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing the dataset and splitting into train , test and validation\n# input: train dataframe\ndef preparing_training_data(train_df):\n    training_data_array = train_df.to_numpy()\n    print(\"[+] Training data shape: {}\".format(training_data_array.shape))\n    trainig = training_data_array[:,0:21]\n    label   = training_data_array[:,-1]\n    # binarizing the label\n    label_binarized = label_binarize(label, classes=[1 ,2, 3]) # similar to one hotencoding\n    n_classes = label_binarized.shape[1]\n    X_train, X_test, y_train, y_test = train_test_split(trainig, label_binarized, test_size=0.3, random_state=0)\n    print(\"[+] X_train shape: {}\".format(X_train.shape))\n    print(\"[+] X_test shape: {}\".format(X_test.shape))\n    print(\"[+] Y_train shape: {}\".format(y_train.shape))\n    return (n_classes, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calling the functio: preparing_training_data(train_df)\nn_classes, X_train, y_train, X_test, y_test = preparing_training_data(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing an SVM model and trying\ndef linearSVM(X_train, y_train):\n    clf = OneVsRestClassifier(LinearSVC(random_state=0, tol=1e-5))\n    clf = make_pipeline(StandardScaler(), clf)\n    clf.fit(X_train,y_train)\n    return clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trying kernel method on SVM \ndef kernelSVM(X_train, y_train):\n    kernel_clf = OneVsRestClassifier(SVC(decision_function_shape = 'ovo', class_weight = 'balanced'))\n    kernel_clf = make_pipeline(StandardScaler(), kernel_clf)\n    kernel_clf.fit(X_train,y_train)\n    return kernel_clf\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trying decision tree on the classifier\ndef decison_tree_classifer(X_train, y_train):\n    decison_tree_clf = tree.DecisionTreeClassifier(criterion='entropy')\n    decison_tree_clf = decison_tree_clf.fit(X_train, y_train)\n    return decison_tree_clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training the linear and kernel classifier classifier\ndef training_classifier(X_train, y_train, classifier):\n    model_classifer = None\n    if classifier == \"linear_svm\":\n        print(\"[+] ####### Testing with linear SVM ############\")\n        model_classifer = linearSVM(X_train, y_train)\n        test_score = model_classifer.score(X_test, y_test)\n        print(\"Test Score : {}\".format(test_score))\n    elif classifier == \"kernel_svm\":\n        print(\"[+] ####### Testing with kernel SVM ############\")\n        model_classifer = kernelSVM(X_train, y_train)\n        k_test_score = model_classifer.score(X_test, y_test)\n        print(\"Test Score : {}\".format(k_test_score))\n    elif classifier == \"decision_tree\":\n        print(\"[+] ####### Testing with decision tree ############\")\n        model_classifer = decison_tree_classifer(X_train, y_train)\n        score = model_classifer.score(X_test, y_test)\n        print(\"Test Score for decision ttree: {}\".format(score))\n        tree.plot_tree(model_classifer) \n    \n    return model_classifer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to predict test set\ndef predict_classifier(X_test, y_test, classifier, model_classifier):\n    if classifier == \"linear_svm\":\n        pred = model_classifier.predict(X_test)\n        print(\"[+] Pred Linear shape : {}\".format(pred.shape))\n    elif classifier == \"kernel_svm\":\n        pred = model_classifier.predict(X_test)\n        print(\"[+] Pred Kernel shape : {}\".format(pred.shape))\n    elif classifier == \"decision_tree\":\n        pred = model_classifier.predict(X_test)\n        print(\"[+] Pred Kernel shape : {}\".format(pred.shape))\n\n    return pred  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating Area under the Precision-Recall Curve, FI Score and Area under the ROC Curve\n# Compute ROC curve and ROC area for each class\nclassifier = \"decision_tree\"\nmodel_classifier = training_classifier(X_train, y_train, classifier)\npred = predict_classifier(X_test, y_test, classifier, model_classifier)\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n# Plot of a ROC curve for a specific class\nfor i in range(n_classes):\n    plt.figure()\n    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic for class {}'.format(i))\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n# computing FI score\nf1score = f1_score(y_test, pred, average = 'macro')\nprint(\"[+] F1 Score for {} {}\".format(classifier, f1score))\n\n# plotting precison and recall curves\naverage_precision_linear = average_precision_score(y_test, pred, average = \"macro\")\nprint(\"[+] Average Precision for {} is {}\".format(classifier, average_precision_linear))\n\n# plotting precision and recall curve\nprecision = dict()\nrecall = dict()\nfor i in range(n_classes):\n    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n                                                        pred[:, i])\n    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n\nplt.xlabel(\"recall\")\nplt.ylabel(\"precision\")\nplt.legend(loc=\"best\")\nplt.title(\"precision vs. recall curve for kernel SVM\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}