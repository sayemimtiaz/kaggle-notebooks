{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Goodreads Books EDA and Clusterization\n\n![Books](https://cdn.pixabay.com/photo/2016/03/26/22/21/books-1281581_960_720.jpg)\n\nThere are six main types of data analysis:\n\n* Descriptive.\n* Exploratory\n* Predictive\n* Inferential\n* Causal\n* Mechanistic or Technical\n\nWe can use an acronym DEPICT\n\nWhat are the differences between Descriptive Data Analysis (DDA) and Exploratory Data Analysis (EDA)?  DDA's main purpose it to just structure and present the data. The most salient example of DDA is a Census - you just gather the data and describe who lives where. EDA's purpose is to show relationships between the data in your sample, what features correlate and how, what are the distributions, connections etc. This is what we are going to do in this notebook.\n\n## TOC <a href id=\"TOC\"></a>\n1. [Preliminaries](#Intro)\n1. [Books](#Books)\n2. [Authors](#Authors)\n3. [Publishers](#Publishers)\n4. [Clusterization](#Cluster)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Preliminaries <a id=\"Intro\"/>[⇧](#TOC)\n\nImport necessary modules.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport warnings\n\nfrom matplotlib.ticker import FormatStrFormatter\n\nfrom scipy.stats import zscore\nfrom scipy.stats import norm\nfrom scipy.stats import f_oneway\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import silhouette_score\n\nfrom IPython.display import display\n\nplt.style.use('seaborn-talk')  # nice readable plot style\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading the data. Some lines are corrupt (i.e lines 3350, 4704, 5879, 8981), so we're skipping those. One column has extra spaces in the title. Also during the initial data analysis I used toy dataframes with random samples of 2000 books out of our total dataframe, it is also done here.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"brows = [3349,4703,5878,8980]  # bad or corrupt row, extra comma in the 'Authors' field\nraw = pd.read_csv('../input/goodreadsbooks/books.csv', skiprows=brows)\nraw.rename(columns={'  num_pages':'num_pages'}, inplace=True)  # there is a problem with column name, extra spaces.\n\nrandom_indexes = np.random.choice(raw.shape[0], size=2000, replace=False)  # create random sample of books to explore\ntoy = raw.loc[random_indexes,:]  # create toy dataframe to wrangle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I want to create new features from the __authors__ feature: a feature __authors_count__ that would show how many authors worked on the book. For subsequent analysis we will need normalized __ratings_count__, __text_reviews_count__ and __num_pages__ features, so I create those in advance. And also our __publication_date__ feature is now a string, I transform it into three numerical features - __pub_year__, __pub_month__ and __pub_day__.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"## DATA PREPARATION ##\n\ndata = raw.copy()  # when I finish the analysis just change from 'toy' to 'raw'\n\ndata.reset_index(inplace=True, drop=True)\n# create new feature with number of authors\ndata['n_authors'] = [len(s) for s in data['authors'].str.split('/')]\n# normalize several features\ndata['log_ratings'] = np.log1p(data['ratings_count'])\ndata['log_reviews'] = np.log1p(data['text_reviews_count'])\ndata['log_pages'] = np.log1p(data['num_pages'])\n\n# create categorical feature:\ndata['bins_pages'] = pd.cut(data['log_pages'], bins=5, labels=['tiny','small','average','large','huge'])\n\n# transform publication_date from string to three integer columns\ndates = data['publication_date'].str.split('/', expand=True).astype(int)\ndates.columns = ['pub_month','pub_day', 'pub_year']\ndata = pd.concat((data,dates),axis=1)\n# drop the old 'publication_date' feature\ndata.drop(['publication_date'],axis=1, inplace=True)\n\ndisplay(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic summary:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"There are {} books in the dataframe.\".format(data.shape[0]))\nprint(\"Maximum number of authors for one book is {0} for {1}.\".format(data['n_authors'].max(), \n                                                                     data.loc[data['n_authors'].idxmax(), 'title']))\nprint(\"Maximum number of pages is in {1} with {0} pages.\".format(data['num_pages'].max(),\n                                                                 data.loc[data['n_authors'].idxmax(), 'title']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Books <a id=\"Books\"/> [⇧](#TOC)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In one random sample I explored during the initial analysis I found that \"Moby Dick\" from \"Penguin Audio\" publisher had only 6 pages and an average rating of 3.5 on 8858 ratings. It seems that there is a category of audiobooks in our database. It may be interesting to explore these.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# a dive into small books\nsmall_books = data.loc[data['num_pages'] <=50,:]\nprint(\"There are a total of:\",small_books.shape[0], \"small books\")\n\n# some publisher names contain the word 'Audio'\naudio = small_books.loc[small_books['publisher'].str.contains('Audio'),:].shape[0]\nprint(\"At least\", audio, \"of them are audiobooks, i.e. their publishers have a word 'Audio' in their name\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# books distribution by publication year\nsns.countplot(data['pub_year'], palette='inferno')\nplt.xlabel('Publication Year')\nplt.ylabel('Published Books')\nplt.title('Published Books by Year')\nfig = plt.gcf()\nfig.set_size_inches(15,8)\nplt.xticks(rotation=75, size=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that prior 1989 and after 2007 there are less than 100 books per year in our sample. Statistical inquiry would be futile on those samplings, so I should keep that in mind and craft a subset of our data during these 18 years, where the number of samples per year is plenty. It also seems like the data for 2007 is incomplete, so I will take only the range between 1989 and 2006 (i.e. 17 years) for subsequent analysis.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data_17 = data.loc[(data['pub_year'] >= 1989) & (data['pub_year']<=2006),:]  # subsample of data for 17 years\nprint('After all we are left with {} samples from years 1989 to 2006.'.format(data_17.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# crosstable with monthly fractions of yearly published books\npd.crosstab(data_17['pub_year'], data_17['pub_month'], \n            normalize='index').plot(kind='bar',stacked=True,cmap='tab20c', width=1)\nplt.legend(bbox_to_anchor=(-0.01, -0.2), loc='upper left', ncol=6, \n           labels=['January','February','March','April','May','June','July',\n                   'August','September','October','November','December'])\nplt.xlabel('Publication Year')\nplt.ylabel('Fraction of monthly published books')\nplt.title('Published books by month and year')\nf = plt.gcf()\nf.set_size_inches(15,7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# books distribution by publication month\nsns.countplot(data_17['pub_month']);\nplt.xlabel('Publication month')\nplt.ylabel('Count')\nplt.xticks(np.arange(12), labels=['January','February','March','April','May','June','July',\n                                  'August','September','October','November','December'], rotation=75)\nplt.title('Books published each month')\nf = plt.gcf(); f.set_size_inches(15,7)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is very interesting. September and October seem to stand out in terms of published books. Let's see whether this difference is statistically significant. We will use Analysis of Variance technique (ANOVA). ANOVA tells us whether there is a statistical significant difference between means of several groups. However, we should understand, that because of growind number of books each year since 1989 to 2006, we have to compare fractions per month.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"by_month = pd.crosstab(index=data_17['pub_year'], columns=data_17['pub_month'], normalize='index')\npval = f_oneway(by_month[1],by_month[2],by_month[3],\n                by_month[4],by_month[5],by_month[6],\n                by_month[7],by_month[8],by_month[9],\n                by_month[10],by_month[11],by_month[12])\nprint('Null hypothesis is: There is no difference between aforementioned sample means')\nprint(\"Statistic:\", np.round(pval[0],4))\nprint(\"p-value:\", pval[1])\nprint('Null hypothesis REJECTED' if pval[1]<=0.05 else \"Null hypothesis ACCEPTED\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nYes, it is definitely the fact and ANOVA test suggests that considering these 12 samples there is a statistical difference between their means. Now we can use Tukey Range Test to find out which months have statistically different means.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cm = by_month.melt()  # making one column dataframe\ntukey_results = pairwise_tukeyhsd(cm['value'], cm['pub_month'], 0.05)\ntukey_results.plot_simultaneous(comparison_name=8,\n                                xlabel='Mean Fraction of Books Per Month',\n                                ylabel='Month')\nf = plt.gcf(); f.set_size_inches(15,7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see that in September and October there are published significantly more books. Also some significant differences are visible in February and January.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# book average rating by year and average book size by year\nf, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n\ndata_17.groupby(by='pub_year')['average_rating'].mean().plot(kind='barh', width=1, edgecolor='white',ax=ax1);\ndata_17.groupby(by='pub_year')['num_pages'].mean().plot(kind='barh',width=1, edgecolor='white',ax=ax2);\n\nax1.set_ylabel('Publication Year')\nax2.set_ylabel('')\nax2.set_yticklabels('')\nax1.set_xlabel('Average Rating')\nax2.set_xlabel('Book Size')\n\nplt.suptitle('Other Yearly Distributions', y=1.04, size=18, weight='bold')\nf = plt.gcf(); f.set_size_inches(15,5)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be no significant difference in average rating of books per year and in average book size per year except for book sizes in 1989.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# distribution of books by rating\nplt.hist(data['average_rating'], bins=40, edgecolor='white', color='lightcoral')\nplt.xlabel('Book Average Rating')\nplt.ylabel('Count')\nplt.title('Book Distribution by Rating')\nf = plt.gcf(); f.set_size_inches(15,7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I read once that a human being can not make a distinction between more tha three categories, i.e. \"good\", \"bad\" and \"neutral\". And if you present a scale with more than 3 grades then most of the ratings would still locate near only three values. Here we can see a perfect illustration of this thesis. Categories \"0\", \"1\" and \"2\" are almost empty.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(15,14))\n\n# distribution of books by size\nax1.hist(data_17['num_pages'], bins=40, edgecolor='darkgreen', color='palegreen')\nax1.set_xlabel('Number of Pages')\nax1.set_ylabel('Count')\n\n# distribution of books by size on a logarithmic scale\nax2.hist(data_17['log_pages'], bins=40, edgecolor='white', color='forestgreen')\nmaximum = data_17['log_pages'].value_counts(bins=40).idxmax().mid\nax2.axvline(ymin=0,ymax=1, x=maximum, ls='--', c='indigo',lw=2,\n            label=maximum)\nax2.legend()\nax2.set_xlabel('Number of Pages Logarithm')\nax2.set_ylabel('Count')\n\nplt.suptitle('Books Distribution by Number of Pages', y=1.03,\n             fontweight='bold', fontsize=18)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the most frequent number of pages in a book is $e^{5.824}-1 \\simeq 337$. There is an interesting tail in front of our distribution with number of pages less than 50.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### How to get rid of outliers in one line.\nOutliers are data points which are so far in the distribution that they skew the parameters of the model applied to the whole distribution. They also make plots hard to understand, so it is often beneficial to get rid of them.\n`scipy.stats` has a function `zscore` which calculates the [standard score](https://en.wikipedia.org/wiki/Standard_score) for the given data point, so we can remove outliers using this:\n\n```python\ndata.loc[zscore(data['log_ratings'])<3]\n```","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,7))\n\n# we have some outliers both on ratings and text reviews counts\nax1.scatter(data_17['ratings_count'], data_17['text_reviews_count']);\nax1.set_title(\"with outliers\")\nax1.xaxis.set_major_formatter(FormatStrFormatter('%d'))\nax1.tick_params('x', labelrotation=45)\nax1.set_xlabel('Ratings Count')\nax1.set_ylabel('Text Reviews Count')\n\n\n# plot with removed outliers\nax2.scatter(data_17.loc[zscore(data_17['ratings_count'])<3, 'ratings_count'],\n            data_17.loc[zscore(data_17['ratings_count'])<3, 'text_reviews_count']);\nax2.set_title(\"without outliers\")\nax2.tick_params('x', labelrotation=45)\nax2.set_xlabel('Ratings Count')\nax2.set_ylabel('Text Reviews Count')\n\nplt.suptitle('Ratings Count vs Text Reviews Count', y=1.03,\n             fontweight='bold', fontsize=18)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_data = data_17.loc[zscore(data_17['ratings_count'])<3,['ratings_count','text_reviews_count','average_rating']]\nplot_data['rating_quartile'] = pd.qcut(plot_data['average_rating'], q=4, labels=['1','2','3','4'])\n\ng = sns.relplot(kind='scatter', data=plot_data, alpha=0.5,\n                x='ratings_count',y='text_reviews_count',\n                hue='rating_quartile',)\ng._legend.texts[0].set_text(\"\")\ng._legend.set_title(\"Rating Quartile\")\ng._legend.set_bbox_to_anchor([0.23,0.73])\n\ng.ax.set_xlabel('Ratings Count')\ng.ax.set_ylabel('Text Reviews Count')\ng.ax.set_title('Book Ratings and Text Reviews')\nf = plt.gcf(); f.set_size_inches(15,7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's study the outliers. What are they?","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# plot only outliers\nplt.scatter(data.loc[zscore(data['ratings_count'])>=3, 'ratings_count'],\n            data.loc[zscore(data['ratings_count'])>=3, 'text_reviews_count']);\nplt.title('Outliers Plot')\nplt.xlabel('Ratings Count')\nplt.ylabel('Text Reviews Count')\nax = plt.gca()\nax.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data[zscore(data['ratings_count'])>=3].sort_values(by='ratings_count', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"labels=['Average Rating','Ratings Count','Ratings Count Logarithm', \n        'Text Reviews Count','Number of Pages','Number of Authors']\n\nf, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10,15))\n# correlation heatmap\ncorr_df = data_17[['average_rating','ratings_count','log_ratings','text_reviews_count','num_pages','n_authors']].corr()\nsns.heatmap(corr_df, annot=True, vmax=0.5, fmt='.2f', cmap='viridis', ax=ax1, \n            linewidth=0.5,\n            xticklabels='', yticklabels=labels);\nax1.set_title('All books')\n\n# what about books with reasonable amount of ratings?\ncorr_df = data_17.loc[data_17['log_ratings']> 4.6, ['average_rating','ratings_count','log_ratings','text_reviews_count','num_pages','n_authors']].corr()\nsns.heatmap(corr_df, annot=True, vmax=0.5, fmt='.2f', cmap='RdYlGn', ax=ax2, \n            linewidth=0.5,\n            xticklabels=labels, yticklabels=labels);\nax2.set_title('Books with more than 100 ratings')\n\nplt.suptitle('Correlations In Data', y=1.04, weight='bold',size=18)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a slight, although noticeable correlation between number of pages and average rating of the book. People like thick books.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# connections between average rating, ratings count and book size category\ncolors = data_17.loc[data_17['log_ratings']>4.6, 'bins_pages'].map({'tiny':'violet',\n                                                              'small':'cyan',\n                                                              'average':'red',\n                                                              'large':'forestgreen',\n                                                              'huge':'blue'})\nplot_data = data_17.loc[data_17['log_ratings']>4.6]\nf, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(15,11))\nsns.violinplot(y='bins_pages', x='average_rating', data=plot_data, ax=ax1)\nax1.set_xlabel('Average Rating')\nax1.set_ylabel('Book Size Category')\n\nsns.violinplot(y='bins_pages', x='log_ratings', data=plot_data, ax=ax2)\nax2.set_xlabel('Ratings Count Logarithm')\nax2.set_ylabel('Book Size Category')\n\nplt.suptitle('Book Size and Ratings when Ratings Count > 100', weight='bold',size=18, y=1.04)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These plots show us that the average rating of \"huge\" books is higher and that the bigger the book the more ratings it gets.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,7))\n\nax1.hist(data['ratings_count'], bins=20, )\nax1.set_title('Ratings Histogram')\nax1.set_xlabel('Number of ratings')\nax1.xaxis.set_major_formatter(FormatStrFormatter('%d'))\nax1.set_ylabel('Book count')\n\nax2.hist(data['log_ratings'], bins=20, edgecolor='white')\nax2.set_title('Ratings\\' Logarithms Histogram')\nax2.set_xlabel('Logarithm of Ratings Count')\n\nplt.suptitle('Book Ratings Count Distributions', y=1.04, weight='bold', size=18)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.scatter(data_17['log_ratings'], data_17['average_rating'], alpha=0.5);\nplt.xlabel('Ratings Count Logarithm')\nplt.ylabel('Average Rating')\nf = plt.gcf()\nf.set_size_inches(15, 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My attention is caught by the $(0,0)$ dots - books with zero ratings and zero score, I would like to see them. It is also interesting to look at \"bad\" books - those that have a lot of ratings but the score less than 3.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# zero rating count\ndata.loc[data['log_ratings'] == 0].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"There are {} unrated books in our dataframe\".format(data.loc[data['log_ratings'] == 0].shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is somewhat interesting that some of these books still have __average_rating__ field filled. But I don't see any pattern there yet. Now let's take a look at \"bad\" books - those books where there are more than 100 ratings, but the average rating is less than 3.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# list of \"bad\" books\nbbooks = data.loc[(data['log_ratings'] > 4.6) & (data['average_rating'] < 3),:]\nbbooks.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('There are {} \"bad\" books in our dataframe.'.format(bbooks.shape[0]))\nprint('Their average rating of \"bad\" books is {0:5.2f}'.format(bbooks['average_rating'].mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Authors [⇧](#TOC)<a id=\"Authors\"/>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"authors = data['authors'].str.split('/')  # create Series object with lists of authors per each book\nauthors = authors.values  # transform to flattened numpy array of lists\nauthors = np.concatenate(authors)  # transform to one list\nauthors = np.unique(authors)  # get uniques\nprint(\"There are {} authors in our base\".format(len(authors)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Embrace the difference between `pandas.Series.str.contains` and `pandas.Series.str.match`","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# how many books did the author (co)write\nnum_books = [data_17[data_17['authors'].str.contains(author)]['title'].count() for author in authors]\n# how many publishers did the author work with\nnum_publishers = [data_17[data_17['authors'].str.contains(author)]['publisher'].count() for author in authors] \n# what books did the author write\nbook_indexes = [data_17[data_17['authors'].str.contains(author)]['bookID'].ravel().tolist() for author in authors]\n# what publishers did he or she work with\npub_names = [data_17[data_17['authors'].str.contains(author)]['publisher'].ravel().tolist() for author in authors]\n\ntotal_pages = [data_17[data_17['authors'].str.contains(author)]['num_pages'].sum() for author in authors]\nmean_pages = [data_17[data_17['authors'].str.contains(author)]['num_pages'].mean() for author in authors]\navg_rating = [data_17[data_17['authors'].str.contains(author)]['average_rating'].mean() for author in authors]\n\n# in what year was the first book published\nfirst_book = [data_17[data_17['authors'].str.contains(author)]['pub_year'].min() for author in authors]\n# in what year was the latest book published\nlatest_book = [data_17[data_17['authors'].str.contains(author)]['pub_year'].max() for author in authors]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"authors_df = pd.DataFrame({'author_name': authors, \n                           'num_books': num_books,\n                           'num_publishers': num_publishers,\n                           'tot_pages': total_pages,\n                           'avg_pages': mean_pages,\n                           'avg_rating': avg_rating,\n                           'first': first_book,\n                           'latest': latest_book,\n                           'publishers': pub_names,\n                           'book_ids': book_indexes})\ndisplay(authors_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('{0} wrote most number of books, {1}'.format(authors_df.loc[authors_df.num_books.idxmax,'author_name'],authors_df.num_books.max()))\nprint('{0} (co)authored books ending up with most total pages, {1}'.format(authors_df.loc[authors_df.tot_pages.idxmax,'author_name'],authors_df.tot_pages.max()))\nprint('{0} worked with most number of publishers, {1}'.format(authors_df.loc[authors_df.num_publishers.idxmax,'author_name'],authors_df.num_publishers.max()))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plot_data = authors_df.sort_values(by='tot_pages', ascending=False)[['author_name', 'num_books','tot_pages']][:20]\n\nplt.bar(x=plot_data['author_name'], height=plot_data['tot_pages'])\nplt.title('Most Prolific Authors')\nplt.ylabel('Total Number of Published Pages')\nplt.xticks(rotation=90)\nf = plt.gcf()\nf.set_size_inches(15,6)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"labels = ['Number of Books','Number of Publishers','Total Pages','Average Pages per Book','Average Rating per Book']\ncorr_df = authors_df[['num_books','num_publishers','tot_pages','avg_pages','avg_rating']].corr()\nsns.heatmap(corr_df, annot=True, fmt='.2f', xticklabels=labels, yticklabels=labels, \n            linewidth=0.5, cmap='viridis');\nax = plt.gca()\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The only thing that caught my attention here is that there is slightly more than zero correlation between Average Rating per Book and Average Pages per Book. And also it seems interesting that there are no correlation between number of books and average rating per book. My prior was that if you write a lot you become better at writing. It seems to be not the case.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Publishers [⇧](#TOC)<a id=\"Publishers\"/>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# top 20 publishers distribution by book count\ndata_17['publisher'].value_counts()[:20].plot(kind='bar', width=0.8)\nf = plt.gcf()\nf.set_size_inches(15,7)\nplt.title(\"Top 20 publishers\")\nplt.ylabel('Published Books')\nplt.xlabel('Publisher')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"There are {} publishers in our dataframe\".format(data['publisher'].value_counts().shape[0]))\nprint(\"Top 20 publishers published {} books\".format(data['publisher'].value_counts()[:20].sum()))\nprint(\"So, top 20 publishers are {0:1.2f}% of all publishers\".format(20/data['publisher'].value_counts().shape[0]*100))\na = data['publisher'].value_counts()[:20].sum() / data.shape[0] * 100\nprint(\"However, they published {0:2.2f}% of all books in our dataset\".format(a))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"a = data['publisher'].value_counts()[:20].sum() / data.shape[0]\np = 20/data['publisher'].value_counts().shape[0]\nc = ['royalblue','mistyrose']\nplot_data = pd.DataFrame({'publisher':['Top 20', 'All Other'],\n                          'proportion': [p,1-p],\n                          'share':[a,1-a]})\n\nf, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n\nax1.pie(plot_data['proportion'], labels=plot_data['publisher'], colors=c,explode=[0.1,0],\n        wedgeprops={'linewidth': 1, 'edgecolor':'k'})\nax1.set_title('Publishers')\nax2.pie(plot_data['share'], labels=plot_data['publisher'], colors=c, explode=[0.1,0],\n        wedgeprops={'linewidth': 1, 'edgecolor':'k'})\nax2.set_title('Books Published')\n\nplt.suptitle('Distribution of publishers',y=1.04, weight='bold', size=18)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n = int(np.round(data['publisher'].value_counts().shape[0]*0.2))\na = data['publisher'].value_counts()[:n].sum() / data.shape[0]*100\nprint(\"Top 20% of publishers published {:5.2f}% of books.\".format(a))\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"a = data['publisher'].value_counts()[:n].sum() / data.shape[0]\np = int(np.round(data['publisher'].value_counts().shape[0]*0.2)) / data['publisher'].value_counts().shape[0]\nc = ['royalblue','honeydew']\nplot_data = pd.DataFrame({'publisher':['Top 20%', 'All Other'],\n                          'proportion': [p,1-p],\n                          'share':[a,1-a]})\n\nf, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n\nax1.pie(plot_data['proportion'], labels=plot_data['publisher'], colors=c,explode=[0.1,0],\n        wedgeprops={'linewidth': 1, 'edgecolor':'k'})\nax1.set_title('Publishers')\nax2.pie(plot_data['share'], labels=plot_data['publisher'], colors=c, explode=[0.1,0],\n        wedgeprops={'linewidth': 1, 'edgecolor':'k'})\nax2.set_title('Books Published')\n\nplt.suptitle('Distribution of publishers',y=1.04, weight='bold', size=18)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's the proverbial Pareto 80/20 rule!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Clusterization <a id=\"Cluster\"/> [⇧](#TOC)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"to_cluster = data_17[['average_rating', 'num_pages', 'ratings_count', 'n_authors']]  # take the data for the 17 years\nto_cluster = to_cluster.loc[(zscore(to_cluster['ratings_count'])<3) &  # remove the outliers in ratings count\n                            (zscore(to_cluster['n_authors'])<3)     &  # --//-- in number of authors\n                            (zscore(to_cluster['num_pages'])<3)].reset_index(drop=True)  # --//-- in number of pages\nscaler = MinMaxScaler()  # scale the data\npreprocessed = scaler.fit_transform(to_cluster)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# a metric for elbow method to determine best number of clusters\nSum_of_squared_distances = []\nK = range(1,15)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(preprocessed)\n    Sum_of_squared_distances.append(km.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.plot(K, Sum_of_squared_distances, marker='x')\nplt.xlabel('k')\nplt.ylabel('Sum of squared distances')\nplt.title('Elbow Method For Optimal k', weight='bold')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"km = KMeans(n_clusters=7)\nkm = km.fit(preprocessed)\nclusters = km.predict(preprocessed)\nsilh = silhouette_score(preprocessed, clusters)\nprint(\"Average cluster silhouette score is: {0:5.3f}\".format(silh))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"clusters = pd.Series(clusters, name='cluster', dtype='category')\nplot_data = pd.concat((to_cluster, clusters), axis=1)\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\nsns.scatterplot(x='average_rating',y='num_pages',hue='cluster',data=plot_data, ax=ax1)\nsns.scatterplot(x='ratings_count',y='num_pages',hue='cluster',data=plot_data, ax=ax2)\nsns.scatterplot(x='average_rating',y='n_authors',hue='cluster',data=plot_data, ax=ax3, alpha=0.5)\nsns.scatterplot(x='ratings_count',y='n_authors',hue='cluster',data=plot_data, ax=ax4, alpha=0.5)\n\nax1.set_xlabel('')\nax2.set_xlabel('')\nax2.tick_params('x', labelrotation=60)\nax3.set_xlabel('Average Rating')\nax4.set_xlabel('Ratings Count')\nax4.tick_params('x', labelrotation=60)\n\nax1.set_ylabel('Number of Pages')\nax2.set_ylabel('')\nax3.set_ylabel('Number of Authors')\nax4.set_ylabel('')\nplt.suptitle('Clusters of Books', y=1.04, weight='bold', size=18)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lessons Learned and What To Do Next:\n\n[Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) article in Scikit-learn library documentation suggests using Mean Shift clustering algorithm when the number of clusters is unknown. My next step will be to study this algorithm and try to use it in clusterization.\n\nHow to change matplotlib style:\n\n```python\nplt.style.available  # show matplotlib styles available \nplt.style.use('seaborn-talk')  # nice readable style\n```\n\nHow to reset matplotlib styling parameters to defaults\n```python\nimport matplotlib as mpl\nmpl.rcParams.update(mpl.rcParamsDefault)\n```","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}