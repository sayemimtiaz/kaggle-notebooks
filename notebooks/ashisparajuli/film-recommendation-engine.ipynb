{"nbformat":4,"cells":[{"cell_type":"markdown","source":"___\n# PROJECT: *Content-based recommendation engine*\n___\nThis notebook aims at building a film recommendation engine from the content of the *'movie\\_metadata.csv'* dataset.\nThis dataset contains around 5000 movies and TV series, and their description has been retrieved from the public IMDb database.\nBasically, the engine will work as follows: after the user has provided the name of a film he liked, the engine should be able to select in the database a list of 5 films that the user will enjoy. In practice, recommendation engines are of three kinds:\n- **popularity-based** engines: usually the most simple to implement be also the most impersonal\n- **content-based** engines: the recommendations are based on the description of the products\n- **collaborative filtering** engines: records from various users provide recommendations based on user similarities\n\nIn the current case, since the dataset only describes the content of the films and TV series, collaborative filtering is excluded and I will thus build an engine that uses both the content and the popularity of the entries.\n___\nThis notebook is organized as follows:\n\n**1. Exploration**\n- 1.1 Keywords\n- 1.2 Filling factor: missing values\n- 1.3 Number of films per year\n- 1.4 Genres\n\n** 2. Cleaning**\n- 2.1 Duplicated entries\n- 2.2 Cleaning of the keywords\n    * 2.2.1 Grouping by roots\n    * 2.2.2 Groups of synonyms\n- 2.3 Correlations\n- 2.4 Missing values\n    * 2.4.1 Setting missing title years\n    * 2.4.2 Extracting keywords from the title\n    * 2.4.3 Imputing from regressions\n    \n**3. Recommendation Engine**\n- 3.1 Basic functioning of the engine \n- 3.1 Definition of the recommendation engine functions\n- 3.2 Exemple of recommendation: test-case\n\n","metadata":{"_execution_state":"idle","_uuid":"58cc9290957c3a15436167cefb72cb5a0e1521de","_cell_guid":"12110ed7-61c4-489c-b121-2bc993162856"}},{"cell_type":"markdown","source":"___\n## 1. Exploration\n\n I first load in a single place all the packages that will be used throughout the notebook:","metadata":{"_uuid":"9bda582d9b59fc16a387fd6d2ede814d6fc8cc37","_cell_guid":"c5033d2e-b634-4e86-8432-1ce567123818"}},{"cell_type":"code","execution_count":null,"source":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math, nltk, warnings\nfrom nltk.corpus import wordnet\nfrom sklearn import linear_model\nfrom sklearn.neighbors import NearestNeighbors\nfrom fuzzywuzzy import fuzz\nfrom wordcloud import WordCloud, STOPWORDS\n%matplotlib inline\nwarnings.filterwarnings('ignore')\nPS = nltk.stem.PorterStemmer()\n\nsns.set_context(\"poster\")\nfont = {'family' : 'normal', 'weight' : 'normal', 'size'   : 14}\nmpl.rc('font', **font)","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"d0e85d63468e4a059b675964a183e3055eff8511","_cell_guid":"1e4af250-2809-40cb-afc8-7b4ab9a3f8b4"},"outputs":[]},{"cell_type":"markdown","source":"and then load the dataset:","metadata":{"_uuid":"b7a8daea31fd38b4e478684f81b8604bb0d2d0e8","_cell_guid":"8e4189f2-9619-4cd9-913a-13e383a53650"}},{"cell_type":"code","execution_count":null,"source":"df_initial = pd.read_csv(\"../input/movie_metadata.csv\")\ndf_initial.info(verbose=False)","metadata":{"_execution_state":"idle","_uuid":"f0e3834241f01ff9f6cd065f29cb59040150a151","_cell_guid":"439ba324-94d8-4a86-a8d2-281226c7302d"},"outputs":[]},{"cell_type":"markdown","source":"___\n### 1.1 Keywords","metadata":{"_uuid":"8e8eb02bfe046b1b6b25f4bafe09346108e4d275","_cell_guid":"8df2466f-4f0b-41f2-8e12-bc5ac3f8ed38"}},{"cell_type":"markdown","source":"To develop the recommendation engine, I plan to make an extensive use of the keywords that describe the films. Indeed, a basic assumption is that films described by similar keywords should have similar contents. Hence, I plan to have a close look at the way keywords are defined and as a first step, I quickly characterize what's already in there. To do so, I first list the keywords which are in the dataset:","metadata":{"_uuid":"3f6970af97fefeff98fe56834bbbc59a4bfcf49e","_cell_guid":"a6529f26-82f8-4d8d-a44a-5b8d8066d156"}},{"cell_type":"code","execution_count":null,"source":"set_keywords = set()\nfor liste_keywords in df_initial['plot_keywords'].str.split('|').values:\n    if type(liste_keywords) == float: continue  # only happen if liste_keywords = NaN\n    set_keywords = set_keywords.union(liste_keywords)","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"b345e2711ec7a4740bbcc9ff60e6136895ede89f","_cell_guid":"faff47cb-32ec-44e0-9a8c-1596beb278e9"},"outputs":[]},{"cell_type":"markdown","source":"and then define a function that counts the number of times each of them appear:","metadata":{"_uuid":"1d95387b7e27aded359c4a85c85ed5587fff9498","_cell_guid":"e6979e54-4118-4e69-84dd-98d65b2b4574"}},{"cell_type":"code","execution_count":null,"source":"def count_word(df, ref_col, liste):\n    keyword_count = dict()\n    for s in liste: keyword_count[s] = 0\n    for liste_keywords in df[ref_col].str.split('|'):\n        if type(liste_keywords) == float and pd.isnull(liste_keywords): continue\n        for s in liste_keywords: \n            if pd.notnull(s): keyword_count[s] += 1\n    #______________________________________________________________________\n    # convert the dictionary in a list to sort the keywords by frequency\n    keyword_occurences = []\n    for k,v in keyword_count.items():\n        keyword_occurences.append([k,v])\n    keyword_occurences.sort(key = lambda x:x[1], reverse = True)\n    return keyword_occurences, keyword_count","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"ac1b82ed313891dbc2c6d094761224cfafb420fd","_cell_guid":"967d942e-6217-49d0-b8b8-7fe1b207ae25"},"outputs":[]},{"cell_type":"markdown","source":"Note that this function will be used again in other sections of this notebook, when exploring the content of the *'genres'* variable and subsequently, when cleaning the keywords. Finally, calling this function gives access to a list of keywords which are sorted by decreasing frequency:","metadata":{"_uuid":"ead32eb2c4d82521fb99eab26f9b6c901270c012","_cell_guid":"90f90d47-35a2-4873-975b-4229287a3ca5"}},{"cell_type":"code","execution_count":null,"source":"keyword_occurences, dum = count_word(df_initial, 'plot_keywords', set_keywords)\nkeyword_occurences[:5]","metadata":{"_execution_state":"idle","_uuid":"c24235201ef028a66b581e106b8454674d28cb91","_cell_guid":"b97262b8-6bc4-4d6b-b435-beaecd57c730"},"outputs":[]},{"cell_type":"markdown","source":"At this stage, the list of keywords has been created and we know the number of times each of them appear in the dataset. In fact, this list can be used  to have a feeling of the content of the *most popular movies*. A fancy manner to give that information makes use of the *wordcloud* package. In this kind of representation, all the words are arranged in a figure with sizes that depend on their respective frequencies. Instead of a wordcloud, we can use histograms to give the same information. This allows to have a figure where the keywords are ordered by occurence and most importantly, this gives the number of times they appear, an information that can not be retrieved from the wordcloud representation. In the following figure, I compare both types of representations:","metadata":{"_uuid":"e1f9a4d185f926a97c5cd0162fce6531f85c4a81","_cell_guid":"6c8ef6c4-2551-4465-b8a1-cbda94ebfdf6"}},{"cell_type":"code","execution_count":null,"source":"#_____________________________________________\n# Function that control the color of the words\n#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n# WARNING: the scope of variables is used to get the value of the \"tone\" variable\n# I could not find the way to pass it as a parameter of \"random_color_func()\"\n#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\ndef random_color_func(word=None, font_size=None, position=None,\n                      orientation=None, font_path=None, random_state=None):\n    h = int(360.0 * tone / 255.0)\n    s = int(100.0 * 255.0 / 255.0)\n    l = int(100.0 * float(random_state.randint(70, 120)) / 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n#_____________________________________________\n# UPPER PANEL: WORDCLOUD\nfig = plt.figure(1, figsize=(18,13))\nax1 = fig.add_subplot(2,1,1)\n#_______________________________________________________\n# I define the dictionary used to produce the wordcloud\nwords = dict()\ntrunc_occurences = keyword_occurences[0:50]\nfor s in trunc_occurences:\n    words[s[0]] = s[1]\ntone = 55.0 # define the color of the words\n#________________________________________________________\nwordcloud = WordCloud(width=1000,height=300, background_color='black', \n                      max_words=1628,relative_scaling=1,\n                      color_func = random_color_func,\n                      normalize_plurals=False)\nwordcloud.generate_from_frequencies(words)\nax1.imshow(wordcloud, interpolation=\"bilinear\")\nax1.axis('off')\n#_____________________________________________\n# LOWER PANEL: HISTOGRAMS\nax2 = fig.add_subplot(2,1,2)\ny_axis = [i[1] for i in trunc_occurences]\nx_axis = [k for k,i in enumerate(trunc_occurences)]\nx_label = [i[0] for i in trunc_occurences]\nplt.xticks(rotation=85, fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.xticks(x_axis, x_label)\nplt.ylabel(\"Nb. of occurences\", fontsize = 18, labelpad = 10)\nax2.bar(x_axis, y_axis, align = 'center', color='g')\n#_______________________\nplt.title(\"Keywords popularity\",bbox={'facecolor':'k', 'pad':5},color='w',fontsize = 25)\nplt.show()","metadata":{"_execution_state":"idle","_uuid":"66a8dfa956a5987577e17979fcdc4a9f20905ec3","_cell_guid":"687534d1-4930-4c87-a19f-6ada504e9eba"},"outputs":[]},{"cell_type":"markdown","source":"___\n### 1.2 Filling factor: missing values\n\nThe dataset consists in 5043 films or TV series which are described by 28 variables. As in every analysis, at some point, we will have to deal with the missing values and as a first step, I determine the amount of data which is missing in every variable:","metadata":{"_uuid":"12c260e242c5433415d892a93b7f751492617dca","_cell_guid":"20b4ced7-1aca-44ef-b8e1-799384ef329a"}},{"cell_type":"code","execution_count":null,"source":"col_filling = []\nfor s in df_initial.columns:\n    ratio = (len(df_initial[s])-df_initial[s].isnull().sum()) / len(df_initial[s])*100\n    number = df_initial[s].notnull().sum()\n    col_filling.append([ratio, s, number])\ncol_filling.sort(key = lambda x:x[0])\n#------------------------------------\nfor ratio, s, number in col_filling:\n    print(\"{:<30} -> {:<6}%\".format(s, round(ratio,2)))","metadata":{"_execution_state":"idle","_uuid":"aad4972a1f5fc6c09d09ae88942f00b0bee5ef02","_cell_guid":"18aadb88-4b2e-44e5-b07d-174ba771bba4"},"outputs":[]},{"cell_type":"markdown","source":"We can see that most of the variables are well filled since only 2 of them have a filling factor below 93%.","metadata":{"_uuid":"b2c96e18d56cc71862ec7b98a79b68ba147cd3ba","_cell_guid":"bd8d7a6a-e81e-414a-816e-12de72394fab"}},{"cell_type":"markdown","source":"___\n### 1.3 Number of films per year","metadata":{"_uuid":"0bbd998612332a6f20da3d123a42fd267d928795","_cell_guid":"e312d4f4-4887-458a-ac4f-ffa43c83d8c1"}},{"cell_type":"markdown","source":"The variable '*title_year*' deals with the year the films came out. In order to have a global look at the way films are distributed according to this variable, I group the films by decades:","metadata":{"_uuid":"ebbb301e42d23c1b6448b133c0bfc31f25aaccd6","_cell_guid":"56b5904c-e64f-4c29-9d54-f8dcd21cc2de"}},{"cell_type":"code","execution_count":null,"source":"df_initial['decade'] = df_initial['title_year'].apply(lambda x:((x-1900)//10)*10)\n#__________________________________________________________________\n# function that extract statistical parameters from a grouby objet:\ndef get_stats(group):\n    return {'min': group.min(), 'max': group.max(),\n            'count': group.count(), 'mean': group.mean()}\n#______________________________________________________________\n# Creation of a dataframe with statitical infos on each decade:\ntest = df_initial['title_year'].groupby(df_initial['decade']).apply(get_stats).unstack()","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"4d31554e7683baa153157092bb721091cac8a599","_cell_guid":"cb9c0079-813c-43dd-82e5-a419fc248b54"},"outputs":[]},{"cell_type":"markdown","source":"and represent the results in a pie chart:","metadata":{"_uuid":"4960cd4eebd0ff77014ad56fe6c3f3510a9bc4ca","_cell_guid":"e4c397a6-92a1-445e-b479-4ba6ae91358e"}},{"cell_type":"code","execution_count":null,"source":"sns.set_context(\"poster\", font_scale=0.85)\n#_______________________________\n# funtion used to set the labels\ndef label(s):\n    val = (1900 + s, s)[s < 100]\n    chaine = '' if s < 50 else \"{}'s\".format(val)\n    return chaine\n#    if s < 50:        \n#        return ''\n#    elif s < 100:\n#        return \"{}'s\".format(int(s))\n#    else:\n#        return \"{}'s\".format(int(1900+s))\n#____________________________________\nplt.rc('font', weight='bold')\nf, ax = plt.subplots(figsize=(14, 6))\nlabels = [label(s) for s in  test.index]\nsizes  = test['count'].values\nexplode = [0.2 if sizes[i] < 100 else 0.01 for i in range(11)]\nax.pie(sizes, explode = explode, labels=labels,\n       autopct = lambda x:'{:1.0f}%'.format(x) if x > 1 else '',\n       shadow=False, startangle=0)\nax.axis('equal')\nax.set_title('% of films per decade',\n             bbox={'facecolor':'k', 'pad':5},color='w', fontsize=16);","metadata":{"_execution_state":"idle","_uuid":"51a04eb63529f7552ba60c0cb0cd21de4ea80978","_cell_guid":"26d9c015-c900-4843-9c71-5976e580c70f"},"outputs":[]},{"cell_type":"markdown","source":"___\n### 1.4 Genres","metadata":{"_uuid":"1e4246012eafcad44861046fffa8d72faf16df3c","_cell_guid":"b36bdcca-b93e-48da-9b6d-9b57cd7e5438"}},{"cell_type":"markdown","source":"The *'genres'* variable will surely be important while building the recommendation engines since it describes the content of the film (i.e. Drama, Comedy, Action, ...). To see exactly which genres are the most popular, I use the same approach than for the keywords (hence using similar lines of code), first making a census of the genres:","metadata":{"_uuid":"dccd6736257491b6eed0e62fd2769b73c8c5ff21","_cell_guid":"b6903dcb-ec86-4030-b30e-9e02a2c9bf19"}},{"cell_type":"code","execution_count":null,"source":"genre_labels = set()\nfor s in df_initial['genres'].str.split('|').values:\n    genre_labels = genre_labels.union(set(s))","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"0bad725974f487bfd59bc83b9c3ab6b87e8e3db4","_cell_guid":"c197648d-541b-4b50-b7f2-6accf02b5c4b"},"outputs":[]},{"cell_type":"markdown","source":"and then counting how many times each of them occur:","metadata":{"_uuid":"6f0c6aff62d4487aa8062089b03a0746ab61062b","_cell_guid":"20f128d8-ba23-4eed-813e-54a8e0605b73"}},{"cell_type":"code","execution_count":null,"source":"keyword_occurences, dum = count_word(df_initial, 'genres', genre_labels)\nkeyword_occurences[:5]","metadata":{"_execution_state":"idle","_uuid":"74ba8cbae89e5948d79ecf27682888dada29638e","_cell_guid":"313cfdcf-ac91-4eec-ada9-3ec9e2ea15ef"},"outputs":[]},{"cell_type":"markdown","source":"Finally, the results is shown as a wordcloud:","metadata":{"_uuid":"219418cf3a50c21fe04d74c02ffdffd768c25fbf","_cell_guid":"8aed2009-1224-4498-9816-b5724f87a3f1"}},{"cell_type":"code","execution_count":null,"source":"words = dict()\ntrunc_occurences = keyword_occurences[0:50]\nfor s in trunc_occurences:\n    words[s[0]] = s[1]\ntone = 100 # define the color of the words\nf, ax = plt.subplots(figsize=(14, 6))\nwordcloud = WordCloud(width=550,height=300, background_color='black', \n                      max_words=1628,relative_scaling=0.7,\n                      color_func = random_color_func,\n                      normalize_plurals=False)\nwordcloud.generate_from_frequencies(words)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"_execution_state":"idle","_uuid":"d10282c2993c6d45cbb496bb47e235f9bb0d5360","_cell_guid":"171e21ce-79d4-42a7-b66f-bc88b3b1030b"},"outputs":[]},{"cell_type":"markdown","source":"___\n## 2. Cleaning\n___\n\n### 2.1 Duplicated entries\n\nUntill there, I only looked at a few variables and mainly tried to represent their content to have an idea on their meaning. Hence, the cleaning of the dataset really starts at this point.\n\nThe first stage consists in checking for the existence of duplicated entries (**thanks [ArjoonnSharma](https://www.kaggle.com/arjoonn/movie-recommendations) for pointing this out !!!**). As a first step, I check if duplicated entries are present at identical. In python, this is made quite easy using the *duplicated()* method: \n","metadata":{"_execution_state":"idle","_uuid":"8fddab99c6ca27d3584d03ff7c4da68d3e6636da","_cell_guid":"8c070672-757f-4eaf-9611-2122189692ac"}},{"cell_type":"code","execution_count":null,"source":"doubled_entries = df_initial[df_initial.duplicated()]\ndoubled_entries.info(verbose=False)","metadata":{"_execution_state":"idle","_uuid":"3ea65d2dc86d8cc8fbcb601461b51cb0ab9eba66","_cell_guid":"fb715af6-1431-48a0-be18-ad71fff9cee3"},"outputs":[]},{"cell_type":"markdown","source":"Here, I created the *doubled_entries* dataframe that contains all the films and TV series that appear at least two times in the database. Hence, displaying its characteristics, we are told that 45 entries are doubled. I simply remove these entries:","metadata":{"_uuid":"e77e9c6e3e397ed4c5da9342d358cd1aef308e3f","_cell_guid":"c7c941a0-979d-403f-a969-f6c7225e1b18"}},{"cell_type":"code","execution_count":null,"source":"df_temp = df_initial.drop_duplicates()","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"e41961677edd23ea00ca6e1d297cc84d65b9f7f4","_cell_guid":"8208c1f3-2c6a-4a55-bb5d-edd6e5f8d3e6"},"outputs":[]},{"cell_type":"markdown","source":"Now, I examine the rows with entries **only** duplicated according to the  *'movie\\_title', 'title\\_year', and 'director\\_name'* variables. First, I create a list of the entries with identical titles:","metadata":{"_execution_state":"idle","_uuid":"2908d11c164bb86a85c4c1e6d361cd78ccfa58f0","_cell_guid":"5a325b89-d071-4088-b029-ba1364bdf793"}},{"cell_type":"code","execution_count":null,"source":"liste_duplicates = df_temp['movie_title'].map(df_temp['movie_title'].value_counts() > 1)\nprint(\"Nb. of duplicate entries: {}\".format(\n    len(df_temp[liste_duplicates][['movie_title','title_year','director_name']])))","metadata":{"_execution_state":"idle","_uuid":"9992b2b52320245ea76d407df8e35599178e0657","_cell_guid":"88a14ae5-f010-4ed8-8a0d-dff53519014a"},"outputs":[]},{"cell_type":"markdown","source":"and then, I examine in a few cases the variables for which the entries differ. Given that there are not so many duplicated values, I don't try to automate the process and have a look, *by eyes*, at the data:","metadata":{"_uuid":"1c38e58374733c584de77d76013a2bbf84b621af","_cell_guid":"51d9c637-8c57-448e-9faf-17c2650e09f1"}},{"cell_type":"code","execution_count":null,"source":"df_temp[liste_duplicates][['movie_title','title_year',\n                           'director_name']].sort_values('movie_title')[31:41]","metadata":{"_execution_state":"idle","_uuid":"f09a972861c02bfb0048761c50f6b4a4fcbd1cb0","_cell_guid":"c33662c1-8e9a-45a6-9ce8-655a00110e51"},"outputs":[]},{"cell_type":"markdown","source":"At this point, I checked on a few entries the reason why two entries with the same title are not identical (i.e. if the *duplicated()* method did not catch them, it means that these entries differ according to a few variables). Hence, I look in a few cases the variables on which differences exist, and for example in the case of the *Disturbia* film, the differences concern the *'actor_3_facebook_likes' *, *'num_voted_user'* and *'cast_total_facebook_likes'* variables:","metadata":{"_uuid":"a59c6a6b07bf0ef300ccaec6ad2971159834b82f","_cell_guid":"4da4ca60-e474-48ff-be69-c06a6b7786e7"}},{"cell_type":"code","execution_count":null,"source":"df_initial.iloc[2166] != df_initial.iloc[1319]","metadata":{"_execution_state":"idle","_uuid":"c28cb4d2c16beb69e434dcfae4e134bc35663959","_cell_guid":"fbf5b033-a0d2-46e4-8fe0-9ce119901441"},"outputs":[]},{"cell_type":"markdown","source":"\nIn most of the cases I've checked, the differences deal with minor points (as e.g. the number of facebook likes) and I thus remove the duplicated rows, with the arbitrary choice of keeping the last entry that appear in the index:","metadata":{"_uuid":"b47154954e6b7342cf99e762caa7b3d413fa7a31","_cell_guid":"fbfa28b4-66c2-44ae-8168-51eb3bc2e821"}},{"cell_type":"code","execution_count":null,"source":"df_duplicate_cleaned = df_temp.drop_duplicates(subset=['movie_title',\n                                'title_year', 'director_name'], keep='last')","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"d6831ad743e9e2f3500fc8ca347af6dc53962fb9","_cell_guid":"abdcb1d3-0e87-465e-8927-d68c02e0d9c2"},"outputs":[]},{"cell_type":"markdown","source":"___\n### 2.2 Cleaning of the keywords\n#### 2.2.1 Grouping by *roots*\n\nI collect the keywords that appear in the *'plot\\_keywords'* variable. This list is then cleaned using the NLTK package. Finally, I look for the number of occurence of the various keywords.","metadata":{"_execution_state":"idle","_uuid":"0c4c124d589daa3925cfed199cc4cc7aea7022bf","_cell_guid":"334aaf94-70d4-42a8-ad53-62a9c9b7acc4"}},{"cell_type":"code","execution_count":null,"source":"# Collect the keywords\n#----------------------\ndef keywords_inventory(dataframe, colonne = 'plot_keywords'):\n    PS = nltk.stem.PorterStemmer()\n    keywords_roots  = dict()  # collect the words / root\n    keywords_select = dict()  # association: root <-> keyword\n    category_keys = []\n    icount = 0\n    for s in dataframe[colonne]:\n        if pd.isnull(s): continue\n        for t in s.split('|'):\n            t = t.lower() ; racine = PS.stem(t)\n            if racine in keywords_roots:                \n                keywords_roots[racine].add(t)\n            else:\n                keywords_roots[racine] = {t}\n    \n    for s in keywords_roots.keys():\n        if len(keywords_roots[s]) > 1:  \n            min_length = 1000\n            for k in keywords_roots[s]:\n                if len(k) < min_length:\n                    clef = k ; min_length = len(k)            \n            category_keys.append(clef)\n            keywords_select[s] = clef\n        else:\n            category_keys.append(list(keywords_roots[s])[0])\n            keywords_select[s] = list(keywords_roots[s])[0]\n                   \n    print(\"Nb of keywords in variable '{}': {}\".format(colonne,len(category_keys)))\n    return category_keys, keywords_roots, keywords_select","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"ffb44a711a765a47557b8480192cc5ccb667c331","_cell_guid":"b121647d-1f5b-4f9b-af9e-6026fa53d03a"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"keywords, keywords_roots, keywords_select = keywords_inventory(df_duplicate_cleaned,\n                                                               colonne = 'plot_keywords')","metadata":{"_execution_state":"idle","_uuid":"7136ff25d34f689e5464c3d8e33a4d720c381bea","_cell_guid":"24a939ab-6556-48a4-b1aa-cb9cf35a6a6a"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# Plot of a sample of keywords that appear in close varieties \n#------------------------------------------------------------\nicount = 0\nfor s in keywords_roots.keys():\n    if len(keywords_roots[s]) > 1: \n        icount += 1\n        if icount < 15: print(icount, keywords_roots[s], len(keywords_roots[s]))","metadata":{"_execution_state":"idle","_uuid":"12bcc43bdd2c010dddaef2247adba337f21e95f3","_cell_guid":"7542eb0b-2b92-42f5-bfd0-88dfcbe6a663"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# Replacement of the keywords by the main form\n#----------------------------------------------\ndef remplacement_df_keywords(df, dico_remplacement, roots = False):\n    df_new = df.copy(deep = True)\n    for index, row in df_new.iterrows():\n        chaine = row['plot_keywords']\n        if pd.isnull(chaine): continue\n        nouvelle_liste = []\n        for s in chaine.split('|'): \n            clef = PS.stem(s) if roots else s\n            if clef in dico_remplacement.keys():\n                nouvelle_liste.append(dico_remplacement[clef])\n            else:\n                nouvelle_liste.append(s)       \n        df_new.set_value(index, 'plot_keywords', '|'.join(nouvelle_liste)) \n    return df_new","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"d9391610e7646e1a8467b76e9b6014ac6107a4de","_cell_guid":"548a0283-057e-47cb-9b14-87ee10350178"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# Replacement of the keywords by the main keyword\n#-------------------------------------------------\ndf_keywords_cleaned = remplacement_df_keywords(df_duplicate_cleaned, keywords_select,\n                                               roots = True)","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"25bbe012af8d8b001a0d84633cb6c59521bd2d02","_cell_guid":"bb89844a-37c4-4a7b-9ce9-62d3c6f7fe2f"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# Count of the keywords occurences\n#----------------------------------\nkeyword_occurences, keywords_count = count_word(df_keywords_cleaned,'plot_keywords',keywords)\nkeyword_occurences[:5]","metadata":{"_execution_state":"idle","_uuid":"b48eeb3016887225e2637eba60bcab601cf276ce","_cell_guid":"30f52dd3-43ca-4faa-a50c-de11577c9334"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# Graph showing the number of keywords occurences\n#-------------------------------------------------------------------\ntrunc_occurences = keyword_occurences[:]\ny_axis = [i[1] for i in trunc_occurences]\nx_axis = [k for k,i in enumerate(trunc_occurences)]\n\nf, ax = plt.subplots(figsize=(9, 5))\nax.plot(x_axis, y_axis,'r-')\nplt.ylim((0,25))\nplt.axhline(y=5, linewidth=2, color = 'k')\nplt.xlabel(\"Keyword Index\", family='fantasy', fontsize = 15)\nplt.ylabel(\"Occurences\", family='fantasy', fontsize = 15);","metadata":{"_execution_state":"idle","_uuid":"1194ee561b2bca54c27e505b7d41ea6d1891fab0","_cell_guid":"ca6f5fe5-9163-45d0-bb35-882e2e3b350c"},"outputs":[]},{"cell_type":"markdown","source":"#### 2.2.2 Groups of *synonyms*\n\nI clean the list of keywords in two steps. As a first step, I suppress the keywords that appear less that 5 times and replace them by a synomym of higher frequency. As a second step, I suppress all the keywords that appear in less than 3 films","metadata":{"_execution_state":"idle","_uuid":"2832d640d8b3c37c83534a076c768a32b49e12f6","_cell_guid":"15a9ef13-de7c-4644-8bfb-003b34b6b0c6"}},{"cell_type":"code","execution_count":null,"source":"# get the synomyms of the word 'mot_cle'\n#--------------------------------------------------------------\ndef get_synonymes(mot_cle):\n    lemma = set()\n    for ss in wordnet.synsets(mot_cle):\n        for w in ss.lemma_names():\n            #_______________________________\n            # We just get the 'nouns':\n            index = ss.name().find('.')+1\n            if ss.name()[index] == 'n': lemma.add(w.lower().replace('_',' '))\n    return lemma   ","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"0a9979a6802e4d19c3b4244768b751db32811953","_cell_guid":"4f2c2038-f322-4b05-bffb-8319c09aeadc"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# Exemple of a list of synonyms given by NLTK\n#---------------------------------------------------\nmot_cle = 'alien'\nlemma = get_synonymes(mot_cle)\nfor s in lemma:\n    print(' \"{:<30}\" in keywords list -> {} {}'.format(s, s in keywords,\n                                                keywords_count[s] if s in keywords else 0 ))","metadata":{"_execution_state":"idle","_uuid":"bd8db5b6f187db5ceb9c0cb84b9b8482e13a13a5","_cell_guid":"cb0e53b9-e2fc-4ff7-820b-a299137a4d2b"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# check if 'mot' is a key of 'key_count' with a test on the number of occurences   \n#----------------------------------------------------------------------------------\ndef test_keyword(mot, key_count, threshold):\n    return (False , True)[key_count.get(mot, 0) >= threshold]","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"192f075099517dbc9efb681ddb88a741d7c8f09d","_cell_guid":"a725d0f4-6e4d-410a-8671-719d7a340a72"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"keyword_occurences.sort(key = lambda x:x[1], reverse = False)\nkey_count = dict()\nfor s in keyword_occurences:\n    key_count[s[0]] = s[1]\n#__________________________________________________________________________\n# Creation of a dictionary to replace keywords by higher frequency keywords\nremplacement_mot = dict()\nicount = 0\nfor index, [mot, nb_apparitions] in enumerate(keyword_occurences):\n    if nb_apparitions > 5: continue  # only the keywords that appear less than 5 times\n    lemma = get_synonymes(mot)\n    if len(lemma) == 0: continue     # case of the plurals\n    #_________________________________________________________________\n    liste_mots = [(s, key_count[s]) for s in lemma \n                  if test_keyword(s, key_count, key_count[mot])]\n    liste_mots.sort(key = lambda x:(x[1],x[0]), reverse = True)    \n    if len(liste_mots) <= 1: continue       # no replacement\n    if mot == liste_mots[0][0]: continue    # replacement by himself\n    icount += 1\n    if  icount < 8:\n        print('{:<12} -> {:<12} (init: {})'.format(mot, liste_mots[0][0], liste_mots))    \n    remplacement_mot[mot] = liste_mots[0][0]\n\nprint(90*'_'+'\\n'+'The replacement concerns {}% of the keywords.'\n      .format(round(len(remplacement_mot)/len(keywords)*100,2)))","metadata":{"_execution_state":"idle","_uuid":"322e9eef8e0b8bb72ba9f5ff62a72d98a6f0bd1c","_cell_guid":"66aaaa30-b7b4-4ceb-9dc0-5a4a391013b8"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# 2 successive replacements\n#---------------------------\nprint('Keywords that appear both in keys and values:'.upper()+'\\n'+45*'-')\nicount = 0\nfor s in remplacement_mot.values():\n    if s in remplacement_mot.keys():\n        icount += 1\n        if icount < 10: print('{:<20} -> {:<20}'.format(s, remplacement_mot[s]))\n\nfor key, value in remplacement_mot.items():\n    if value in remplacement_mot.keys():\n        remplacement_mot[key] = remplacement_mot[value]                    ","metadata":{"_execution_state":"idle","_uuid":"5a487f048d7be20cd0f12c122c9b5f30dd5beb2a","_cell_guid":"af5cad55-1e85-41ff-b9b0-e0e9dc96f94b"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# replacement of keyword varieties by the main keyword\n#----------------------------------------------------------\ndf_keywords_synonyms = \\\n            remplacement_df_keywords(df_keywords_cleaned, remplacement_mot, roots = False)   \nkeywords, keywords_roots, keywords_select = \\\n            keywords_inventory(df_keywords_synonyms, colonne = 'plot_keywords')","metadata":{"_execution_state":"idle","_uuid":"880770ea60e1906befe9e16f56a25155f78cea00","_cell_guid":"dcae46b6-8d6c-4f93-acda-6d30ac3d2b5c"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# New count of keyword occurences\n#-------------------------------------\nnew_keyword_occurences, keywords_count = count_word(df_keywords_synonyms,\n                                                    'plot_keywords',keywords)\nnew_keyword_occurences[:5]","metadata":{"_execution_state":"idle","_uuid":"c3c5bfa803a2e551022e1015a8daa8dfe9de2b33","_cell_guid":"99acaa6c-02b7-4e58-8c2c-f977d3240533"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# deletion of keywords with low frequencies\n#-------------------------------------------\ndef remplacement_df_low_frequency_keywords(df, keyword_occurences):\n    df_new = df.copy(deep = True)\n    key_count = dict()\n    for s in keyword_occurences: \n        key_count[s[0]] = s[1]    \n    for index, row in df_new.iterrows():\n        chaine = row['plot_keywords']\n        if pd.isnull(chaine): continue\n        nouvelle_liste = []\n        for s in chaine.split('|'): \n            if key_count.get(s, 4) > 3: nouvelle_liste.append(s)\n        df_new.set_value(index, 'plot_keywords', '|'.join(nouvelle_liste))\n    return df_new","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"bd493cfc91d54ed80725079f920f400c0223680a","_cell_guid":"99428eb9-30ff-4eb2-9996-879ec15e60ed"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# Creation of a dataframe where keywords of low frequencies are suppressed\n#-------------------------------------------------------------------------\ndf_keywords_occurence = \\\n    remplacement_df_low_frequency_keywords(df_keywords_synonyms, new_keyword_occurences)\nkeywords, keywords_roots, keywords_select = \\\n    keywords_inventory(df_keywords_occurence, colonne = 'plot_keywords')    ","metadata":{"_execution_state":"idle","_uuid":"9b80ea897c9708925c2c595e287826a574272151","_cell_guid":"cda2d19f-bfb1-470c-848e-bfa9a305ce1d"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# New keywords count\n#-------------------\nnew_keyword_occurences, keywords_count = count_word(df_keywords_occurence,\n                                                    'plot_keywords',keywords)\nnew_keyword_occurences[:5]","metadata":{"_execution_state":"idle","_uuid":"99b47da31fde5b3b8965535099f3c07d7cf380eb","_cell_guid":"c39a0f6c-23ac-436d-83a8-d74884da34ad"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"# Graph of keyword occurences\n#----------------------------\nfont = {'family' : 'fantasy', 'weight' : 'normal', 'size'   : 15}\nmpl.rc('font', **font)\n\nkeyword_occurences.sort(key = lambda x:x[1], reverse = True)\n\ny_axis = [i[1] for i in keyword_occurences]\nx_axis = [k for k,i in enumerate(keyword_occurences)]\n\nnew_y_axis = [i[1] for i in new_keyword_occurences]\nnew_x_axis = [k for k,i in enumerate(new_keyword_occurences)]\n\nf, ax = plt.subplots(figsize=(9, 5))\nax.plot(x_axis, y_axis, 'r-', label='before cleaning')\nax.plot(new_x_axis, new_y_axis, 'b-', label='after cleaning')\n\n# Now add the legend with some customizations.\nlegend = ax.legend(loc='upper right', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nfor label in legend.get_texts():\n    label.set_fontsize('medium')\n            \nplt.ylim((0,25))\nplt.axhline(y=3.5, linewidth=2, color = 'k')\nplt.xlabel(\"keywords index\", family='fantasy', fontsize = 15)\nplt.ylabel(\"Nb. of occurences\", family='fantasy', fontsize = 15)\n#plt.suptitle(\"Nombre d'occurences des mots clés\", fontsize = 18, family='fantasy')\nplt.text(3500, 4.5, 'threshold for keyword delation', fontsize = 13)\nplt.show()","metadata":{"_execution_state":"idle","_uuid":"b1056d614700579905cdd0b23948e80ad2d06546","_cell_guid":"99a2b8c1-9095-4cd7-b1d9-1148b0ff5311"},"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Correlations","metadata":{"_execution_state":"idle","_uuid":"dc427dca19eafaa74b98c66db776e3a79f7bec38","_cell_guid":"d1c1ff38-6508-46a1-853d-7a047e1391ad"}},{"cell_type":"code","execution_count":null,"source":"# Graph showing Pearsons correlation coefficients\n#------------------------------------------------\nsns.set(context=\"paper\", font=\"fantasy\")\n#____________________________\n# Set up the font parameters\nfont = {'family' : 'fantasy',\n        'weight' : 'normal',\n        'size'   : 8}\nmpl.rc('font', **font)\n#_____________________________\n# calculations of correlations\ncorrmat = df_keywords_occurence.corr()\n#_____________________________________\nf, ax = plt.subplots(figsize=(14, 10))\nf.text(0.45, 0.93, \"Correlation coefficients\", ha='center', fontsize = 18, family='fantasy')\nsns.heatmap(corrmat, square=True, linewidths=0.01, cmap=\"coolwarm\", annot=True);","metadata":{"_execution_state":"idle","_uuid":"9174be71334c784ab4b7940ac46613aa00b89b2a","_cell_guid":"5967614e-49c4-4522-80a3-a6c40fd70105"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"df_keywords_occurence['content_rating'].unique()","metadata":{"_execution_state":"idle","_uuid":"d3770d524b67ee669a59b15f503a6751b54ad608","_cell_guid":"39e6f581-c2e9-4d58-8e2a-1d2918ab7ab8"},"outputs":[]},{"cell_type":"markdown","source":"According to the values reported above, I delete a few variables from the dataframe and then re-order the columns.","metadata":{"_execution_state":"idle","_uuid":"2fbb698728b0b5ec09d0b2d994fac406bd057ff0","_cell_guid":"a0954d22-c00e-4313-823d-94fcef88bc5f"}},{"cell_type":"code","execution_count":null,"source":"dropped_var = ['aspect_ratio', 'budget', 'facenumber_in_poster',\n               'content_rating', 'cast_total_facebook_likes']\ndf_var_cleaned = df_keywords_occurence.drop(dropped_var, axis = 1)","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"9cbf423cc2bcb9b1fbed730c528e1339ee56d54f","_cell_guid":"9260e5e6-7933-41d9-aff7-4ea52ca10d9e"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"df_var_cleaned.columns","metadata":{"_execution_state":"idle","_uuid":"9c173dc171c15ac01921492ea59f0f8ab6a936ce","_cell_guid":"8aff3fe9-7431-4194-989d-4c9724692483"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"new_col_order = ['movie_title', 'title_year', 'genres', 'plot_keywords', \n                 'director_name', 'actor_1_name', 'actor_2_name', 'actor_3_name',\n                 'director_facebook_likes', 'actor_1_facebook_likes', 'actor_2_facebook_likes',\n                 'actor_3_facebook_likes', 'movie_facebook_likes', 'num_critic_for_reviews', \n                 'num_user_for_reviews', 'num_voted_users', 'language', 'country',\n                 'imdb_score', 'movie_imdb_link', 'color', 'duration', 'gross', ]\ndf_var_cleaned = df_var_cleaned[new_col_order]","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"88bf4a7eb404a72cc10b4caf4d48fd137b35c7ca","_cell_guid":"619c35af-65c7-44ff-894a-cf4d189e992d"},"outputs":[]},{"cell_type":"markdown","source":"###  2.4 Missing values\nI examine the number of missing values in each variable and then choose a methodology to complete the dataset.","metadata":{"_execution_state":"idle","_uuid":"669d2e853a80d3f78eee906e421ebc2a68a21523","_cell_guid":"2daadcf5-3ede-442f-b5d1-6e429827d5c1"}},{"cell_type":"code","execution_count":null,"source":"df = df_var_cleaned.copy(deep = True)\n#__________________________________\n# Filling factor of the variables\n#----------------------------------\ncol_filling = []\nfor s in df.columns:\n    ratio = (len(df[s])-df[s].isnull().sum()) / len(df[s])*100\n    number = df[s].notnull().sum()\n    col_filling.append([ratio, s, number])\ncol_filling.sort(key = lambda x:x[0])\n#------------------------------------\nfor ratio, s, number in col_filling:\n    print(\"{:<30} -> {:<6}%\".format(s, round(ratio,2)))","metadata":{"_execution_state":"idle","_uuid":"a70e2a0d16ffefb38e4c2f3e60989b63fb11052c","_cell_guid":"74cf776f-c59c-4de6-9376-ce974b724b98"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"#________________________________________________\n# Graph showing how the variables are filled\n#------------------------------------------------\ny_axis = [i[0] for i in col_filling]\nx_axis = [k for k,i in enumerate(col_filling)]\nx_label = [i[1] for i in col_filling]\n\nfig = plt.figure(figsize=(14, 6))\nplt.xticks(rotation=80, fontsize = 14)\nplt.yticks(fontsize = 13)\n\nN_thresh = 5\nplt.axvline(x=N_thresh-0.5, linewidth=2, color = 'r')\nplt.text(N_thresh-4.8, 30, 'filling factor \\n < {}%'.format(round(y_axis[N_thresh],1)),\n         fontsize = 15, family = 'fantasy', bbox=dict(boxstyle=\"round\",\n                   ec=(1.0, 0.5, 0.5),\n                   fc=(0.8, 0.5, 0.5)))\n\nN_thresh = 17\nplt.axvline(x=N_thresh-0.5, linewidth=2, color = 'g')\nplt.text(N_thresh, 30, 'filling factor \\n = {}%'.format(round(y_axis[N_thresh],1)),\n         fontsize = 15, family = 'fantasy', bbox=dict(boxstyle=\"round\",\n                   ec=(1., 0.5, 0.5),\n                   fc=(0.5, 0.8, 0.5)))\n\nplt.xticks(x_axis, x_label,family='fantasy', weight = 'bold', fontsize = 14 )\nplt.ylabel('Filling factor (%)', family='fantasy', fontsize = 16, weight = 'bold')\nplt.bar(x_axis, y_axis);","metadata":{"_execution_state":"idle","_uuid":"201be9a8d2d5053e149e81e83b42d6eff29ae0c2","_cell_guid":"c41c98cc-febe-46f2-ad89-e64041256f57"},"outputs":[]},{"cell_type":"markdown","source":"#### 2.4.1 Setting missing title years\n\n To infer the title year, I use the list of actors and the director. For each of them, I determine the mean year of activity, using the current dataset. I then average the values obtained to estimate the title year.","metadata":{"_execution_state":"idle","_uuid":"46c29eb7cbc309d7e2f73fa2bd6a95ea38c0e5df","_cell_guid":"2a951eee-851a-41b8-ae0b-93bdab2b75f4"}},{"cell_type":"code","execution_count":null,"source":"df_filling = df_var_cleaned.copy(deep=True)\nmissing_year_info = df_filling[df_filling['title_year'].isnull()][[\n            'director_name','actor_1_name', 'actor_2_name', 'actor_3_name']]\nmissing_year_info[:10]","metadata":{"_execution_state":"idle","_uuid":"80212b3384a36fc36e55ecd01ca9c21ad37f2676","_cell_guid":"48d0100b-6474-47d3-9fa2-ff4324b880ae"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"def fill_year(df):\n    col = ['director_name', 'actor_1_name', 'actor_2_name', 'actor_3_name']\n    usual_year = [0 for _ in range(4)]\n    var        = [0 for _ in range(4)]\n    #_____________________________________________________________\n    # I get the mean years of activity for the actors and director\n    for i in range(4):\n        usual_year[i] = df.groupby(col[i])['title_year'].mean()\n    #_____________________________________________\n    # I create a dictionnary collectinf this info\n    actor_year = dict()\n    for i in range(4):\n        for s in usual_year[i].index:\n            if s in actor_year.keys():\n                if pd.notnull(usual_year[i][s]) and pd.notnull(actor_year[s]):\n                    actor_year[s] = (actor_year[s] + usual_year[i][s])/2\n                elif pd.isnull(actor_year[s]):\n                    actor_year[s] = usual_year[i][s]\n            else:\n                actor_year[s] = usual_year[i][s]\n        \n    #______________________________________\n    # identification of missing title years\n    missing_year_info = df[df['title_year'].isnull()]\n    #___________________________\n    # filling of missing values\n    icount_replaced = 0\n    for index, row in missing_year_info.iterrows():\n        value = [ np.NaN for _ in range(4)]\n        icount = 0 ; sum_year = 0\n        for i in range(4):            \n            var[i] = df.loc[index][col[i]]\n            if pd.notnull(var[i]): value[i] = actor_year[var[i]]\n            if pd.notnull(value[i]): icount += 1 ; sum_year += actor_year[var[i]]\n        if icount != 0: sum_year = sum_year / icount \n\n        if int(sum_year) > 0:\n            icount_replaced += 1\n            df.set_value(index, 'title_year', int(sum_year))\n            if icount_replaced < 10: \n                print(\"{:<45} -> {:<20}\".format(df.loc[index]['movie_title'],int(sum_year)))\n    return ","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"777a8bf33774302fdcdd6dca98293d45078d3fb3","_cell_guid":"69103243-063a-4420-9c7b-3dfe24f68167"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"fill_year(df_filling)","metadata":{"_execution_state":"idle","_uuid":"3880a1845967b1ebee89cce627f99d6c79e8d168","_cell_guid":"4003dd66-21ba-4147-9b89-9fc210d2ef93"},"outputs":[]},{"cell_type":"markdown","source":"#### Comparing some predictions with actual values show a reasonable agreement:\n- Bewitched: **1951** -> on TV  between 1964 et 1972 \n- The A-team: **1977** -> on TV between 1982 et 1987\n- Sleepy Hollow: **2012** -> on TV between 2013 et 2017","metadata":{"_execution_state":"idle","_uuid":"b15313f7352086829fe4fc057aec2248e4037d88","_cell_guid":"c724c594-e99f-4232-a241-5e2241cd5db7"}},{"cell_type":"markdown","source":"#### 2.4.2 Extracting keywords from the title","metadata":{"_uuid":"e093750e29f10cd54b5ab1eb834d6feb8780113c","_cell_guid":"21267377-9e73-47cb-9b9b-2ef28fee32fd"}},{"cell_type":"code","execution_count":null,"source":"icount = 0\nfor index, row in df_filling[df_filling['plot_keywords'].isnull()].iterrows():\n    icount += 1\n    liste_mot = row['movie_title'].strip().split()\n    new_keyword = []\n    for s in liste_mot:\n        lemma = get_synonymes(s)\n        for t in list(lemma):\n            if t in keywords: \n                new_keyword.append(t)                \n    if new_keyword and icount < 15: \n        print('{:<30} -> {:<30}'.format(row['movie_title'], str(new_keyword)))\n    if new_keyword:\n        df_filling.set_value(index, 'plot_keywords', '|'.join(new_keyword)) ","metadata":{"_uuid":"594f3ca1c69e1b48d3991ec25bbdc8532dff36cc","_cell_guid":"46559ffb-0d45-49fa-8998-64bf3cdf4cd7"},"outputs":[]},{"cell_type":"markdown","source":"#### 2.4.3 Imputing from regressions\n\nFor the 'gross' variable, I use the fact it is correctly correlated to the  *'num\\_voted\\_users'* variable.","metadata":{"_execution_state":"idle","_uuid":"4301edd6073e284b0dbbb309285e56926505baa1","_cell_guid":"11a7de2e-efd6-4994-814f-0b9ffad65092"}},{"cell_type":"code","execution_count":null,"source":"fig, ax = plt.subplots(figsize=(8, 5))\nplt.plot(df_filling['num_voted_users'], df_filling['gross'], 'r.')\nplt.show()","metadata":{"_execution_state":"idle","_uuid":"2fd020bd7f83d9426253d669caaebb69a617ca77","_cell_guid":"4a94c079-8246-4b47-8b52-0f3f1335bc2d"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"def variable_linreg_imputation(df, col_to_predict, ref_col):\n    regr = linear_model.LinearRegression()\n    test = df[[col_to_predict,ref_col]].dropna(how='any', axis = 0)\n    X = np.array(test[ref_col])\n    Y = np.array(test[col_to_predict])\n    X = X.reshape(len(X),1)\n    Y = Y.reshape(len(Y),1)\n    regr.fit(X, Y)\n    \n    test = df[df[col_to_predict].isnull() & df[ref_col].notnull()]\n    for index, row in test.iterrows():\n        value = float(regr.predict(row[ref_col]))\n        df.set_value(index, col_to_predict, value)","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"41762b5b00ed016f1b4430bf7255ea7eddf21d35","_cell_guid":"7c321adc-ba0e-47ca-ba61-50695c232452"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"fig, ax = plt.subplots(figsize=(8, 5))\nplt.xticks(rotation=30, fontsize = 13)\nplt.yticks(fontsize = 13)\nregr = linear_model.LinearRegression()\ntest = df_filling[['gross','num_voted_users']].dropna(how='any', axis = 0)\nX = np.array(test['num_voted_users'])\nY = np.array(test['gross'])\nX = X.reshape(len(X),1)\nY = Y.reshape(len(Y),1)\nregr.fit(X, Y)\nplt.scatter(X, Y,  color='black')\nplt.plot(X, regr.predict(X), color='blue', linewidth=3)\nax.set_xlabel('num_voted_users')\nax.set_ylabel('gross');","metadata":{"_execution_state":"idle","_uuid":"77d92006b2480e11dde8b6d44bd50c78196265e7","_cell_guid":"31c5c204-79d9-4b23-8d65-40273748d35f"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"variable_linreg_imputation(df_filling, 'gross', 'num_voted_users')","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"e433a818a7d406396a046e56982181d2ed0830b4","_cell_guid":"2d78f0b3-3ae6-4867-93a1-b629cf20a1f4"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"#df_filling.plot(x = 'num_critic_for_reviews', y = 'num_voted_users', kind = 'scatter')\ndf_filling.plot(x = 'num_critic_for_reviews', y = 'num_user_for_reviews', kind = 'scatter')\n#df_filling.plot(x = 'num_critic_for_reviews', y = 'movie_facebook_likes', kind = 'scatter')","metadata":{"_execution_state":"idle","_uuid":"821ac4e9befb5a5936de80bc03ca207836641482","_cell_guid":"3c972106-c2dc-49b3-8460-b35ad1721b40"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"variable_linreg_imputation(df_filling, 'num_critic_for_reviews', 'num_user_for_reviews')\nvariable_linreg_imputation(df_filling, 'num_critic_for_reviews', 'num_voted_users')\nvariable_linreg_imputation(df_filling, 'num_user_for_reviews', 'num_critic_for_reviews')","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"e5476e87d028dd2f2c969e8d46327c65f2e0a1bc","_cell_guid":"96dd044f-bfac-4982-b9eb-b92160a58cfb"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"df = df_filling.copy(deep = True)\n#______________________________________________\n# Filling factor of the various variables\n#----------------------------------------------\ncol_filling = []\nfor s in df.columns:\n    ratio = (len(df[s])-df[s].isnull().sum()) / len(df[s])*100\n    number = df[s].notnull().sum()\n    col_filling.append([ratio, s, number])\ncol_filling.sort(key = lambda x:x[0])\n#------------------------------------\nfor ratio, s, number in col_filling:\n    print(\"{:<30} -> {:<6}%\".format(s, round(ratio,2)))","metadata":{"_execution_state":"idle","_uuid":"207ebe11431d3dff7a9e39da675decf215f038b2","_cell_guid":"de6962e2-a2eb-43d7-8bcd-16c199573ac3"},"outputs":[]},{"cell_type":"markdown","source":"___\n## 3. RECOMMENDATION ENGINE","metadata":{"_execution_state":"idle","_uuid":"6ccb23c18bf235400abd5e0b2a4eca49f7b3adaf","_cell_guid":"b853bf02-6421-49d8-be8c-551e5a8a82e8"}},{"cell_type":"markdown","source":"___\n### 3.1 Basic functioning of the engine \nI order to build the recommendation engine, I will basically proceed in two steps:\n- 1/ determine N films with a content similar to the entry provided by the user\n- 2/ select the 5 most popular films among these N films\n\nWhen builing the engine, the first step thus consists in defining a criteria that would tell us how close two films are. To do so, I start from the description of the film that was selected by the user: from it, I get the director name, the names of the actors and a few keywords. I then build a matrix where each row corresponds to a film of the database and where the columns correspond to the previous quantities (director + actors + keywords) plus the *k* genres that were described in section 1.4:\n","metadata":{"_uuid":"9620e9305d238f723ed55be57774b92b50a4056c","_cell_guid":"a9d2ee56-2408-4064-a1be-7be6bc625dc6"}},{"cell_type":"markdown","source":"|  movie title |director   |actor 1   |actor 2   |actor 3   | keyword 1  | keyword 2   | genre 1 | genre 2 | ... | genre k |\n|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n|Film 1   | $a_{11}$  |  $a_{12}$ |   |   |  ... |   |   |   |   | $a_{1N}$  |\n|...   |   |   |   |   | ...  |   |   |   |   |   |\n|Film i   |  $a_{i1}$ | $a_{i2}$ |   |   | $a_{ij}$  |   |   |   |   |  $a_{iN}$ |\n|...   |   |   |   |   | ...  |   |   |   |   |   |\n| Film N   |$a_{M1}$   | $a_{M2}$  |   |   | ...  |   |   |   |   | $a_{MN}$  |","metadata":{"_uuid":"e96f32c1757e052c68dd46b716c1a1932ed72d75","_cell_guid":"8069a12f-08c3-4ebe-8bad-99c696cde2c2"}},{"cell_type":"markdown","source":"In this matrix, the $a_{ij}$ coefficients take either the value 0 or 1 depending on the correspondance between the significance of column $j$ and the content of film $i$. For exemple, if \"keyword 1\" is in film $i$, we will have $a_{ij}$ = 1 and 0 otherwise. Once this matrix has been defined, we determine the distance between two films according to:\n\n\\begin{eqnarray}\nd_{m, n} = \\sqrt{  \\sum_{i = 1}^{N} \\left( a_{m,i}  - a_{n,i} \\right)^2  } \n\\end{eqnarray}\n\nAt this stage, we just have to select the N films which are the closest from the entry selected by the user.\n","metadata":{"_uuid":"093a84e5a928136e52197bdb00951e6a1dddc4fa","_cell_guid":"561bfc16-d5e4-4d1b-9f88-81ddead25388"}},{"cell_type":"code","execution_count":null,"source":"gaussian_filter = lambda x,y,sigma: math.exp(-(x-y)**2/(2*sigma**2))","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"a8a467e39794a3f5edcd5487ad36b1f5df6b69cc","_cell_guid":"8090f679-085d-4891-803e-cda9d0fe9dd4"},"outputs":[]},{"cell_type":"code","execution_count":null,"source":"df = df_filling.copy(deep=True)\ndf.reset_index(inplace = True, drop = True)","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"a6a83154e83b90a5f7f3573ab01f427a80ac5bd5","_cell_guid":"9259947b-a585-4902-8fee-54190bce9339"},"outputs":[]},{"cell_type":"markdown","source":"### 3.1 Definition of the engine functions","metadata":{"_execution_state":"idle","_uuid":"5d7f9885516fc7947cfb34b285a2732dbea0f589","_cell_guid":"cee6241c-f240-4b2f-9f76-596d535f8f72"}},{"cell_type":"markdown","source":" **Function collecting some variables content**: the *entry\\_variables()* function returns the values taken by the variables *'director\\_name', 'actor\\_N\\_name'* (N $\\in$ [1:3]) and *'plot\\_keywords'* for the film selected by the user.","metadata":{"_execution_state":"idle","_uuid":"892feb73f1f43979191540d417a2ec2f772d92c0","_cell_guid":"237b934a-36e4-47e5-8150-740de0a3dc01"}},{"cell_type":"code","execution_count":null,"source":"def entry_variables(df, id_entry): \n    col_labels = []    \n    if pd.notnull(df['director_name'].iloc[id_entry]):\n        for s in df['director_name'].iloc[id_entry].split('|'):\n            col_labels.append(s)\n            \n    for i in range(3):\n        column = 'actor_NUM_name'.replace('NUM', str(i+1))\n        if pd.notnull(df[column].iloc[id_entry]):\n            for s in df[column].iloc[id_entry].split('|'):\n                col_labels.append(s)\n                \n    if pd.notnull(df['plot_keywords'].iloc[id_entry]):\n        for s in df['plot_keywords'].iloc[id_entry].split('|'):\n            col_labels.append(s)\n    return col_labels","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"1d90e930fbaa1d25d4723bd18032315f197abc08","_cell_guid":"85c5b8f4-fc43-4aa7-a416-4f21d5519476"},"outputs":[]},{"cell_type":"markdown","source":" **Function adding variables to the dataframe**: the function *add\\_variables()* add a list of variables to the dataframe given in input and initialize these variables at 0 or 1 depending on the correspondance with the description of the films and the content of the REF_VAR variable given in input.","metadata":{"_execution_state":"idle","_uuid":"92724d4c93fc72db9cf2f6491307701cddb43fce","_cell_guid":"130b1f06-8563-47ff-b441-a1a6cd36e794"}},{"cell_type":"code","execution_count":null,"source":"def add_variables(df, REF_VAR):    \n    for s in REF_VAR: df[s] = pd.Series([0 for _ in range(len(df))])\n    colonnes = ['genres', 'actor_1_name', 'actor_2_name', 'actor_3_name', 'director_name', 'plot_keywords']\n    for categorie in colonnes:\n        for index, row in df.iterrows():\n            if pd.isnull(row[categorie]): continue\n            for s in row[categorie].split('|'):\n                if s in REF_VAR: df.set_value(index, s, 1)            \n    return df","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"7a2dd4d35fc6cd6d0654c03f45626ab8a367005f","_cell_guid":"75a1a621-1e82-4a99-a91c-ab8461349d37"},"outputs":[]},{"cell_type":"markdown","source":" **Function creating a list of films**: the *recommand()* function create a list of N (= 31) films similar to the film selected by the user.","metadata":{"_execution_state":"idle","_uuid":"de9831121580d3340bfc17cd7bf4245fc12789be","_cell_guid":"1fafda6a-cc7f-43b8-9b5f-ed1dc40ccdd6"}},{"cell_type":"code","execution_count":null,"source":"def recommand(df, id_entry):    \n    df_copy = df.copy(deep = True)    \n    liste_genres = set()\n    for s in df['genres'].str.split('|').values:\n        liste_genres = liste_genres.union(set(s))    \n    #_____________________________________________________\n    # Create additional variables to check the similarity\n    variables = entry_variables(df_copy, id_entry)\n    variables += list(liste_genres)\n    df_new = add_variables(df_copy, variables)\n    #____________________________________________________________________________________\n    # determination of the closest neighbors: the distance is calculated / new variables\n    X = df_new.as_matrix(variables)\n    nbrs = NearestNeighbors(n_neighbors=31, algorithm='auto', metric='euclidean').fit(X)\n\n    distances, indices = nbrs.kneighbors(X)    \n    xtest = df_new.iloc[id_entry].as_matrix(variables)\n    xtest = xtest.reshape(1, -1)\n\n    distances, indices = nbrs.kneighbors(xtest)\n\n    return indices[0][:]\n    ","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"82b1e339bf506c77bbfe8d286b1a84f07f517dcb","_cell_guid":"26a12500-7211-4b62-92d0-017ced42be10"},"outputs":[]},{"cell_type":"markdown","source":" **Function extracting some parameters from a list of films**: the *create\\_film\\_selection()* function extracts some variables of the dataframe given in input and returns this list for a selection of N films. This list is ordered according to criteria established in the *critere\\_selection()* function.","metadata":{"_execution_state":"idle","_uuid":"32ae86b33647f04602d539d3882fff9032862ec1","_cell_guid":"7a43a8fc-46cd-42b8-a189-40d12af3cf58"}},{"cell_type":"code","execution_count":null,"source":"def extract_parameters(df, liste_films):     \n    parametres_films = ['_' for _ in range(31)]\n    i = 0\n    max_users = -1\n    for index in liste_films:\n        parametres_films[i] = list(df.iloc[index][['movie_title', 'title_year',\n                                        'imdb_score', 'num_user_for_reviews', \n                                        'num_voted_users']])\n        parametres_films[i].append(index)\n        max_users = max(max_users, parametres_films[i][4] )\n        i += 1\n        \n    title_main = parametres_films[0][0]\n    annee_ref  = parametres_films[0][1]\n    parametres_films.sort(key = lambda x:critere_selection(title_main, max_users,\n                                    annee_ref, x[0], x[1], x[2], x[4]), reverse = True)\n\n    return parametres_films ","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"446ff6e9a17fd4846af97814d19320380bd9dd40","_cell_guid":"8ac3da95-3e1f-42d0-b3d7-e8af68838a5f"},"outputs":[]},{"cell_type":"markdown","source":" **Function comparing 2 film titles**: the sequel *sequel()* function compares the 2 titles passed in input and defines if these titles are similar or not.","metadata":{"_execution_state":"idle","_uuid":"322d73e737403a7a13f45072376ec74f5f16e171","_cell_guid":"963b7f76-f471-4c37-985c-b2be09925cb8"}},{"cell_type":"code","execution_count":null,"source":"def sequel(titre_1, titre_2):    \n    if fuzz.ratio(titre_1, titre_2) > 50 or fuzz.token_set_ratio(titre_1, titre_2) > 50:\n        return True\n    else:\n        return False","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"eea92a88f609b43180774347baef19957db59ea8","_cell_guid":"c95d6c81-3dd0-4470-bbf5-10dd4fe4bb7b"},"outputs":[]},{"cell_type":"markdown","source":" **Function giving marks to films**: the *critere\\_selection()* function gives a mark to a film depending on its IMDB score,  the title year and the number of users who have voted for this film.","metadata":{"_execution_state":"idle","_uuid":"046f69a734e8d4ad681080974eb1129767de83e0","_cell_guid":"702e07ac-249d-4805-bad6-4f617704624c"}},{"cell_type":"code","execution_count":null,"source":"def critere_selection(title_main, max_users, annee_ref, titre, annee, imdb_score, votes):    \n    if pd.notnull(annee_ref):\n        facteur_1 = gaussian_filter(annee_ref, annee, 20)\n    else:\n        facteur_1 = 1\n        \n    sigma = max_users * 1.0\n\n    if pd.notnull(votes):\n        facteur_2 = gaussian_filter(votes, max_users, sigma)\n    else:\n        facteur_2 = 0\n        \n    if sequel(title_main, titre):\n        note = 0\n    else:\n        note = imdb_score**2 * facteur_1 * facteur_2\n    \n    return note","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"48b315a6e757587d1315b6c56dd9337df685ea7d","_cell_guid":"581eee7e-e59e-4682-93d9-275bfa390af7"},"outputs":[]},{"cell_type":"markdown","source":" **Function adding films**: the *add\\_to\\_selection()* function complete the *film\\_selection* list which contains 5 films that will be recommended to the user. The films are selected from the *parametres\\_films* list and are taken into account only if the title is different enough from other film titles. ","metadata":{"_execution_state":"idle","_uuid":"d0966aa6b920e1cb337736bb8eb3b1d6917c3386","_cell_guid":"931b569b-e514-4a6b-87bd-12b6f9cccc65"}},{"cell_type":"code","execution_count":null,"source":"def add_to_selection(film_selection, parametres_films):    \n    film_list = film_selection[:]\n    icount = len(film_list)    \n    for i in range(31):\n        already_in_list = False\n        for s in film_selection:\n            if s[0] == parametres_films[i][0]: already_in_list = True\n            if sequel(parametres_films[i][0], s[0]): already_in_list = True            \n        if already_in_list: continue\n        icount += 1\n        if icount <= 5:\n            film_list.append(parametres_films[i])\n    return film_list","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"7d29d3af3d1af2c0fdaa6c1e3e704f3a4c035553","_cell_guid":"5d135a13-9d18-42f0-bb11-fadd222ab5c4"},"outputs":[]},{"cell_type":"markdown","source":" **Function filtering sequels**: the *remove\\_sequels()* function remove sequels from the list if more that two films from a serie are present. The older one is kept.","metadata":{"_execution_state":"idle","_uuid":"ea6d6fd278d33e07a1dd03d3aee916060d2ab1e4","_cell_guid":"904c97a3-96e9-4725-bb2e-f716818f64b9"}},{"cell_type":"code","execution_count":null,"source":"def remove_sequels(film_selection):    \n    removed_from_selection = []\n    for i, film_1 in enumerate(film_selection):\n        for j, film_2 in enumerate(film_selection):\n            if j <= i: continue \n            if sequel(film_1[0], film_2[0]): \n                last_film = film_2[0] if film_1[1] < film_2[1] else film_1[0]\n                removed_from_selection.append(last_film)\n\n    film_list = [film for film in film_selection if film[0] not in removed_from_selection]\n\n    return film_list   ","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"cad0f673f0ad065a4733d858e5ac1fa04ece5b3f","_cell_guid":"a2b84c57-11ef-4243-a580-a3c6b27f9f07"},"outputs":[]},{"cell_type":"markdown","source":"**Main function**: create a list of 5 films that will be recommended to the user.","metadata":{"_execution_state":"idle","_uuid":"7fa2e75cfc0fc52cc72b9603e0be2193acab69f2","_cell_guid":"8b200e48-ddc1-4c8a-93ea-4baf5e66a928"}},{"cell_type":"code","execution_count":null,"source":"def find_similarities(df, id_entry, verbose = False):    \n    if verbose: \n        print(90*'_' + '\\n' + \"QUERY: films similar to id={} -> '{}'\".format(id_entry,\n                                df.iloc[id_entry]['movie_title']))\n    #____________________________________\n    liste_films = recommand(df, id_entry)\n    #__________________________________\n    # Create a list of 31 films\n    parametres_films = extract_parameters(df, liste_films)\n    #_______________________________________\n    # Select 5 films from this list\n    film_selection = []\n    film_selection = add_to_selection(film_selection, parametres_films)\n    #__________________________________\n    # delation of the sequels\n    film_selection = remove_sequels(film_selection)\n    #______________________________________________\n    # add new films to complete the list\n    film_selection = add_to_selection(film_selection, parametres_films)\n    #_____________________________________________\n    selection_titres = []\n    for i,s in enumerate(film_selection):\n        selection_titres.append([s[0].replace(u'\\xa0', u''), s[5]])\n        if verbose: print(\"nº{:<2}     -> {:<30}\".format(i+1, s[0]))\n\n    return selection_titres","metadata":{"collapsed":true,"_execution_state":"idle","_uuid":"813652bfc50c5039ccee918c788d83c697db2184","_cell_guid":"5bb9e4ef-5738-40b5-9932-413d3ebfaee0"},"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Exemple of recommendation: test-case","metadata":{"_execution_state":"idle","_uuid":"7829d84f33e21476eacd3ff0817089672fb297a9","_cell_guid":"f24f6a4b-5fe3-4729-a3f8-87f1f17b2ea9"}},{"cell_type":"code","execution_count":null,"source":"selection = dict()\nfor i in range(0, 20, 3):\n    selection[i] = find_similarities(df, i, verbose = True)","metadata":{"_execution_state":"idle","_uuid":"f9458123bdb67554a13ee84c1ca00fffa52ebe74","_cell_guid":"77d8abd4-b367-4f22-b60e-5f4503acfe2d"},"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","name":"python","version":"3.6.1","file_extension":".py","mimetype":"text/x-python"}},"nbformat_minor":2}