{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EDA for categorical variables\n\nfig, ax = plt.subplots(2,3, figsize=(16,9))\nfig.suptitle(\"Categorical Variables Distribution\")\n\n#Death Count\nsns.set_theme(style=\"darkgrid\")\nsns.countplot(ax=ax[0][0], x=\"DEATH_EVENT\", data=data)\n\n#Anemia Count\nsns.set_theme(style=\"darkgrid\")\nsns.countplot(ax=ax[1][0], x=\"anaemia\", data=data)\n\n#Diabetes Count\nsns.set_theme(style=\"darkgrid\")\nsns.countplot(ax=ax[0][1], x=\"diabetes\", data=data)\n\n#Fumadores Count\nsns.set_theme(style=\"darkgrid\")\nsns.countplot(ax=ax[1][1], x=\"smoking\", data=data)\n\n#Sex Count\nsns.set_theme(style=\"darkgrid\")\nsns.countplot(ax=ax[0][2], x=\"sex\", data=data)\n\n#high_blood_pressure Count\nsns.set_theme(style=\"darkgrid\")\nsns.countplot(ax=ax[1][2], x=\"high_blood_pressure\", data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EDA for numerican variables\n\nfig, ax = plt.subplots(2,3, figsize=(16,9))\nfig.suptitle(\"Numerical Variables Distribution\")\n\n#Age Distribution\nsns.set_theme(style=\"darkgrid\")\nsns.histplot(ax=ax[0][0], x=\"age\", data=data, kde= True, stat=\"count\")\n\n#Creatin distribution\nsns.set_theme(style=\"darkgrid\")\nsns.histplot(ax=ax[0][1], x=\"creatinine_phosphokinase\", data=data, kde= True, stat=\"count\")\n                      \n#serum_creatin distribution\nsns.set_theme(style=\"darkgrid\")\nsns.histplot(ax=ax[0][2],x=\"serum_creatinine\", data=data, kde= True, stat=\"count\")\n\n#platelets distribution\nsns.set_theme(style=\"darkgrid\")\nsns.histplot(ax=ax[1][0],x=\"platelets\", data=data, kde= True, stat=\"count\")\n\n#sodium distribution\nsns.set_theme(style=\"darkgrid\")\nsns.histplot(ax=ax[1][1],x=\"serum_sodium\", data=data, kde= True, stat=\"count\")\n\n#ejection_fraction distribution\nsns.set_theme(style=\"darkgrid\")\nsns.histplot(ax=ax[1][2], x = \"ejection_fraction\", data=data, kde= True, stat=\"count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making grapical analisis with the categorical variables\n\nsns.catplot(x=\"DEATH_EVENT\", col=\"sex\", data=data, kind=\"count\")\n\nsns.catplot(x=\"DEATH_EVENT\", col=\"smoking\", data=data, kind=\"count\")\n\nsns.catplot(x=\"DEATH_EVENT\", col=\"high_blood_pressure\", data=data, kind=\"count\")\n\nsns.catplot(x=\"DEATH_EVENT\", col=\"diabetes\", data=data, kind=\"count\")\n\nprint(\"We can have an primary aproach with this catplots. It seems to be that being an smoker, having diabities and high blood pleasure increase the probability of death\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking for correlations in the numerical data\nplt.figure(figsize=(16,9))\ncorr_matrix = data.corr()\nsns.heatmap(corr_matrix, annot=True, cbar=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clearly the dataset is unbalanced\n\ncant_heart_pos = len(data[data[\"DEATH_EVENT\"]==1])\ncant_heart_neg = len(data[data[\"DEATH_EVENT\"]==0])\n\ntasa = cant_heart_pos / len(data)\n\nprint(\"Only the {}% of the dataset are DEATH_EVENTS=1. We will use the SMOTE tecnique to balance the data\".format(tasa*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors = data.columns.values.tolist()[:12]\ntarget = data.columns.values.tolist()[12]\nX = data[predictors]\nY = data[target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making the prediction with Desicion Tree without features balancing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[target] = np.where(data[target]==1, \"yes\", \"no\")\nY = data[target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tree(max_depth, min_samples_split, n_splits):\n    \n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.model_selection import KFold, cross_val_score\n    from sklearn.metrics import classification_report\n    \n    tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=max_depth, \n                                  min_samples_split=min_samples_split, random_state=99)\n    tree.fit(X,Y)\n    \n    pred = tree.predict(X)\n    \n    \n    cv = KFold(n_splits= n_splits, shuffle=True, random_state=1)\n    \n    score = np.mean(cross_val_score(tree, X, Y, scoring = \"accuracy\", cv=cv))*100\n    \n\n\n    print(classification_report(Y, pred, target_names= [\"NO\", \"YES\"]))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree(n_splits=4, max_depth=3, min_samples_split=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making the prediction with Random Forest without features balancing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rand_for(n_estimators,min_samples_leaf):\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.model_selection import KFold, cross_val_score\n    from sklearn.metrics import classification_report\n    \n    forest_clas = RandomForestClassifier(n_jobs=2, oob_score=True, n_estimators=n_estimators, \n                                         min_samples_leaf=min_samples_leaf)\n    forest_clas.fit(X,Y)\n    \n    pred = forest_clas.predict(X)\n    \n    cv = KFold(n_splits= 4, shuffle=True, random_state=1)\n    \n    score = np.mean(cross_val_score(forest_clas, X, Y, scoring = \"accuracy\", cv=cv))*100\n    \n    print(classification_report(Y, pred, target_names= [\"NO\", \"YES\"]))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_for(n_estimators=500,min_samples_leaf=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making the prediction with Logistic Regression without features balancing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic(n_splits):\n    from sklearn import linear_model\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import KFold, cross_val_score\n    from sklearn.metrics import classification_report\n\n    logit_model = linear_model.LogisticRegression()\n    logit_model.fit(X,Y)\n    \n    pred = logit_model.predict(X)\n    \n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n\n    scores = cross_val_score(linear_model.LogisticRegression(), X, Y, scoring=\"accuracy\", cv=cv)\n    scores\n\n\n    print(classification_report(Y, pred, target_names= [\"NO\", \"YES\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic(n_splits=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SMOTE for unbalanced data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n\n\nsm = SMOTE(random_state = 2)\nX_train_res, Y_train_res = sm.fit_resample(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree with balanced Data\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import classification_report\n    \ntree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, \n                                  min_samples_split=10, random_state=99)\ntree.fit(X_train_res,Y_train_res)\npred_tree = tree.predict(X_test)\n\ntree_smote = classification_report(Y_test, pred_tree, output_dict=True,target_names= [\"NO\", \"YES\"] )\n\nprint(classification_report(Y_test, pred_tree, target_names= [\"NO\", \"YES\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest with balanced Data\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import classification_report\n\n\nforest_clas = RandomForestClassifier(n_jobs=2, oob_score=True, n_estimators=500, \n                                         min_samples_leaf=20)\nforest_clas.fit(X_train_res,Y_train_res)\n    \npred_for = forest_clas.predict(X_test)\n\nrand_smote = classification_report(Y_test, pred_for, output_dict=True,target_names= [\"NO\", \"YES\"] )\n\nprint(classification_report(Y_test, pred_for, target_names= [\"NO\", \"YES\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression with balanced Data\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import classification_report\n\nlogit_model = linear_model.LogisticRegression()\nlogit_model.fit(X_train_res,Y_train_res)\n    \npred_log = logit_model.predict(X_test)\n\nlog_smote = classification_report(Y_test, pred_log, output_dict=True,target_names= [\"NO\", \"YES\"] )\n\n\nprint(classification_report(Y_test, pred_log, target_names= [\"NO\", \"YES\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_smote = [tree_smote[\"YES\"][\"recall\"] , rand_smote[\"YES\"][\"recall\"] , log_smote[\"YES\"][\"recall\"]]\nrecall = [0.71, 0.70, 0.66] #Recall values for modeling without smote\nlabel = [\"Tree Model\", \"Random Forest\", \"Logaritm\"]\n\naccuracy_smote = [tree_smote[\"accuracy\"] , rand_smote[\"accuracy\"] , log_smote[\"accuracy\"]]\naccuracy = [0.86, 0.87, 0.82] #Recall values for modeling without smote","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(label, recall, c=\"r\", label=\"Unbalanced\")\nplt.plot(label, recall_smote,label=\"Balanced\")\nplt.legend(loc=\"upper right\")\nplt.title(\"Recall Comparision Between Unbalanced and SMOTE Balanced\")\nplt.show()\n\nplt.plot(label, accuracy, c=\"r\", label=\"Unbalanced\")\nplt.plot(label, accuracy_smote, label=\"Balanced\")\nplt.legend(loc=\"upper right\")\nplt.title(\"Accuracy Comparision Between Unbalanced and SMOTE Balanced\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Conclusion:\n\n    #Balancing the data with SMOTE and NeverMiss tecniques doensÂ´t have an impovement in mean accuracy in the 3 models i choose. But the random forest mantains almost with the same acc rate.\n    \n    #In this particulary dataset is more important to predcit as much as possible the heart attacks, even if to achive that you increase the FN rate. Balancing the data shows an highly increas on the Recall rate, that means that the model predict more heart attacks.\n    \n    #The best model is the Random Forest with the data balanced because it achives an an 82% of recall","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}