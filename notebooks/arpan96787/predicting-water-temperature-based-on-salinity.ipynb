{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This is a kernel I am doing to practice Linear Regression in one variable. Goal is to find a relationship between Water Salinity and Water Temperature based on the calcofi dataset(bottle.csv)**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/calcofi/bottle.csv\")\ndata.head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the dataset, our sole purpose is to find relationship between water temperature and salinity so looking at water salinity values we have quite a lot of rows with NaN values and since most of the values are around 33 we fill those NaN values with the mean of Salinity."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Salnty\"].head\ndata[\"Salnty\"].isna().sum()\nmean_salinity = np.mean(data[\"Salnty\"])\nprint(mean_salinity)\nSalinity_Independent_X = data[\"Salnty\"].fillna(mean_salinity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also check for any NaN values in the Temperature column and see what we can do to fill those NaN values up."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"T_degC\"].head\nmax_temp = data[\"T_degC\"].max()\nmin_temp = data[\"T_degC\"].min()\nmean_temp = np.mean(data[\"Salnty\"])\ntemperature_dependent_y = data[\"T_degC\"].fillna(random.uniform(min_temp, max_temp))\n#we are also going to fill up those NaN values with random values \n#ranging between min and max which is 1.44 and 31.14 since there are a lot of NaN values and corresponding salinity values are all around 33.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (20.0, 10.0)\nplt.scatter(Salinity_Independent_X, temperature_dependent_y, c=\"#ef5423\", label= \"Scatter Plot\")\nplt.xlabel(\"Salinity\")\nplt.ylabel(\"Temperature\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am going to just use the ordinary least squares method to find the theta0 and theta1 values. These are the values that provide with a best fit for the given data."},{"metadata":{"trusted":true},"cell_type":"code","source":"numerator = 0\ndenominator = 0\nm = data[\"Salnty\"].size\n\nx = Salinity_Independent_X\ny = temperature_dependent_y\n\nx_mean = np.mean(x)\ny_mean = np.mean(y)\n\nfor i in range(m):\n    numerator += (x[i] - x_mean)* (y[i] - y_mean)\n    denominator += (x[i] - x_mean)**2\n\ntheta1 = numerator / denominator\ntheta0 = y_mean - (theta1 * x_mean)\n\nprint(theta1, theta0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From using this method of ordinary least squares, we find that theta1 value is -4.613 which determines that there is a negative correlation between the 2 variables.\n\nNow we will display a few diagnostic plots that I learned while doing Rachel's Regression Challenge Day 2.\nWant more info on the uses and significance of the diagnostic plots. Read this https://www.kaggle.com/rtatman/regression-challenge-day-2"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nimport statsmodels.graphics\nimport seaborn as sns\nfrom statsmodels.graphics.gofplots import ProbPlot\n\n#Using the Gaussian family we see that we are fitting a Simple Linear Regression Model.\nmodel = sm.GLM(temperature_dependent_y, Salinity_Independent_X, family=sm.families.Gaussian()).fit()\n\nsns.regplot(Salinity_Independent_X, model.resid_deviance, fit_reg=False)\nplt.title('Residual plot')\nplt.xlabel('Salinity_Independent_x')\nplt.ylabel('Residuals');\n\n#From the below plot we can see that the residual plot is like a shapeless cloud so this diagnostic plot is quite correct as we want a normally distributed\n#cloud of values from this plot.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# statsmodels Q-Q plot on model residuals\nQQ = ProbPlot(model.resid_deviance)\nfig = QQ.qqplot(alpha=0.5, markersize=5);\n#From this plot we see that we have a diagonal line from bottom to top right with a high skew at the bottom which is what we want. This is how \n#we want the plot to be more or less.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get data relating to high leverage points using statsmodels\n\n# fit OLS regression model \nmodel_g = sm.OLS(temperature_dependent_y, Salinity_Independent_X).fit()\n\n# leverage, from statsmodels\nmodel_leverage = model_g.get_influence().hat_matrix_diag\n# cook's distance, from statsmodels\nmodel_cooks = model_g.get_influence().cooks_distance[0]\n\n# plot cook's distance vs high leverage points\nsns.regplot(model_leverage, model_cooks, fit_reg=False)\nplt.xlim(xmin=-0.005, xmax=0.02)\nplt.xlabel('Leverage')\nplt.ylabel(\"Cook's distance\")\nplt.title(\"Cook's vs Leverage\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that all the data points are highly clustered around the 0 side which is how we want it to be."},{"metadata":{},"cell_type":"markdown","source":"We now use the Sklearn Linear Regression and use train test split to train the model and then see how it works on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression \nimport pandas as pd  \nimport numpy as np  \nimport matplotlib.pyplot as plt  \nimport seaborn as sns \nfrom sklearn.model_selection import train_test_split \nfrom sklearn import metrics\n%matplotlib inline\n\nX = Salinity_Independent_X.values.reshape(-1,1)\ny = temperature_dependent_y.values.reshape(-1,1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = LinearRegression()  \nregressor.fit(X_train, y_train) #training the algorithm\n#To retrieve the intercept:\nprint(regressor.intercept_)\n#For retrieving the slope:\nprint(regressor.coef_)\n\n#The regressor.coef is our theta1 value where as regressor.intercept value is our theta0 value from our ordinary least squares method.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = regressor.predict(X_test)\ndf = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred.flatten()})\nprint(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.head(25)\ndf1.plot(kind='bar',figsize=(16,10))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Many of the predicted values are close to the actual value which means that our model is quite good in predicting \nthe temperature based on salinity values. Now we will plot a regression line. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X_test, y_test,  color='gray')\nplt.plot(X_test, y_pred, color='red', linewidth=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we calculate some metrics from the fitted model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we come to an end to this kernel. My first Kernel after reading and practicing by myself with some common datasets. I went through Rachel Taetman's Datasets for Regression Analysis and thought I would practice on this and write my first Kernel of Linear Regression.\n\nI would love everyone to give me feedback on what all I could have done with this dataset and how I could improve on while practicing data science. Thank you."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}