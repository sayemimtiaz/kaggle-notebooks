{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This kernel explores different factors driving satisfaction of the airline passengers. The data set was quite clean devoid of the hassles related to imbalance or too many missing values etc. Hence, I concentrated mainly on doing extensive EDA through putting different visualizations here. Finally, I applied eight models and compared their performances to select the best one.\n\n# Importing Data","metadata":{}},{"cell_type":"code","source":"# Importing data\nimport pandas as pd\ntrain = pd.read_csv('../input/airline-passenger-satisfaction/train.csv')\ntest = pd.read_csv('../input/airline-passenger-satisfaction/test.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get row and column count\ntrain.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a snapshot of data\ntrain.head(10)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop unnecessary columns\ntrain = train.drop('Unnamed: 0', axis=1)\ntrain = train.drop('id', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check size of the data set\ntrain.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Repeating the same steps for test data set as well...","metadata":{}},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.drop('Unnamed: 0', axis=1)\ntest = test.drop('id', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace spaces in the column names with underscore\ntrain.columns = [c.replace(' ', '_') for c in train.columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.columns = [c.replace(' ', '_') for c in test.columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['satisfaction'].replace({'neutral or dissatisfied': 0, 'satisfied': 1},inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['satisfaction'].replace({'neutral or dissatisfied': 0, 'satisfied': 1},inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking for Imbalance","metadata":{}},{"cell_type":"code","source":"# Checking the nature of data set: balanced or imbalanced?\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize = (8,5))\ntrain.satisfaction.value_counts(normalize = True).plot(kind='bar', color= ['darkorange','steelblue'], alpha = 0.9, rot=0)\nplt.title('Satisfaction Indicator (0) and (1) in the Dataset')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plot shows a distribution of around 55%:45% between neutral/dissatisfied passengers and satisfied passengers respectively. So the data is quite balanced and it does not require any special treatment/resampling.\n\n# Handling of Missing Data","metadata":{}},{"cell_type":"code","source":"# Missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\nmissing = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imputing missing value with mean\ntrain['Arrival_Delay_in_Minutes'] = train['Arrival_Delay_in_Minutes'].fillna(train['Arrival_Delay_in_Minutes'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Arrival_Delay_in_Minutes'] = test['Arrival_Delay_in_Minutes'].fillna(test['Arrival_Delay_in_Minutes'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the list of categorical variables\ntrain.select_dtypes(include=['object']).columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace NaN with mode for categorical variables\ntrain['Gender'] = train['Gender'].fillna(train['Gender'].mode()[0])\ntrain['Customer_Type'] = train['Customer_Type'].fillna(train['Customer_Type'].mode()[0])\ntrain['Type_of_Travel'] = train['Type_of_Travel'].fillna(train['Type_of_Travel'].mode()[0])\ntrain['Class'] = train['Class'].fillna(train['Class'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Gender'] = test['Gender'].fillna(test['Gender'].mode()[0])\ntest['Customer_Type'] = test['Customer_Type'].fillna(test['Customer_Type'].mode()[0])\ntest['Type_of_Travel'] = test['Type_of_Travel'].fillna(test['Type_of_Travel'].mode()[0])\ntest['Class'] = test['Class'].fillna(test['Class'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nwith sns.axes_style(style='ticks'):\n    g = sns.catplot(\"satisfaction\", col=\"Gender\", col_wrap=2, data=train, kind=\"count\", height=2.5, aspect=1.0)  \n    g = sns.catplot(\"satisfaction\", col=\"Customer_Type\", col_wrap=2, data=train, kind=\"count\", height=2.5, aspect=1.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Gender:** \nIt is observed that gender-wise distribution of dissatisfied and satisfied customers are quite same. For both male and female passengers, no. of dissatisfied customers are on the higher side compared to no. of satisfied customers.\n\n**Customer Type:**\nLoyal passengers are very high in number. Even among loyal passengers, the ratio of satisfied and dissatidfied ones are almost 49:51. ","metadata":{}},{"cell_type":"code","source":"with sns.axes_style('white'):\n    g = sns.catplot(\"Age\", data=train, aspect=3.0, kind='count', hue='satisfaction', order=range(5, 80))\n    g.set_ylabels('Age vs Passenger Satisfaction')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Age:**\nFrom age 7-to-38 and from age 61-to-79, quotient of dissatisfied passengers is very high compared to satisfied passengers. On the contrary, in age range 39-60, quotient of satisfied passengers is higher compared to dissatisfied passengers.","metadata":{}},{"cell_type":"code","source":"with sns.axes_style('white'):\n    g = sns.catplot(x=\"Flight_Distance\", y=\"Type_of_Travel\", hue=\"satisfaction\", col=\"Class\", data=train, kind=\"bar\", height=4.5, aspect=.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Type of Travel, Class, Flight Distance:**\nFor business travel in business class category, the number of satisfied passengers are quite on the higher side for longer flight distance. For other combinations, there is almost equal distribution of satisfied and dissatisfied passengers.\n","metadata":{}},{"cell_type":"code","source":"with sns.axes_style('white'):\n    g = sns.catplot(x=\"Departure/Arrival_time_convenient\", y=\"Online_boarding\", hue=\"satisfaction\", col=\"Class\", data=train, kind=\"bar\", height=4.5, aspect=.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Online Boarding, Departure/Arrival Time Convenience grouped by Class:**\nFor Eco Plus class, very inconvenient Departure/Arrival time (Departure/Arrival_time_convenient = 0) has really high no. of dissatisfied passengers, even when online boarding is done very well. For other combinations, no. of satisfied passengers are on the higher side compared to no. of dissatisfied passengers. ","metadata":{}},{"cell_type":"code","source":"with sns.axes_style('white'):\n    g = sns.catplot(x=\"Class\", y=\"Departure_Delay_in_Minutes\", hue=\"satisfaction\", col=\"Type_of_Travel\", data=train, kind=\"bar\", height=4.5, aspect=.8)\n    g = sns.catplot(x=\"Class\", y=\"Arrival_Delay_in_Minutes\", hue=\"satisfaction\", col=\"Type_of_Travel\", data=train, kind=\"bar\", height=4.5, aspect=.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Departure Delay, Arrival Delay grouped by Type of Travel:**\nFor personal travel (specially Eco Plus and Eco), the no. of dissatisfied passengers are really high when arrival delay in minutes is high. Now, this is quite obvious. By minute comparison, all combinations have higher no. of dissatisfied passengers compared to no. of satisfied passengers.","metadata":{}},{"cell_type":"code","source":"with sns.axes_style('white'):\n    g = sns.catplot(x=\"Gate_location\", y=\"Baggage_handling\", hue=\"satisfaction\", col=\"Class\", data=train, kind=\"box\", height=4.5, aspect=.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Baggage Handling, Gate Location grouped by Class:**\nFor business class, it is observed that all gate locations have higher no. of dissatisfied passengers when baggage handling is not done perfectly well (rating <= 4). For Eco Plus, when the gate location is 1 and for Eco, when the gate location is 2, even when the baggages are handled in a mediocre way (rating in range 2.0 - 4.0), passengers remained dissatisfied.","metadata":{}},{"cell_type":"code","source":"with sns.axes_style('white'):\n    g = sns.catplot(x=\"Inflight_wifi_service\", y=\"Inflight_entertainment\", hue=\"satisfaction\", col=\"Class\", data=train, kind=\"box\", height=4.5, aspect=.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inflight Entertainment, Inflight wi-fi Service grouped by Class:**\nIt is interesting to find that Eco Plus passengers are mostly satisfied without in-flight wi-fi service (rating 0) and medium level of in-flight entertainment (rating 2 - 4). For Business class passengers, only highest level of in-flight entertainment (rating 5) can make them satisfied. For Eco passengers, high level of in-flight entertainment (rating 3 - 5) and very high wi-fi service availability (rating 5) can make them satisfied.","metadata":{}},{"cell_type":"code","source":"with sns.axes_style(style='ticks'):\n    g = sns.catplot(\"satisfaction\", col=\"Ease_of_Online_booking\", col_wrap=6, data=train, kind=\"count\", height=2.5, aspect=.9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with sns.axes_style(style='ticks'):\n    g = sns.catplot(\"satisfaction\", col=\"Seat_comfort\", col_wrap=6, data=train, kind=\"count\", height=2.5, aspect=.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with sns.axes_style(style='ticks'):\n    g = sns.catplot(\"satisfaction\", col=\"Cleanliness\", col_wrap=6, data=train, kind=\"count\", height=2.5, aspect=.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with sns.axes_style(style='ticks'):\n    g = sns.catplot(\"satisfaction\", col=\"Food_and_drink\", col_wrap=6, data=train, kind=\"count\", height=2.5, aspect=.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ease of Online Booking, Seat Comfort, Cleanliness, Food and Drink:**\nFor all of these features, maximum no. of satisfied passengers belong to the category of 4 and 5 rating givers. Below rating 4, passengers are mostly dissatisfied.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nfig, axarr = plt.subplots(2, 2, figsize=(12, 8))\n\ntable1 = pd.crosstab(train['satisfaction'], train['Checkin_service'])\nsns.heatmap(table1, cmap='Oranges', ax = axarr[0][0])\ntable2 = pd.crosstab(train['satisfaction'], train['Inflight_service'])\nsns.heatmap(table2, cmap='Blues', ax = axarr[0][1])\ntable3 = pd.crosstab(train['satisfaction'], train['On-board_service'])\nsns.heatmap(table3, cmap='pink', ax = axarr[1][0])\ntable4 = pd.crosstab(train['satisfaction'], train['Leg_room_service'])\nsns.heatmap(table4, cmap='bone', ax = axarr[1][1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checkin Service, Inflight Service, On-board Service, Leg-room Service:**\nFor checkin service, 0-2 rating givers are predominantly dissatisfied. For other three services, only 4 and 5 rating givers belong to satisfied passengers category.  ","metadata":{}},{"cell_type":"markdown","source":"# Label Encoding of Categorical Variables","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlencoders = {}\nfor col in train.select_dtypes(include=['object']).columns:\n    lencoders[col] = LabelEncoder()\n    train[col] = lencoders[col].fit_transform(train[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lencoders_t = {}\nfor col in test.select_dtypes(include=['object']).columns:\n    lencoders_t[col] = LabelEncoder()\n    test[col] = lencoders_t[col].fit_transform(test[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outliers Detection and Removal","metadata":{}},{"cell_type":"code","source":"Q1 = train.quantile(0.25)\nQ3 = train.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing outliers from dataset\ntrain = train[~((train < (Q1 - 1.5 * IQR)) |(train > (Q3 + 1.5 * IQR))).any(axis=1)]\ntrain.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlation among Features","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20, 20))\ncmap = sns.diverging_palette(150, 1, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=None, center=0,square=True, annot=True, linewidths=.5, cbar_kws={\"shrink\": .9})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"Ease_of_Online_booking\" is highly correlated with \"Inflight_wifi_service\". Also \"Inflight_service\" is highly correlated with \"Baggage_handling\". But no pair is having corr. coefficient exactly equal to 1. So there is no perfect multicollinearity. Hence we are not discarding any variable. \n\n# Top 10 Feature Selection through Chi-Square","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nr_scaler = preprocessing.MinMaxScaler()\nr_scaler.fit(train)\n#modified_data = pd.DataFrame(r_scaler.transform(train), index=train['id'], columns=train.columns)\nmodified_data = pd.DataFrame(r_scaler.transform(train), columns=train.columns)\nmodified_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, chi2\nX = modified_data.loc[:,modified_data.columns!='satisfaction']\ny = modified_data[['satisfaction']]\nselector = SelectKBest(chi2, k=10)\nselector.fit(X, y)\nX_new = selector.transform(X)\nprint(X.columns[selector.get_support(indices=True)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These are top 10 features impacting on passenger satisfaction. We will check feature importance with other methods as well.\n# Feature Importance using Wrapper Method","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier as rf\n\nX = train.drop('satisfaction', axis=1)\ny = train['satisfaction']\nselector = SelectFromModel(rf(n_estimators=100, random_state=0))\nselector.fit(X, y)\nsupport = selector.get_support()\nfeatures = X.loc[:,support].columns.tolist()\nprint(features)\nprint(rf(n_estimators=100, random_state=0).fit(X,y).feature_importances_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So only these six features are inherently important in contributing towards passenger satisfaction. However, we will again cross-check with another feature importance deciding method.\n# Feature Permutation Importance","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rf(n_estimators=100, random_state=0).fit(X,y),random_state=1).fit(X,y)\neli5.show_weights(perm, feature_names = X.columns.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From all above results, finally we can combine and conclude the list of important features.\n\n**Really Important Features:** Type_of_Travel, Inflight_wifi_service, Online_boarding, Seat_comfort\n\n**Important Features:** Class, Flight_Distance, Inflight_entertainment, On-board_service, Leg_room_service, Cleanliness, Checkin_service, Inflight_service, Baggage_handling\n\n# Building Models ","metadata":{}},{"cell_type":"code","source":"features = ['Type_of_Travel','Inflight_wifi_service','Online_boarding','Seat_comfort','Flight_Distance',\n            'Inflight_entertainment','On-board_service','Leg_room_service','Cleanliness','Checkin_service', \n            'Inflight_service', 'Baggage_handling']\ntarget = ['satisfaction']\n\n# Split into test and train\nX_train = train[features]\ny_train = train[target].to_numpy()\nX_test = test[features]\ny_test = test[target].to_numpy()\n\n# Normalize Features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report, plot_confusion_matrix, plot_roc_curve\nfrom matplotlib import pyplot as plt \ndef run_model(model, X_train, y_train, X_test, y_test, verbose=True):\n    t0=time.time()\n    if verbose == False:\n        model.fit(X_train,y_train.ravel(), verbose=0)\n    else:\n        model.fit(X_train,y_train.ravel())\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    roc_auc = roc_auc_score(y_test, y_pred) \n    time_taken = time.time()-t0\n    print(\"Accuracy = {}\".format(accuracy))\n    print(\"ROC Area under Curve = {}\".format(roc_auc))\n    print(\"Time taken = {}\".format(time_taken))\n    print(classification_report(y_test,y_pred,digits=5))\n    plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.pink, normalize = 'all')\n    plot_roc_curve(model, X_test, y_test)                     \n    \n    return model, accuracy, roc_auc, time_taken","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model-1: Logistic Regression penalized with Elastic Net (L1 penalty = 50%, L2 penalty = 50%)**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nparams_lr = {'penalty': 'elasticnet', 'l1_ratio':0.5, 'solver': 'saga'}\n\nmodel_lr = LogisticRegression(**params_lr)\nmodel_lr, accuracy_lr, roc_auc_lr, tt_lr = run_model(model_lr, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since Logistic Regression is a white-box model (explainable), we can dive deeper into it to get more insight. ","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sm\nlogit_model=sm.Logit(y_train,X_train)\nresult=logit_model.fit()\nprint(result.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see, among 12 features, except 6th feature (Inflight_entertainment), rest 11 features have p-value < 0.05. So these are really important features impacting highly towards the target variable. Also, a pseudo R-square value **(McFadden's Pseudo R-Squared Value)** of 0.55 represents an excellent fit. \n\n**Model-2: Naive Bayes Classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nparams_nb = {}\n\nmodel_nb = GaussianNB(**params_nb)\nmodel_nb, accuracy_nb, roc_auc_nb, tt_nb = run_model(model_nb, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model-3: K-Nearest Neighbor Classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nparams_kn = {'n_neighbors':10, 'algorithm': 'kd_tree', 'n_jobs':4}\n\nmodel_kn = KNeighborsClassifier(**params_kn)\nmodel_kn, accuracy_kn, roc_auc_kn, tt_kn = run_model(model_kn, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model-4: Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nparams_dt = {'max_depth': 12,    \n             'max_features': \"sqrt\"}\n\nmodel_dt = DecisionTreeClassifier(**params_dt)\nmodel_dt, accuracy_dt, roc_auc_dt, tt_dt = run_model(model_dt, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since Decision Tree is a white-box (explainable) model, we can deep-dive into its visualization to get more valuable insight below. From tree-visualization, we can extract rules which are contributing towards passenger-satisfaction.","metadata":{}},{"cell_type":"code","source":"import graphviz\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nfeatures_n = ['Type_of_Travel', 'Inflight_wifi_service', 'Online_boarding', 'Seat_comfort']\nX_train_n = scaler.fit_transform(train[features_n])\ndata = export_graphviz(DecisionTreeClassifier(max_depth=3).fit(X_train_n, y_train), out_file=None, \n                       feature_names = features_n,\n                       class_names = ['Dissatisfied (0)', 'Satisfied (1)'], \n                       filled = True, rounded = True, special_characters = True)\n# we have intentionally kept max_depth short here to accommodate the entire visual-tree, best result comes with max_depth = 12\n# we have taken only really important features here to accommodate the entire tree picture\ngraph = graphviz.Source(data)\ngraph","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above tree visualization, it can be easily spotted that rule \"Type_of_Travel <=0.227 and Seat_comfort <= -0.089 and Online_boarding <= 0.045\" (all normalized values) contributes towards passenger satisfaction indicator= 1. Like that, many other rules can be extracted easily by going through the nodes.\n\n**Model-5: Neural Network (Multilayer Perceptron)**","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n\nparams_nn = {'hidden_layer_sizes': (30,30,30),\n             'activation': 'logistic',\n             'solver': 'lbfgs',\n             'max_iter': 100}\n\nmodel_nn = MLPClassifier(**params_nn)\nmodel_nn, accuracy_nn, roc_auc_nn, tt_nn = run_model(model_nn, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model-6: Random Forest**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nparams_rf = {'max_depth': 16,\n             'min_samples_leaf': 1,\n             'min_samples_split': 2,\n             'n_estimators': 100,\n             'random_state': 12345}\n\nmodel_rf = RandomForestClassifier(**params_rf)\nmodel_rf, accuracy_rf, roc_auc_rf, tt_rf = run_model(model_rf, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, we see that Random Forest has performed very well on both Accuracy and area under ROC curve. So, we are now interested to see **how many decision trees are minimally required make the Accuarcy consistent** (recalling the fact that Random Forest is actually a bagged ensemble of decision trees).","metadata":{}},{"cell_type":"code","source":"import numpy as np\n%matplotlib inline\n\ntrees=range(100)\naccuracy=np.zeros(100)\n\nfor i in range(len(trees)):\n    clf = RandomForestClassifier(n_estimators = i+1)\n    model1 = clf.fit(X_train, y_train.ravel())\n    y_predictions = model1.predict(X_test)\n    accuracy[i] = accuracy_score(y_test, y_predictions)\n\nplt.plot(trees,accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above graph, it is evident that **minimum 40 trees** are required to make accuracy fairly consistent (though minimal fluctuation is still there, and we can try the graph after increasing the no. of iterations).\n\n**Model-7: Extreme Gradient Boosting**","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nparams_xgb ={'n_estimators': 500,\n            'max_depth': 16}\n\nmodel_xgb = xgb.XGBClassifier(**params_xgb)\nmodel_xgb, accuracy_xgb, roc_auc_xgb, tt_xgb = run_model(model_xgb, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model-8: Adaptive Gradient Boosting**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier as adab\nparams_adab ={'n_estimators': 500,\n              'random_state': 12345}\n\nmodel_adab = adab(**params_adab)\nmodel_adab, accuracy_adab, roc_auc_adab, tt_adab = run_model(model_adab, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Region Plotting for Different Models\n\nWe will observe the boundary of decision regions plotted by all the models on training data. Also we will observe the number of misclassified data points in the plots.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.plotting import plot_decision_regions\n\nvalue = 1.70\nwidth = 0.85\n\nclf1 = LogisticRegression(random_state=12345)\nclf2 = GaussianNB()\nclf3 = KNeighborsClassifier()\nclf4 = DecisionTreeClassifier(random_state=12345) \nclf5 = MLPClassifier(random_state=12345, verbose = 0)\nclf6 = RandomForestClassifier(random_state=12345)\nclf7 = xgb.XGBClassifier(random_state=12345)\nclf8 = AdaBoostClassifier(random_state=12345)\neclf = EnsembleVoteClassifier(clfs=[clf6, clf7, clf8], weights=[1, 1, 1], voting='soft')\n\nX_list = train[[\"Type_of_Travel\", \"Inflight_wifi_service\", \"Online_boarding\", \"Seat_comfort\"]] #took only really important features\nX = np.asarray(X_list, dtype=np.float32)\ny_list = train[\"satisfaction\"]\ny = np.asarray(y_list, dtype=np.int32)\n\n# Plotting Decision Regions\ngs = gridspec.GridSpec(3,3)\nfig = plt.figure(figsize=(18, 14))\n\nlabels = ['Logistic Regression',\n          'Naive Bayes',\n          'KNN',\n          'Decision Tree',\n          'Neural Network',\n          'Random Forest',\n          'XGBoost',\n          'AdaBoost',\n          'Ensemble']\n\nfor clf, lab, grd in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf7, clf8, eclf],\n                         labels,\n                         itertools.product([0, 1, 2],\n                         repeat=2)):\n    clf.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=clf, \n                                filler_feature_values={2: value, 3: value}, \n                                filler_feature_ranges={2: width, 3: width}, \n                                legend=2)\n    plt.title(lab)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Comparison:\nWe will compare the performace of various models by their respective ROC_AUC score and total time taken for execution.","metadata":{}},{"cell_type":"code","source":"roc_auc_scores = [roc_auc_lr, roc_auc_nb, roc_auc_kn, roc_auc_dt, roc_auc_nn, roc_auc_rf, roc_auc_xgb, roc_auc_adab]\ntt = [tt_lr, tt_nb, tt_kn, tt_dt, tt_nn, tt_rf, tt_xgb, tt_adab]\n\nmodel_data = {'Model': ['Logistic Regression','Naive Bayes','K-NN','Decision Tree','Neural Network','Random Forest','XGBoost','AdaBoost'],\n              'ROC_AUC': roc_auc_scores,\n              'Time taken': tt}\ndata = pd.DataFrame(model_data)\n\nfig, ax1 = plt.subplots(figsize=(14,8))\nax1.set_title('Model Comparison: Area under ROC Curve and Time taken for execution by Various Models', fontsize=13)\ncolor = 'tab:blue'\nax1.set_xlabel('Model', fontsize=13)\nax1.set_ylabel('Time taken', fontsize=13, color=color)\nax2 = sns.barplot(x='Model', y='Time taken', data = data, palette='Blues_r')\nax1.tick_params(axis='y')\nax2 = ax1.twinx()\ncolor = 'tab:orange'\nax2.set_ylabel('ROC_AUC', fontsize=13, color=color)\nax2 = sns.lineplot(x='Model', y='ROC_AUC', data = data, sort=False, color=color)\nax2.tick_params(axis='y', color=color)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nWe observe, Random Forest and AdaBoost have performed equally well on producing high ROC_AUC score (90%). But **Random Forest** has taken lesser amount of time compared to time taken by AdaBoost. So, we will stick to Random Forest as the best model. \n\n**Acknowledgement:** Thank you [TJ Klein](http://www.kaggle.com/teejmahal20) for your excellent kernel. I took the basic structure of run_model function from your work. Thank you [Prageeth Anjula](http://www.kaggle.com/praanj) for your great tree visualization. I took the codes for graphviz from your Titanic kernel. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}