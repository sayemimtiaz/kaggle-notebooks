{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Leveraging a Knowledge Base of Covid-19 Terms to Construct a Bayesian Risk Model\n\nIn this notebook, we will rely on topic modeling to extract all clinically relevant articles. Afterwards, we’ll utilize the articles to build a knowledge base of clinical terms. The knowledge base will be constructed in a semi-unsupervised manner. It will categorize our medical terms into classes, such as _symptoms_, and _comorbidities_. We’ll subsequently use the knowledge base to power an intelligent search tool. Our tool will leverage known medical concepts to return highly-relevant results for risk-factor queries. Based on these results, we will construct a baseline Bayesian model for identifying high-risk patients. Using our knowledge base, we’ll build a wrapper function around our model, so that it can be applied to unstructured text. Our final result will be a baseline model for prediction patient risk, based on a written description of the patient.\n\nHere is an outline of the notebook:\n\n1. **Data Exploration**\n2. **Topic Modeling**\n    1. Topic Visualization\n    2. Ranking Topics by Task Relevance\n3. **Exploring Clinically Relevant Articles**\n4. **Extracting Medical Terms from Relevant Sentences**\n5. **Exploring the Extracted Medical Terms**\n6. **Building a Knowledge Base of Medical Terms**\n    1. Uncovering the Symptoms of Covid-19\n    2. Uncovering Covid-19 Comorbidities\n7. **Building a KB-Powered Search Tool to Probe for Risks**\n8. **Building a Classifier to Predict Covid-19 Progression**\n\nLet’s get started!\n\n# 1. Data Exploration\n\nWe’ll start by loading the metadata into a Pandas DataFrame. Afterwards, we’ll count the number of records, while sampling the first few rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('../input/CORD-19-research-challenge/metadata.csv', low_memory=False)\nrecord_count = df.shape[0]\nprint(f\"Our metadata includes {record_count} total records.\")\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have over 47,000 records. Some of the records lack an abstract. This is a problem. Our initial plan is to cluster our records based on abstract similarity. Let’s filter out the records lacking an abstract. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna(subset=['abstract'])\npercent_abstracts = 100 * df.shape[0] / (record_count)\nprint(f\"{percent_abstracts:.2f}% of our records contain abstracts.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"18% of our records lack an abstract. We’ve filtered these out. Most of the initial records remain. Are any of these records duplicates? Let’s find out."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['title', 'abstract']].describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Approximately 38.6K of our 39K abstracts are unique. The rest are duplicates. We’ll delete these duplicates below."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\npercent_remaining = 100 * df.shape[0] / (record_count)\nprint(f\"{percent_remaining:.2f}% of our records remain after duplicate deletion.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the deletion, 81.75% of our initial records remain. Now, we’ll filter these records even further. Our goal is to explore SARS / Coronavirus risks. Thus, we want to ensure that every record mentions a SARS / Coronavirus related-term. We check for relevant mentions by applying a regular expression to the title and the abstract of each record. All records that lack a relevant match will be deleted."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom functools import partial\n\n\ndef compile_pattern_list(pattern_list, add_bounderies=True, escape=True):\n    \"\"\"This function compiles an aggregated regular expression from a list of string\n    patterns. It is frequently used elsewhere within this notebook.\n    \n    Parameters\n    ----------\n    pattern_list: [str]:\n        A list of strings to be OR-ed together into a single regular expression.\n    add_bounderies (optional): Bool\n        If True, then word boundaries are added to the regular expression.\n    escape (optional): Bool\n        If True, then we run `re.escape` on each pattern string, to ensure proper compilation.\n        \n    Returns\n    -------\n    regex: re.Pattern\n        A compiled regular expression object.\n    \"\"\"\n    if escape:\n        pattern_list = [re.escape(p) for p in pattern_list]\n    \n    regex_string = r'|'.join(pattern_list)\n    if add_bounderies:\n        regex_string = r'\\b%s\\b' % regex_string\n    \n    return compile_(regex_string)\n\n# Compiles a case-insenstiive regular expression. Re-used frequently elsewhere in the Notebook.\ncompile_ = partial(re.compile, flags=re.I)\n\npattern_list = ['SARS', 'Severe Acute Respiratory Syndrome', 'COVID-19', 'SARS-COV', \n                'coronavirus', 'corona virus', '2019-nCoV', 'SARS-CoV-2']\n\nregex = compile_pattern_list(pattern_list)\nis_match = [any([regex.search(e) for e in [title, abstract]])\n            for title, abstract in df[['title', 'abstract']].values]\ndf = df[is_match]\n\nprint(f\"{df.shape[0]} of our records reference either SARS or some coronavirus.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After filtering, approximately 10,000 records remain. We’ll now run topic modeling on the records. Afterwards, we’ll focus our attention on those topics that relate to risk-factor analysis.\n\n# 2. Topic Modelling\n\nThere are [multiple techniques](https://towardsdatascience.com/2-latent-methods-for-dimension-reduction-and-topic-modeling-20ff6d7d547) for topic modelling. [LDA](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158) is one very popular approach. However, the quality of the LDA outputs vary depending on the algorithm’s implementation. There are two ways to compute LDA’s posterior distribution: [MCMC and Variational Inference](https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29). MCMC yields accurate results but will run slowly on 5,000 samples. Meanwhile Variational Inference (VI)  is much faster. Unfortunately, based on my experience, the VI topic outputs tend to be of substandard quality.  Hence, when I run topic-modeling, I prefer the following approach:\n\n1. Extract initial topics using simple [LSA](https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python).\n\n2. Cluster the LSA outputs for improved topic results.\n\nLet’s prepare to execute LSA. First, we’ll vectorize our 5,000 abstracts, yielding a matrix of TFIDF vectors. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words='english')\n# This matrix has been normalized under default settings\ntfidf_matrix = vectorizer.fit_transform(df.abstract)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we’ll finish LSA by dimensionally reducing the matrix using Scikit-Learn’s `TruncatedSVD` class. We’ll set the number of reduced dimensions to 100, per Scikit-Learn’s recommended [LSA parameter settings](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.decomposition import TruncatedSVD\n# Truncated SVD is a stochastic algorithm. We set the random seed to ensure a consistant output.\nnp.random.seed(0)\nlsa_matrix = TruncatedSVD(n_components=100).fit_transform(tfidf_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nEssentially, LSA assigns a dimensionally-reduced vector to each abstract. We can leverage these vectors to further cluster our abstracts by topic. Personally, my preference is to cluster our LSA matrix in a completely unsupervised way, using the [markov clustering](https://medium.com/analytics-vidhya/demystifying-markov-clustering-aeb6cdabbfc7). However, good markov clustering requires a time-consuming [matrix filtration step](https://academic.oup.com/bioinformatics/article/27/3/326/319615) to be effective. Therefore, in this exercise, we’ll carry out a simpler text clustering approach:\n\n1. First, we’ll normalize our vectors. \n2. Next, we’ll carry out K-means. Since our normalized vectors are now points along a hypersphere, our K-means clustering output should be reliable (see [here](https://livebook.manning.com/book/data-science-bookcamp/chapter-15/v-3/206) for additional theoretical justifications).\n\nBelow, we’ll execute these steps, using an arbitrary K of 20. If necessary, we’ll adjust that K during our topic-exploration phase."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import normalize\n\nclusters = KMeans(n_clusters=20).fit_predict(normalize(lsa_matrix))\ndf['Index'] = range(clusters.size)\ndf['Cluster'] = clusters\n# Clusters are stored as DataFrames for easier analysis.\ncluster_groups = [df_cluster for  _, df_cluster in df.groupby('Cluster')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Topic Visualization\n\nNow, we are ready to visualize our clusters. I prefer to visualize NLP-based clusters using word clouds. Some members of the Data Science community look down upon word clouds. However, in my opinion, they can be an amazing visualization tool if their input is tweaked properly. Here is my preferred technique for word-cloud-based text-cluster visualization.\n\nSelect the indices of the documents that are present in a single cluster.\nSum-up the TFIDF vectors associated with that index list. These sums allow us to rank the words in our vocabulary based on their importance to the cluster. In my experience, these  TFIDF rankings are much more useful than the standard frequency rankings. See [here](https://livebook.manning.com/book/data-science-bookcamp/chapter-15/v-3/249) for more details.\nVisualize a word cloud containing top 10 ranked words within the cluster. The size of each word in the cloud is determined by its summed TFIDF value.\n\nLet’s implement this strategy by defining a `cluster_to_image` to function. The function will convert a single text cluster into a word-cloud image. We’ll then apply that function to visualize the first of our 20 clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\n\ndef cluster_to_image(df_cluster, max_words=10, tfidf_matrix=tfidf_matrix,\n                     vectorizer=vectorizer):\n    \"\"\"This function converts a text-cluster into a word-cloud image.\n\n    Parameters\n    ----------\n    df_cluster: DataFrame\n        This DataFrame contains the document ids associated with a single cluster.\n    max_words (optional): int\n        The number of top-ranked words to include in the word-cloud image.\n    tfidf_matrix (optional): csr_matrix\n        A matrix of TFIDF values. Each row i corresponds to the ith abstract.\n    vectorizer (optional): TfidfVectorizer\n        A vectorizer object that tracks our vocabulary of words.\n    \n    Returns\n    -------\n    word_cloud_image: WordCloud\n        A word-cloud image containing the top words within the cluster.\n\n    \"\"\"\n    indices = df_cluster.Index.values\n    summed_tfidf = np.asarray(tfidf_matrix[indices].sum(axis=0))[0]\n    data = {'Word': vectorizer.get_feature_names(),'Summed TFIDF': summed_tfidf}  \n    # Words are ranked by their summed TFIDF values.\n    df_ranked_words = pd.DataFrame(data).sort_values('Summed TFIDF', ascending=False)\n    words_to_score = {word: score\n                     for word, score in df_ranked_words[:max_words].values\n                     if score != 0}\n    \n    # The word cloud's color parameters are modefied to maximize readability.\n    cloud_generator = WordCloud(background_color='white',\n                                color_func=_color_func,\n                                random_state=1)\n    wordcloud_image = cloud_generator.fit_words(words_to_score)\n    return wordcloud_image\n\ndef _color_func(*args, **kwargs):\n    # This helper function will randomly assign one of 5 easy-to-read colors to each word.\n    return np.random.choice(['black', 'blue', 'teal', 'purple', 'brown'])\n\nwordcloud_image = cluster_to_image(cluster_groups[0])\nplt.imshow(wordcloud_image, interpolation=\"bilinear\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our first visualized cluster describes [bat-born](https://en.wikipedia.org/wiki/Bat-borne_virus#Coronaviruses) coronaviruses. What sort of information is found in the other 19 clusters? Let's find out. We'll proceed to visualize all clusters in a 5-by-4 subplot grid."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_wordcloud_grid(cluster_groups, num_rows=5, num_columns=4):\n    # This function plots all clusters as word-clouds in 5x4 subplot grid.\n    figure, axes = plt.subplots(num_rows, num_columns, figsize=(20, 15))\n    cluster_groups_copy = cluster_groups[:]\n    for r in range(num_rows):\n        for c in range(num_columns):\n            if not cluster_groups_copy:\n                break\n                \n            df_cluster = cluster_groups_copy.pop(0)\n            wordcloud_image = cluster_to_image(df_cluster)\n            ax = axes[r][c]\n            ax.imshow(wordcloud_image, interpolation=\"bilinear\")   \n            # The title of each subplot contains the cluster id, as well as the cluster size.\n            ax.set_title(f\"Cluster {df_cluster.Cluster.iloc[0]}: {df_cluster.shape[0]}\")\n            ax.set_xticks([])\n            ax.set_yticks([])\n\nplot_wordcloud_grid(cluster_groups)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our 20 clusters cover a diverse collection of topics. These include:\n\n* The [Feline Coronavirus](https://en.wikipedia.org/wiki/Feline_coronavirus), which thus far is only found in cats (Cluster 2).\n* Efforts to develop a SARS vaccine (Cluster 5).\n* The deadly [MERS](https://en.wikipedia.org/wiki/Middle_East_respiratory_syndrome) Coronavirus, originating in the Middle East, after a Camel-to-Person transmission (Cluster 6). \n* [RNA virus](https://en.wikipedia.org/wiki/RNA_virus) genetics (Cluster 7).\n* Respiratory infections by the [Avian Coronavirus (IBV)](https://en.wikipedia.org/wiki/Avian_coronavirus), which originates in chickens (Cluster 9).\n* The [protease cleavage](https://en.wikipedia.org/wiki/Protease#Viruses) of Coronavirus genomes into functional units, and how it could be inhibited (Cluster 12).\n* The [fusion membrane proteins](https://en.wikipedia.org/wiki/Viral_protein#Viral_membrane_fusion_proteins)  used by the coronavirus to get inside our cells (Cluster 18).\n\nEach of these clusters are interesting, and is worth exploring. However, the goal of this exercise is to analyze clinical risk factors, not membrane proteins or protease cleavage. Hence, many of the clusters are not relevant to the task at hand. Can we somehow rank the clusters based on their relevance to the task? Yes we can! We just need to use text similarity.\n\n### 2.2 Ranking Topics by Task Relevance\nWe want to compare our clusters to our assigned Kaggle task. Let’s start by storing the description of the task within a string."},{"metadata":{"trusted":true},"cell_type":"code","source":"task_string = \"\"\"Task Details\nWhat do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\n\nSpecifically, we want to know what the literature reports about:\n\nData on potential risks factors\nSmoking, pre-existing pulmonary disease\nCo-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities\nNeonates and pregnant women\nSocio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\nTransmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\nSeverity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\nSusceptibility of populations\nPublic health mitigation measures that could be effective for control\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Next, we’ll compute a vector of cosine similarities between each abstract and the task string."},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words='english')\nmatrix = vectorizer.fit_transform(list(df.abstract) + [task_string])\nsimilarities_to_task = matrix[:-1] @ matrix[-1].T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our `similarities_to_task` array contains the task similarities across all clustered abstracts. For any given cluster, we can combine these cosine similarities into a relevance score, by taking their mean. This allows us to sort all clusters by relevance. Let’s sort the 20 clusters and re-plot the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_mean_similarity(df_cluster):\n    # Computes the mean cosine similarity of a cluster to the task string.\n    indices = df_cluster.Index.values\n    return similarities_to_task[indices].mean()\n\nmean_similarities = [compute_mean_similarity(df_cluster) \n                     for df_cluster in cluster_groups]\nsorted_indices = sorted(range(len(cluster_groups)),\n                        key=lambda i: mean_similarities[i], reverse=True)\nsorted_cluster_groups = [cluster_groups[i] for i in sorted_indices]\nplot_wordcloud_grid(sorted_cluster_groups)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The four most relevant clusters appear in the top row of our sorted results. These clusters focus on the clinical consequences of the coronavirus outbreak. The relevant clusters are dominated by terms such as _patients_, _health_, _disease_, _covid-19_, _hospital_,  and _clinical_. Beyond the top row, our relevancy rapidly decreases. The subsequent two clusters in the second row focus on flu infections, and on MERS. However, they don’t reference Covid-19. Hence, in our analysis, we’ll focus on just the clusters in Row One. All other clustered papers will deleted."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat(sorted_cluster_groups[:4])\nprint(f'Our 4 most relevant clusters cover {df.shape[0]} articles.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The top four clusters contain 2,786 clinically relevant articles articles. Our intermediate goal is to build a smart search engine that’s powered by these articles. However, we will first need to explore these relevant articles in more detail. \n\n# 3. Exploring Clinically Relevant Articles\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_relevant = pd.concat(sorted_cluster_groups[:4])\nprint(f'Our 4 most relevant clusters cover {df.shape[0]} relevant articles.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All remaining articles are clinically relevant. However, not all articles are necessarily relevant to the 2020 coronavirus Pandemic. Some articles may refer to the Pandemics of the past, such as the SARS outbreak of 2003. Additionally, some articles may focus on flu-outbreaks, while mentioning coronaviruses only in passing. Therefore, to maintain relevance, we need to filter the articles even further. Let’s only keep those articles that reference the Pandemic in their titles."},{"metadata":{"trusted":true},"cell_type":"code","source":"# These phrases are explicity associated with the SARS-Cov-2 Pandemic.\npattern_list = ['COVID-19', 'novel coronavirus', 'novel corona virus', '2019-nCoV', 'SARS-CoV-2']\nregex = compile_pattern_list(pattern_list)\n\ndf = df[[regex.search(t) is not None for t in df.title.values]]\nprint(f\"{df.shape[0]} of our clinically relevant records reference the Pandemic in their title.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Over 1000 articles remain. We’ll explore these articles on a sentence-by-sentence level. To start, we’ll tokenize all abstracts into sentences with the spaCy NLP library. Some of our tokenized sentences might be duplicates, containing slight variations of spacing or punctuation. As a precautionary measure, we will eliminate such duplicates from our results.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy  \nimport string\nnlp = spacy.load(\"en_core_web_sm\")  \n\nabstract_sentences = set() # Contains all unique sentences.\nseen_sentences = set() # Used to tract duplicates.\nduplicate_count = 0 # Used to count duplicates.\n\ntrans_table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\ndef simplify_text(text):\n    # Removes all spacing, casing and punctuation, for better duplicate detection.\n    return text.translate(trans_table).lower().replace(' ', '')\n\nfor abstract in df.abstract.values:\n    for sentence in nlp(abstract).sents:\n        simplified_text = simplify_text(sentence.text)\n        if simplified_text not in seen_sentences:\n            # Sentence is not a duplicate.\n            seen_sentences.add(simplified_text)\n            abstract_sentences.add(sentence.text)\n            \n        else:\n            duplicate_count += 1\n            \nnum_unique = len(abstract_sentences)\npercent_unique = 100 * num_unique / (num_unique + duplicate_count)\nprint(f\"Our 1334 abstracts contain {num_unique} unique sentences.\")\nprint(f\"{int(percent_unique)}% of all sentences are unique.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"89% of the abstract sentences are unique. This leaves us with 13.2K unique sentences total. We can increase our sentence count by also including all full articles that are associated with `df`. Let’s now load the file paths to all relevant article JSONs at our disposal."},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\ndf_full = df[df.has_pdf_parse == True]\nrelevant_fnames  = {sha + '.json' for sha in df_full.sha.values}\nrelevant_fpaths = [p for p in glob.glob('../input/CORD-19-research-challenge/**/*.json', recursive=True)\n                   if p.split('/')[-1] in relevant_fnames]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we’ll parse all relevant JSONs and extend our `all_sentences` list with the tokenized full-article texts."},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\nbody_sentences = set()\nfor file_path in relevant_fpaths:\n    with open(file_path) as f:\n        data = json.load(f)\n        full_text = ' '.join(t['text'] \n                             for t in data['body_text'])\n        \n        for sentence in nlp(full_text).sents:\n            simplified_text = simplify_text(sentence.text)\n            if simplified_text not in seen_sentences:\n                # Sentence is not a duplicate.\n                seen_sentences.add(simplified_text)\n                body_sentences.add(sentence.text)\n                \nall_sentences = abstract_sentences | body_sentences\nprint(f\"Article body inclusion gives us {len(all_sentences)} unique sentences.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We’ve obtained nearly 100K unique sentences. However, not all our sentences are equally relevant. Consider for instance, those sentences that mention _Diabetes_. They are quite different from sentences that mention _Ebola_. Why? Well, Diabetes is a well-known comorbidity of Covid-19. Meanwhile, Ebola carries very little relevance to the current Pandemic. It’s only mentioned in passing, in reference to previous types of outbreaks. We can confirm this by printing five random sentences that mention Diabetes. Afterwards, we’ll print five Ebola sentences for comparison."},{"metadata":{"trusted":true},"cell_type":"code","source":"def search(regex_string):\n    # Returns sentences that match the inputted regex.\n    regex = compile_(regex_string)\n    return sorted([s for s in all_sentences if regex.search(s)])\n\ndiabetes_sentences = search('Diabetes')\nebola_sentences = search('Ebola')\n\nprint('DIABETES:')\nfor sentence in diabetes_sentences[-10:]:\n    print(sentence)\n    \nprint('\\nEBOLA')\nfor sentence in ebola_sentences[-10:]:\n    print(sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The Diabetes sentences are obviously more relevant. They frequently mention other types of Covid-19 comorbidities and complications. Such mentions are occasionally accompanied by percentages corresponding to measurements. The mentioned terms are important enough to be measured across patients. Meanwhile, the Ebola sentences are much less important. Hence, they lack any sort of measured percentages. Of course, we’ve sampled 20 sentences total. Perhaps the observed percentage trend will not hold. Let’s count the number of Diabetes matches that contain percentages. We’ll then compare this value to the Ebola-percentage total."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_percent_diabetes = len([s for s in diabetes_sentences if '%' in s])\nnum_percent_ebola = len([s for s in ebola_sentences if '%' in s])\n\nprint(f\"{num_percent_diabetes} of the {len(diabetes_sentences)} Diabetes sentences contain a percentage.\")\nprint(f\"{num_percent_ebola} of the {len(ebola_sentences)} Ebola sentences contain a percentage\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Nearly one-third of the Diabetes sentences contain a percentage. Meanwhile, less than 2% of our less-relevant Ebola sentences contain a measured percentege. This provides some evidence for the following hypothesis: **Sentences with percentages are more likely to be relevant than sentences that lack percentages**. How many of our sentences contain percentages? We’ll now find out."},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_sentences = {s for s in all_sentences if '%' in s}\nprint(f\"{len(percent_sentences)} of our {len(all_sentences)} sentences contain a percentage.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Over 7% of our sentences contain a percentage. We hypothesize that these sentences are clinically relevant. Hence, the medical terms within the sentences should be clinically relevant. We’ll now proceed to extract these medical terms from the texts in `percent_sentences`. Afterwards, we’ll leverage these terms in order to populate our knowledge base.\n\n# 4. Extracting Medical Terms from Relevant Sentences\n\nClinical terms such as _Diabetes_ and _Hypertension_, appear as noun chunks within our relevant sentences. We can extract all noun chunks from a sentence with the help of the spaCy library. Below, we will define an `extract_noun_chunks` function, that we’ll then apply to an example sentence."},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\ndef extract_noun_chunks(text):\n    # Returns a set of unique noun chunks present in a text.\n    text = remove_parans(text) # Strips our parenthesized text for better noun chunk parsing.\n    return {noun_chunk.text.lower()\n            for noun_chunk in nlp(text).noun_chunks}\n\ndef remove_parans(text):\n    # Removes short stretches of parenthesized text for more accurate parsing.\n    return re.sub(r'\\((.{,20})\\)', '', text).replace('  ', ' ')\n\ntext = (\"Most of the infected patients were men (30 [73%] of 41); less than half \"\n        \"had underlying diseases (13 [32%]), including diabetes (eight [20%]), \"\n        \"hypertension (six [15%]), and cardiovascular disease (six [15%]).'\")\nextract_noun_chunks(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the relevant medical terms in our example sentence are included within the extracted noun chunks. However, not all these noun-chunks are medical terms. Noun chunks such as _less than half_ and _men_ are a source of noise in our results. We need a way of extracting just the medical texts while ignoring all other results. One very simple way to extract medical phrases is with the [WordNet](https://en.wikipedia.org/wiki/WordNet) lexical database, which can be accessed using [NLTK](https://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html). WordNet allows us to access the hypernyms of any given word. A word’s hypernym is a broad ontological category to which that word belongs. For instance, both _Diabetes_ and _Hypertension_ fall under the hypernym of _Disease_. By leveraging a set of common medical hypernyms, we can define an `is_medical_term` function. The function extracts all hypernyms for each word within a text, and returns `True` if any hypernym is medical."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import wordnet as wn \n# Common medical hypernyms\nmedical_hypernyms = {'symptom', 'ill_health', 'negative_stimulus',\n                    'bodily_process', 'disease', 'illness', 'pain',\n                    'body_substance', 'medicine', 'drug', 'pathology',\n                    'breathing', 'therapy', 'medical_care', 'treatment',\n                    'disorder'}\n\ndef is_medical(text):\n    # Returns True any an hypernym of an word in `text` overlaps with `medical_hypernyms`.\n    for word in text.lower().split():\n        hypernyms = set(get_hypernyms(word))\n        if hypernyms & medical_hypernyms:\n            return True\n        \n    return False \n\ndef get_hypernyms(word):\n    # Returns a set of hypernyms associated with each word.\n    hypernym_list = []\n    # Accesses all synsets (synonyms and and usage-categories) of word\n    synsets = wn.synsets(word)\n    count = 0\n    while synsets and count < 4:\n        # Extracts all hypernyms of most common sysnset. We limit ourselves to just the \n        # first 4 layers of the hypernym hierarchy.\n        hypernyms = synsets[0].hypernyms()\n        hypernym_list.extend([h.name().split('.')[0]\n                              for h in hypernyms])\n        synsets = hypernyms\n        count += 1\n    return hypernym_list\n\n\nassert is_medical('Diabetes')\nassert is_medical('Breathing problems')\nassert not is_medical('problems')\nassert not is_medical('men')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The main downside of hypernym analysis is lack of precision. The hypernyms of each word are analyzed individually, without regard to context. This leads to unexpected errors. For instance, consider the previously discussed phrase _less than half_. It just so happens WordNet records the word _less_ as a synonym of Lupus. Thus, _disease_ is erroneously recorded as a hypernym of _less_. Consequently, our `is_medical` function erroneously treats the phrase as a medical term.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"First synset of 'less':\\n{wn.synsets('less')[0]}\")\nprint(f\"\\nHypernyms of 'less':\\n{get_hypernyms('less')}\")\nassert is_medical('less than half')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need a more precise way of validating medical terms. There’s one approach that guarantees high precision; scraping Wikipedia! Medical Wikipedia pages are very easy to spot. They almost always contain a _Medical Specility_ field within their info box. With this in mind, we’ll now define a `search_wikipedia` function. The function will take as input a noun-chunk. Afterwards, it will scrape the Wikipedia page associated with that noun chunk. If the page is not found, the function will return `None`. Otherwise, the function will look for a specialty field in the info-box. If no specialty is found, we’ll return None. Otherwise, we will return the medical specialty, along with the page title. That title will allow us to gauge differences between the inputted noun chunk and the final redirected results."},{"metadata":{"trusted":true},"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup as bs\n\ndef search_wikipedia(noun_chunk):\n    \"\"\"Searches Wikipedia for a noun chunk. If a noun chunk is associated with a medical \n    specialty, then  the function returns both the specialty and the page title. Otherwise,\n    it returns a None\"\"\"\n    \n    # Heuristic cleaning is carried out on the noun chuck\n    name = _clean_string(noun_chunk)\n    # The Wikipedia page that we'll try to load.\n    url = f'https://en.wikipedia.org/wiki/{name.replace(\" \", \"_\")}'\n    response = _scrape_url(url)\n    if response is None:\n        # No page loaded.\n        return None\n    \n    # We parse the page using Beautiful Soup.\n    soup = bs(response.text)\n    table = soup.find('table', class_='infobox')\n    if not table:\n        # No Info Box has been found.\n        return None\n    \n    specialty = None\n    for table_row in table.find_all('tr'):\n        text = table_row.text\n        if text.startswith('Specialty'):\n            # We've uncovered a medical specialy.\n            specialty = text[len('Specialty'):].strip()\n            break\n    \n    if not specialty:\n        # No specialty has been found.\n        return None\n    \n    # We clean the title, prior to returning the medical specialy and the title.\n    title = _clean_title(soup.title.text, name)\n            \n    return specialty, title\n\ndef _clean_string(text):\n    # Filters out common patterns at the start of medical strings that interfere with Wiki crawling.\n    deletion_patterns = [r'^(a|an|the)\\b',  r'^(mild|low|moderate|severe|high|old)\\b']\n    for regex_string in deletion_patterns:\n        text = re.sub(regex_string, '', text, flags=re.I).strip()\n        \n    return text\n\n\ndef _scrape_url(url):\n    # Scrapes the url.\n    for _ in range(3):\n        # Repeat 3 times in case of timeout.\n        try:\n            response = requests.get(url, timeout=10)\n            # Return scraped response if page has loaded.\n            return response if response.status_code == 200 else None\n        \n        except timeout:\n            continue\n    \n    return None\n\ndef _clean_title(title, name):\n    # Removes noise from the end of the string.\n    title = title.replace('- Wikipedia', '').strip()\n    # Sometimes symptom-terms redirect to diease pages. This could lead to confusion later in our\n    # analysis, when we execute symptom extraction. Hence, we use a regex to deal with this edge-case.\n    if title.endswith('disease') and re.search(r'symptoms?$', name):\n        return name.capitalize()\n        \n    return title\n\nfor term in ['a moderate fever', 'dyspnea', 'diabetes', 'hypertension']:\n    print(f\"Searching for '{term}'.\")\n    specialty, alias = search_wikipedia(term)\n    print(f\"The term is also know as '{alias}.'\")\n    print(f\"Its associated clinical specialties are: {specialty}.\\n\")\n    \nassert search_wikipedia('less than half') is None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Using Wikipedia to uncover medical terms carries multiple benefits over hypernym strategy.\n\n1. Term identification is more precise. \n2. Wikipedia redirects allow us to identify multiple aliases for the same medical terms. We thus can leverage these aliases to group synonymous medical terms together.\n3. Accessing the clinical specialty allows us to group the terms by specialty-type (lung disease, heart-disease, etc). This allows us to group the  medical terms by specialty, for a more nuanced investigation.\n\nUnfortunately the strategy also has its downsides. The running-time for crawling individual wikipedia pages is not very efficient. Ideally, we’d have access to a local Wiki data-dump in order to speed-up the query time. However, storing that data would require many gigabytes of memory. Currently a 20 GB partial Wikipedia dump is [available on Kaggle](https://www.kaggle.com/jkkphys/english-wikipedia-articles-20170820-sqlite) Regrettably, that data does not include redirects or info-box information. Hence, we’ll need to scrape Wikipedia ourselves. It takes about 0.5 seconds to scrape an individual page. Hence, it takes on average 1-3 seconds to crawl all the noun-chunks in a sentence. Of course, we could speed things up using concurrency. However, given the resource limitations of our Kaggle notebook, concurrency should be avoided. With this in mind, we’ll comprise and execute the following strategy.\n\n* We’ll extract all unique noun-chunks from the percentage sentences found in abstracts.\n* We’ll query the noun-chunks against Wikipedia, thus obtaining a subset of medical terms (while also tracking their associated clinical specialties).\n* For every medical term, we’ll obtain the list of all percentage sentences that mention that term. Both abstract and body sentences will be included in the list.\n\nOf course, we will miss clinical terms that are mentioned in the body but not in the abstract. However, the importance of the missed terms should be minimal, since important observations are usually referenced in the abstract.\n\nLet’s now use Wikipedia to extract medical terms from our relevant sentences."},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\n# A mapping between medical terms and sentence mentions.\nmedical_terms = defaultdict(list)\n# A mapping between medical terms and the associated page-titles / specialties \n# that were scraped from Wikipedia\nterm_to_wiki_data = {}\n# A cache of all encountered noun chunks that are not medical terms.\nbad_noun_chunks = set()\n\n# We filter out certain noun-chunks to speed-up search results. This includes numeric nouns\n# associated with percentages as well as various variations of the term SARS.\nbad_patterns = [r'respiratory(\\s+distress)? syndrome', r'[0-9]']\nbad_regex = compile_pattern_list(bad_patterns, escape=False, add_bounderies=False)\n\nfor sent in percent_sentences & abstract_sentences:\n    # We start by iterating over the relevant sentences from the abstracts.\n    for noun_chunk in extract_noun_chunks(sent):\n        if noun_chunk in bad_noun_chunks or bad_regex.search(noun_chunk):\n            continue\n                \n        if noun_chunk in medical_terms:\n            # This noun chunk is a known medical term. We append this sentence to that term's \n            # associated sentence list.\n            medical_terms[noun_chunk].append(sent)\n            continue\n\n        # We do a wiki-search for previously unseen noun-chunk\n        wiki_result = search_wikipedia(noun_chunk)\n        if wiki_result is not None:\n            # New medical term discovered using Wikipedia.\n            medical_terms[noun_chunk].append(sent)\n            term_to_wiki_data[noun_chunk] = wiki_result      \n        else:\n            # We cache the non-medical term.\n            bad_noun_chunks.add(noun_chunk)\n            \nfor sent in percent_sentences & body_sentences:\n    for noun_chunk in extract_noun_chunks(sent):\n        # We iterate over the relevant sentences in the article bodies and subsequently\n        # update our sentence mentions.\n        if noun_chunk in medical_terms:\n            medical_terms[noun_chunk].append(sent)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 5. Exploring the Extracted Medical Terms\nTo start, we’ll count the total number of medical terms. We’ll also output the top 10 most frequently mentioned terms.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"We've obtained {len(medical_terms)} medical terms.\")\nprint(\"The top 10 terms / mention-counts are:\\n\")\nfor term, sentences in sorted(medical_terms.items(), \n                              key=lambda x: len(x[1]), reverse=True)[:10]:\n    print(term, len(sentences))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We’ve extracted 168 medical terms. Not surprisingly, the most frequently mentioned term happens to be _fever_. Let’s output a few random fever mentions."},{"metadata":{"trusted":true},"cell_type":"code","source":"for sentence in medical_terms['fever'][:5]:\n    print(sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Our mentions contain fluctuating percentages that detail the observed fraction of feverish patients. Of course, not all percentages are associated with fever. Some percentages pertain to other symptoms and observations. Is there a way to extract these percentages automatically, such that each percentage maps to its associated medical term? Yes! If we leverage the BERT-SQuAD model for question-answering.\n\n## 5.1 Using BERT-SQUAD to Extract Measured Percentages\n\nSuppose we have a sentence that mentions a fever. We want to know the percentage that’s associated with fever. How do we obtain that percentage? By asking!! Using the [BERT-SQuAD](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15848021.pdf) model, we can run question-answering on [any](https://towardsdatascience.com/testing-bert-based-question-answering-on-coronavirus-articles-13623637a4ff) on any text of our choosing. So we can ask “What percentage is associated with Fever?” and obtain an answer. Or, better yet, we can simply ask “% Fever?” and still obtain a result. Below, we load a [Hugging Face](https://huggingface.co/transformers/) SQuAD-trained transformer model in order to define an `answer_question` function. Afterwards, we query an example fever mention in order to obtain the fever percentage."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pytorch_transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom transformers import DistilBertTokenizer                    \nfrom transformers import DistilBertForQuestionAnswering\n                                           \ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad') \nmodel = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\ndef answer_question(question, text):\n    # Uses DistilBERT SQuaD trained model for question-answering.\n    input_text = f\"{question} [SEP] {text}\" \n    input_ids = tokenizer.encode(input_text) \n    start_scores, end_scores = model(torch.tensor([input_ids])) \n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n    # Strips out pound-signs and extra spaces returned by the model.\n    answer = answer.replace(' ##', '').strip()\n    return answer\n\ntext = (\"The most common symptoms at the onset of illness were fever (82.1%), cough (45.8%), \"\n       \"fatigue (26.3%), dyspnea (6.9%) and headache (6.5%).\")\nanswer_question(\"Percent fever?\", text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Our strategy worked! Let’s now define a task-specific `ask_percentage` function. The function will query an input text for the percentage that’s associated with an input term. If found, the extracted percentage will be converted to a numeric quantity."},{"metadata":{"trusted":true},"cell_type":"code","source":"def ask_percentage(term, text):\n    # Asks for the percentage that's associated with an input medical term.\n    question = f'Percent {term}?'\n    answer = answer_question(question, text)\n    # Strips out extra-spaced returned by DistilBERT model.\n    answer = answer.replace(' %', '%').replace(' . ', '.')\n    # Extracts the percentage from the asnwer, if any.\n    percentage = _extract_percentage(answer)\n    # Converts the percentage into a float.\n    return float(percentage[:-1]) if percentage else None\n    \nregex_percent = compile_( r'\\b[0-9]{1,2}(\\.[0-9]{1,2})?%')\ndef _extract_percentage(text):\n    # Uses a regex to extract a percentage from the text.\n    match = regex_percent.search(text)\n    return text[match.start(): match.end()] if match else None \n\nask_percentage('fever', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Given our list of fever mentions, we can attempt to extract a fever percentage for each mention. Afterwards, we can compute a single representative percentage using either the median or mean of our results. Subsequently, we can choose a representative mention whose percentage matches our representative value. We’ll now define a `match_percentage` function that will execute these steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"def match_percentages(term, text_list):\n    \"\"\"The function returns a representative percentage of a term across of a list of mentions, along\n    with its representative mention.\n    \n    Parameters\n    ----------\n    term: str\n        A clinical term whose percentage we wish to obtain (Example: Fever)\n    text_list: [str]\n        A list of of text mention. Each metion potentially contains percentage match to `term`.\n    \n    Results\n    -------\n    representative_percentage: float\n        An extracted percentage that is closest to the median.\n    representative_mention: str\n        The element of `text_list` in which `representative_percentage` has been found.\n    \"\"\"\n    matches = []\n    for text in text_list:\n        percentage = ask_percentage(term, text)\n        if percentage is not None and percentage != 0.95:\n            # We filter all references to 95%. These mostly correspond with statistical significance,\n            # and not with observed fequencies.\n            matches.append((percentage, text))\n    \n    if not matches:\n        # No percentages have been found.\n        return None, None\n    \n    percentages = np.array([m[0] for m in matches])\n    # We use the median instead of the mean, due to risk of overweighed extreme values.\n    dist_to_median = np.abs(percentages - np.median(percentages))\n    return matches[dist_to_median.argmin()]\n    \nrep_percentage, rep_mention = match_percentages('fever', medical_terms['fever'])\nprint(f\"\\nThe representative percentage of fever is {rep_percentage}%.\")\nprint(f\"It occurs in the following sentence:\\n{rep_mention}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We’ve obtained a single fever percentage, along with its associated sentence. However, our analysis of the _Fever_ term is incomplete. Currently, there exist multiple terms within our `medical_terms` dictionary that redirect to the _Fever_ Wiki-page. All these terms are essentially aliases of _Fever_. Hence, their mentions should be combined into a single `all_fever_sentences` list. Afterwards, we’ll want to re-run `match_percentage` on that list."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_fever_sentences = []\nfever_aliases = []\n\n_, fever_title =  term_to_wiki_data['fever']\n# The term 'fever' redirects to a Wiki=page titled 'Fever'/\nassert fever_title == 'Fever'\n\nfor term, sentences in medical_terms.items():\n    _, wiki_title = term_to_wiki_data[term]\n    if wiki_title == fever_title:\n        fever_aliases.append(term)\n        all_fever_sentences.extend(sentences)\n\n\nprint(f\"The following {len(fever_aliases)} terms are aliases of Fever:\\n {fever_aliases}\")\n\nrep_percentage, rep_mention = match_percentages('fever', all_fever_sentences)\nprint(f\"\\nThe representative percentage of Fever is {rep_percentage}%.\")\nprint(f\"It occurs in the following sentence:\\n{rep_mention}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We’ve obtained a representative percentage for all our fever mentions. In the process, we have also aggregated all the distinct alises of the symptom under a single _Fever_ name. Now, we will apply a similar strategy to all our remaining medical terms. For every given term we will:\n\n1. Uncover all the aliases that redirect to the same Wiki-page as the term.\n2. Aggregate together all the aliases and assign them their representative Wiki-page title.\n3. Compute a representative percentage and a representative mention.\n4. Store the results in a Pandas DataFrame.\n\nThe DataFrame will include aliases, and the representative percentages. It will also include mention counts. Additionally, we’ll add the Wikipedia-extracted medical specialty to the DataFrame, for further analysis.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Maps a medical word to its aliases, based on wikipedia redirects.\naliases = defaultdict(list)\n# An aggregation of all mentions that's associated with a term\naggregated_sentences = defaultdict(list)\nfor term, sentences in medical_terms.items():\n    _, name = term_to_wiki_data[term]\n    aliases[name].append(term)\n    aggregated_sentences[name].extend(sentences)\n    \ntable = {'Name': [], 'Count': [], 'Specialties': [], 'Percentage': [],\n         'Percentage Text': [], 'Aliases': []}\n\n# Sorting ensures constitancy of indices in DataFrame.\nfor name, alias_list in sorted(aliases.items()):\n    # Iterate over each aggregated medical concept.\n    sentences = aggregated_sentences[name]\n    count = len(sentences)\n    percentage, percent_text = match_percentages(name, sentences)\n    specialties = term_to_wiki_data[alias_list[0]][0]\n    table['Name'].append(name)\n    # The count of total aggregated sentences.\n    table['Count'].append(count)\n    # The wiki-determined medical specialties associated with the concept.\n    table['Specialties'].append(specialties)\n    # The computed representative percentage.\n    table['Percentage'].append(percentage)\n    table['Percentage Text'].append(percent_text)\n    # The aggregated aliases of the medical concept\n    table['Aliases'].append(alias_list)\n    \ndf_medical = pd.DataFrame(table)\ndf_medical.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have a table of aggregated medical terms, along with their measured percentages. This table will serve as the basis for our medical knowledge base. We’ll proceed to develop that medical knowledge base, by differentiating between symptoms and comorbidities.\n\n# 6. Building a Knowledge Base of Medical Terms\n\nWe’ll start by probing our table in more detail. First, we’ll sort the table by mention-count and also by measured percentage. Afterwards, we’ll output the top 10 sorted medical terms.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_medical.sort_values(['Count', 'Specialties'], ascending=False, inplace=True)\ndf_medical.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our top 10 terms fall into several broad categories. Some terms, like _Fever_ and _Cough_, are symptoms of Covid-19. Other terms, _Diabetes_ and _Hypertension_ are its comorbidities. Can we separate the comorbidities from the symptoms? We can try! Let’s turn our attention to extracting the symptoms.\n\n### 6.1 Uncovering the Symptoms of Covid-19\n\nGiven a line of text, how do extract all medical symptoms mentioned in that text? Well, we can just ask for the symptoms! After-all, we still have our transformer-powered `answer_question` function at our disposal. All we need to is input `’What are the symptoms?’` into `answer_question` along with the text. Afterwards, we’ll obtain our result."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"Fever ( 89.8% ) and Cough ( 67.2% ) were common clinical symptoms, while diabetes and hypertension were a common comorbidity.\" \n# Please note that question-answering better if we strip out the parantheses fom the text.\nanswer_question('What were the symptoms?', remove_parans(text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this in mind, let’s define an `is_symptom` function. The function will take as input some medical term, such as _Fever_, _Cough_, or _Diabetes_. Afterwards, the function will iterate over all sentences that mention that term. For each sentence, the function will ask for symptoms. It will then check whether the term is found among the symptoms. If at-least 10% of the sentences reference the term amongst the symptoms, then `is_symptom` will return `True`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Constructs a symptom regular expression. Symptoms are sometimes referenced as \"manifestations.\"\nsymptom_synonyms = ['symptom', 'manifestation']\n# A word-boundary is not added to the right-side of the regex, to allow for plurality.\nsymptom_regex = compile_pattern_list([r'\\b' + s for s in symptom_synonyms],\n                                     escape=False, add_bounderies=False,)\n\ndef is_symptom(term, min_fraction=0.1): \n    \"\"\"Returns True a medical term is referred to as a symptom among its mentions.\n    \n    Parameters\n    ----------\n    term: str\n        A medical term within our `df_medical` DataFrame.\n    min_fraction (optional): float\n        The minimum fraction of `term` sentences that must refer to the term \n        as symptom for the function to return True. Preset to 10%.\n    \"\"\"\n    aliases = df_medical.loc[df_medical.Name == term].Aliases.values[0]\n    # Compiles a regular expression containing all the aliases of `term`.\n    alias_regex = aliases_to_regex(aliases)\n    count = 0\n    # The minimum number of symptom matches required to return True.\n    min_count = int(min_fraction  * len(aggregated_sentences[term]))\n    for sentence in aggregated_sentences[term]:\n        # We ask for the symptoms of the sentence.\n        answer = ask_for_symptoms(sentence)\n        if answer and alias_regex.search(answer):\n            # The term is referred to as a symptom in the sentence.\n            count += 1\n            if count >= min_count:\n                return True\n            \n    return False        \n    \n\ndef ask_for_symptoms(text):\n    \"\"\"Using question-answering to ask for the symptoms in the text\"\"\"\n    text = remove_parans(text)\n    for question in [f'What are the {s}s?' for s in symptom_synonyms]:\n        # We ask for symptoms, as well as clincial manifestations.\n        answer = answer_question(question, text)\n\n        if not answer:\n            continue\n        \n        # In order to up precision, we check for the mention of symptoms \n        # in the answer, or right before the answer.\n        if re.search(r'\\b(symptoms|manifestations)\\s+(were|are)$',\n                     text.split(answer)[0].strip()):\n            return f\"symptoms are {answer}\"\n        \n        if symptom_regex.search(answer):\n            return answer\n    \n    return None\n                \ndef aliases_to_regex(aliases):\n    # Transforms the alieases into reguler expersions. The longer aliases \n    # are matched first.\n    aliases = sorted(aliases, key=len, reverse=True)\n    return compile_pattern_list(aliases)\n\nfor term in ['Fever', 'Cough', 'Diabetes', 'Hypertension']:\n    if is_symptom(term):\n        print(f\"{term} is a symptom.\")\n    else:\n        print(f\"{term} is not a symptom.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We’ll now proceed to isolate all symptoms in a separate `df_symptoms` DataFrame. All other medical terms will be added to `df_not_symptoms`.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"are_symptoms = np.array([is_symptom(name) for name in df_medical.Name.values])\ndf_symptom = df_medical[are_symptoms]\ndf_not_symptom = df_medical[~are_symptoms]\nprint(f\"{df_symptom.shape[0]} of our medical terms are symptoms.\")\nprint(f\"{df_not_symptom.shape[0]} of our medical terms are not symptoms.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"20 of our medical terms are symptoms. Let’s output these symptoms for review."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_symptom","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Our symptoms all make sense. Now let’s sample from our `df_not_symptom` DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_not_symptom.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Some of our printed terms are comorbidities (_Diabetes_, _Hypertension_). Other terms like _Pneuomonia_ represent medical consequences of COV-19 onset.  We’ll now turn our efforts to extracting out the comorbidities.\n\n\n## 6.2 Uncovering Covid-19 Comorbidities\n\nUncovering comorbidities is trickier than uncovering the symptoms. The language around comorbidities tends to be more varied. Hence, it’s harder to extract these comorbidities out without some supervision. We’ll need to manually sample from `df_not_symptom` in order to extract our initial comorbidity data. Purely random sampling will not be productive. We need to sample in an intelligent manner, to ensure that the most common comorbidities appear in our sampled results. One way to sample intelligently is to leverage our existing _Specialty_ column. Medical specialties associated with common comorbidities are expected to occur more frequently. Hence, sampling from the top-most frequent specialties should yield some insightful results. Therefore, let’s print out the most frequently-mentioned specialties. We’ll only print those specialties that are mentioned more than twice."},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n# We count specilties by the number of terms that fall within each specialty. Each\n# unique term is counted only once. Alternatively, we could weighing each specialty\n# by the total number of term mentions.\nspecialties = Counter(df_not_symptom.Specialties.values)\nprint(f\"We have {len(specialties)} medical specialties in total.\")\nprint(\"The top-ranking specialties are:\\n\")\nfor specialty, count in specialties.most_common():\n    if count <= 2:\n        break\n        \n    print(f\"{specialty}:  {count}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We’ve outputted seven top-ranking specialties. Let’s now examine these specialties one-by-one. For each specialty, we will gather up all the associated terms. Afterwards, we will print out representative sentences for each term. Within each sentence, we’ll highlight the term in red, for easier readability. We’ll also highlight each term’s measured percentage in blue. Furthermore, we’ll include the term’s DataFrame index. This will allow us to isolate those indices thR correspond to comorbidities.\n\nIn order to highlight medical term matches, we’ll convert all our outputs to HTML. Subsequently, we’ll render that HTML. For this purpose, we’ll define a `display_specialty` function. Given an input specialty, the function  will display a rendered summary, in the manner described above.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.display import display, HTML\n\ndef display_specialty(specialty):\n    # Visualizes key information about a specialty using HTML.\n    html = specialty_to_html(specialty)\n    display(HTML(html))\n    \ndef specialty_to_html(specialty):\n    # Extracts key information about a specialty, highlighting features using HTML.\n    df = df_not_symptom[df_not_symptom.Specialties.isin([specialty])]\n    #html = f'<h3>{specialty}</h3></br>'\n    html = ''\n    for index, row in df.iterrows():\n        # We iterate of all non-symptometic medical terms within the specialty.\n        tup = row[['Name', 'Count', 'Aliases', 'Percentage', 'Percentage Text']]\n        name, count, aliases, percentage, text = tup\n        # For each medical term, we output its index as well as count.\n        html += f'<h4>{index} {name.upper()}: Count {count}</h4>' \n        \n        if text:\n            # A representative percentage is associated with the medical term.\n            # We'll highlight that percentage within the representative text.\n            percentage = str(int(percentage)) if int(percentage) == percentage else str(percentage)\n            regex = re.compile(r'\\b%s\\b' % percentage)\n            # The percentage is boldened and colored blue.\n            text = add_markup(regex, text, multi_matches=False,\n                              color='blue', bold=True)     \n        else:\n            # If no percentage is found, then we choose a random sentence that's\n            # associated with the medical term.\n            text = aggregated_sentences[name][0]\n        \n        regex = aliases_to_regex(aliases)\n        # We color the medical term red within the text, for a better display.\n        text = add_markup(regex, text, color='red')\n        html += text + '</br></br>'\n    \n    return f'<html>{html}</html>'\n    \n        \ndef add_markup(regex, text, multi_matches=True, **kwargs):\n    \"\"\" Adds markup to all matches of a regular expression within the text.\n    \n    Parameters\n    ----------\n    regex: re.Pattern\n        A compiled regular expression that we match against the text.\n    text: str\n        Our inputted text string.\n    multi_matches (optional): Bool\n        If True, then multiple regex matches will be marked up within the\n        text\n        \n    Returns\n    -------\n    marked_text: str\n        Text with HTML markup added to all regex matches.\n    \"\"\"\n    offset = 0\n    for match in regex.finditer(text):\n        old_length = len(text)\n        span = (match.start() + offset, match.end() + offset)\n        text = _add_span_markup(span, text, **kwargs)\n        # Offset tracks length-shift in the text due to markup addition.\n        offset += len(text) - old_length\n        if not multi_matches:\n            break\n        \n    return text\n    \ndef _add_span_markup(span, text, color='black', bold=False):\n    \"\"\"Adds markup across a single span of text. Colors and optionally\n    bolds that span.\n    \n    Parameters\n    ----------\n    span: (int, int)\n        The start and end span of text that we'll markup.\n    text: str\n        The complete text\n    color (optional): str\n        The color to assign the marked-up span.\n    bold (optional): bool\n        If True, than boldens the marked-up span.\n        \n    Returns\n    -------\n    marked_text: str\n        Text with HTML markup added across the specified span.\n    \"\"\"\n    start, end = span\n    snippet = text[start: end]\n    html_snippet = f'<font color=\"{color}\">{snippet}</font>'\n    if bold:\n        html_snippet = f'<b>{html_snippet}</b>'\n    \n    return text[:start] + html_snippet + text[end:]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now ready to investigate the specialties. Let’s start by viewing the infectious diseases."},{"metadata":{"trusted":true},"cell_type":"code","source":"display_specialty('Infectious disease')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The infectious disease output does not appear that useful. No comorbidities are present. Most of the output refers to other infectious epidemics that are analogous to SARS-CoV-2. Perhaps we’ll have more luck with Pulmonology, which is next on our specialty list."},{"metadata":{"trusted":true},"cell_type":"code","source":"display_specialty('Pulmonology')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this output we see several comorbidities including COPD (Index 22) and Asthma (Index 9).  Let’s store these comorbidity indices in a set for later use. Also, in our output, we observe several symptoms that belong in our `df_symptom` DataFrame. These symptoms include Sputum (Index 96 ), Wheeze (Index 104) and Crackles (Index 33) are actually Covid-19 symptoms. Let’s save the symptom indices in a separate set. Later, we will transfer these indices to `df_symptom` for maximum recall."},{"metadata":{"trusted":true},"cell_type":"code","source":"comorbidity_indices = {22, 9}\nsymptom_indices = {96, 104, 33}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we'll examine Hematology."},{"metadata":{"trusted":true},"cell_type":"code","source":"display_specialty('Hematology')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No comorbidities have been observed. Let's move on to Opthalmology."},{"metadata":{"trusted":true},"cell_type":"code","source":"display_specialty('Ophthalmology')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conjunctivitis or pink-eye (29) is a known ocular symptom of Covid-19. So is Dry Eye Syndrome (42). We’ll add these to our `symptom_indices` set. Meanwhile, diabetic retinopathy (37) is a comorbidity. We'll add it to `comorbidity_indices`."},{"metadata":{"trusted":true},"cell_type":"code","source":"comorbidity_indices.add(37)\nsymptom_indices.update([52, 42])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we'll take a look at Cardiology"},{"metadata":{"trusted":true},"cell_type":"code","source":"display_specialty('Cardiology')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hypertension (56) and Cardiovascular Disease (15) are comorbidities."},{"metadata":{"trusted":true},"cell_type":"code","source":"comorbidity_indices.update([56, 15])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's move on to Endocrinology."},{"metadata":{"trusted":true},"cell_type":"code","source":"display_specialty('Endocrinology')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Diabetes (36), Obesity (80) and Hypothyroidism (60) are comorbidities."},{"metadata":{"trusted":true},"cell_type":"code","source":"comorbidity_indices.update([36, 80, 60])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we'll examine Nephrology."},{"metadata":{"trusted":true},"cell_type":"code","source":"display_specialty('Nephrology')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Chronic Kidney Disease (21) is a comorbidity."},{"metadata":{"trusted":true},"cell_type":"code","source":"comorbidity_indices.add(21)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We’ve examined the top 7 of our 47 specialties by hand. Do we really need to examine the remaining 40 specialties? Not necessarily. All the remaining specialties contain no more than 2 medical terms. There’s a reasonable likelihood that these terms aren’t actual comorbidities. However, some terms are comorbid. We thus can heuristically assume that these terms are mentioned with some other comorbidity. Furthermore, many top comorbidities have already been recorded in our `comorbidity_indices` Hence, let’s execute the following heuristic. We’ll output only those remaining specialties that contain a term which is co-mentioned with a recorded comorbidity. Afterwards, we’ll examine these specialties by hand, and add additional comorbidities to our set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import chain\ncomorbid_names = df_not_symptom[df_not_symptom.index.isin(comorbidity_indices)].Name.values\n# A set of sentences that mention a comorbidity.\ncomorbid_sentences = set((chain.from_iterable([aggregated_sentences[name] \n                                               for name in comorbid_names])))\nfor specialty, count in specialties.most_common()[7:]:\n    # We iterate over the remaining 40 specialtities.\n    df_tmp = df_not_symptom[df_not_symptom.Specialties.isin([specialty])]\n    for name in df_tmp.Name.values:\n        if comorbid_sentences & set(aggregated_sentences[name]):\n            # The term in the specialty is mentioned in a sentence that also\n            # mentions a comorbidity.\n            display_specialty(specialty)\n            break\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cancer (14), Lung Cancer (72), Cerebrovascular disease (18),  Cerebral infarction (17), Cirrhosis (23), Acute Pancreatitis (2), Coronary Artery Disease (30), Tuberculosis (99), Immunodeficiency (61), and Kidney Disease (65) are all additional comorbidities that we have missed. Let’s add them to `comorbidity_indices`."},{"metadata":{"trusted":true},"cell_type":"code","source":"comorbidity_indices.update([14, 72, 18, 17, 23, 2, 30, 99, 61, 65])\nprint(f\"We uncovered {len(comorbidity_indices)} total comorbidities\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We’ve uncovered 19 total comorbidities. Let’s transfer these comorbidities to a separate `df_comorbidities` DataFrame. We'll then print and review the comorbidities."},{"metadata":{"trusted":true},"cell_type":"code","source":"is_comorbid = df_not_symptom.index.isin(comorbidity_indices)\ndf_comorbidity = df_not_symptom[is_comorbid]\ndf_comorbidity.sort_values(['Count', 'Specialties'], ascending=False)\ndf_comorbidity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our commorbities cover diabetes, heart issues, lung issues, kidney / liver issues, They also include cancer, obesity and immunodeficiencies. Co-infection by tuberculosis also makes our list. With comorbidities in-place, our knowledge is nearly complete. We just need to update our `df_symptom` DataFrame with the newly-discovered symptoms in `symptom_indices`.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_symptom = df_symptom.append(df_not_symptom.loc[symptom_indices])\ndf_not_symtpom = df_not_symptom.drop(index=symptom_indices)\nprint(f\"We uncovered {df_symptom.shape[0]} total symptoms\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We’ve uncovered 25 total symptoms, as well as 19 total comorbidities. We will assume that all remaining medical terms are neither symptoms nor comorbidities. Let’s shift these terms to a `df_other` DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnot_comorbid = ~df_not_symptom.index.isin(comorbidity_indices)\ndf_other = df_not_symptom[not_comorbid]\n\ntotal = df_medical.shape[0]\npercent_other = int(100 * df_other.shape[0] / total)\nprint(f\"{100 - percent_other}% of our medical terms are symptoms or comorbidities.\")\nprint(f\"The remaining {percent_other}% of terms fall into the 'Other' category.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Nearly 40% of medical terms have been effectively categorized as either symptoms or comorbidities. Our knowledge base is ready! Now, we’ll leverage that knowledge base to build an intelligent search tool. The search will allow us to probe for critical risk factors within our data. It will yield insights that will allow us to construct a risk prediction model.\n\n# 7. Building a KB-Powered Search Tool to Probe for Risks\n\n\nWe want to construct a search tool that will rank matched texts by matches to comorbidities, symptoms, and other medical terms. Furthermore, we want the tool to color all such matches within the text by category type. With this in mind, we’ll construct three large-scale regular expressions to match against our three DataFrames, `df_comorbidity`, `df_symptom`, and `df_other`. Additionally, we’ll include a fourth regular expression that detects valuable percentage matches. The regular expressions will be leveraged by a `mark_matches` function, which will color-code all matched terms within a text. The function will return a marked HTML string. It will also return a count of matches carried out by each of our four regular expressions. Later, we will use these counts as our ranking criteria within the search tool\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def aliases_to_regex(df): \n    # Converts all aliases with a medical DataFrame into a regular expression\n    patterns = set(np.hstack(df.Aliases.values))\n    patterns.update([name.lower() for name in df.Name.values])\n    patterns = sorted(patterns, key=len, reverse=True)\n    return compile_pattern_list(patterns)\n\n# Our regular expressions match comorbidities, symptoms, other medical terms, and also percentages.\nregex_list = [aliases_to_regex(df)\n              for df in [df_comorbidity, df_symptom, df_other]] + [regex_percent]\n\n# Markup colors are assigned to each match type.\ncolors = ['blue', 'green', 'brown', 'black']\nformat_kwargs = [{'color': c} for c in colors]\n# Percentages will be boldened in the marked up text.\nformat_kwargs[-1]['bold'] = True\n\ndef mark_matches(text):\n    \"\"\"A input text is converted into a marked up HTML string, based on \n    terminology matches. All match counts are also returned.\n    \n    Parameters\n    ----------\n    text: str\n        The text we match against.\n        \n    Returns\n    -------\n    marked_text: str\n        A marked-up version of the text based on regex matches.\n        \n    match_counts: [int, int, int, int]\n        A list of four match counts, one for each regex in `regex_list`.\n    \"\"\"\n    match_counts = []\n    for regex, kwargs in zip(regex_list, format_kwargs):\n       \n        match_counts.append(len(regex.findall(text)))\n        text = add_markup(regex, text, **kwargs)\n    \n    return text, match_counts\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Let’s test-out `mark_matches` by applying it to random _Diabetes_ sentence. We’ll expect that function to match at-least one comorbidity (Diabetes) and also to match at-least one percentage."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = aggregated_sentences['Diabetes'][0]\nmarked_text, match_counts = mark_matches(text)\nfor count, name in zip(match_counts, ['comorbidities', 'symptoms', 'other terms', 'percentages']):\n    print(f\"The text matches {count} {name}.\")\n\nprint('\\nDisplaying the marked-up text:')\ndisplay(HTML(mark_matches(text)[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our function works! We’ll now utilize it to create a ranked-search tool. Basically, we’ll define a `ranked_search` function. The function will take as input a regular expression that will be matched against all 97.6K clinically relevant sentences. All matching results will subsequently be ranked in following manner:\n\n1. Abstract sentences will take precedence over non-abstract sentences, since clinically critical sentences are more likely to wind up in the abstract.\n2. Sentences with a diversity of matched categories (symptoms, comorbidities, percentages, etc) will take precedence over sentences with just a single type of matched category (symptoms only, etc). Knowledge diversity is more interesting, since it allows to examine the interplay between comorbidities, measurements, etc.\n3. Sentences with a higher total match count will take precedence over sentences with a lower total match-count. \n\nOur top-ranking sentences will be displayed using HTML. All medical-term matches will be marked appropriately, by color category. Additionally, our query term will be marked in red within the output.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ranked_search(query_string, results_per_page=7, page=1):\n    \"\"\"A ranked search tool for extracting relevant sentences.\n    The top-ranked matches are displayed as HTML.\n    \n    Parameters\n    ----------\n    query_string: str:\n        Our query string that's used to match the sentences. This string\n        can be a regular expression.\n    results_per_page (optional): int\n        The number of results to dispaly within a page of output.\n    page (optional): int\n        The page-number. It allows us to flip through multiple \n        pages of results.\n    \"\"\"\n    regex = compile_(query_string)\n    matches = [s for s in all_sentences if regex.search(s)]\n    # This dictionary maps matched sentences to a tuple that is used for ranking purpuses.\n    matches_to_ranking = {}\n    for match in matches:\n        # All matches to the query are marked in red.\n        marked_match = add_markup(regex, match, color='red', bold=True)\n        marked_match, match_counts = mark_matches(marked_match)\n        # The number of diverse categorical matches.\n        num_matches = len([count for count in match_counts if count])\n        # The total number of matches.\n        num_total_matches = sum(match_counts)\n        # Assigns each match a tuple, for ranking purposes.\n        matches_to_ranking[marked_match] = (match in abstract_sentences, num_matches,\n                                            num_total_matches)\n        \n    # Ranks matches by importance\n    sorted_matches = [m[0] for m in sorted(matches_to_ranking.items(),\n                                           key=lambda x: x[1], reverse=True)]\n    start = (page - 1) * results_per_page\n    end = start + results_per_page\n    # Displays the top results using HTML.\n    html = '<br>'.join(sorted_matches[start: end])\n    display(HTML(html))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s take our `ranked_search` function for a spin! We’ll start by searching for _risk factors_."},{"metadata":{"trusted":true},"cell_type":"code","source":"ranked_search('risk factors')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The query yields some valuable insights. Symptoms such as _dyspnea_, _chest pain_, _cough_ are increased risk factors for COVID-10 pneumonia. So is age, as well as underlying comorbidities (such _hypertension_ and _diabetes_). Additionally, we’ve learned that _asthma_ and _COPD_ and not risk factors for SARS-CoV-2 infection. That’s good to know!\n\nNext, we’ll search for pregnancy and prenatal-related sentences."},{"metadata":{"trusted":true},"cell_type":"code","source":"ranked_search(r'(pregnan(t|cy)|pre-?natal)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well we’ve gained some insight on how pregnant women react to other types of respiratory diseases. For instance, in 2009, pregnant women accounted for 5% of all H1N1 related deaths. Also, we learned that previous Coronavirus outbreaks (SARS-CoV / MERS) were known to be responsible for severe complications during pregnancy. However, we learned little about how pregnant women respond to SARS-CoV-2. Perhaps the next page of results will yield greater insights."},{"metadata":{"trusted":true},"cell_type":"code","source":"ranked_search(r'(pregnan(t|cy)|pre-?natal)', page=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"It seems that pregnancy complications (including fetal distress) have been observed in multiple pregnant women after the onset of COVID-10. Fetal destral and premature membrane ruptures  can happen if the infection occurs during the third trimester of pregnancy. However, on a more positive note, there is no evidence of intrauterine infection in pregnant women. Thus, it is unlikely that pregnant women can pass-on the virus to the fetus.\n\nNow, let’s search for _coinfections_."},{"metadata":{"trusted":true},"cell_type":"code","source":"ranked_search(r'co-?infection')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We learned that MTB (Mycobacterium tuberculosis) co-infection is linked with disease severity. Also, coinfection is common in pediatric patients. Thus, coronavirus-infected children are potentially at risk for other infections (even though generally, the effects of the virus are less severe in children).\n\nWe’ll proceed to search for comorbidities."},{"metadata":{"trusted":true},"cell_type":"code","source":"ranked_search(r'co-?morbidities')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"72.2% of patients treated in the ICU had underlying comorbidities. This is relative to the 37.3% of comorbid patients who did not require intensive care.. Such recorded percentages are quite valuable. Essentially, they serve as conditional probabilities of disease-features given latent outcomes. The probability of a comorbidity given a severe (ICU-worthy) outcome is 0.722. Meanwhile, the probability of a comorbidity given a milder outcome if only 0.373. These conditional values can allow us to build a Bayesian model. Hence, the values are worth recording. Also, there are other conditional percentages within our output. Patients with severe progression are more likely to have  dyspnea (63.9% vs 19.6%) and anorexia (66.7% vs 30.4%). Let’s store all these percentages within a `conditionals` dictionary. Later, we will utilize that dictionary for modeling purposes.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"conditionals = {'comorbidities': (72.2, 37.3), 'dyspnea': (63.9, 19.6), 'anorexia': (66.7, 30.4)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Let’s extract more progression stats. We’ll now search for _progression group_."},{"metadata":{"trusted":true},"cell_type":"code","source":"ranked_search(r'progression group')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"27.3% of patients with patients with progression have a history of smoking. This is relative to the 3% of the patients in the stable group that have a history of smoking. We’ve just obtained another conditional percentage: **Smoking: 27.3% vs 3%**. Let’s add it to our conditional dictionary.\n\n_NOTE: Not all our conditional percentages are equal. Different measurements correspond to different levels of disease progression / severity. Thus, any model built around these values serves as a baseline-effort, not as a rigorous statistical analsyis_.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"conditionals['smoking'] = (27.3, 3.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Now, let’s examine patient mortality stats. We’ll search for _non_survivor_."},{"metadata":{"trusted":true},"cell_type":"code","source":"ranked_search(r'non-?survivor')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We’ve obtained two additional conditional variables relative to non-survivors / survivors. These variables include _disseminated intravascular coagulation_ (71.4% vs 0.6%) and _BMI > 25 kg / m2_ (88.24% vs 18.95%). Should we add these conditionals to our dictionary? Well, it depends. Our previous conditionals all reflect disease progression. These conditionals reflect death; the ultimate end-point of disease progression. Still, it’s difficult for us to compare death and disease progression directly. Nonetheless, if we treat these numbers as estimates, we can cautiously construct a baseline model. The model won’t be based on rigorous statistics. However, it will provide some baseline predictive power that otherwise would not be available. Hence, we’ll actually add these two conditionals to our dictionary."},{"metadata":{"trusted":true},"cell_type":"code","source":"conditionals.update({'disseminated intravascular coagulation': (71.4, 0.6),\n                     'BMI>25': (88.24, 18.95)})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we’ll run a final search to get some stats on patients who have stabilized."},{"metadata":{"trusted":true},"cell_type":"code","source":"ranked_search(r'\\bstabilized\\b')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"85.9% of observed have improved/stabilized, according to one article. The remaining 14.1% percent of patients have deteriorated. We’ll treat these two percentages as our prior probabilities of deterioration / stabilization."},{"metadata":{"trusted":true},"cell_type":"code","source":"prior_probs = [0.141, 0.859]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We now have prior probabilities, in addition to conditional probabilities of observed variables (given deterioration / stabilization). Let’s quickly output these conditional probabilities for review."},{"metadata":{"trusted":true},"cell_type":"code","source":"for condition, tup in conditionals.items():\n    prob_a, prob_b = np.array(tup) / 100\n    print(f'P({condition} | Deterioration) = {prob_a:.3f}')\n    print(f'P({condition} | Stabilization) = {prob_b:.3f}')\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given these probabilities, we can now construct a simple Bayes classifier for prediction deterioration / stabilization.\n\n# 8. Building a Classifier to Predict Covid-19 Progression\n\nOur classifier will be a simple Naive Bayes model. The model will assume that all input features are independent of each other. Clearly, that is not the case. For instance, Anorexia and BMI are two features that are closely related. Nonetheless, it has been shown the Naive Bayes can work [surprisingly well](http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf), even when some features are interdependent.  Hence, despite our vast oversimplifications, the model should provide some useful baseline insights. \n\nWe’ll now define a `naive_bayes` function in a few lines of code, following the standard [Naive Bayes math](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Constructing_a_classifier_from_the_probability_model). Our function will take a vector of 6 binary features, corresponding to the keys within `conditionals`. It will then return a binary value demarcating whether a patient is at risk for COVID-19 progression."},{"metadata":{"trusted":true},"cell_type":"code","source":"# A matrix of conditional probabilities. Each column corresponds to a feature in `conditionals`.\n# The first row corresponds to the prob(feature | deterioration). The second row corresponds\n# to prob(feature | stabilization)\nconditional_matrix = np.vstack([np.array(tup) / 100 \n                                for tup in conditionals.values()]).T\n# We'll use Maximum a posteriori estimation, so taking the Log of the\n# conditional probabilities will be sufficient.\nlog_conditional_matrix = np.log(conditional_matrix)\n\ndef naive_bayes(feature_vector):\n    results = log_conditional_matrix @ feature_vector + np.log(prior_probs)\n    # Returns 1 if the conditional probability of deterioration (index 0) is maximized, and 0 otherwise.\n    return 1 - results.argmax()\n\nassert naive_bayes([0] * 6) == 0\nassert naive_bayes([1] * 6) == 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Suppose a coronavirus patient has diabetes, which is a comorbidity. Is that patient more likely to progress or to stabilize? Let’s find out!"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = np.array([1] + 5 * [0])\nphrase = 'deteriorate' if naive_bayes(features) else 'stabilize'\nprint(f\"The patient is more likely to {phrase}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The patient is more likely to stabilize. However, now suppose the patient begins to show dyspnea (shortness-of-breath). How will that affect the patient’s prognosis? We’ll check below."},{"metadata":{"trusted":true},"cell_type":"code","source":"features[1] = 1\nphrase = 'deteriorate' if naive_bayes(features) else 'stabilize'\nprint(f\"The patient is more likely to {phrase}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The patient is now more likely to deteriorate. Our model has some rudimentary predictive power. However, extracting binary features for the model is a bit inconvenient. We have to construct the binary features by hand, while remembering each feature’s vector index. It would be more convenient if we could extract the features automatically, from a textual description of the patient. For instance, suppose we encounter the following patient description in our records:\n\n_The patient suffers from diabetes and is a chronic smoker. He has a BMI of 28_.\n\nHow do we extract features from that text? Well, extracting the comorbidity feature is easy! We simply need to match  our comorbidity regular expression to the text!"},{"metadata":{"trusted":true},"cell_type":"code","source":"text =\"The patient suffers from diabetes and is a chronic smoker. He has a BMI of 28.\"\ncomorbidity_regex = aliases_to_regex(df_comorbidity)\nassert comorbidity_regex.search(text) is not None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In that same manner, we can construct text-matching regular expressions for the subsequent four features in our model. However, as caveat, we should note that these regexes will not cover negation (ex: “no history of smoking”). Still, as a baseline, this solution ought to be sufficient."},{"metadata":{"trusted":true},"cell_type":"code","source":"smoking_regex = compile_(r'\\bsmok(er|es|ed|ing)\\b')\n# We include all aliases of the various terms, such as \"Shortness of Breath\", an alias of dyspnea.\ntup = [aliases_to_regex(df_medical[[term in aliases \n                                    for aliases in df_medical.Aliases.values]])\n       for term in ['dyspnea', 'anorexia', 'disseminated intravascular coagulation']]\n\ndyspnea_regex, anorexia_regex, coagulation_regex = tup\nfeature_regex_list = [comorbidity_regex, smoking_regex, dyspnea_regex, anorexia_regex, coagulation_regex]\ndef features_from_regex(text):\n    return [int(regex.search(text) is not None) for regex in feature_regex_list]\n\nprint(features_from_regex(text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obtaining the final feature is more tricky. We need a way to extract the BMI. One solution is to just ask for it! Inputting “What is the patient’s BMI?” into `answer_question` will yield the BMI information."},{"metadata":{"trusted":true},"cell_type":"code","source":"answer_question('What is the patient\\'s BMI?', text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence, we can define an `is_large_bmi` function, which extracts the final binary feature from the text."},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_large_bmi(text):\n    # Returns 1 if a BMI of greater > 25 is mentioned in the text, and zero otherwise.\n    answer = answer_question('What is the patient\\'s BMI?', text)\n    match = re.search(r'[0-9]+', text)\n    if match:\n        bmi_value = int(text[match.start(): match.end()])\n        return int(bmi_value > 25)\n    \n    return 0\n\nassert is_large_bmi(text) == 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have all pieces required to define a `predict_risk` function. Given a textual description of a patient, the function will extract all features from the text. Subsequently, it will process the feature vector with our predefined `naive_bayes` classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_risk(text):\n    # Predicts a patient's risk of deterioration from a textual description.\n    features = features_from_regex(text) \n    # Please note, we are assuming that a BMI description is always included in the text.\n    features.append(is_large_bmi(text))\n    return(naive_bayes(features))\n\nphrase = 'deteriorate' if predict_risk(text) else 'stabilize'\nprint(f\"The patient is more likely to {phrase}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Based on the textual description of the patient, we know that they’re at risk of deterioration. Thus, our baseline model allows us to prioritize high risk patients, using just recorded clinical notes."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}