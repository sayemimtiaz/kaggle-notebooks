{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In today's world abound with information that comes in all shapes and forms, it's crucial to stay vigilant when it comes to deciding which information we will consume and take to be real. Of various types of misinformation out there, fake news articles have been becoming increasingly easier to encounter in the recent years due to the relative ease in forging them and the profits they can generate (read [this fascinating article](https://www.nytimes.com/2016/11/20/business/media/how-fake-news-spreads.html) on how and why fake news can spread so  quickly and easily).\n\nThankfully, there are simple natural language processing algorithms that can be trained to help discriminate between real and fake news. In this notebook, I've described one of such text classification models and used it on a set of news articles.","metadata":{}},{"cell_type":"markdown","source":"# Table of contents\n1. [Importing data & exploration](#1)\n2. [Data cleaning / Prepping](#2)\n3. [Feature extraction](#3)\n4. [Model training](#4)\n5. [Further exploration](#5)\n6. [Conclusion](#6)","metadata":{}},{"cell_type":"markdown","source":"<div id='1'></div>\n\n# 1. Importing data & exploration\n\nBelow, I start by taking a look at the data we're dealing with.","metadata":{}},{"cell_type":"code","source":"# Basic set-up\nimport os\nimport numpy as np\nimport pandas as pd\n\n# ML toolkits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\nfrom sklearn.utils.extmath import density\nfrom sklearn.pipeline import make_pipeline\n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mlxtend.plotting import plot_confusion_matrix","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-20T16:02:28.999255Z","iopub.execute_input":"2021-05-20T16:02:28.999758Z","iopub.status.idle":"2021-05-20T16:02:29.006882Z","shell.execute_reply.started":"2021-05-20T16:02:28.999715Z","shell.execute_reply":"2021-05-20T16:02:29.005843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_path = '/kaggle/input/fake-and-real-news-dataset'\nfake = pd.read_csv(os.path.join(input_path,'Fake.csv'))\nreal = pd.read_csv(os.path.join(input_path,'True.csv'))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:29.008408Z","iopub.execute_input":"2021-05-20T16:02:29.008807Z","iopub.status.idle":"2021-05-20T16:02:30.509705Z","shell.execute_reply.started":"2021-05-20T16:02:29.008762Z","shell.execute_reply":"2021-05-20T16:02:30.508527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(fake.head())","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:30.51181Z","iopub.execute_input":"2021-05-20T16:02:30.512237Z","iopub.status.idle":"2021-05-20T16:02:30.527995Z","shell.execute_reply.started":"2021-05-20T16:02:30.512193Z","shell.execute_reply":"2021-05-20T16:02:30.526748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(real.head())","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:30.530189Z","iopub.execute_input":"2021-05-20T16:02:30.530676Z","iopub.status.idle":"2021-05-20T16:02:30.548078Z","shell.execute_reply.started":"2021-05-20T16:02:30.530629Z","shell.execute_reply":"2021-05-20T16:02:30.547024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like a simple dataset that contains four columns, namely the article's title, the actual body of text, the subject, and date. There may not even be that much data cleaning to do, given how simple the dataset is!","metadata":{}},{"cell_type":"code","source":"display(fake.info())\nprint('\\n')\ndisplay(real.info())","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:30.549727Z","iopub.execute_input":"2021-05-20T16:02:30.550048Z","iopub.status.idle":"2021-05-20T16:02:30.609593Z","shell.execute_reply.started":"2021-05-20T16:02:30.550017Z","shell.execute_reply":"2021-05-20T16:02:30.608601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(fake.subject.value_counts())\nprint('\\n')\ndisplay(real.subject.value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:30.611077Z","iopub.execute_input":"2021-05-20T16:02:30.611391Z","iopub.status.idle":"2021-05-20T16:02:30.637482Z","shell.execute_reply.started":"2021-05-20T16:02:30.611346Z","shell.execute_reply":"2021-05-20T16:02:30.636511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id='2'></div>\n\n# 2. Data Cleaning / Prepping\nHmmm... it looks like the `subject` column is perhaps too informative -- there are clearly no overlapping \"subjects\" between fake and true news articles. Since I want to build a model that can differentiate fake vs. true news based on its content, I will drop this column.\n\nMoreover, the best way to train the model on both fake and true news data will be to use concatenate two kinds, and shuffle them. I should first add labels to make sure we know which ones are which.","metadata":{}},{"cell_type":"code","source":"fake['label'] = 'fake'\nreal['label'] = 'real'","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:30.639634Z","iopub.execute_input":"2021-05-20T16:02:30.639954Z","iopub.status.idle":"2021-05-20T16:02:30.649992Z","shell.execute_reply.started":"2021-05-20T16:02:30.639923Z","shell.execute_reply":"2021-05-20T16:02:30.64872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([fake, real], axis=0)\ndata = data.sample(frac=1).reset_index(drop=True)\ndata.drop('subject', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:30.653242Z","iopub.execute_input":"2021-05-20T16:02:30.653659Z","iopub.status.idle":"2021-05-20T16:02:30.718632Z","shell.execute_reply.started":"2021-05-20T16:02:30.653616Z","shell.execute_reply":"2021-05-20T16:02:30.717481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I can split up our dataset into training vs. test dataset.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.25)\ndisplay(X_train.head())\nprint('\\n')\ndisplay(y_train.head())\n\nprint(\"\\nThere are {} documents in the training data.\".format(len(X_train)))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:30.720081Z","iopub.execute_input":"2021-05-20T16:02:30.720424Z","iopub.status.idle":"2021-05-20T16:02:30.748798Z","shell.execute_reply.started":"2021-05-20T16:02:30.720391Z","shell.execute_reply":"2021-05-20T16:02:30.747456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div id='3'></div>\n\n# 3. Feature extraction\n\nBefore getting into the actual feature extraction, I want to add some explanations to the method that was used here and why. This may be the most text-heavy section, but I believe it's also crucial to be able to reason the use of your choice of methodology, so please bear with me! But if you so wish, you could also just skip to the \"TL;DR\" below.","metadata":{}},{"cell_type":"markdown","source":"## Some terms to know when dealing with... terms\n\nAmong a few common ways of extracting numerical features from text are tokenizing, counting occurrence, and tf-idf term weighting; I've chosen **tf-idf term weighting** here as the feature to extract from these text.\n<br>\n\n### (1) Term frequency\nThe first portion of this method, \"tf\", refers to the **term frequency**, which simply indicates how often terms can be found in documents. Tf's alone are often insufficient as features, however; since there are many commonly-used words such as \"is\", \"are\", \"the\", etc. that do not carry much information about the document, we do not want to weigh these terms as heavily as other more rare but more informative terms. These uninformative terms are actually referred to as **stop words**, and are often cleaned out during data cleansing/feature extraction steps as they do not hold much value in enhancing the model's ability to predict information.\n\n### (2) Inverse document frequency\nThis is where the \"idf\", short for **inverse document frequency**, comes into play. Idf is used to penalize such terms that occur commonly across different contexts without adding interesting information. The exact equation for computing inverse document frequency is:\n\n$$ idf(t) = log \\frac{1 + n}{1 + df(t)} + 1. $$\n\nHere, $n$ represents the total number of documents, $t$ represents the term in question, $df(t)$ represents the document frequency of that term; i.e., the number of documents within the set of documents that contain that term. As one can imagine, for common terms such as \"is\", \"are\", etc., $idf(t)$ will most likely be 1, since all documents are highly likely to contain them (thus, $df(t) = n$). On the other hand, the less often a term occurs across different documents, the smaller the denominator will be, making the fraction bigger and in turn, $idf(t)$ bigger.\n\n### (3) Tf-idf\nFinally, **tf-idf** is the product of term-frequency and inverse document frequency, mathematically computed as: \n\n$$tf-idf(t,d) = tf(t,d) * idf(t). $$\n\nWhere in addition to notations used above, $d$ represents a document. The more commonly the word appears, the greater the value of tf will be, but if this is the case across different documents, it will be penalized with a small idf. On the other hand, a rarely-occurring word might have a smaller value of tf, but be highlighted by bigger idf values for not occurring often in different documents.","metadata":{}},{"cell_type":"markdown","source":"### TL;DR\n>`Tf-idf` term weighting lets you assign importance to tokens that actually carry some information by balancing overall token frequency with its frequency across documents.\n\nBelow, I first initialize a `TfidfVectorizer` object. It takes as input the set of document strings and outputs the normalized tf-idf vectors; then, using `fit_transform` like any other transformers and predictors in scikit-learn, we can *fit* the vectorizer to data and *tranform* them. It has an option to use the `max_df` to indicate the cut-off document-frequency for stop words, if being used. Here, I will set the cut-off document-frequency to be 0.7, which is the lowest possible value that this parameter can take. The final output of fitting & transforming data will give a sparse matrix with the size of `n_samples` by `n_features`, i.e., `number of documents` by `number of unique words`.","metadata":{}},{"cell_type":"code","source":"my_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\n\n# fit the vectorizer and transform X_train into a tf-idf matrix,\n# then use the same vectorizer to transform X_test\ntfidf_train = my_tfidf.fit_transform(X_train)\ntfidf_test = my_tfidf.transform(X_test)\n\ntfidf_train","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:30.750706Z","iopub.execute_input":"2021-05-20T16:02:30.751188Z","iopub.status.idle":"2021-05-20T16:02:50.896809Z","shell.execute_reply.started":"2021-05-20T16:02:30.75114Z","shell.execute_reply":"2021-05-20T16:02:50.895805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, we see that there are as many rows as the number of documents, and we have extracted over a hundred thousand features, or tokens.\n\nAs a side note, the output is in a `Compressed Sparse Row` format, which refers to how the resulting tf-idf matrix is stored in memory. Sparse matrices are matrices that contain few non-zero elements -- for example, as many non-zeros as number of rows or columns (i.e., the sparse matrix might contain one non-zero element in each row or each column). Because so many elements are zero, it's wasteful to store all the zero elements in memory. The compressed sparse row format presents a solution to this problem by storing the non-zero values and their locations instead. \n","metadata":{}},{"cell_type":"markdown","source":"<div id='4'></div>\n\n# 4. Model training\n\nThe model I've chosen to use is the **Passive-Aggressive (PA) Classifier** (see original paper [here](https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf)). In essence, the PA classifier is an algorithm that only updates its weights (**\"aggressive\"** action) when it encounters examples for which its predictions are wrong, but otherwise remains unchanged (**\"passive\"** action).\n\nThe PA classifier is an *online* algorithm, meaning it uses one example at a time to update its weights and moves on, never seeing the same example again. This is in contrast to a *batch* algorithm, which would use the same set of multiple examples and updates weights in each iteration of training. Because of this, the PA classifier is particularly useful when dealing with a dataset containing a large or rapidly increasing number of examples, like news articles or Tweets! Of course, the data I'm using in this notebook are toy static data, but you can imagine its advantages in real-life applications. Other Kagglers, like [Ayushi Mishra](https://www.kaggle.com/ayushimishra2809/fake-news-prediction) have shown that the PA classifier outperforms several other types of models as well, so I can be confident that it is a good choice.\n\nIf you'd like to learn more about the mathematics behind the PA classifier algorithm, check out [this video](https://www.youtube.com/watch?v=uxGDwyPWNkU) by Dr. Victor Lavrenko that explains the steps in very clear steps!","metadata":{}},{"cell_type":"markdown","source":"Now, let's instantiate the `PassiveAggressiveClassifier` and train it with our features.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import PassiveAggressiveClassifier\n\npa_clf = PassiveAggressiveClassifier(max_iter=50)\npa_clf.fit(tfidf_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:50.898186Z","iopub.execute_input":"2021-05-20T16:02:50.898498Z","iopub.status.idle":"2021-05-20T16:02:51.317783Z","shell.execute_reply.started":"2021-05-20T16:02:50.898467Z","shell.execute_reply":"2021-05-20T16:02:51.316807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we can implement the same algorithm to the test dataset and see how well it performs!","metadata":{}},{"cell_type":"code","source":"y_pred = pa_clf.predict(tfidf_test)\n\nconf_mat = confusion_matrix(y_test, y_pred)\nplot_confusion_matrix(conf_mat,\n                      show_normed=True, colorbar=True,\n                      class_names=['Fake', 'Real'])\n\naccscore = accuracy_score(y_test, y_pred)\nf1score = f1_score(y_test,y_pred,pos_label='real')\n\nprint('The accuracy of prediction is {:.2f}%.\\n'.format(accscore*100))\nprint('The F1 score is {:.3f}.\\n'.format(f1score))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:51.319043Z","iopub.execute_input":"2021-05-20T16:02:51.319351Z","iopub.status.idle":"2021-05-20T16:02:51.707332Z","shell.execute_reply.started":"2021-05-20T16:02:51.319321Z","shell.execute_reply":"2021-05-20T16:02:51.706307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Amazing! The model does a very good job predicting whether the news is real or fake, with 99% accuracy; it has a phenomenal F1 score as well, scoring 0.993. The next few things that I'm curious to find out are the following:\n- What criteria did the model learn to be able to make such accurate predictions?\n- Will this model generalize well to other articles that were not included in this dataset (including the test data), or was there something characteristic about this toy dataset in particular?\n\nI'll explore these questions in the next section.","metadata":{}},{"cell_type":"markdown","source":"<div id='5'></div>\n\n# 5. Further exploration\n\nFirst, to see what the model's criteria ended up being, I want to investigate more into the resulting model's attributes.","metadata":{}},{"cell_type":"code","source":"# Dimensionality and density of features\n\nprint(\"Dimensionality (i.e., number of features): {:d}\".format(pa_clf.coef_.shape[1]))\nprint(\"Density (i.e., fraction of non-zero elements): {:.3f}\".format(density(pa_clf.coef_)))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:51.708771Z","iopub.execute_input":"2021-05-20T16:02:51.709083Z","iopub.status.idle":"2021-05-20T16:02:51.715307Z","shell.execute_reply.started":"2021-05-20T16:02:51.70905Z","shell.execute_reply":"2021-05-20T16:02:51.713898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Out of the features identified, the algorithm found that a little less than half of them were not useful in determining the realness of the article. What does the rest of them look like?","metadata":{}},{"cell_type":"code","source":"# Sort non-zero weights\nweights_nonzero = pa_clf.coef_[pa_clf.coef_!=0]\nfeature_sorter_nonzero = np.argsort(weights_nonzero)\nweights_nonzero_sorted =weights_nonzero[feature_sorter_nonzero]\n\n# Plot\nfig, axs = plt.subplots(1,2, figsize=(9,3))\n\nsns.lineplot(data=weights_nonzero_sorted, ax=axs[0])\naxs[0].set_ylabel('Weight')\naxs[0].set_xlabel('Feature number \\n (Zero-weight omitted)')\n\naxs[1].hist(weights_nonzero_sorted,\n            orientation='horizontal', bins=500,)\naxs[1].set_xlabel('Count')\n\nfig.suptitle('Weight distribution in features with non-zero weights')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:51.717303Z","iopub.execute_input":"2021-05-20T16:02:51.717808Z","iopub.status.idle":"2021-05-20T16:02:56.085073Z","shell.execute_reply.started":"2021-05-20T16:02:51.717758Z","shell.execute_reply":"2021-05-20T16:02:56.083969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So it appears that even among the features that had non-zero weights, most of them had a value close to zero. This is not surprising; there were close to a hundred thousand tokens; it's very unlikely that a large majority of them would have been much informative for our task. This leads to the next question -- which tokens *were* actually useful?","metadata":{}},{"cell_type":"markdown","source":"## Extracting \"Indicator\" Tokens","metadata":{}},{"cell_type":"code","source":"# Sort features by their associated weights\ntokens = my_tfidf.get_feature_names()\ntokens_nonzero = np.array(tokens)[pa_clf.coef_[0]!=0]\ntokens_nonzero_sorted = np.array(tokens_nonzero)[feature_sorter_nonzero]\n\nnum_tokens = 10\nfake_indicator_tokens = tokens_nonzero_sorted[:num_tokens]\nreal_indicator_tokens = np.flip(tokens_nonzero_sorted[-num_tokens:])\n\nfake_indicator = pd.DataFrame({\n    'Token': fake_indicator_tokens,\n    'Weight': weights_nonzero_sorted[:num_tokens]\n})\n\nreal_indicator = pd.DataFrame({\n    'Token': real_indicator_tokens,\n    'Weight': np.flip(weights_nonzero_sorted[-num_tokens:])\n})\n\nprint('The top {} tokens likely to appear in fake news were the following: \\n'.format(num_tokens))\ndisplay(fake_indicator)\n\nprint('\\n\\n...and the top {} tokens likely to appear in real news were the following: \\n'.format(num_tokens))\ndisplay(real_indicator)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:56.086358Z","iopub.execute_input":"2021-05-20T16:02:56.08667Z","iopub.status.idle":"2021-05-20T16:02:57.722283Z","shell.execute_reply.started":"2021-05-20T16:02:56.08664Z","shell.execute_reply":"2021-05-20T16:02:57.721497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_contain_fake = fake.text.loc[[np.any([token in body for token in fake_indicator.Token])\n                                for body in fake.text.str.lower()]]\nreal_contain_real = real.text.loc[[np.any([token in body for token in real_indicator.Token])\n                                for body in real.text.str.lower()]]\n\nprint('Articles that contained any of the matching indicator tokens:\\n')\n\nprint('FAKE: {} out of {} ({:.2f}%)'\n      .format(len(fake_contain_fake), len(fake), len(fake_contain_fake)/len(fake) * 100))\nprint(fake_contain_fake)\n\nprint('\\nREAL: {} out of {} ({:.2f}%)'\n      .format(len(real_contain_real), len(real), len(real_contain_real)/len(real) * 100))\nprint(real_contain_real)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-20T16:02:57.723506Z","iopub.execute_input":"2021-05-20T16:02:57.723944Z","iopub.status.idle":"2021-05-20T16:02:59.779583Z","shell.execute_reply.started":"2021-05-20T16:02:57.72391Z","shell.execute_reply":"2021-05-20T16:02:59.778055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How interesting! Here are several of my speculations on these 20 tokens that were likely to indicate either real or fake news (hence so-called \"indicator tokens\"):\n- Fake news seems to use Getty Images a lot! This might be because many of such fake articles do not have associated photographs taken by real journalists, and they need to outsource.\n- Real news must often state the day of the week an event took place, since four of the top 10 real-news indicator tokens were `tuesday`, `wednesday`, `thursday`, and `friday`.\n- Although the categories were supposedly not just limited to politics, many of the indicator terms seem relevant to it, and specifically U.S. politics (this is likely a sampling bias of sources being American): `gop`, `hillary` (assuming, and likely, it refers to Hillary Clinton), `sen` (short for senator), `republican`, `spokeswoman` are all such terms. \n- Interestingly, `gop` has been used more often in fake news, while `republican` more so in real news -- maybe this is related to the fact that GOP is a nickname for the Republican party.\n\nMoreover, some questions remain to be answered:\n- Why are the top two fake-news indicator tokens `read` and `featured`? Perhaps, the author was trying to convince the reader of the article's supposed realness by stating that it has been \"read\" many times and \"featured\" as an important story?\n- Why are `nov` and `washington` the second- and third-most likely to indicate real news? Perhaps `nov` is short for \"November\", which happens to be the election month and `washington` may have been present in headings whenever a U.S. government/politics news was reported.\n- One crystal clear conclusion is that Reuters is a reputable source of news... in case that was not clear prior to this analysis. This must have been an obvious identifier for the algorithm that the article is real, since Reuters' articles always begin with \"CITY NAME (Reuters)\". A necessary follow-up should be to see whether the algorithm performance holds up when this information is masked.\n\nSince we can only make speculations based on the predictiveness of each token, it would be interesting to find out in which ways these terms were used within text, but most of the investigations rooting from comments laid out above are beyond the scope of this project.","metadata":{}},{"cell_type":"markdown","source":"## Algorithm \"Generalizability\" (?)\n\nOne final question I would like to ask is whether the removal of \"Reuters\" will change the algorithm's performance, since it had a disproportionately strong weights (although this may have been to counterbalance the effects of the `idf`). So let's find out what happens.","metadata":{}},{"cell_type":"code","source":"def FakeNewsDetection(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n    \n    # vectorizer\n    my_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\n    tfidf_train = my_tfidf.fit_transform(X_train)\n    tfidf_test = my_tfidf.transform(X_test)\n    \n    # model\n    my_pac = PassiveAggressiveClassifier(max_iter=50)\n    my_pac.fit(tfidf_train, y_train)\n    y_pred = my_pac.predict(tfidf_test)\n    \n    # metrics\n    conf_mat = confusion_matrix(y_test, y_pred)\n    plot_confusion_matrix(conf_mat,\n                          show_normed=True, colorbar=True,\n                          class_names=['Fake', 'Real'])\n    \n    accscore = accuracy_score(y_test, y_pred)\n    f1score = f1_score(y_test,y_pred,pos_label='real')\n\n    print('The accuracy of prediction is {:.2f}%.\\n'.format(accscore*100))\n    print('The F1 score is {:.3f}.\\n'.format(f1score))\n    \n    # Sort non-zero weights\n    weights_nonzero = my_pac.coef_[my_pac.coef_!=0]\n    feature_sorter_nonzero = np.argsort(weights_nonzero)\n    weights_nonzero_sorted =weights_nonzero[feature_sorter_nonzero]\n    \n    # Sort features by their associated weights\n    tokens = my_tfidf.get_feature_names()\n    tokens_nonzero = np.array(tokens)[my_pac.coef_[0]!=0]\n    tokens_nonzero_sorted = np.array(tokens_nonzero)[feature_sorter_nonzero]\n\n    num_tokens = 10\n    fake_indicator_tokens = tokens_nonzero_sorted[:num_tokens]\n    real_indicator_tokens = np.flip(tokens_nonzero_sorted[-num_tokens:])\n\n    fake_indicator = pd.DataFrame({\n        'Token': fake_indicator_tokens,\n        'Weight': weights_nonzero_sorted[:num_tokens]\n    })\n\n    real_indicator = pd.DataFrame({\n        'Token': real_indicator_tokens,\n        'Weight': np.flip(weights_nonzero_sorted[-num_tokens:])\n    })\n\n    print('The top {} tokens likely to appear in fake news were the following: \\n'.format(num_tokens))\n    display(fake_indicator)\n\n    print('\\n\\n...and the top {} tokens likely to appear in real news were the following: \\n'.format(num_tokens))\n    display(real_indicator)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-20T16:05:41.87288Z","iopub.execute_input":"2021-05-20T16:05:41.873274Z","iopub.status.idle":"2021-05-20T16:05:41.88671Z","shell.execute_reply.started":"2021-05-20T16:05:41.873233Z","shell.execute_reply":"2021-05-20T16:05:41.88563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a copy of the \"real news\" dataset and remove headings f\n\nreal_copy = real.copy()\nfor i,body in real.text.items():\n    if '(reuters)' in body.lower():\n        idx = body.lower().index('(reuters)') + len('(reuters) - ')\n        real_copy.text.iloc[i] = body[idx:]\n        \nreal_copy.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:02:59.795803Z","iopub.execute_input":"2021-05-20T16:02:59.796327Z","iopub.status.idle":"2021-05-20T16:03:07.860684Z","shell.execute_reply.started":"2021-05-20T16:02:59.796278Z","shell.execute_reply":"2021-05-20T16:03:07.859328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create new data, and run the algorithm\ndata2 = pd.concat([fake, real_copy], axis=0)\ndata2 = data2.sample(frac=1).reset_index(drop=True)\ndata2.drop('subject', axis=1)\n\nFakeNewsDetection(data2['text'], data2['label'])","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:05:44.249535Z","iopub.execute_input":"2021-05-20T16:05:44.250127Z","iopub.status.idle":"2021-05-20T16:06:06.633789Z","shell.execute_reply.started":"2021-05-20T16:05:44.250077Z","shell.execute_reply":"2021-05-20T16:06:06.632523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overall, the algorithm does slightly less well but still holds up very nicely! The list of real news indicator tokens have changed minutely, and the weight distribution has shifted in both fake and real news indicator tokens. The most noticeable difference would be, of course, that the token \"reuters\" is not as heavy of an indicator, and now the remaining one day of the week (\"monday\") as well as the word \"reporters\" have replaced \"washington\" and \"saying\" as two of the top 10 real news indicator tokens. My guess is that \"washington\" was largely related to the presence of \"reuters\" as a heading previously.","metadata":{}},{"cell_type":"markdown","source":"<div id='6'></div>\n\n# 6. Conclusion\n\nIn this notebook, I used the `TfidfVectorizer` and `PassiveAggressiveClassifier` algorithms to detect \"fake news\" in the dataset. If you found these interesting, I highly encourage you to do \nfurther research yourself! \n\nThank you for reading until this far -- please upvote if you enjoyed my work and feel free to leave a comment and let me know your thoughts. All kinds of feedback & constructive criticism are appreciated!","metadata":{}},{"cell_type":"markdown","source":"## References\n\n*Below are some links to guides I've referred to and gained inspiration from (some were mentioned throughout the notebook):*\n- [Detecting Fake News with Python and Machine Learning](https://data-flair.training/blogs/advanced-python-project-detecting-fake-news/) by **DataFlair**: a simple set of guidelines on implementation of toolkits used here\n- [Fake News Prediction](https://www.kaggle.com/ayushimishra2809/fake-news-prediction) by [**Ayushi Mishra**](https://www.kaggle.com/ayushimishra2809): a comparison of different model performances using accuracy scores and confusion matrices\n- [Classification of text documents using sparse features](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py) by **Scikit-learn**: similar to above, but benchmarking accuracy scores and computation times\n- [The original paper on the Passive-Aggressive Algorithms](https://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf) (**Crammer et al.**, 2006)\n- [A lecture on Passive-Aggressive Classifiers](https://www.youtube.com/watch?v=uxGDwyPWNkU) by **Dr. Victor Lavrenko**: a great resource to learn more about the mathematics behind the PA classifier algorithm","metadata":{}}]}