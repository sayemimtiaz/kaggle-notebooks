{"cells":[{"metadata":{"_uuid":"0d40d735f8d4c4cf5e7904c4071aea7d64b322ee"},"cell_type":"markdown","source":"# Topic Modelling Demo"},{"metadata":{"_uuid":"e01cff506c2e725b9f21630f979d7cdc1a01ba3e"},"cell_type":"markdown","source":"Exploring Topic Modelling using:\n    - Latent Dirichlet Allocation (LDA) following both Bag of words and TF-IDF approach"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65a611267b74f05cd1abce42b8415be603e47457"},"cell_type":"markdown","source":"For this test case we will be using the US Consumer Finance Complaints, which holds verbatim complaints as well as product information among other fields.\nIn this exercise we will focus on the customer verbatim to distinguish topics following an unsupervised learning approach"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/consumer_complaints.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9a4914488db01e77909f334a3a5174576c1338c"},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53e87137690067c98dd88ed3b3151f9b352be139"},"cell_type":"markdown","source":"We take a subset of our data"},{"metadata":{"trusted":true,"_uuid":"2409d069303f98f88e90f670e065472aad4d412a"},"cell_type":"code","source":"verbatim_product = data[['consumer_complaint_narrative','product']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5311d3c5f88f52e818fb4f3b8be3fb83769f68c"},"cell_type":"code","source":"verbatim_product.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65975c49ecb27764861ae7b16c386279d1ba1720"},"cell_type":"markdown","source":"We remove any complaints which don't have any verbatim to analyse, "},{"metadata":{"trusted":true,"_uuid":"4895c8fe2918edef356c859ef3e764e75eb51169"},"cell_type":"code","source":"filtered_verbatim = verbatim_product.dropna()\nfiltered_verbatim.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85cf7f281de049ff62ac9ee4fc022dbab649e2c1"},"cell_type":"markdown","source":"How many complaints are there with Verbatim?"},{"metadata":{"trusted":true,"_uuid":"ceb2a0f2c49c786f6699486de09a369980ca182b"},"cell_type":"code","source":"len(filtered_verbatim.consumer_complaint_narrative)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02fa9e0c0638cfcad30a72fd47551a6b7d23c3cc"},"cell_type":"markdown","source":"What are the most popular products owned"},{"metadata":{"trusted":true,"_uuid":"f820e62998dc21c1da4b50514c6e7f1159491372"},"cell_type":"code","source":"filtered_verbatim['product'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03d809f14e39c3f471f9443d833b82b05516b15c"},"cell_type":"markdown","source":"Can we see this as a chart?"},{"metadata":{"trusted":true,"_uuid":"0214df98ebaca9e32d28d639f61d131b12f35972"},"cell_type":"code","source":"filtered_verbatim['product'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d052fab3f1db7c714d58f6cf5cf7fb0b6e68bf0e"},"cell_type":"markdown","source":"Let's select a single complaint to start doing some NLP on"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"02ad065ee9da746d5812477b6861ff03a2dc5c1a"},"cell_type":"code","source":"complaint = filtered_verbatim.iloc[1]['consumer_complaint_narrative']\npd.options.display.max_colwidth = 1000\nprint(complaint)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd7e7f0b61b3a68bcbd9c3e8feda0206ecc5a219"},"cell_type":"markdown","source":"Lets import the required libraries for our natural language processing"},{"metadata":{"trusted":true,"_uuid":"d947e92dabf0f90c532c977821e6b72d3c660d39"},"cell_type":"code","source":"import spacy #for our NLP processing\nimport nltk #to use the stopwords library\nimport string # for a list of all punctuation\nfrom nltk.corpus import stopwords # for a list of stopwords","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"714674944b9ed20b684c05981ac3a29453f2c4ed"},"cell_type":"markdown","source":"Now we can load and use spacy to analyse our complaint"},{"metadata":{"trusted":true,"_uuid":"2a36e641ad63ca592666c2dabbc809f9647ff051"},"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')\ntext = nlp(complaint)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"784adbb769d65a8575e5ac2a41a6c3cb7d6504ba"},"cell_type":"code","source":"text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"947f56f4420a444ec13d13ef3893122e61a03d99"},"cell_type":"markdown","source":"Let's start by tokenising our complaint -- Splitting it out into words"},{"metadata":{"trusted":true,"_uuid":"98dc51e73c9ebbd074810f584cd70b017736688c"},"cell_type":"code","source":"tokens = [tok for tok in text]\ntokens.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8cb9d800b41634045f1cacfb5869155fe41c749"},"cell_type":"markdown","source":"For our bag of words to have less overlap - lets lemmatize our words"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"247ca331bb06009a9d71a5df66f7c255c933441b"},"cell_type":"code","source":"tokens = [tok.lemma_ for tok in text]\ntokens","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59e106af4f4de49e3498a647f690ece4ef837d78"},"cell_type":"markdown","source":"To ensure our words match up and there are no sneaky spaces let's strip any whitespace around the tokens, and lowercase our text to ensure words like 'Credit' and 'credit' are matched"},{"metadata":{"trusted":true,"_uuid":"123f50b4903f5038a0db40c486f14c776cf84241"},"cell_type":"code","source":"tokens = [tok.lemma_.lower().strip() for tok in text]\ntokens","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cbdfb095dfb1af02f793644dd6ac876bac0c73e"},"cell_type":"markdown","source":"Now let's get rid of all the -PRON- lemmas as they will add no value to our analysis"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"263c2e0d333e3c2f4934175356499bc35c385efa"},"cell_type":"code","source":"tokens = [tok.lemma_.lower().strip() for tok in text if tok.lemma_ != '-PRON-']\ntokens","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7da56c9c3171d495ad31f617acecb483759b4688"},"cell_type":"markdown","source":"Lets now use another library - NLTK to get a list of stopwords (think: I, me, you ,they etc.) Words that won't really add much value to our analysis, more like fillers between the important words"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"eaf9a37da1b19def1438bfe907bd2a002cd07d67"},"cell_type":"code","source":"stop_words = stopwords.words('english')\npunctuations = string.punctuation\nstop_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdf12fcb3aeb250e84832c6b2fdea5321e27dfba"},"cell_type":"markdown","source":"We will now be looking at the tokens in our tokens list (no longer using the spacy/ nlp(text) object) - and remove any puntuation and stop_words"},{"metadata":{"trusted":true,"_uuid":"952dad29cb9df9775508221d5ea557326d67e75c"},"cell_type":"code","source":"tokens = [tok for tok in tokens if tok not in stop_words and tok not in punctuations]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc784b373877abe7a822c8131f477acb7a4149e4"},"cell_type":"markdown","source":"And there we have! We've tokenized, lemmatied, lowercased, stripped of white spaces + removed stopwords and punctuations"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"7331b0b4068e1f6459b836d1f4b909f761b5806e"},"cell_type":"code","source":"tokens","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b70ded3153e1ff3b17063ba8c317ba2b52e56778"},"cell_type":"markdown","source":"Let's put it all together into a function so we can apply these steps to every complaint"},{"metadata":{"trusted":true,"_uuid":"1413127f248c4e724ca83bdae88c6e5887c71b87"},"cell_type":"code","source":"def cleanup_text(complaint):\n    doc = nlp(complaint, disable=['parser', 'ner'])\n    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n    tokens = [tok for tok in tokens if tok not in stop_words and tok not in punctuations]\n    return tokens\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b3f64943512155105e4413880df65f510c1abf2"},"cell_type":"markdown","source":"We'll look to see if this looks correct on the first 100 complaints - pre-check  \nWe'll also declare **doc_sample** as the complaints verbatim"},{"metadata":{"trusted":true,"_uuid":"7b0e87046b7f067e51313d96a1c066236b56c46f","scrolled":true},"cell_type":"code","source":"limit = 100\ndoc_sample = filtered_verbatim.consumer_complaint_narrative\nprint('tokenized and lemmatized document: ')\n\nfor idx, complaint in enumerate(doc_sample):\n    print(cleanup_text(complaint))\n    if idx == limit:\n        break\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"964a0d254543fb49c87fbe1eb44adf5ab7008041"},"cell_type":"markdown","source":"In the interest of time (as we don't have much time in this interactive demo), we'll run the rest of the demo on 10k complaints"},{"metadata":{"trusted":true,"_uuid":"1e58b694128eabbb93761b83d659aa87b049fb55"},"cell_type":"code","source":"doc_sample = doc_sample[0:10000]\n#doc_sample = doc_sample[:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8c338203b502f8a2117915f5782b0140db22a88"},"cell_type":"markdown","source":"We can now apply our function to doc_sample and process our 10k complaints using the .map function"},{"metadata":{"trusted":true,"_uuid":"9466bbeb3b06ad6e406ba692adf04a0d5697d423"},"cell_type":"code","source":"processed_docs = doc_sample.map(cleanup_text)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5c233b61f73679c92fbe39aac769e3d52974a19"},"cell_type":"markdown","source":"# Bag of Words"},{"metadata":{"_uuid":"6bc5473c08830267bf81893f47b755bad0de0cc2"},"cell_type":"markdown","source":"The dictionary encapsulates the mapping between normalized words and their integer ids"},{"metadata":{"trusted":true,"_uuid":"a48cd4a9107a296424477a7ce97c74ee2c0fdbf7"},"cell_type":"code","source":"import gensim\ndictionary = gensim.corpora.Dictionary(processed_docs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff1d80814bbb2ce207946372dc6cd885913cefc4"},"cell_type":"markdown","source":"The dictionary is then filtered to remove extreme values using the following parameters:\n- *no_below* parameter is an absolute number - Words appearing less than 10 times in the entire corpus are removed from the analysis\n- *no_above* parameter is a fraction - Words appearing more than 50% of the time are removed from the analysis"},{"metadata":{"trusted":true,"_uuid":"0ef6cc05550450ad746cebc373cf0091a2dd2ca0"},"cell_type":"code","source":"dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=100000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb8e54fffcdc29dfd17c91b6e9a8356abda01d09"},"cell_type":"markdown","source":"The dictionary is then converted to a bag of words format"},{"metadata":{"trusted":true,"_uuid":"5cb5f395cca83aa691bb92ee5d5bda53833e3b9d"},"cell_type":"code","source":"bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f38133cb6afb5cb7057a98a8dd848019b89e3cc5"},"cell_type":"markdown","source":"We display an example of the bag-of-words format on a single complaint to ensure it worked (here on complaint 4310)"},{"metadata":{"trusted":true,"_uuid":"ab44d27722776ed7c46bc846cac3adec4677b23b"},"cell_type":"code","source":"bow_doc_4310 = bow_corpus[4310]\n\nfor i in range(len(bow_doc_4310)):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n                                                     dictionary[bow_doc_4310[i][0]], \n                                                     bow_doc_4310[i][1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3eeb36f7435876276b5dcab3937bca112cd8b897"},"cell_type":"markdown","source":"# LDA"},{"metadata":{"_uuid":"c92cfb8d9fbff753d3e483df29b18cce3cf3606e"},"cell_type":"markdown","source":"Latent Dirichlet allocation (LDA), is an  **unsupervised** algorithm: only the words in the documents are modeled. \n- The goal is to _infer topics that maximize the likelihood (or the posterior probability) of the collection_."},{"metadata":{"_uuid":"3eae79bce08ef49679be3a86e355d0d59b5d6457"},"cell_type":"markdown","source":"## Running LDA on Bag of Words"},{"metadata":{"_uuid":"8e2472d3efbee3790885234b379337b42e7d7821"},"cell_type":"markdown","source":"The LDA algorithm has a number of parameters than can be used to calibrate the output:\n- num_topics: In this example we have prescribed a number 10, in a previous run without a prescribed number, the LDA produced 99 clusters which is not very informative for our usecase\n- id2word: The previously defined dictionary mapping from word IDs to Words\n- Workers: for parralelisation\n- chunksize: number of documents to use in each training chunk\n- passes: no. passes through the corpus during training\n- alpha: Can be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topicsâ€™ probability.\n- decay: A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten when each new document is examined.\n- iterations: Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n- gamma_threshold: Minimum change in the value of the gamma parameters to continue iterating."},{"metadata":{"trusted":true,"_uuid":"eb2444aa56f3dd1ce610ad213c1abc3ac1569ea1"},"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0749947f76c7bf365d2e81588ca2fbaa4aede113"},"cell_type":"markdown","source":"Lets display our topics"},{"metadata":{"trusted":true,"_uuid":"d5f12a54b16efd8da9843a585418c5cf4aececa0"},"cell_type":"code","source":"for idx, topic in lda_model.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d166c0328f5221bc695f9bf907eece3e7ee0190"},"cell_type":"markdown","source":"We can also use pyLDA vis to inspect our outputs in a more interactive way"},{"metadata":{"trusted":true,"_uuid":"e899c4473e80ee2565448a1bbc18b9d262dbb7b6"},"cell_type":"code","source":"import pyLDAvis\nimport pyLDAvis.gensim as gensimvis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f1db920f4d526c99404e976b8f544642c7ab713"},"cell_type":"code","source":"vis_data = gensimvis.prepare(lda_model, bow_corpus, dictionary)\npyLDAvis.display(vis_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fadb351f4d313b3f247d36329982489cd315df11"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3577bec6beef983ccce83eaad7ed6a9f484426b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}