{"cells":[{"metadata":{"_uuid":"c8ed9988e11c17bf244d4a3ed2fd76702f1726f6"},"cell_type":"markdown","source":"> **Objectives**<br>\n> \n> 1. Build a model that can predict the health of a bee hive by analyzing images of bees.\n> 2. Deploy the model as a prototype web based tool that bee keepers and researchers can use. They should be able to submit a picture of a bee and  instantly get an assessment of the health of the hive where the bee lives.\n\n\n****\n\nIn this quick kernel we will use transfer learning to infer the health of bee hives by analyzing images of bees. We will use a pre-trained MobileNet model together with image augmentation. MobileNet was trained on the ImageNet dataset which includes pictures of bees. This means that it's already able to recognize bees. Here we will see how well it can be trained to evaluate bee health.\n\nWe won't be doing extensive data exploration. Other kernels have already expertly covered this area. Please refer to the excellent kernels by @gpreda and  @dmitrypukhov.<br>\n\nThis kernel will focus on:\n\n1. Creating a directory structure\n2. Creating generators\n3. Data augmentation\n4. Model building and training\n5. Assessing the quality of the model via a confusion matrix and F1 score.\n\n**Results**\n\nThe model's validation accuracy and F1 score are greater than 0.9\n\n****\n\nAll the html, css, and javascript code used to build the web app is available on Github.\n\nWeb app:<br>\nhttp://bee.test.woza.work/<br>\nGithub:<br>\nhttps://github.com/vbookshelf/Bee-Hive-Health-Analyzer\n"},{"metadata":{"trusted":true,"_uuid":"371731306c3e504b191979706e826c247def88dc","_kg_hide-input":false},"cell_type":"code","source":"from numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\n#import keras\n\nimport tensorflow\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.metrics import categorical_accuracy\n\nimport os\nimport cv2\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79d5a2536c1728e28045684f6e6f4a4065218c9e"},"cell_type":"markdown","source":"\n**A lesson learned...**\n\nIn this kernel we are using tf.keras and not native Keras. tf.keras is Keras that lives within Tensorflow.\n\nWhen using native Keras I found that the accuracy as calculated manually from the confusion matrix does not match the accuracy obtained during training and evaluation. The problem was solved when I switched to tf.keras. I think there's a problem with predict_generator() in native Keras or maybe I'm using it wrong.\n\nAnother complication is that a tf.keras model cannot be converted into a Tensorflowjs model for use in the app. To solve this problem I had to create another kernel using native Keras and convert that model to Tensorflowjs - a complicated story, I know :-)\n"},{"metadata":{"trusted":true,"_uuid":"b015ceac61f52106049ab8dee7e10433c1633de8"},"cell_type":"code","source":"# To reduce the class imbalance in the train and validation set, the number of \n# rows in class 'healthy' will be reduced to this number:\n\nHEALTHY_SAMPLE_SIZE = 579\n\n# The approx. total number of images we want in each class after doing image augmentation.\n# We won't be doing image augmentation on the fly.\nNUM_IMAGES_WANTED = 3000 # incl. class 'healthy'\n\n# MobileNet needs input images with shape 224x224x3\nIMAGE_SIZE = 224\nIMAGE_CHANNELS = 3\n\nIMAGE_PATH = \"../input/bee_imgs/bee_imgs/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dac48f5259e7cb5d31283a662f8ab8e7c1641794"},"cell_type":"code","source":"# What files are available?\n\nos.listdir(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2d043abbee34d9d0c02e92231d4bd27e79a58b7"},"cell_type":"markdown","source":"### LABELS\nWhat are the labels and what is the class distribution?"},{"metadata":{"trusted":true,"_uuid":"9f6d1f42790155d5ed901bc3ec4f64c4ee51f87e"},"cell_type":"code","source":"df = pd.read_csv('../input/bee_data.csv')\ndf['health'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"086162161ba405b800863e7d545b5917e5205984"},"cell_type":"markdown","source":"### Create the directory structure\n\nIn these folders we will store the images that will later be fed to the Keras generators. "},{"metadata":{"trusted":true,"_uuid":"d24ef21f9f2359b8bf6b3e7a0b8ab5a43daaf566","_kg_hide-input":true},"cell_type":"code","source":"# Key:\n# healthy = healthy\n# fvar = few varrao, hive beetles\n# var = Varroa, Small Hive Beetles\n# ant = ant problems\n# robbed = hive being robbed\n# queen = missing queen\n\n# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# train_dir\n    # 1_healthy\n    # 2_fvar\n    # 3_var\n    # 4_ant\n    # 5_robbed\n    # 6_queen\n \n# val_dir\n    # 1_healthy\n    # 2_fvar\n    # 3_var\n    # 4_ant\n    # 5_robbed\n    # 6_queen\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\nhealthy = os.path.join(train_dir, '1_healthy')\nos.mkdir(healthy)\nfvar = os.path.join(train_dir, '2_fvar')\nos.mkdir(fvar)\nvar = os.path.join(train_dir, '3_var')\nos.mkdir(var)\nant = os.path.join(train_dir, '4_ant')\nos.mkdir(ant)\nrobbed = os.path.join(train_dir, '5_robbed')\nos.mkdir(robbed)\nqueen = os.path.join(train_dir, '6_queen')\nos.mkdir(queen)\n\n\n# create new folders inside val_dir\nhealthy = os.path.join(val_dir, '1_healthy')\nos.mkdir(healthy)\nfvar = os.path.join(val_dir, '2_fvar')\nos.mkdir(fvar)\nvar = os.path.join(val_dir, '3_var')\nos.mkdir(var)\nant = os.path.join(val_dir, '4_ant')\nos.mkdir(ant)\nrobbed = os.path.join(val_dir, '5_robbed')\nos.mkdir(robbed)\nqueen = os.path.join(val_dir, '6_queen')\nos.mkdir(queen)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ae8d37fdee293aaffa71a79019dd7277f8288fc"},"cell_type":"markdown","source":"### Create Train and Val Sets"},{"metadata":{"trusted":true,"_uuid":"268503398ef61904e05a2c0b0667d589f08a19a8"},"cell_type":"code","source":"df_data = pd.read_csv('../input/bee_data.csv')\n\ndf_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e3a3a5fcabd8f06ed7099c90bfb733496b6de6b"},"cell_type":"markdown","source":"### Downsample the 'healthy' class to reduce the class imbalance"},{"metadata":{"trusted":true,"_uuid":"1a332efcbc2fc282a76094552858b8c79f02da61"},"cell_type":"code","source":"# take a random sample of class 'healthy'\ndf = df_data[df_data['health'] == 'healthy'].sample(HEALTHY_SAMPLE_SIZE, random_state=101)\n\n# remove class 'healthy' from the dataframe\ndf_data = df_data[df_data['health'] != 'healthy']\n\n# concat df and df_data\ndf_data = pd.concat([df_data, df], axis=0).reset_index(drop=True)\n\n# shuffle the new dataframe\ndf_data = shuffle(df_data)\n\n# check the new class distribution\ndf_data['health'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db267968b6e95ad401a28313fbf3468a53c08f89"},"cell_type":"markdown","source":"### Creat train and val sets"},{"metadata":{"trusted":true,"_uuid":"df735da903622942a61e08391b3c86d6dcdb266f"},"cell_type":"code","source":"\ny = df_data['health']\n\ndf_train, df_val = train_test_split(df_data, test_size=0.1, random_state=101, stratify=y)\n\nprint(df_train.shape)\nprint(df_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b976a9018b1bd2dc0522c68339c5861534a1571"},"cell_type":"code","source":"df_train['health'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1581d5a3e86f9673ae175102112017e30229bc37"},"cell_type":"code","source":"df_val['health'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8812ad87c4fa18d2d82497df42c3895c7f10bc39"},"cell_type":"markdown","source":"### Transfer the Images into the Folders\nWe now transfer the train and val images into the directory structure that we created. Keras needs this directory structure in order to load images from folders into the model during training - and to infer the class of the images."},{"metadata":{"trusted":true,"_uuid":"4acee2b7879762e50b52df118a9b691515fe7ac0"},"cell_type":"code","source":"# Set the 'file' column as the index in df_data\ndf_data.set_index('file', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eca02fbf066c8124d0cb465295bbd2593f5f045a"},"cell_type":"code","source":"\n# Get a list of train and val images\ntrain_list = list(df_train['file'])\nval_list = list(df_val['file'])\n\n# Key:\n# healthy = healthy\n# fvar = few varrao, hive beetles\n# var = Varroa, Small Hive Beetles\n# ant = ant problems\n# robbed = hive being robbed\n# queen = missing queen\n\n\n\n# Transfer the train images.\n# Note that we re-name the files during the transfer using a number sequence. This is just a\n# safety measure help the generators create a repeatable sequence of images. \n# Otherwise I think they could mix them up and give us confusing results.\n\nlength = len(train_list)\n\nfor i in range(0,length):\n    \n    fname = train_list[i]\n    image = fname\n    description = df_data.loc[image,'health']\n    \n    # map the class descriptions to folder names\n    if description == 'healthy':\n        label = '1_healthy'\n    if description == 'few varrao, hive beetles':\n        label = '2_fvar'\n    if description == 'Varroa, Small Hive Beetles':\n        label = '3_var'\n    if description == 'ant problems':\n        label = '4_ant'\n    if description == 'hive being robbed':\n        label = '5_robbed'\n    if description == 'missing queen':\n        label = '6_queen'\n        \n        \n    # source path to image\n    src = os.path.join('../input/bee_imgs/bee_imgs', fname)\n    # chage the file name\n    new_name = str(i) + '_' + 'train'+ '.png'\n    # destination path to image\n    dst = os.path.join(train_dir, label, new_name)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n\n\n# Transfer the val images\n\nlength = len(val_list)\n\nfor i in range(0,length):\n    \n    fname = val_list[i]\n    image = fname\n    description = df_data.loc[image,'health']\n    \n     # map the class descriptions to folder names\n    if description == 'healthy':\n        label = '1_healthy'\n    if description == 'few varrao, hive beetles':\n        label = '2_fvar'\n    if description == 'Varroa, Small Hive Beetles':\n        label = '3_var'\n    if description == 'ant problems':\n        label = '4_ant'\n    if description == 'hive being robbed':\n        label = '5_robbed'\n    if description == 'missing queen':\n        label = '6_queen'\n    \n    # source path to image\n    src = os.path.join('../input/bee_imgs/bee_imgs', fname)\n    # chage the file name\n    new_name = str(i) + '_' + 'val' + '.png'\n    # destination path to image\n    dst = os.path.join(val_dir, label, new_name)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a4847c4cc799c23e57bf2531d92117cb95e1b07"},"cell_type":"code","source":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir/train_dir/1_healthy')))\nprint(len(os.listdir('base_dir/train_dir/2_fvar')))\nprint(len(os.listdir('base_dir/train_dir/3_var')))\nprint(len(os.listdir('base_dir/train_dir/4_ant')))\nprint(len(os.listdir('base_dir/train_dir/5_robbed')))\nprint(len(os.listdir('base_dir/train_dir/6_queen')))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd05c08cbfa00418dc333f5b67d1ff6e98aa973e"},"cell_type":"code","source":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir/val_dir/1_healthy')))\nprint(len(os.listdir('base_dir/val_dir/2_fvar')))\nprint(len(os.listdir('base_dir/val_dir/3_var')))\nprint(len(os.listdir('base_dir/val_dir/4_ant')))\nprint(len(os.listdir('base_dir/val_dir/5_robbed')))\nprint(len(os.listdir('base_dir/val_dir/6_queen')))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cebcb5242ff542efb03be5086bf3796bea70c591"},"cell_type":"markdown","source":"### Copy the train images  into aug_dir\nWe will not be doing image augmentation on the fly. We will augment the images and then add them to the train folders before training begins. This way the augmented images will be combined with the original images. The training process will also run faster."},{"metadata":{"trusted":true,"_uuid":"8fe970d74e9d5a284420af4ad37d8aae89dc1c15"},"cell_type":"code","source":"# note that we are not augmenting class 'healthy'\nclass_list = ['1_healthy', '2_fvar','3_var','4_ant','5_robbed','6_queen']\n\nfor item in class_list:\n    \n    # We are creating temporary directories here because we delete these directories later.\n    \n    # create a base dir\n    aug_dir = 'aug_dir'\n    os.mkdir(aug_dir)\n    # create a dir within the base dir to store images of the same class\n    img_dir = os.path.join(aug_dir, 'img_dir')\n    os.mkdir(img_dir)\n\n    # Choose a class\n    img_class = item\n\n    # list all images in that directory\n    img_list = os.listdir('base_dir/train_dir/' + img_class)\n\n    # Copy images from the class train dir to the img_dir e.g. class 'healthy'\n    for fname in img_list:\n            # source path to image\n            src = os.path.join('base_dir/train_dir/' + img_class, fname)\n            # destination path to image\n            dst = os.path.join(img_dir, fname)\n            # copy the image from the source to the destination\n            shutil.copyfile(src, dst)\n\n\n    # point to a dir containing the images and NOT to the images themselves\n    path = aug_dir\n    save_path = 'base_dir/train_dir/' + img_class\n\n    # Create a data generator to generate augmented images for each class.\n    datagen = ImageDataGenerator(\n        #rotation_range=180,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.01,\n        #horizontal_flip=True,\n        #vertical_flip=True,\n        brightness_range=(0.9,1.1),\n        fill_mode='nearest')\n\n    batch_size = 9\n\n    aug_datagen = datagen.flow_from_directory(path,\n                                        save_to_dir=save_path, # this is where the images are saved\n                                        save_format='jpg',\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=batch_size)\n\n\n\n    # Generate the augmented images and add them to the training folders\n    \n    # NUM_IMAGES_WANTED = total number of images we want to have in each class\n    # We will use image augmentation to create the additional images.\n    \n    num_files = len(os.listdir(img_dir))\n    \n    # Just a calculation to get approx. the same amount of images for each class.\n    num_batches = int(np.ceil((NUM_IMAGES_WANTED-num_files)/batch_size))\n\n    # Run the generator and create augmented images.\n    # Note that these images are automatically stored in a folder. The path\n    # to the save folder is specified as a parameter in the generator above.\n    for i in range(0,num_batches):\n\n        imgs, labels = next(aug_datagen)\n        \n    # delete temporary directory with the raw image files\n    shutil.rmtree('aug_dir')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9bbc56bd25441150d2430dca2b07d8ebae57d95"},"cell_type":"code","source":"# Check how many train images we now have in each folder.\n# This is the original images plus the augmented images.\n\nprint(len(os.listdir('base_dir/train_dir/1_healthy')))\nprint(len(os.listdir('base_dir/train_dir/2_fvar')))\nprint(len(os.listdir('base_dir/train_dir/3_var')))\nprint(len(os.listdir('base_dir/train_dir/4_ant')))\nprint(len(os.listdir('base_dir/train_dir/5_robbed')))\nprint(len(os.listdir('base_dir/train_dir/6_queen')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21de03bdc63ecf78cc061d364d14d3216a544b43"},"cell_type":"code","source":"# Check how many val images we have in each folder.\n\nprint(len(os.listdir('base_dir/val_dir/1_healthy')))\nprint(len(os.listdir('base_dir/val_dir/2_fvar')))\nprint(len(os.listdir('base_dir/val_dir/3_var')))\nprint(len(os.listdir('base_dir/val_dir/4_ant')))\nprint(len(os.listdir('base_dir/val_dir/5_robbed')))\nprint(len(os.listdir('base_dir/val_dir/6_queen')))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"767cb7d35e301369f020cdbb705da1620ba8e594"},"cell_type":"markdown","source":"### Visualize a batch of augmented images"},{"metadata":{"trusted":true,"_uuid":"5f0e13a8455af926fe449e1b3ea818b704724202"},"cell_type":"code","source":"# plots images with labels within jupyter notebook\n# source: https://github.com/smileservices/keras_utils/blob/master/utils.py\n\ndef plots(ims, figsize=(12,6), rows=2, interp=False, titles=None): # 12,6\n    if type(ims[0]) is np.ndarray:\n        ims = np.array(ims).astype(np.uint8)\n        if (ims.shape[-1] != 3):\n            ims = ims.transpose((0,2,3,1))\n    f = plt.figure(figsize=figsize)\n    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis('Off')\n        if titles is not None:\n            sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i], interpolation=None if interp else 'none')\n        \nplots(imgs, titles=None) # titles=labels will display the image labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3e2126a39c06568a1f95da2ab42353447d1be20"},"cell_type":"code","source":"# End of Data Preparation\n### ===================================================================================== ###\n# Start of Model Building","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32dad10b7c104d2baa972da8cbadc7d6038af05c"},"cell_type":"markdown","source":"### Set Up the Generators"},{"metadata":{"trusted":true,"_uuid":"aa1041d69b0e8313324b91e3e9475799e1ad61c2"},"cell_type":"code","source":"train_path = 'base_dir/train_dir'\nvalid_path = 'base_dir/val_dir'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\nimage_size = 224\n\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edcafa3869045130020d364c640959622459977a"},"cell_type":"markdown","source":"#### A Note on Image Pre-propcessing to Suit MobileNet\n\nWe will be applying the same pre-processing to the bee images as was applied to the original ImageNet  images that were used to train MobileNet. We will add this pre-processing as a preprocessing_function in the generators below."},{"metadata":{"trusted":true,"_uuid":"d0e5aede7139196b0d4e1344b278e7621f005550"},"cell_type":"code","source":"\ndatagen = ImageDataGenerator(preprocessing_function=\n                             tensorflow.keras.applications.mobilenet.preprocess_input)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(image_size,image_size),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(image_size,image_size),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical')\n\n# Note: shuffle=False causes the test dataset to not be shuffled.\n# Here we will be using the val set as the test dataset because we need to run predict\n# in order to generate the confusion matrix.\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(image_size,image_size),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ee4ee41f1b16083bd9fc20ee9dec40acccc97dd"},"cell_type":"markdown","source":"### Modify MobileNet Model"},{"metadata":{"trusted":true,"_uuid":"ad582cb8ea0ca2d563fc367aa89b7edfafc1a57f"},"cell_type":"code","source":"# Create a copy of a mobilenet model.\n# Please ensure your kaggle kernel is set to 'Internet Connected'.\n\nmobile = tensorflow.keras.applications.mobilenet.MobileNet()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"960449ec7ecdda92ba733ad23b00b7be605f3d4b","_kg_hide-output":true},"cell_type":"code","source":"mobile.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b7922bdf625675834d9b63ec0e85351bd9f3c0f"},"cell_type":"code","source":"type(mobile.layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f832e5865c65a013a06dbf5d500c0381020c56d5"},"cell_type":"code","source":"# How many layers does MobileNet have?\nlen(mobile.layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dd9dcf26d85a57a113e6b158cf8fceeca7f99de"},"cell_type":"code","source":"# CREATE THE MODEL ARCHITECTURE\n\n# Exclude the last 5 layers of the above model.\n# This will include all layers up to and including global_average_pooling2d_1\nx = mobile.layers[-6].output\n\n# Create a new dense layer for predictions\n# 6 corresponds to the number of classes\n#x = Dropout(0.25)(x)\npredictions = Dense(6, activation='softmax')(x)\n\n# inputs=mobile.input selects the input layer, outputs=predictions refers to the\n# dense layer we created above.\n\nmodel = Model(inputs=mobile.input, outputs=predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b38734b72afc4289ab187a9e683cbda6bf3269bc","_kg_hide-output":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9d74e44630c3d07a596460c8fbfda3ae7cae1e9"},"cell_type":"code","source":"# We need to choose how many layers we actually want to be trained.\n\n# Here we are freezing the weights of all layers except the\n# last 23 layers in the new model.\n# The last 23 layers of the model will be trained.\n\nfor layer in model.layers[:-23]:\n    layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13cf63a53e5195cb8a9725d2506c71108bc478b9"},"cell_type":"markdown","source":"### Train the Model"},{"metadata":{"trusted":true,"_uuid":"2013ff1abae70fed845af94e7ab3d95cefad0d61"},"cell_type":"code","source":"model.compile(Adam(lr=0.001), loss='categorical_crossentropy', \n              metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a5e3bc3cf44f1d4326c34ad880a302ba082e9d5","scrolled":false,"_kg_hide-output":true},"cell_type":"code","source":"\nfilepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                            validation_data=val_gen,\n                            validation_steps=val_steps,\n                            epochs=30, verbose=1,\n                           callbacks=callbacks_list)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3e43e3f2943db4be9d75831fe23661ae9deb44b"},"cell_type":"markdown","source":"### Evaluate the model using the val set\n"},{"metadata":{"trusted":true,"_uuid":"710ee26097924153647ac432c8ade29383fe42f1"},"cell_type":"code","source":"# Get the metric names so that we can see what the output from evaulate_generator will be.\nmodel.metrics_names","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cb4ce7768cb0eb37f128178e173bface9bb267c"},"cell_type":"markdown","source":"We see that the output metrics are loss and accuracy. Therefore, we now know that model.evaluate_generator() outputs val loss and val accuracy. "},{"metadata":{"trusted":true,"_uuid":"897f066da922d81fefa165a6b911a741c52ef7f5"},"cell_type":"code","source":"# Here the best epoch will be used.\n\nmodel.load_weights('model.h5')\n\nval_loss, val_acc = \\\nmodel.evaluate_generator(test_gen, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3fffba5e0aa9088cda1865c7b8d75d72c20d0f6"},"cell_type":"markdown","source":"### Plot the Training Curves"},{"metadata":{"trusted":true,"_uuid":"0cbd11ef4286a751ef2918361af035d356f341ae"},"cell_type":"code","source":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4204e4056c8d12c1fee72b97912879cad4ee483f"},"cell_type":"markdown","source":"### Create a Confusion Matrix\n\nThe confusion matrix and F1 score will tell us how well our model is able to perform on each individual class. "},{"metadata":{"trusted":true,"_uuid":"701dafc5874aa60a054a74c04170cb7e8d750e94"},"cell_type":"code","source":"# make a prediction\nfrom tensorflow.keras.models import load_model\n\npredictions = model.predict_generator(test_gen, steps=len(df_val), verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcce17ac0488ff90d29b11592c9226ed1bb210fb"},"cell_type":"code","source":"# Get the index of the class with the highest probability score\ny_pred = np.argmax(predictions, axis=1)\n\n# Get the labels of the test images.\ny_true = test_gen.classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cfd9bdbbd27e27d9c5de7c6593527686445ea89","_kg_hide-input":true},"cell_type":"code","source":"# Source: Scikit Learn website\n# http://scikit-learn.org/stable/auto_examples/\n# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n# selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.figure(figsize=(100,100))\n    plt.tight_layout()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"940b71bb2b37d847ba81dd67ca50c7fd5785fd35"},"cell_type":"code","source":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97c6b493c368ff6565782c1bb15827f5d349ef79"},"cell_type":"code","source":"test_gen.class_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ddbd33a93468075c64ba49188a6d272a5c7828f"},"cell_type":"code","source":"# Key:\n# healthy = healthy\n# fvar = few varrao, hive beetles\n# var = Varroa, Small Hive Beetles\n# ant = ant problems\n# robbed = hive being robbed\n# queen = missing queen\n\n# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = ['1_healthy', '2_fvar', '3_var', '4_ant', '5_robbed','6_queen']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a478ccbc3ba97e21308448b2963b1b9b907e6295"},"cell_type":"code","source":"len(df_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fd36d09f28ec1db282736664b59858411c81bd6"},"cell_type":"markdown","source":"### Generate the Classification Report"},{"metadata":{"trusted":true,"_uuid":"324404b4febf12c31b8f0dd959e177ceae002e66"},"cell_type":"code","source":"# Get the filenames, labels and associated predictions\n\n# This outputs the sequence in which the generator processed the test images\ntest_filenames = test_gen.filenames\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14a8978010ccb767006e63d6a5ca4e9026a292bb"},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Generate a classification report\n\nnames = ['healthy', 'few varrao, hive beetles', 'Varroa, Small Hive Beetles', \n               'ant problems', 'hive being robbed','missing queen']\n\nreport = classification_report(y_true, y_pred, target_names=names)\n\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b462feaa2de3c325b9d9f484facfdb71e6828a78"},"cell_type":"markdown","source":"**Recall ** = Given a class, will the classifier be able to detect it?<br>\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?<br>\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\n"},{"metadata":{"_uuid":"01795e9b2a4d45c450e71a549aab4c4a13928b44"},"cell_type":"markdown","source":"The accuracy is above 90%. The F1 score is also above 90%. This tells us that our model is really good at detecting all classes, even the 'missing queen' class where only a 29  images are available.\n\n*As a side note - I'm wondering if the model is performing so well because it's looking at the bees or if it is actually looking at the background colour of the images. Maybe a more experienced data scientist can answer this question.*"},{"metadata":{"trusted":true,"_uuid":"4c46f3f1d257241f96b4aac7eb96831ff8bbea33"},"cell_type":"code","source":"# End of Model Building\n### ===================================================================================== ###\n# Convert the Model from Keras to Tensorflow.js","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed22201310ba3ca93178fbfb52b9ee07d9bdde39"},"cell_type":"markdown","source":"### What is Tensorflow.js?"},{"metadata":{"_uuid":"831ef2acf6cd7976c59498a994f4b0b26dcaea63"},"cell_type":"markdown","source":"Tensorflow.js is a new library that allows machine learning models to run in the browser - without having to download or install any additional software. Because the model is running locally, any data that a user submits never leaves his or her pc or mobile phone. \n\nEvery image that a user submits needs to be pre-processed before being passed to the model for prediction. MobileNet pre-prcessing involves scaling the pixel values between 1 and -1. In this kernel we used a built in function to pre-process images for MobileNet:<br>\ntensorflow.keras.applications.mobilenet.preprocess_input()\n\nWhen using Tensorflowjs in the app, this pre-processing needs to be implemented manually.\n"},{"metadata":{"_uuid":"29880b4e0d1c540f667ac0e4adc9c3c866368753"},"cell_type":"markdown","source":"*The code that follows could run in Kaggle kernels a few days ago, now it can't. <br>\n\"import tensorflowjs as tfjs\" gives an error.*\n\n*I've included the model conversion steps for reference - because I'm sure the error will be fixed at some point.*"},{"metadata":{"_uuid":"7f9017d69bf0b84522e34841c1876b613cae1535"},"cell_type":"markdown","source":"### Install Tensorflow.js"},{"metadata":{"trusted":true,"_uuid":"2da93a52657b786a8eb7a0d5df6d6a2bcbd0f1c6","_kg_hide-output":true},"cell_type":"code","source":"# !pip install tensorflowjs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a85d7889e2bada2ebe3b84fc1571a89b1a66b7b0"},"cell_type":"markdown","source":"### Convert the model from Keras to Tensorflow.js"},{"metadata":{"trusted":true,"_uuid":"9977179251a2feb129a946028fb74d30b9eb7341"},"cell_type":"code","source":"# create a directory to store the model files\n#os.mkdir('tfjs_dir')\n\n# convert to Tensorflow.js\n#import tensorflowjs as tfjs\n\n#tensorflowjs.converters.save_keras_model(model, 'tfjs_dir')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7df391a2792ddfb7fa2a980776aeac744612f702"},"cell_type":"code","source":"# check the the directory containing the model is available\n#!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71dd9a6a021d4ffcc159dafd52e7f86ecc6558cb"},"cell_type":"code","source":"# view the files that make up the tensorflow.js model\n#os.listdir('tfjs_dir')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9658677176b67f9ac35259d8d291cee175338e1"},"cell_type":"markdown","source":"### Delete base_dir"},{"metadata":{"trusted":true,"_uuid":"f774cd15c6de188d4bb150f25ab600e5cbc06031"},"cell_type":"code","source":"# Delete the image data directory we created to prevent a Kaggle error.\n# Kaggle allows a max of 500 files to be saved.\n\nshutil.rmtree('base_dir')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b6056bb27006ebc85fb54bbc8b9b989bd756ff1"},"cell_type":"markdown","source":"### Resources\n\nThese are some helpful resources:"},{"metadata":{"_uuid":"d5f5d88e7cda18fb86c7e9715e488536e4424673"},"cell_type":"markdown","source":"1. Excellent tutorial series by deeplizard on how to use Mobilenet with Tensorflow.js<br>\nhttps://www.youtube.com/watch?v=HEQDRWMK6yY\n\n2. Tutorial by Minsuk Heo on Accuracy, Precision and F1 Score<br>\nhttps://www.youtube.com/watch?v=HBi-P5j0Kec\n\n3. Tutorial by Data School on how to evaluate a classifier<br>\nhttps://www.youtube.com/watch?v=85dtiMz9tSo\n\n3. Tensorflow.js gallery of projects<br>\nhttps://github.com/tensorflow/tfjs/blob/master/GALLERY.md\n\n"},{"metadata":{"_uuid":"55623033f552beec0101f2d8241c122404b1f82f"},"cell_type":"markdown","source":"### Conclusion"},{"metadata":{"trusted":true,"_uuid":"ffb96375ce44ef009192bf6ddb44f6eb53a43838"},"cell_type":"markdown","source":"Many thanks to Jenny Yang (@jenny18) for posting this well curated dataset. Thanks again Kaggle for the free GPU.\n\nThank you for reading. "},{"metadata":{"trusted":true,"_uuid":"d3fd098aae4fdd509942e58cf61a3bb338f5d4ac"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}