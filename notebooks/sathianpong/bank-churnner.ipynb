{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\nsns.set()\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1) Import data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/credit-card-customers/BankChurners.csv')\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset provider note\ndata = data.iloc[:,:-2]\n\n# Drop identity\ndata.drop('CLIENTNUM',axis=1, inplace=True)\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Input shape =',data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Declare categorical, numerical columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = data.select_dtypes(include='object')\nnum_cols = data.select_dtypes(exclude='object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical Columns\ncat_cols.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numerical Columns\nnum_cols.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mark color to each target <br>\n- loss : is a color for customer who move to other company (Churn).\n- still : is a color for customer who doesn't churn."},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = sns.color_palette(\"Paired\")[5]\nstill = sns.color_palette(\"Paired\")[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) Categorical columns"},{"metadata":{},"cell_type":"markdown","source":"### Distribution of each categorical column."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,axes = plt.subplots(1,6)\nfig.set_size_inches(23,6)\nj=0\nfor col in cat_cols.columns:\n    if col=='Attrition_Flag':\n        color='darkorange'\n    else:\n        color='mediumpurple'\n    sns.countplot(data=data, x=col, ax=axes[j], color=color)\n    axes[j].set_xticklabels( data[col].unique(), rotation=90)\n    axes[j].set_ylabel('')\n    axes[j].xaxis.set_label_coords(0.5,1.05)\n    j+=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see the exact same distribution of 'Education_Level' in 'Marital_Status', and vice-versa."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2)\nfig.set_size_inches(12,4)\nsns.countplot(data=cat_cols, x='Education_Level', hue='Marital_Status', ax=ax[0])\nax[0].tick_params(axis='x', labelrotation= 40.0)\nax[0].xaxis.set_label_coords(0.5,1.1)\nsns.countplot(data=cat_cols, hue='Education_Level', x='Marital_Status', ax=ax[1], hue_order=['Uneducated', 'High School', 'College', 'Graduate', 'Post-Graduate', 'Doctorate'])\nax[1].tick_params(axis='x', labelrotation= 40.0)\nax[1].xaxis.set_label_coords(0.5,1.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see the same distribution of 'Income' in all 'Education_Level', and vice-versa. <br>\nMost people have $<40K income in all types of 'Education_Level' even in 'Doctorate'. <br>\nIn all Income range, Graduate have the largest number."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2)\nfig.set_size_inches(12,4)\n\nsns.countplot(ax=ax[0], data=cat_cols, x='Education_Level', hue='Income_Category', color='indigo',hue_order=['Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +'])\nax[0].tick_params('x', labelrotation=30)\nax[0].xaxis.set_label_coords(0.5,1.1)\n\nsns.countplot(ax=ax[1], data=cat_cols, hue='Education_Level', x='Income_Category')\nax[1].set_xticklabels(['Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +'], Rotation= 30)\nax[1].xaxis.set_label_coords(0.5,1.1)\n\nplt.legend(bbox_to_anchor=(1, 1), loc='upper left')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2)\nfig.set_size_inches(12,4)\nsns.countplot(data=cat_cols, x='Marital_Status', hue='Income_Category', ax=ax[0], color='indigo',hue_order=['Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +'])\nax[0].tick_params(axis='x', labelrotation= 40.0)\nax[0].xaxis.set_label_coords(0.5,1.1)\nsns.countplot(data=cat_cols, x='Income_Category', hue='Marital_Status', ax=ax[1])\nax[1].set_xticklabels(['Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +'], Rotation= 40)\nax[1].xaxis.set_label_coords(0.5,1.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't see any missing value which is kinf of wierd. <br>"},{"metadata":{},"cell_type":"markdown","source":"Let's try looking into each value in each category."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in cat_cols.columns:\n    print('---------'+col+'---------')\n    print(cat_cols[col].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that missing values are denoted with 'unknown'. So, we have to replace 'unknown' with NaN."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the missing values\npd.Series(cat_cols.replace('Unknown', np.nan).isnull().sum()/len(cat_cols)*100, name='Missing value').apply(lambda x:round(x,4)).apply(lambda x:str(x)+' %')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1) Ordinal encoding, manually"},{"metadata":{},"cell_type":"markdown","source":"We'll do ordinal encoding to 'Education_Level', 'Income_Category', and 'Card_Category' feature. For 'Unknown' category, we will replace it with average of order."},{"metadata":{"trusted":true},"cell_type":"code","source":"Edu_level = {'Uneducated':0, 'High School':1, 'College':2, 'Graduate':3, 'Post-Graduate':4, 'Doctorate':5, 'Unknown':15/6}\nIncome_cat = {'Less than $40K':0, '$40K - $60K':1, '$60K - $80K':2, '$80K - $120K':3,  '$120K +':4, 'Unknown':10/4}\nCard_cat = {'Blue':0, 'Silver':1, 'Gold':2, 'Platinum':3}\n\ndata_ordinal_encoded = data.copy()\ndata_ordinal_encoded['Education_Level'] = data_ordinal_encoded['Education_Level'].map(Edu_level)\ndata_ordinal_encoded['Income_Category'] = data_ordinal_encoded['Income_Category'].map(Income_cat)\ndata_ordinal_encoded['Card_Category'] = data_ordinal_encoded['Card_Category'].map(Card_cat)\ndata_ordinal_encoded.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other categorical columns will be one-hot encoded in the later section. This means we consider 'Unknown' value in 'Marital_Status' as a new class."},{"metadata":{},"cell_type":"markdown","source":"# 3) Continuous columns"},{"metadata":{},"cell_type":"markdown","source":"### Pairwise scatterplot of continuous variables\nThe picture is in ..\\scatterplot\\\\.png"},{"metadata":{},"cell_type":"markdown","source":"### Missing value"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,ax=plt.subplots()\nfig.set_size_inches(10,10)\nsns.heatmap(data=num_cols.corr(), ax=ax, annot=True, fmt='.1f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that **Months_on_book** and **Customer_Age**  , **Total_Trans_Amt** and **Total_Trans_Ct** are highly linearly correlated."},{"metadata":{},"cell_type":"markdown","source":"### Variance."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data_var = pd.DataFrame(data.var(), columns=['Var']).apply(lambda x:round(x,2))\ndata_var.style.background_gradient(sns.light_palette('green',as_cmap=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will deal with cantinuous features using RFE(Recursive Feature Elimination) in the next section. <br>\nRecursive feature elimination (RFE) is a **feature selection** method that do \n1. fitting a model\n2. removes the weakest feature(s) by feature importance.<br>Features' importances are ranked by the modelâ€™s **coef_** or **feature\\_importances\\_** attributes, and by recursively eliminating a small number of features per loop, RFE attempts to eliminate dependencies and collinearity that may exist in the model.\n3. repeat until the specified number of features is reached\n"},{"metadata":{},"cell_type":"markdown","source":"# 4) Feature selection, One-hot encoding, and Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, plot_roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_ordinal_encoded.drop('Attrition_Flag',axis=1)\nY = data_ordinal_encoded['Attrition_Flag']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, stratify=Y)\n\nct = ColumnTransformer(\n    [(\"one hot encode\",OneHotEncoder(sparse=False),[1,4]),\n     (\"scale\", StandardScaler(), [e for e in range(len(X.columns)) if e not in {1,4}])],\n    remainder='passthrough')\nX_train = ct.fit_transform(X_train)\nX_test = ct.transform(X_test)\n\nle=LabelEncoder()\nY_train = le.fit_transform(Y_train)\nY_test = le.transform(Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Note: <br>\nLucklily we don't have a problem here.<br> \nFor small dataset, I recommend **fit the OneHotEncoder object to entire dataset** instead of X_train because ,after splitting the data, X_train sometimes might not contain all the unique values of all categories. If so, there will be a problem when trying to transform X_test since OneHotEncoder object doesn't know some values occured in X_test (but didn't occur in X_train that it fits)."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"rfe = RFE(estimator = RandomForestClassifier(), n_features_to_select=11, verbose=1)\nrfe.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selected"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train[:,rfe.support_]\nX_test = X_test[:,rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5) Building models"},{"metadata":{},"cell_type":"markdown","source":"We'll use Random Forest Classifier with its default parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier().fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rf.predict(X_test)\n\nprint('Confusion matrix\\n',confusion_matrix(y_pred, Y_test))\nprint('\\nroc_auc_score\\n',roc_auc_score(Y_test, y_pred))\nprint('\\nClassification report\\n',classification_report(y_pred, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(rf, X_test, Y_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that Random Forest has done a very good job!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}