{"cells":[{"metadata":{},"cell_type":"markdown","source":"**            Test of ML classifier using Heart Disease Data**\n\n* Data Analysis\n* Data Visiualization\n* Feature Encoding\n* Feature Scaling\n* Logistic Regression\n* Stochastic Gradient Decsent Classifier\n* Random Forest Classifier\n* Support Vector Machine\n* Random Forest\n* Gradient Boosting Classifier\n* Voting Classifier"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/heart.csv\")\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data discription:\n**\n* age - age in years \n* sex - (1 = male; 0 = female) \n* cp - chest pain type \n* trestbps - resting blood pressure (in mm Hg on admission to the hospital) \n* chol - serum cholestoral in mg/dl \n* fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n* restecg - resting electrocardiographic results \n* thalach - maximum heart rate achieved \n* exang - exercise induced angina (1 = yes; 0 = no) \n* oldpeak - ST depression induced by exercise relative to rest \n* slope - the slope of the peak exercise ST segment \n* ca - number of major vessels (0-3) colored by flourosopy \n* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect \n* target - have disease or not (1=yes, 0=no)\n\nThere are several categorical fetures, so we have to be careful on how to deal with them."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\ncorr_matrix = data.corr()\nplt.figure(figsize=(20,10))\nsns.heatmap(corr_matrix,annot=True,cmap=\"coolwarm\",fmt=\".2f\",annot_kws={'size':16})\ncorr_matrix[\"target\"].sort_values(ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on correlation matrix figure, we do quick data analysis with visualization."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax1 = plt.subplot(1, 2, 1)\nax = sns.scatterplot(x=\"thalach\", y=\"trestbps\", hue=\"target\",data=data)\n\nax1 = plt.subplot(1, 2, 2)\nax = sns.scatterplot(x=\"thalach\", y=\"oldpeak\", hue=\"target\",data=data)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The figure left shows more overlap compared to the right. This shows trestbps(-0.14) is weak feature compared to oldpeak(-0.43). It also shows due to low correlation values we have to use higher dimention features do better classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax1 = plt.subplot(1, 2, 1)\nsns.distplot(data[\"thalach\"][data[\"target\"]==0], label=\"Negative\")\nsns.distplot(data[\"thalach\"][data[\"target\"]==1], label=\"Positive\")\nplt.ylabel(\"density\")\nplt.xlabel(\"maximum heart rate achieved\")\nplt.legend()\nax2 = plt.subplot(1, 2, 2)\nax = sns.scatterplot(x=\"thalach\", y=\"target\", hue=\"target\",data=data)\nplt.xlabel(\"maximum heart rate achieved\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature \"thalach\" a.k.a maximum heart rate achieved is interesting feature. As you can see, the person with higher max heart rate is more likely to diagonsed with heart disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"sex\", hue = \"target\",kind=\"count\", data=data);\nplt.xlabel(\"Sex (0:Female, 1:Male)\")\nplt.title(\"Heart Disease (Target 0:Positive, 1:Negitave)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Based on sex, female is more likely diagnosed with heart disease than male."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"cp\",hue=\"target\",kind = \"count\",data=data)\nplt.xlabel(\"Chest Pain (0:Female, 1:Male)\")\nplt.title(\"Heart Disease (Target 0:Positive, 1:Negitave)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even chest pain values looks as numerical, but it positvely correlates with target. It acts like continous variables as higher the chest pain person is more likely to be diagnosed with hear disease which makes sense. This is one of the reason I did not add this feature to categorical."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.iloc[:,:-1].values\nY = data.iloc[:,-1].values\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X,Y,test_size = 0.2, random_state = 5)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nlog_clf = LogisticRegression(solver=\"lbfgs\")\nlog_clf.fit(x_train,y_train)\ny_logclf_pred = log_clf.predict(x_test)\nprint(accuracy_score(y_test,y_logclf_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first classifier is linear regression without any data preprocessing. Note: Linear regression does not need feature scaling, thats why it still works well."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test,y_logclf_pred))\nprint(classification_report(y_test,y_logclf_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Full classifier performance analysis. It is worth to note one can calculate precision, recall, and f1-score from confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoding categorical features\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nfe1_1hot = encoder.fit_transform(data['thal'].values.reshape(-1,1))\nfe2_1hot = encoder.fit_transform(data['ca'].values.reshape(-1,1))\nfe3_1hot = encoder.fit_transform(data['slope'].values.reshape(-1,1))\n\ndatadrop = data.drop(columns=[\"thal\",\"ca\",\"slope\"])\nX_new = datadrop.iloc[:,:-1].values\nY_new = datadrop.iloc[:,-1].values\nX_new = np.concatenate((X_new,fe1_1hot.toarray(),fe2_1hot.toarray(),fe3_1hot.toarray()),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(random_state=3)\nsgd_clf.fit(x_train,y_train)\ny_sgd_clf_pred = sgd_clf.predict(x_test)\nprint(accuracy_score(y_test,y_sgd_clf_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, the SGD accuracy is only 70.5%?! Why?! Again, we have to do feature scaling since a lot of algorithms reuqires this step."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nstand_sca = StandardScaler()\nX_trans = stand_sca.fit_transform(X_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X_trans,Y_new,test_size = 0.2, random_state = 5)\nsgd_clf = SGDClassifier(random_state=3,n_jobs=-1)\nsgd_clf.fit(x_train,y_train)\ny_sgd_clf_pred = sgd_clf.predict(x_test)\nprint(accuracy_score(y_test,y_sgd_clf_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter([0,1],[100*0.704,100*0.868])\nplt.ylabel(\"accuracy\")\nplt.annotate('Before Feature Scaling', xy=(0, 72), xytext=(0, 75), fontsize = 12,\n            arrowprops=dict(facecolor='grey', shrink=0.05, linewidth = 2))\n\nplt.annotate('After Feature Scaling', xy=(1, 85), xytext=(0.6, 82.5), fontsize = 12,\n            arrowprops=dict(facecolor='grey', shrink=0.05, linewidth = 2))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After feature scaling , SGD classification accuracy increased by nearly 17%!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def testclassfiersgd(max_in,tol_in):\n    sgd_clf = SGDClassifier(random_state=3,n_jobs=-1,max_iter=max_in,tol=tol_in)\n    sgd_clf.fit(x_train,y_train)\n    y_sgd_clf_pred = sgd_clf.predict(x_test)\n    acc = accuracy_score(y_test,y_sgd_clf_pred)\n    return acc\n\nmax_iter_test = [10,50,100,500,1000,3000]\ntol = [10,1,0.1,0.2,0.3]\n\nacc_matrix_sgd = np.zeros((len(max_iter_test),len(tol)))\n\nfor i in range(len(max_iter_test)):\n    #print(max_iter_test[i])\n    for j in range(len(tol)):\n        #print(tol[j])\n        acc_matrix_sgd[i,j] = testclassfiersgd(max_iter_test[i],tol[j])\n\nsns.heatmap(acc_matrix_sgd,annot=True,cmap=\"coolwarm\",fmt=\".2f\",annot_kws={'size':16})\nplt.ylabel(\"max_iter\")\nplt.xlabel(\"tol\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is my simple version of hyper parameters tuning, we can also use sklearn grid search to find best parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"log_clf = LogisticRegression(solver=\"lbfgs\")\nlog_clf.fit(x_train,y_train)\ny_logclf_pred = log_clf.predict(x_test)\nprint(\"Logistic Regression accuracy\",100*accuracy_score(y_test,y_logclf_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(n_estimators=100,random_state=1)\nrf_clf.fit(x_train,y_train)\ny_rf_clf_pred = rf_clf.predict(x_test)\nprint(\"Random Forest accuracy\",100*accuracy_score(y_test,y_rf_clf_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc_clf = SVC(kernel=\"rbf\")\nsvc_clf.fit(x_train,y_train)\ny_svc_clf_pred = svc_clf.predict(x_test)\nprint(\"SVM accuracy\",100*accuracy_score(y_test,y_svc_clf_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngrbt_clf = GradientBoostingClassifier(max_depth=2,n_estimators=100,random_state=1)\ngrbt_clf.fit(x_train,y_train)\n\nerrors = np.zeros((100,1))\ni = 0\nfor y_pred in grbt_clf.staged_predict(x_test):\n    errors[i] = accuracy_score(y_test,y_pred)\n    i = i + 1\n    #print(y_pred)\n\nbest_n_estimator = np.argmax(errors)\n\nplt.plot(errors)\nplt.xlabel('number of trees');plt.ylabel('accuracy');plt.show()\n\ngrbt_clf_best = GradientBoostingClassifier(max_depth=2,n_estimators=best_n_estimator+1)\ngrbt_clf_best.fit(x_train,y_train)\ny_test_gbrt = grbt_clf_best.predict(x_test)\n\nprint(\"GBR accuracy is:\",100*accuracy_score(y_test,y_test_gbrt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter([0,1,2,3,4],[86.8,93.4,86.8,90.1,91.8])\nplt.ylabel(\"Accuracy\");plt.title(\"Accuracy comparision of classifiers\")\nplt.xticks([0,1,2,3,4],(\"SGD\",\"RF\",\"LOG\",\"GB\",\"SVC\"))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By comparing one fold validation, random forest performed the best and SGD & Logistic Regression performed the worst. \nFinally, lets put all together, perform 5 fold cross validations to see average performance of classifiers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\nsgd_cval=cross_validate(sgd_clf,X_trans,Y_new,cv=5)\nrf_cval=cross_validate(rf_clf,X_trans,Y_new,cv=5)\nlog_cval=cross_validate(log_clf,X_trans,Y_new,cv=5)\ngrbt_cval=cross_validate(grbt_clf_best,X_trans,Y_new,cv=5)\nsvc_cval=cross_validate(svc_clf,X_trans,Y_new,cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_mod_cval=np.concatenate((sgd_cval[\"test_score\"],rf_cval[\"test_score\"],log_cval[\"test_score\"],\n               grbt_cval[\"test_score\"],svc_cval[\"test_score\"]),axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This result is different than what we got from earlier one!"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,6))\nplt.subplot(1,2,1)\nsns.heatmap(all_mod_cval.reshape((5,5)),annot=True,cmap=\"coolwarm\",fmt=\".2f\",annot_kws={'size':16})\nplt.title(\"5 fold cross validation\")\nplt.yticks([0,1,2,3,4],(\"SGD\",\"RF\",\"LOG\",\"GB\",\"SVC\"))\nplt.subplot(1,2,2)\nplt.scatter([0,1,2,3,4],[sgd_cval[\"test_score\"].mean(),\n                        rf_cval[\"test_score\"].mean(),\n                        log_cval[\"test_score\"].mean(),\n                        grbt_cval[\"test_score\"].mean(),\n                        svc_cval[\"test_score\"].mean()])\nplt.ylabel(\"Avg Accuracy\");plt.title(\"Avg accuracy comparision of classifiers\")\nplt.xticks([0,1,2,3,4],(\"SGD\",\"RF\",\"LOG\",\"GB\",\"SVC\"))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Logistic regression has the highest average accuracy through 5 fold cross validation.\n\n**End Note: The 5-fold cross validation shows different result than earlier one fold result(same random state) for five different classifiers.So it is important to point out one should perform cross validation to determine performance of the classifier. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}