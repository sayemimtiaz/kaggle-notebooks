{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing & Overview"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dia = pd.read_csv('/kaggle/input/diabetes/diabetes.csv')\nprint(dia.shape)\nprint(dia.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dia.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dia.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# bagging ensembles\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Random Forests\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve\n\n# Ada Boost Classifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nX = dia.drop('Outcome', axis = 1)\ny = dia.Outcome\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n\n# bagging ensembles\nbag_clf = BaggingClassifier(DecisionTreeClassifier(random_state = 42), n_estimators = 500,\n                            max_samples = 100, bootstrap = True, n_jobs = 1, random_state = 42)\nbag_clf.fit(X_train, y_train)\n\n# determine accuracy score for the bagging method\nbag_y_pred = bag_clf.predict(X_test)\nbag_score = accuracy_score(y_test, bag_y_pred)\n\n# standard decision tree classifier\ntree_clf = DecisionTreeClassifier(random_state = 42)\ntree_clf.fit(X_train, y_train)\n\n# determine accuracy score for the Decision Tree method\ntree_y_pred = tree_clf.predict(X_test)\ntree_score = accuracy_score(y_test, tree_y_pred)\n\n# Random Forests\nrf_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1, random_state = 42)\nrf_clf.fit(X_train, y_train)\n\n# determine accuracy score for Random Forest method\nrf_y_pred = rf_clf.predict(X_test)\nrf_score = accuracy_score(y_test, rf_y_pred)\n\n# Ada Boost Classifier\nada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1), n_estimators = 200, \n                             algorithm = \"SAMME.R\", learning_rate = 0.5, random_state = 42)\nada_clf.fit(X_train, y_train)\n\n# determine accuracy score for Ada Boost Classifier\nada_y_pred = ada_clf.predict(X_test)\nada_score = accuracy_score(y_test, ada_y_pred)\n\n# summary for all the results\nprint('Accuracy Score Results')\nprint('Bagging Classifier {:.2f}'.format(bag_score))\nprint('Decision Tree Classifier {:.2f}'.format(tree_score))\nprint('Random Forest {:.2f}'.format(rf_score))\nprint('Ada Boost Classifier {:.2f}'.format(ada_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# determine accuracy score for all methods\nbag_y_prob = bag_clf.predict_proba(X_test)\nbag_y_score = bag_y_prob[:, 1]\nbag_fpr, bag_tpr, bag_threshold = roc_curve(y_test, bag_y_score)\n\ntree_y_prob = tree_clf.predict_proba(X_test)\ntree_y_score = tree_y_prob[:, 1]\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_test, tree_y_score)\n\nrf_y_prob = rf_clf.predict_proba(X_test)\nrf_y_score = rf_y_prob[:, 1]\nrf_fpr, rf_tpr, rf_threshold = roc_curve(y_test, rf_y_score)\n\nada_y_prob = ada_clf.predict_proba(X_test)\nada_y_score = ada_y_prob[:, 1]\nada_fpr, ada_tpr, ada_threshold = roc_curve(y_test, ada_y_score)\n\n# plotting ROC Curve to visualize all method\nsns.set_style('whitegrid')\nplt.figure(figsize = (10, 8))\nplt.plot(bag_fpr, bag_tpr, label = 'Bagging Classifier')\nplt.plot(tree_fpr, tree_tpr, label = 'Decision Tree Classifier')\nplt.plot(rf_fpr, rf_tpr, label = 'Random Forest Classifier')\nplt.plot(ada_fpr, ada_tpr, label = 'Ada Boost Classifier')\n\nplt.plot([0, 1], [0, 1], color = 'blue', linestyle = '--')\nplt.axis([0, 1, 0, 1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve', fontsize = 15)\nplt.legend(loc = \"lower right\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}