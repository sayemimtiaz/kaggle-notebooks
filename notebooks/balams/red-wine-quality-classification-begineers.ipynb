{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### Load Libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basics\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Preprocssing\nimport missingno as msno\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, binarize\n\n# Model Selection \nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n# Ensemble\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n\n# Metrics\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, accuracy_score\n\n# Feature Selection\nfrom sklearn.feature_selection import SelectKBest, chi2\n\n# Warnings\nimport warnings as ws\nws.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load dataset\ndata = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary\ndef summary(data):\n    df = {\n     'Count' : data.shape[0],\n     'NA values' : data.isna().sum(),\n     '% NA' : round((data.isna().sum()/data.shape[0]) * 100, 2),\n     'Unique' : data.nunique(),\n     'Dtype' : data.dtypes,\n     'min' : round(data.min(),2),\n     '25%' : round(data.quantile(.25),2),\n     '50%' : round(data.quantile(.50),2),\n     'mean' : round(data.mean(),2),\n     '75%' : round(data.quantile(.75),2),   \n     'max' : round(data.max(),2)\n    } \n    return(pd.DataFrame(df))\n\nprint('Shape is :', data.shape)\nsummary(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no missing values in this dataset. All variables are numeric and we found our target varibale have 6 unique values.\n\n### Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hist(figsize = (10,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target Variables\ndata['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert Target variable into binary\nbins = [2,6.5, 8]\nlabels = ['Bad','Good']\ndata['quality'] = pd.cut(data['quality'], bins = bins, labels = labels)\n\ndata['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset seems like imbalanced dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = data.drop('quality', axis = 1).columns.tolist()\n\nplt.figure(figsize = (15,10))\ni=0\nfor col in col_names:\n    plt.subplot(3,4, i+1)\n    plt.grid(True, alpha = 0.5)\n    sns.kdeplot(data[col][data['quality'] == 'Bad'], label = 'Bad Quality')\n    sns.kdeplot(data[col][data['quality'] == 'Good'], label = 'Good Quality')\n    plt.title(col + ' vs Quality', size = 15)\n    plt.xlabel(col, size = 12)\n    plt.ylabel('Density')    \n    plt.tight_layout()\n    i+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('quality', axis = 1)\nY = data['quality'].replace({'Bad':0, 'Good' : 1})\n\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Selection\nWe don't  know which model is perform well for this dataset. So we validate all the models on trian test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('SVM', SVC()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('ADA', AdaBoostClassifier()))\nmodels.append(('GB', GradientBoostingClassifier()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_selection(X,Y):\n    acc_results = []\n    auc_results = []\n    names = []\n\n    # Set Table\n    col = ['Model Name','ROC AUC Mean','ROC AUC Std','ACC Mean', 'AUC Std']\n    model_results = pd.DataFrame(columns = col)\n\n    i = 0\n    for name, model in models:\n        kfold = KFold(n_splits = 10, random_state = 7)\n\n        cv_acc_results = cross_val_score(model, X,Y, cv = kfold, scoring = 'accuracy')\n        cv_auc_results = cross_val_score(model, X,Y, cv = kfold, scoring =  'roc_auc')\n\n        acc_results.append(cv_acc_results)\n        auc_results.append(cv_auc_results)\n        names.append(name)\n\n        model_results.loc[i] = [name, cv_auc_results.mean(),cv_auc_results.std(), cv_acc_results.mean(), cv_acc_results.std()]\n        i+=1\n\n    model_results = model_results.sort_values(['ROC AUC Mean'], ascending = False)     \n\n    # View Model Results\n    plt.figure(figsize=(10,5))\n    plt.subplot(1,2,1)\n    sns.boxplot(x = names, y = acc_results)\n    plt.title('Accuracy Score')\n\n    plt.subplot(1,2,2)\n    sns.boxplot(x = names, y = auc_results)\n    plt.title('AUC Score')\n    plt.show()\n    \n    return(model_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_selection(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest fits well in this dataset. \n\nTo avoid overfitting in final model we have to use hyper parameters of the models. This basically done by cross valdation technique"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_validation(model,x_test,y_test,thr = 0.5) :\n    \n    y_pred_prob = model.predict_proba(x_test)[:,1]\n    y_pred = binarize(y_pred_prob.reshape(1,-1), thr)[0]\n    \n    cnf_matrix = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize = (10,3))\n    plt.subplot(1,2,1)\n    sns.heatmap(cnf_matrix, annot = True, fmt = 'g')\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted label')\n    plt.ylabel('Actual label')\n\n    fpr, tpr, threshold = roc_curve(y_test, y_pred_prob)\n    plt.subplot(1,2,2)\n    sns.lineplot(fpr, tpr)\n    plt.plot([0,1],[0,1], 'r--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.show()\n\n    \n    print('Classification Report :')\n    print('===' * 20)\n    print(classification_report(y_test, y_pred))\n\n    score = tpr - fpr\n    opt_threshold = sorted(zip(score,threshold))[-1][1]\n    print('='*20)\n    print('Area Under Curve', roc_auc_score(y_test,y_pred))\n    print('Accuracy', accuracy_score(y_test,y_pred))\n    print('Optimal Threshold : ',opt_threshold)\n    print('='*20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n    'bootstrap': [True,False],\n    'max_depth': [10, 50, 100],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [10,100, 200, 300, 1000]\n}\n\nrf = RandomForestClassifier()\ngrid = GridSearchCV(rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 1)\n\ngrid.fit(x_train, y_train)\ngrid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_validation(grid, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final Model\nfinal_model = grid.best_estimator_\nfinal_model.fit(x_train, y_train)\n\nmodel_validation(final_model,x_test,y_test, 0.106)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall for 1 in final model has improved lot. which means 90% of True positive predicted as True.\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}