{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Assign Research Value Scores along the following dimensions:**\n\nSTUDY TARGET:\nLab, Animal, Human\n\nSIGNIFICANCE:\nBased on occurrences of significance vs no significance\n\nMETHODS:\nGood = Review,\nBetter = Observational,\nBest = Clinical trials\n\nCOHORT:\nBased on occurrences of number following subject.\n\nAFFILIATION:\nPrimary Institution matched to TWUR Top 200\n\nWhere applicable, weights are assigned using scale of 1-3 (good, better, best)\n\nCreated by Chris Busch & Piyush Madan"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install fuzzywuzzy\n#!pip install nltk\n#!pip install plotly\n#!pip install swifter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport json\nfrom tqdm import tqdm_notebook as tqdm\nimport re\nfrom functools import reduce\nimport operator\nfrom fuzzywuzzy import fuzz\n#import swifter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get Tags using regex"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pos_tag(text,pos_type=('NNS')):\n    if type(text)!=str:\n        text = str(text)\n    tokenized = nltk.word_tokenize(text)\n    nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if(pos[:2] in pos_type)]\n    if len(nouns):\n        #print(text,nltk.pos_tag(tokenized))\n        return text\n    else:\n        return None\n\n\ndef get_tags(text='',\n             type_of_extractor='type_of_study', # 'type_of_study', 'animal_model' , 'cohort','is_significant'\n             text_useless_on_its_own=('study'),\n             stop_words=set(stopwords.words('english')),\n             *args, **kwargs\n            ):\n    \n    regex_map={\n            'type_of_study':'(?:\\S+\\s)(?:study|trial|retrospective|prospective|case-control|ecological)',\n            'animal_model': '(?:\\S+\\s)(?:animal)',\n            'human_model':  '(?:\\S+\\s)(?:human|person|person|man|men|woman|women|child|children|patient|patients)',\n            'lab_model':    '(?:\\S+\\s)(?:lab|laboratory|culture|specimen|microbiology|molecula)',\n            'cohort':       '[0-9]+\\s[a-zA-Z]+',\n            'is_significant': '(is|not)\\sstatistically significant'\n             }\n        \n    if not regex_map.get(type_of_extractor,None):\n        print(\"Invalid Implementation\",type_of_extractor)\n        return None\n    \n    regex_filter = regex_map[type_of_extractor]\n    selected_words = re.findall(regex_filter,text.lower())\n    \n    if type_of_extractor in ['type_of_study','animal_model','human_model','lab_model']:\n        value = pd.Series(selected_words).apply(\n                                lambda x: pd.Series(x.split(\" \"))\n                                        ).apply( \n                                lambda x:\n                                    x.apply(\n                                        (lambda y: y if y not in stop_words  else None )\n                                        )                                \n                                    ).apply(\n                                        lambda x: ' '.join(x.dropna()),\n                                        axis=1\n                                    ).apply(\n                                        lambda x: x if x not in text_useless_on_its_own else None\n                                        ).dropna().tolist()\n        \n        \n            \n\n    if type_of_extractor in ['cohort']:\n        selected_words_parsed = pd.Series(selected_words).apply(\n                                lambda x: get_pos_tag(x,('NNS'))\n                            ).apply(\n#this is to include only specific words\n                                lambda x: { x.split(\" \")[1]:x.split(\" \")[0]} if ( x and \n                                                                                 len(x.split(\" \"))==2 and \n                                                                                 x.split(\" \")[1] in kwargs.get('inclusion_keyword',[])) else None\n#                                 lambda x: { x.split(\" \")[1]:x.split(\" \")[0]} if ( x and \n#                                                                                  len(x.split(\" \"))==2 ) else None\n            \n                    ).dropna()\n\n        if len(selected_words_parsed): \n            value = reduce(lambda a, b: dict(a, **b), selected_words_parsed)\n        else:\n            value = {}\n\n    if type_of_extractor == 'is_significant':\n        value = pd.Series(selected_words).value_counts().to_dict()\n        \n    return value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_keywords(list_of_files):\n    \n    df = pd.DataFrame()\n\n    for txt_file in tqdm(list_of_files):\n        txt_file_text = open(txt_file, \"r\").read()\n        \n        author_info=get_authors_info(txt_file)\n\n        df = df.append({\n                'paper_id': re.findall(\"[\\w-]+\\.\",txt_file)[0][:-1],\n                'path': txt_file,\n                'study': get_tags(text=txt_file_text,\n                                 type_of_extractor='type_of_study',\n                                 stop_words=  set(stopwords.words('english')).union( ['present','one','recent','largest',\n                                                         'first','different','previous','prior',\n                                                         'this','our','current','another','pilot',\n                                                         '\"this','\"a','\"our'\n                                                        ]\n                                                    )\n                                ),\n                'subject_animal': get_tags(text=txt_file_text,\n                                 type_of_extractor='animal_model',\n                                 stop_words=  set(stopwords.words('english'))\n                                 ),\n                'subject_human': get_tags(text=txt_file_text,\n                                 type_of_extractor='human_model',\n                                 stop_words=  set(stopwords.words('english'))\n                                 ),        \n                'subject_lab': get_tags(text=txt_file_text,\n                                 type_of_extractor='lab_model',\n                                 stop_words=  set(stopwords.words('english'))\n                                 ),   \n                'cohort_info': get_tags(text=txt_file_text,\n                                 type_of_extractor='cohort',\n                                 stop_words=  set(),\n                                 inclusion_keyword =  ('people','patient','patients','children','infant', 'females','males',\n                                                         'women','men','subjects','animals','control','controls','cases',\n                                                         'mice','dogs','calves','cats','samples','groups','individuals','participants',\n                                                         'adult','adults','candidate','candidates')\n                                ),  \n                'is_significant': get_tags(text=txt_file_text,\n                                 type_of_extractor='is_significant',\n                                ),\n                **author_info\n                },ignore_index=True)\n\n    df=df.set_index('paper_id')\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Give appropriate weights based on extracted keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"study_scoring_df = pd.read_csv(\"../input/study-scoring/study_scoring.csv\")\nstudy_scoring_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_study_method_match(study_name):\n    #print(\"study_scoring_df.study_method\",study_scoring_df.study_method)\n#    print(\"study_name\",study_name)\n    best_match_index = np.argmax(study_scoring_df.study_method.apply(lambda x: \n                                    fuzz.token_sort_ratio(x.lower(), study_name.lower())))\n    \n    return study_scoring_df.value[best_match_index]\n\n\ndef get_cohort_info_score(df_row):\n    try:\n        data= df_row['cohort_info']\n        values = list(data.values())\n\n        if len(values):\n            if '000' in values:\n                return 3\n            try:\n                values = pd.Series(values).apply(int)\n            except:\n                print(\"unable to convert values to int\")\n\n            max_value = max(values)\n\n            if max_value <= 10:\n                return 1\n            elif max_value <=100:\n                return 2\n            elif max_value >100:\n                return 3\n        else:\n            return 0\n    except:\n        return 0 \n\n    \ndef get_significant_score(df_row):    \n    try:\n        data= df_row['is_significant']\n        is_count = data.get('is',0)\n        is_not_count = data.get('not',0)\n    except:\n        print('significant data issue',df_row['is_significant'],df_row['path'])        \n        return 0     \n    \n    weight = (is_count-is_not_count)\n\n    if weight>3:\n        return 3\n    elif weight <-3:\n        return -3\n    else:\n        return weight\n\n\ndef get_study_type(df_row):\n    try:\n        data= df_row['study']\n        if len(data)>0:\n            return max(pd.Series(data).apply(lambda x: get_study_method_match(x)))\n        else:\n            return 0  \n    except:\n        print('study type issue',df_row['study'],df_row['path'])        \n        return 0\n    \ndef get_study_subject_type(df_row):\n\n    try:\n        data= df_row[['subject_animal','subject_human','subject_lab']]\n        data = data.apply(lambda x: len(x))\n\n        if data['subject_animal']:\n            return 2    \n        if data['subject_human'] >  data['subject_lab'] :\n            return 3\n        elif data['subject_lab'] > 0:\n            return 1\n        else:\n            return 0\n    except:\n        return 0\n\n\ndef get_author_score(df_row):    \n    try:\n        data= df_row['Rank']\n        if data < 20:\n            return 3\n        elif data < 100:\n            return 2    \n        elif data < 150:\n            return 1\n        else:\n            return 0 \n    except:\n        return 0\n    \ndef get_weights(df_row):\n    weights =   {\n             \n                'weight_subject_type': get_study_subject_type(df_row), \n                'weight_cohort_info': get_cohort_info_score(df_row),\n                'weight_significance': get_significant_score(df_row),       \n                'weight_study_type':  get_study_type(df_row),\n                'weight_affliation': get_author_score(df_row)\n            }\n\n    weights['cummulative_sum'] = sum(weights.values())\n    \n    return pd.Series(weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for test\n# df = get_keywords(get_filelist(dir_path,type_of_file='.json')[0:40])        \n# df = df.join(df.swifter.apply(lambda x: get_weights(x),axis=1))\n# df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_authors_info(json_filepath):\n    paper_info = json.loads(open(json_filepath,\"r\").read())\n    extracted_paper_info = {\n        'paper_id' : paper_info['paper_id'],\n        'metadata':  paper_info['metadata']['title'],\n        'num_authors': len(paper_info['metadata']['authors']),\n        'path_to_json': json_filepath,\n\n    }\n\n    def get_author_info(author_json,prefix_to_key=''):\n\n        author_details={key: paper_info['metadata']['authors'][0].get(key,None) for key in ('first','middle','last')}\n        affiliation_temp=paper_info['metadata']['authors'][0].get('affiliation',None)\n        if affiliation_temp:\n            author_details['institution'] = affiliation_temp.get('institution')\n            author_details['location'] = affiliation_temp.get('location')\n\n        first_person_keys = list(author_details.keys())\n        for key in first_person_keys:\n            author_details[prefix_to_key+'_'+key] = author_details.pop(key)    \n\n        return author_details\n\n    if len(paper_info['metadata']['authors'])>0:\n        author_info= get_author_info(   paper_info['metadata']['authors'][0],\n                                        prefix_to_key='first_person'\n                                    )\n        if author_info:\n            extracted_paper_info.update(author_info)\n\n\n#     if len(paper_info['metadata']['authors'])>1:\n#         author_info= get_author_info(   paper_info['metadata']['authors'][-1],\n#                                         prefix_to_key='last_person'\n#                                     )\n#         if author_info:\n#             extracted_paper_info.update(author_info)        \n\n    return extracted_paper_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_filelist(dir_path,type_of_file='.json'):\n    \n    txt_files=[]\n    for (dirpath, dirnames, filenames) in os.walk(dir_path):\n        for filename in filenames:\n            if type_of_file in filename:\n                txt_files.append(f'{dirpath}/{filename}')\n    print(\"total files:\",len(txt_files))\n    return txt_files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twur_df = pd.read_csv('../input/twur-top-200/twur.csv')\ntwur_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twur_kag_xwalk_df = pd.read_csv(\"../input/twur-kaggle-crosswalk/twur-kag-xwalk.csv\")\ntwur_kag_xwalk_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loop over all the folders to extract keywords, and calculate repective weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!mkdir -p output","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"test = False\nsave_to_file = True\n\ndf_main= pd.DataFrame()\n\nfor sources in tqdm(['biorxiv_medrxiv','comm_use_subset','custom_license','noncomm_use_subset']):\n    dir_path = f'../input/CORD-19-research-challenge/{sources}/{sources}/'\n    output_file = f'../weights_{sources}.csv'\n    \n    if test:\n        df = get_keywords(get_filelist(dir_path,type_of_file='.json')[0:40])        \n    else:\n        df = get_keywords(get_filelist(dir_path,type_of_file='.json'))\n        \n    if 'first_person_institution' not in df.columns:\n        df['first_person_institution'] = 'nan'   \n\n    # join to twur without crosswalk\n    #df = pd.merge(\n    #    df.reset_index(),\n    #    twur_kag_df[['Rank','TNAME']],\n    #    left_on='first_person_institution',\n    #    right_on='TNAME',\n    #    how='left' \n    #).set_index('paper_id')  \n\n    # join to twur using xwalk\n    df = pd.merge((\n        pd.merge(df.reset_index(), twur_kag_xwalk_df, how='left', left_on='first_person_institution', right_on='kaggle_name')),\n                  twur_df, how='left', left_on='twur_name', right_on='Name').set_index('paper_id')\n    \n    # using swifter\n    #df = df.join(df.swifter.apply(lambda x: get_weights(x),axis=1))\n    \n    # without swifter\n    df = df.join(df.apply(lambda x: get_weights(x),axis=1))\n\n    \n    display(df.shape)\n    display(df.head())\n    \n    df_main = df_main.append(df.reset_index(),ignore_index=True)\n    \n#     if test:\n#         break\n\n    if save_to_file:\n        df.to_csv(output_file)\n\ndf_main = df_main.set_index('paper_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_main","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_main.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_main.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_main.to_csv(\"main_output.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Weight Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_main.cummulative_sum)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Weight Distribution by weight type"},{"metadata":{"trusted":true},"cell_type":"code","source":"for weight_col in list(filter(lambda x: x if 'weight_' in x else None, df_main.columns )):\n    print(weight_col)\n    plt.hist(df_main[weight_col])\n    plt.show()\n    print(\"---\"*10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"value_count_dict = {}\nfor weight_col in list(filter(lambda x: x if 'weight_' in x else None, df_main.columns )):\n    value_count_dict[weight_col] = df_main[weight_col].value_counts().to_dict()\nprint(value_count_dict)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nimport plotly.express as px\nmapping = {\n   'weight_study_type'  :   ['Unknown','Good','Better','Best'],\n   'weight_subject_type':   ['Unknown','Lab','Animal','Human'],\n   'weight_cohort_info'  :   ['Unknown','Good','Better','Best'],\n   'weight_affliation':     ['200+ or unknown','100-200','21-100','Top 20'],    \n   'weight_significance':   ['Unknown','Weak','Good','Strong'] \n} \nfor weight_type in mapping.keys(): \n    keys = list(value_count_dict[weight_type].keys())\n    y = list(value_count_dict[weight_type].values())\n    sorted_key_index = np.argsort(list(value_count_dict[weight_type].keys()))\n    x = [mapping[weight_type][i] for i in keys]\n    if weight_type=='weight_significance':\n        x = mapping[weight_type]\n        w = value_count_dict[weight_type]\n        y = [ \n             w.get(0,0),\n             w.get(-3,0)+w.get(-2,0)+w.get(-1,0)  ,\n             w.get(1,0)+w.get(2,0),\n             w.get(3,0)\n            ]\n        fig = px.bar(x=x, \n                     y=y, \n                     labels={'x':weight_type, 'y':'count'})\n    elif weight_type=='weight_affliation' :\n        fig = px.bar(x=np.array(x)[sorted_key_index.tolist()].tolist()[1:], \n             y=np.array(y)[sorted_key_index.tolist()].tolist()[1:], \n             labels={'x':weight_type, 'y':'count'})\n    else:\n        fig = px.bar(x=np.array(x)[sorted_key_index.tolist()].tolist(), \n                     y=np.array(y)[sorted_key_index.tolist()].tolist(), \n                     labels={'x':weight_type, 'y':'count'})\n    fig.show()\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":4}