{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Part I - Brief Intro / Libraries<br>\nPart II - Variables <br>\nPart III - Visualizations<br>\nPart IV - Clustering (KMeans and Hierarchical)<br>\nPart V - Centroids and Profiling<br>**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Part I - Brief Intro / Libraries\nAs described in the dataset description the main goal is to use the supermarket data to find customer clusters. After finding specific clusters, the company can run personalized campaigns targeting each group. <br>\nThe notebook will first go through some basic descriptive statistics about the variables and later the clusters will be developed. The methods used are KMeans and Hierarchical Clustering.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.manifold import TSNE\nimport scipy.cluster.hierarchy as sch\nmall=mall = pd.read_csv(\"../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv\")\nmall.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part II - Variables\n$First$ $glance$ $at$ $variables:$ <br>\nTheses are initial thoughts, for further details check the table and charts below.<br>\n$Age$<br>\nAge ranges between 18 and 70 years with an average of 38.85 and median of 36. The distribution roughly shows more people of younger ages and less people as age increases. <br>\n$Income$<br>\nIncome ranges between 15k and 137k with an average of 60.56 and median of 61.50. The distribution shows that very few people have a large amount of income (>100k) and most of people earning between 40k and 90k.<br>\n$Spending Score$ <br>\nSpending score distribution seems to be more symmetric with an average of 50.2 and median of 50. It resembles more a normal curve.<br>\n$Gender$ <br>\nGender is distributed as 56% females and 44% males.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mall.shape)\nmall.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distributions - Age, Income and Spending Score\n\n# General fig\nfig, axs =plt.subplots(2,2,figsize=(10,10))\n\n#Age\naxs[0,0].hist(mall['Age'],bins=15,color='#e6a340')\naxs[0,0].set_title('Age Histogram')\naxs[0,0].set_xlabel('Age')\naxs[0,0].set_ylabel('Freq')\n\n# Income\naxs[0,1].hist(mall[\"Annual Income (k$)\"],bins=15,color='#8be640')\naxs[0,1].set_title('Income Histogram')\naxs[0,1].set_xlabel('Income')\naxs[0,1].set_ylabel('Freq')\n\n# Spending\naxs[1,0].hist(mall[\"Spending Score (1-100)\"],bins=15,color='#40a1e6')\naxs[1,0].set_title('Spending Histogram')\naxs[1,0].set_xlabel('Spending')\naxs[1,0].set_ylabel('Freq')\n\n# Gender\n# First creating a table to summarize information\ngender_sum=mall.groupby(['Gender']).Gender.count().to_frame('Count').reset_index()\naxs[1,1].pie(gender_sum.Count,labels=gender_sum.Gender,autopct='%1.1f%%',colors=('#e64040','#40a1e6'))\naxs[1,1].set_title('Gender')\n\n# General Title\nfig.suptitle('Variables Distribution',size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Part III - Visualizations\nIn this section, was to plot some combination of variables to see if there was any pattern, correlation, or difference between genders. In this case, there was no difference between women and men in terms of age, income, and spending score distribution. They are essentially the same. For \"age x income\" and \"age x spending score\" there is no apparent correlation. However, between \"income x spending score\" it seems that there are some observable groups forming a \"X\".<br>\nSomething that I found curious was the positive correlation between Customer_ID and income. In my opinion, this relation should not be observable since Customer_ID is only used to identify a customer.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# The big figure\nfig, axs =plt.subplots(3,2,figsize=(10,17))\n\n# Age\n# Age - histogram by gender\nmetric=\"Age\"\naxs[0,0].hist(mall.query(\"Gender == 'Male'\")[metric],alpha=0.4, bins=20, label='M',color='#40a1e6')\naxs[0,0].hist(mall.query(\"Gender == 'Female'\")[metric],alpha=0.4, bins=20,label='F',color='#e64040')\naxs[0,0].set_title('Age Histogram by Gender')\naxs[0,0].set_xlabel(metric)\naxs[0,0].set_ylabel('Freq')\naxs[0,0].legend()\n\n# Age - Boxplot by gender\nage_boxplot=[mall.query(\"Gender == 'Male'\")[metric],\n             mall.query(\"Gender == 'Female'\")[metric]]\naxs[0,1].boxplot(age_boxplot)\naxs[0,1].set_xticklabels(['Male','Female'])\naxs[0,1].set_ylabel(metric)\naxs[0,1].set_title('Age Boxplot by Gender')\n\n\n#############################################################\n# Income\n#Income - histogram by gender\nmetric=\"Annual Income (k$)\"\naxs[1,0].hist(mall.query(\"Gender == 'Male'\")[metric],alpha=0.4, bins=20, label='M',color='#40a1e6')\naxs[1,0].hist(mall.query(\"Gender == 'Female'\")[metric],alpha=0.4, bins=20,label='F',color='#e64040')\naxs[1,0].set_title('Income Histogram by Gender')\naxs[1,0].set_xlabel(metric)\naxs[1,0].set_ylabel('Freq')\naxs[1,0].legend()\n\n# Income - boxplot by gender\nincome_boxplot=[mall.query(\"Gender == 'Male'\")[metric],\n             mall.query(\"Gender == 'Female'\")[metric]]\naxs[1,1].boxplot(income_boxplot)\naxs[1,1].set_xticklabels(['Male','Female'])\naxs[1,1].set_ylabel(metric)\naxs[1,1].set_title('Income Boxplot by Gender')\n##############################################################\n# Spending Score\n# Spending Score - histogram by gender\nmetric=\"Spending Score (1-100)\"\naxs[2,0].hist(mall.query(\"Gender == 'Male'\")[metric],alpha=0.4, bins=20, label='M',color='#40a1e6')\naxs[2,0].hist(mall.query(\"Gender == 'Female'\")[metric],alpha=0.4, bins=20,label='F',color='#e64040')\naxs[2,0].set_title('Spending Histogram by Gender')\naxs[2,0].set_xlabel(metric)\naxs[2,0].set_ylabel('Freq')\naxs[2,0].legend()\n\n# Spending Score - boxplot by gender\nspend_boxplot=[mall.query(\"Gender == 'Male'\")[metric],\n             mall.query(\"Gender == 'Female'\")[metric]]\naxs[2,1].boxplot(spend_boxplot)\naxs[2,1].set_xticklabels(['Male','Female'])\naxs[2,1].set_ylabel(metric)\naxs[2,1].set_title('Spend Boxplot by Gender')\n\n#plot the figure\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(mall,kind='scatter',hue='Gender',palette=('#40a1e6','#e64040'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part IV - Clustering\nThis section is the main focus of the notebook. Here we will be comparing 2 clusters methodologies: KMeans and Hierarchical Clustering. <br>\nFor this dataset both approaches generate very similar results with almost no difference between them.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### K Means\nBefore finding the optimal number of clusters, I arbitrarily set n_clusters as 3 just to have a rough idea how the algorithm would group the customers. With n_clusters = 3 the clustering is not bad (figure below), however it can be improved...  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, just guessing a number of clusters\n\n# Including gender as a dummy variable\nmall['Gender_2']=mall['Gender'].apply(lambda x: 1 if x=='Male' else 0)\n\n#running kmeans with 3 clusters\nvariables=mall[['Age','Annual Income (k$)','Spending Score (1-100)','Gender_2']]\nmodel_kmeans=KMeans(n_clusters=3)\nmodel_kmeans.fit(variables)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting a 2D approximation for the clusters\n# plot the clusters\ntsne=TSNE()\nvisualization=tsne.fit_transform(variables)\nsns.set(rc={'figure.figsize':(5,5)})\nsns.scatterplot(x=visualization[:,0],y=visualization[:,1],\n               hue=model_kmeans.labels_,\n               palette=sns.color_palette('Set1',3))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To improve the above clustering, I used the \"elbow method\" and found that six clusters would be reasonable. <br>\n(I also tried wih 5 clusters, but in my personal opinion 6 resulted in a better segmentation).<br><br>\n\n\n\"elbow method\" - In a nutshell, is the point where any additional cluster does not significantly decrease the error/variation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Figuring out the optimal number of clusters\n# The idea will be running KMeans with different numbers of clusters\n# and compute the error associated with it\n# We will be using the elbow method to determine the exact number of clusters\n\n# First define a function that returns the number of clusters and error\n\ndef k_means_elbow(n_clust,variables):\n    model_kmeans=KMeans(n_clusters=n_clust)\n    model_kmeans.fit(variables)\n    return [n_clust,model_kmeans.inertia_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot Inertia by number of clusters\nelbow=[k_means_elbow(n_cluster,variables) for n_cluster in range (1,20)]\nelbow=pd.DataFrame(elbow,columns=['n_clusters','Inertia'])\nplt.figure(figsize=(5,5))\nelbow['Inertia'].plot()\nplt.title('Inertia by Number of Clusters',size=15)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same code, but 6 clusters instead of 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# repeat the previous process again but with n_cluters=5\nvariables=mall[['Age','Annual Income (k$)','Spending Score (1-100)','Gender_2']]\nn_clust=6\nmodel_kmeans=KMeans(n_clusters=n_clust)\nmodel_kmeans.fit(variables)\n\n# Including the cluster classification in the dataframe\npredict=model_kmeans.predict(variables)\nmall['kmeans_6']=pd.Series(predict,index=mall.index)\n\ntsne=TSNE()\nvisualization=tsne.fit_transform(variables)\nsns.set(rc={'figure.figsize':(5,5)})\nsns.scatterplot(x=visualization[:,0],y=visualization[:,1],\n               hue=model_kmeans.labels_,\n               palette=sns.color_palette('Set1',n_clust))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hierarchical Clustering\nUsing the hierarchical clustering and dendrogram, we can see how the algorithm is grouping all data points by proximity, starting from 200 cluster (each data point is a cluster) and reaching 1 (all data points in a single cluster). Six clusters also seem to be reasonable. <br>\nThe next section will show the differences between both methods.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dendrogram=sch.dendrogram(sch.linkage(variables,method='ward'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variables=mall[['Age','Annual Income (k$)','Spending Score (1-100)','Gender_2']]\nn_clust=6\nmodel_HC=AgglomerativeClustering(n_clusters=n_clust, affinity='euclidean',linkage='ward')\ngroups_HC=model_HC.fit_predict(variables)\ngroups_HC\nmall['HC_6']=pd.Series(groups_HC,index=mall.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne=TSNE()\nvisualization=tsne.fit_transform(variables)\nsns.set(rc={'figure.figsize':(5,5)})\nsns.scatterplot(x=visualization[:,0],y=visualization[:,1],\n               hue=model_HC.labels_,\n               palette=sns.color_palette('Set1',n_clust))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparison between KMeans and Hierarchical Clustering\nIn this case both methods result in very similar clusters. There are only very few exceptions. <br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs =plt.subplots(1,2,figsize=(10,5))\n\n#kmeans plot\na=axs[0].scatter(x=mall[\"Annual Income (k$)\"],y=mall[\"Spending Score (1-100)\"],c=mall['kmeans_6'])\naxs[0].legend(*a.legend_elements())\naxs[0].set_title('KMeans 6 Clusters')\naxs[0].set_xlabel('Income')\naxs[0].set_ylabel('Spending')\n\n#HC plot\nb=axs[1].scatter(x=mall[\"Annual Income (k$)\"],y=mall[\"Spending Score (1-100)\"],c=mall['HC_6'])\naxs[1].legend(*b.legend_elements())\naxs[1].set_title('Hierarchical 6 Clusters')\naxs[1].set_xlabel('Income')\naxs[1].set_ylabel('Spending')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pair plot\nsns.pairplot(mall[['Age','Annual Income (k$)','Spending Score (1-100)','kmeans_6']],kind='scatter',hue='kmeans_6')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part V - Centroids and Profiling\nIn overall, Spending Score and Income can pretty much divide almost all groups. 1 group per \"corner\" in the chart, totaling 4 (groups 0,2,3,4) and 2 groups (1 and 5) in the middle.<br> <br>\n$group$ $1$ $and$ $5$ - have the same income and spending score. The main differences are on the age, group 1 is younger (27 years) vs group 5 older (56 years) and on gender proportion, group 0 (34.21% men) and group 2 (44.44% men).<br>\n$group$ $0$ - seems to be the most valuable customer, since they have a high spending score (82.12) and high income (86.53k). Average age of 32 years. <br>\n$group$ $4$ - is also valuable. Despite their significant lower income (25.72k), they still have high spending scores (79.36). Average age of 25 years. (could be young early career professionals). <br>\n$group$ $3$ - have the highest potential for gain since they have the highest level of income (88.22k) , but the lowest spending score (17.28). Could be an interesting group to run a campaign. Average age of 41 years. <br>\n$group$ $2$ - Seems to be least interesting group to target. They have the lowest income (25.14k) and the second lowest spending score (19.52). Perhaps there are some products that can increase the spending score of this group. Average age of 44 years.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"variables=mall[['Age','Annual Income (k$)','Spending Score (1-100)','Gender_2']]\nsummary_centroids=pd.DataFrame(model_kmeans.cluster_centers_,columns=variables.columns)\nsummary_centroids","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}