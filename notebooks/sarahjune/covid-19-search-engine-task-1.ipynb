{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **COVID-19 Open Research Dataset Challenge (CORD-19)**"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.ovpm.org/wp-content/uploads/2020/03/chla-what-you-should-know-covid-19-1200x628-01.jpg)"},{"metadata":{},"cell_type":"markdown","source":"This is a joint work by [Moshe Hazoom](https://www.kaggle.com/hazoom), [Sarah June Sachs](https://www.kaggle.com/sarahjune) and [Kevin Benassuly](https://www.kaggle.com/kevinbenassuly)."},{"metadata":{},"cell_type":"markdown","source":"# **Goal**\nOur goal is to build an infrastructure that can serve whoever fights the novel COVID-19 virus (researches, doctors, health care workers, etc.) by finding the most useful information using state-of-the-art NLP tools and algorithms. We hope this project will be useful and that our efforts will yield fruits to make our world without the COVID-19 virus. \n"},{"metadata":{},"cell_type":"markdown","source":"# **Project Description**\nThis project consists of different modules that serve together as a full pipeline in order to extract relevant, useful and accurate information from the the scholarly articles.\nWe believe that our solution is capable to mine information that answers the research question accurately, flexible enough in order to support future research questions and easy to understand. \nWe will describe in details the different modules below:\n1. **Data Preprocessing** - ETL, Keyword Extraction & Word Embeddings.\n2. **Topic Modeling** - LDA model.\n3. **Search Engine for Seed Sentences** - Simple but useful search engine to find seed sentences using keywords.\n4. **Semantic Search for Relevant Sentences** - Find relevant answers from seed sentences from #3 using sentence embedding using smooth weighted average of the word embeddings and removal of 1st principal component using SVD.\n5. **Answer Summarization** - Generate abstractive summary for answers using Facebook's BART model.\n\nAll the code and notebooks are availabe in the Github [repo](https://github.com/Hazoom/covid19)."},{"metadata":{},"cell_type":"markdown","source":"# **Pros/Cons**\n\n## Pros\n1. Our solution finds information to all the research questions in accuracte, consice and informative manner.\n2. It's simple to understand, read and reproduce results.\n3. Easy to expand to other research questions and domains.\n\n## Cons\n1. The keyword generation for each task can be improved to better use the search engine for seed sentences.\n2. There is some manual work needed to pick the best seed sentences for each sub-task in each task. In most cases, we took the first 1-3 sentences that the search engine returned, but it wasn't the case for all search queries. At this point, after that we marked for each query the best results out of 10, we can better improve the search engine against gold-data set with a defined evaluation metric for information retrieval tasks (e.g. [nDCG - Normalized Discounted Cumulative Gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)) . We believe this can be further improved and might be an interesting direction for future research."},{"metadata":{},"cell_type":"markdown","source":"# **Data Preprocessing**\n\n## Article Filtering\nIn order to be focusing on relevant articles only, we filtered out articles that are not related specificaly to COVID-19 by using [@ajrwhite](https://www.kaggle.com/ajrwhite)'s list of keywords (thanks!) and took only articles that contains at least one word from the following list:"},{"metadata":{"trusted":true},"cell_type":"code","source":"[\"2019-ncov\", \"2019 novel coronavirus\", \"coronavirus 2019\", \"coronavirus disease 19\", \"covid-19\", \"covid 19\", \"ncov-2019\", \"sars-cov-2\", \"wuhan coronavirus\", \"wuhan pneumonia\", \"wuhan virus\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ETL\nFor each article we did the following:\n1. Parsed its full text using [scispacy](https://allenai.github.io/scispacy/) and split it into sentence. The sentence segmentation part was done using Microsoft's [BlingFire](https://github.com/microsoft/BlingFire) library since we've noticed that scispacy had difficulties to split some text into sentences and kept a very long text. The Code for sentence segmentation is available in [blingfire_sentence_splitter.py](https://github.com/Hazoom/covid19/blob/master/src/nlp/blingfire_sentence_splitter.py) and [common_sentence_splitter.py](https://github.com/Hazoom/covid19/blob/master/src/nlp/common_sentence_splitter.py).\n2. Cleaned the text by normalizing non ASCII characters, fixing contractions, removing URLs, removing punctuations, removing stop-words, etc. The cleaning code is available in [cleaning.py](https://github.com/Hazoom/covid19/blob/master/src/nlp/cleaning.py).\n3. Transformed the sentence to contains meaningful bi-grams and tri-grams. Detailed explanation below.\n4. Created a metadata CSV file such that each row contains a sentence, its cleaned version, the section it came from (abstract, body) and the article metadata it came from. The code is available in [preprocess.py](https://github.com/Hazoom/covid19/blob/master/src/preprocessing/preprocess.py)."},{"metadata":{},"cell_type":"markdown","source":"The parsing method with scispacy (for demonstration purposes, it doesn't include our custom sentence segmantation):"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install scispacy package\n!pip install scispacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nimport scispacy\n\nnlp = spacy.load(\"../input/scispacymodels/en_core_sci_sm/en_core_sci_sm-0.2.4\")\nnlp.max_length = 2000000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cleaning method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install contractions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nCURRENCIES = {'$': 'USD', 'zł': 'PLN', '£': 'GBP', '¥': 'JPY', '฿': 'THB',\n              '₡': 'CRC', '₦': 'NGN', '₩': 'KRW', '₪': 'ILS', '₫': 'VND',\n              '€': 'EUR', '₱': 'PHP', '₲': 'PYG', '₴': 'UAH', '₹': 'INR'}\n\nRE_NUMBER = re.compile(\n    r\"(?:^|(?<=[^\\w,.]))[+–-]?\"\n    r\"(([1-9]\\d{0,2}(,\\d{3})+(\\.\\d*)?)|([1-9]\\d{0,2}([ .]\\d{3})+(,\\d*)?)|(\\d*?[.,]\\d+)|\\d+)\"\n    r\"(?:$|(?=\\b))\")\n\nRE_URL = re.compile(\n    r'((http://www\\.|https://www\\.|http://|https://)?' +\n    r'[a-z0-9]+([\\-.][a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(/.*)?)')\n\n# English Stop Word List (Standard stop words used by Apache Lucene)\nSTOP_WORDS = {\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\",\n              \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\",\n              \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nfrom typing import List\nimport ftfy\nimport contractions\n\ndef clean_tokenized_sentence(tokens: List[str],\n                             unicode_normalization=\"NFC\",\n                             unpack_contractions=False,\n                             replace_currency_symbols=False,\n                             remove_punct=True,\n                             remove_numbers=False,\n                             lowercase=True,\n                             remove_urls=True,\n                             remove_stop_words=True) -> str:\n    if remove_stop_words:\n        tokens = [token for token in tokens if token not in STOP_WORDS]\n\n    sentence = ' '.join(tokens)\n\n    if unicode_normalization:\n        sentence = ftfy.fix_text(sentence, normalization=unicode_normalization)\n\n    if unpack_contractions:\n        sentence = contractions.fix(sentence, slang=False)\n\n    if replace_currency_symbols:\n        for currency_sign, currency_tok in CURRENCIES.items():\n            sentence = sentence.replace(currency_sign, f'{currency_tok} ')\n\n    if remove_urls:\n        sentence = RE_URL.sub('_URL_', sentence)\n\n    if remove_punct:\n        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n\n    # strip double spaces\n    sentence = re.sub(r' +', ' ', sentence)\n\n    if remove_numbers:\n        sentence = RE_NUMBER.sub('_NUMBER_', sentence)\n\n    if lowercase:\n        sentence = sentence.lower()\n\n    return sentence\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Putting it all together:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_sentence(sentence) -> str:\n    doc = nlp(sentence)\n    tokens = [str(token) for token in doc]\n    return clean_tokenized_sentence(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clean_sentence(\"Let's clean this sentence!\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the output of ETL process:\n\nperhaps add more explanation here?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences_df = pd.read_csv('../input/covid19sentencesmetadata/sentences_with_metadata.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Sentence count: {len(sentences_df)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training bi-gram Model\nSome words on its own doesn't give a lot of information, but when coming together, the meaning is changing to something else. Our goal was to transform meaningful bi-gram phrases to one token, for example: `fake news` to `fake_news`. For that, we used [Gensim](https://radimrehurek.com/gensim/)'s Phrases package, that has two implementations: 1. Data-Driven approach and 2. NPMI (Normalized Pointwise Mutual Information) score. We won't show here the training, since it takes times, but we will load our trained model and see some examples. The trainind code is in our [notebook](https://github.com/Hazoom/covid19/blob/master/notebooks/Taxonomy/Topic_Model_LDA.ipynb).\nWe used a threshold of 10 and minimum count of 5 that worked best for our use case, to build a search engine. One can play with the hyper-parameters for his own use-case, depending on the tradeoff between large number (and less meaningful) of phrases to a smaller (but more meaningful) amount of phrases."},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.phrases import Phraser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_model = Phraser.load(\"../input/covid19phrasesmodels/covid_bigram_model_v0.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_model[\"despite social media often vehicle fake news boast news hype also worth noting tremendous effort scientific community provide free uptodate information ongoing studies well critical evaluations\".split()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training tri-gram model\nWe created a tri-gram model, in addition to the bi-gram model, in order to catch more meaningful phrases, like `Epidemic Preparedness Innovations` for example. We transformed the cleaned sentences (from all articles) with bi-grams using the model above and trained a Phrases model with Gensim again, but with a lower threshold this time. Please note that this method can also crerate 4-grams if the model connects between two bi-grams."},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram_model = Phraser.load(\"../input/covid19phrasesmodels/covid_trigram_model_v0.pkl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add phrases model to the ETL process and change the clean function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_sentence(sentence) -> str:\n    doc = nlp(sentence)\n    tokens = [str(token) for token in doc]\n    cleaned_sentence = clean_tokenized_sentence(tokens)\n    sentence_with_bigrams = bigram_model[cleaned_sentence.split(' ')]\n    sentence_with_trigrams = trigram_model[sentence_with_bigrams]\n    return ' '.join(sentence_with_trigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clean_sentence(\"On 23 January 2020, the Coalition for Epidemic Preparedness Innovations (CEPI) announced that they will fund vaccine development programmes with Inovio\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FastText Word Embeddings\nWe trained word embeddings model on the full corpus (without filtering out articles) using Facebook's [FastText](https://github.com/facebookresearch/fastText) library. This will serve us later in the Sentence Similarity model to find relevant answers for each question. After training, we also created word counts that serves us in the sentence encoder when calculating the smooth weighted average of the word embeddings of all words in the sentence.\nThe code for training word embeddings using FastText is availabe at [train_fasttext.py](https://github.com/Hazoom/covid19/blob/master/src/w2v/train_fasttext.py)\n\nLet's visualize our word embeddings. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.preprocessing import normalize\nfrom sklearn.manifold import TSNE\nfrom matplotlib import pylab\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fasttext_model_dir = '../input/fasttext-no-subwords-trigrams'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read the 400 most frequent word vectors. The vectors in the file are in descending order of frequency."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_points = 400\n\nfirst_line = True\nindex_to_word = []\nwith open(os.path.join(fasttext_model_dir, \"word-vectors-100d.txt\"),\"r\") as f:\n    for line_num, line in enumerate(f):\n        if first_line:\n            dim = int(line.strip().split()[1])\n            word_vecs = np.zeros((num_points, dim), dtype=float)\n            first_line = False\n            continue\n        line = line.strip()\n        word = line.split()[0]\n        vec = word_vecs[line_num-1]\n        for index, vec_val in enumerate(line.split()[1:]):\n            vec[index] = float(vec_val)\n        index_to_word.append(word)\n        if line_num >= num_points:\n            break\nword_vecs = normalize(word_vecs, copy=False, return_norm=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train t-SNE in order to reduce embeddings to 2-dimension for visualization purpose:"},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(perplexity=40, n_components=2, init='pca', n_iter=10000)\ntwo_d_embeddings = tsne.fit_transform(word_vecs[:num_points])\nlabels = index_to_word[:num_points]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot the most frequent 400 word vectors in a 2-dimensions plot:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot(embeddings, labels):\n    pylab.figure(figsize=(20,20))\n    for i, label in enumerate(labels):\n        x, y = embeddings[i,:]\n        pylab.scatter(x, y)\n        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n                       ha='right', va='bottom')\n    pylab.show()\n\nplot(two_d_embeddings, labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The visualization of the word vectors of the 400 most frequent words makes sense.\nWe can use those vectors to find synonyms, or related terms, for each input word (or phrase) by comparing its word vectors to the whole corpus vectors using cosine similarity, or any other vectors similarity functions.\nLet's see some examples:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pprint import pprint\nimport gensim.models.keyedvectors as word2vec\n\nfasttext_model = word2vec.KeyedVectors.load_word2vec_format(os.path.join(fasttext_model_dir, \"word-vectors-100d.txt\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_most_similar(search_term):\n    print(f\"Synonyms of '{search_term}':\")\n    synonyms = fasttext_model.most_similar(search_term)\n    pprint(synonyms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_most_similar(\"new_coronavirus\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_most_similar(\"fake_news\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_most_similar(\"pathogen\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Topic Modeling & Keyword Extraction**\nIn order to understand the content of the corpus and how the text might relate to each task, we extracted relevant topics with the Latent Dirichlet Allocation (LDA) algorithm. We used the [Gensim](https://radimrehurek.com/gensim/) package on the clean version of the sentences within the filtered subset of the corpus.\n\n[Notebook with code for topic modeling](https://github.com/Hazoom/covid19/blob/master/notebooks/Taxonomy/Topic_Model_LDA.ipynb)\n\nSteps taken:\n\n1. Read in cleaned sentences\n2. Build Gensim dictionary with id2word\n3. Structure corpus with doc2bow\n4. Calculate term document frequency\n5. Train the LDA model\n6. Using the full filtered subset of articles did not result in very distinctive topics, below is a snippet with keywords from the 10 topics which were derived:"},{"metadata":{"trusted":true},"cell_type":"code","source":"[(0, '0.079\"•\" + 0.019\"blood\" + 0.015\"associated\" + 0.013\"cells\" + ' '0.012\"ace2\" + 0.012\"protein\" + 0.011\"important\" + 0.011\"levels\" + ' '0.010\"diseases\" + 0.010\"cell\"'), (1, '0.110\"who\" + 0.088\"it\" + 0.056\"response\" + 0.043\"could\" + 0.036\"under\" ' '+ 0.035\"available\" + 0.032\"major\" + 0.032\"as\" + 0.030\"without\" + ' '0.024\"muscle\"'), (2, '0.173\"■\" + 0.020\"some\" + 0.013\"drugs\" + 0.010\"transmission\" + ' '0.009\"surgery\" + 0.009\"must\" + 0.009\"drug\" + 0.009\"there\" + ' '0.008\"increased\" + 0.008\"high\"'), (3, '0.071\"de\" + 0.036\"were\" + 0.025\"patient\" + 0.023\"1\" + 0.022\"after\" + ' '0.018\"a\" + 0.018\"more\" + 0.015\"all\" + 0.015\"when\" + 0.014\"cause\"'), (4, '0.044\"the\" + 0.035\"from\" + 0.028\"should\" + 0.019\"other\" + 0.018\"risk\" ' '+ 0.017\"oral\" + 0.017\"which\" + 0.017\"in\" + 0.013\"use\" + 0.013\"cases\"'), (5, '0.069\"may\" + 0.033\"can\" + 0.031\"have\" + 0.029\"disease\" + 0.028\"dental\" ' '+ 0.022\"also\" + 0.020\"has\" + 0.020\"been\" + 0.018\"health\" + ' '0.016\"virus\"'), (6, '0.051\"la\" + 0.031\"en\" + 0.025\"2\" + 0.023\"3\" + 0.016\"que\" + 0.016\"el\" ' '+ 0.016\"y\" + 0.014\"los\" + 0.014\"4\" + 0.013\"les\"'), (7, '0.045\"s\" + 0.041\"et\" + 0.031\"during\" + 0.023\"al\" + 0.022\"had\" + ' '0.021\"people\" + 0.020\"à\" + 0.018\"local\" + 0.017\"days\" + 0.016\"2020\"'), (8, '0.062\"patients\" + 0.030\"treatment\" + 0.028\"care\" + 0.020\"used\" + ' '0.014\"clinical\" + 0.014\"infection\" + 0.013\"common\" + 0.013\"severe\" + ' '0.013\"respiratory\" + 0.012\"dentistry\"'), (9, '0.030\"using\" + 0.020\"areas\" + 0.018\"ct\" + 0.014\"described\" + ' '0.014\"performed\" + 0.013\"lesions\" + 0.013\"above\" + 0.012\"day\" + ' '0.011\"learning\" + 0.011\"reactions\"')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The lack of distinctive topics is likely due to the corpus range of content which contained a lot of noise. If we run the same LDA on the output from the semantic search for relevant sentences, hopefully clearer topics will emerge. This is an interesting research direction for the future."},{"metadata":{},"cell_type":"markdown","source":"# **Search Engine for Seed Sentences**\nNow, our goal was to find the best (accurate and informative) sentences for each sub-task for each task in the CORD-19 challenge.\nFor that goal, we did the following:\n1. Created a search engine for finding relevant sentences using input keywords. The search engine transforms the input to phrases using our phrases model above and performs Query Expansion technique by adding synonyms (above certain similarity threshold) to the input keywords and ranks sentences by the number of keyword matches and the date of the article the sentence came from (the newest will be ranked higer). In order to focus on articles about COVID-19, the search engine can get optional keywords that boost the sentences containing them. We've tried to use TF-IDF and some other weighting techniques, but our simple method worked best for us.\n2. For each sub-task we created list of keywords the retrieves for us the best result set of sentences that answer the resaerch question in the sub-task.\n3. Picked 1-3 sentences (out of 10) that summarizes the answer in the most accurate, consice and informative manner. Those sentences are considered \"seed sentences\" and will serve us in the next module of sentence similarity. This part was done partially manually and we belive this approach can be automatic with better keyword extraction and a better search engine. This is an interesting area for future research.\n\nAll the examples are listed in the `notebooks` folder in our repo."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_articles_metadata_mapping(sentences_df: pd.DataFrame) -> dict:\n    sentence_id_to_metadata = {}\n    for row_count, row in sentences_df.iterrows():\n        sentence_id_to_metadata[row_count] = dict(\n            paper_id=row['paper_id'],\n            cord_uid=row['cord_uid'],\n            source=row['source'],\n            url=row['url'],\n            publish_time=row['publish_time'],\n            authors=row['authors'],\n            section=row['section'],\n            sentence=row['sentence'],\n        )\n    return sentence_id_to_metadata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_id_to_metadata = create_articles_metadata_mapping(sentences_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator\nfrom datetime import datetime\n\nclass SearchEngine:\n    def __init__(self,\n                 sentence_id_to_metadata: dict,\n                 sentences_df: pd.DataFrame,\n                 bigram_model,\n                 trigram_model,\n                 fasttext_model):\n        self.sentence_id_to_metadata = sentence_id_to_metadata\n        self.cleaned_sentences = sentences_df['cleaned_sentence'].tolist()\n        print(f'Loaded {len(self.cleaned_sentences)} sentences')\n\n        self.bigram_model = bigram_model\n        self.trigram_model = trigram_model\n        self.fasttext_model = fasttext_model\n\n    def _get_search_terms(self, keywords, synonyms_threshold):\n        # clean tokens\n        cleaned_terms = [clean_tokenized_sentence(keyword.split(' ')) for keyword in keywords]\n        # remove empty terms\n        cleaned_terms = [term for term in cleaned_terms if term]\n        # create bi-grams\n        terms_with_bigrams = self.bigram_model[' '.join(cleaned_terms).split(' ')]\n        # create tri-grams\n        terms_with_trigrams = self.trigram_model[terms_with_bigrams]\n        # expand query with synonyms\n        search_terms = [self.fasttext_model.most_similar(token) for token in terms_with_trigrams]\n        # filter synonyms above threshold (and flatten the list of lists)\n        search_terms = [synonym[0] for synonyms in search_terms for synonym in synonyms\n                        if synonym[1] >= synonyms_threshold]\n        # expand keywords with synonyms\n        search_terms = list(terms_with_trigrams) + search_terms\n        return search_terms\n\n    def search(self,\n               keywords: List[str],\n               optional_keywords=None,\n               top_n: int = 10,\n               synonyms_threshold=0.7,\n               keyword_weight: float = 3.0,\n               optional_keyword_weight: float = 0.5) -> List[dict]:\n        if optional_keywords is None:\n            optional_keywords = []\n\n        search_terms = self._get_search_terms(keywords, synonyms_threshold)\n\n        optional_search_terms = self._get_search_terms(optional_keywords, synonyms_threshold) \\\n            if optional_keywords else []\n\n        print(f'Search terms after cleaning, bigrams, trigrams and synonym expansion: {search_terms}')\n        print(f'Optional search terms after cleaning, bigrams, trigrams and synonym expansion: {optional_search_terms}')\n\n        date_today = datetime.today()\n\n        # calculate score for each sentence. Take only sentence with at least one match from the must-have keywords\n        indexes = []\n        match_counts = []\n        days_diffs = []\n        for sentence_index, sentence in enumerate(self.cleaned_sentences):\n            sentence_tokens = sentence.split(' ')\n            sentence_tokens_set = set(sentence_tokens)\n            match_count = sum([keyword_weight if keyword in sentence_tokens_set else 0\n                               for keyword in search_terms])\n            if match_count > 0:\n                indexes.append(sentence_index)\n                if optional_search_terms:\n                    match_count += sum([optional_keyword_weight if keyword in sentence_tokens_set else 0\n                                       for keyword in optional_search_terms])\n                match_counts.append(match_count)\n                article_date = self.sentence_id_to_metadata[sentence_index][\"publish_time\"]\n\n                if article_date == \"2020\":\n                    article_date = \"2020-01-01\"\n\n                article_date = datetime.strptime(article_date, \"%Y-%m-%d\")\n                days_diff = (date_today - article_date).days\n                days_diffs.append(days_diff)\n\n        # the bigger the better\n        match_counts = [float(match_count)/sum(match_counts) for match_count in match_counts]\n\n        # the lesser the better\n        days_diffs = [(max(days_diffs) - days_diff) for days_diff in days_diffs]\n        days_diffs = [float(days_diff)/sum(days_diffs) for days_diff in days_diffs]\n\n        index_to_score = {}\n        for index, match_count, days_diff in zip(indexes, match_counts, days_diffs):\n            index_to_score[index] = 0.7 * match_count + 0.3 * days_diff\n\n        # sort by score descending\n        sorted_indexes = sorted(index_to_score.items(), key=operator.itemgetter(1), reverse=True)\n\n        # take only the sentence IDs\n        sorted_indexes = [item[0] for item in sorted_indexes]\n\n        # limit results\n        sorted_indexes = sorted_indexes[0: min(top_n, len(sorted_indexes))]\n\n        # get metadata for each sentence\n        results = []\n        for index in sorted_indexes:\n            results.append(self.sentence_id_to_metadata[index])\n        return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_engine = SearchEngine(sentence_id_to_metadata, sentences_df, bigram_model, trigram_model, fasttext_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def search(keywords, optional_keywords=None, top_n=10, synonyms_threshold=0.8, only_sentences=False):\n    print(f\"\\nSearch for terms {keywords}\\n\\n\")\n    results = search_engine.search(\n        keywords, optional_keywords=optional_keywords, top_n=top_n, synonyms_threshold=synonyms_threshold\n    )\n    print(\"\\nResults:\\n\")\n    \n    if only_sentences:\n        for result in results:\n            print(result['sentence'] + \"\\n\")\n    else:\n        pprint(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search(keywords=[\"spillover\", \"bats\", \"snakes\", \"exotic animals\", \"seafood\"],\n       optional_keywords=[\"new coronavirus\", \"coronavirus\", \"covid19\"],\n      top_n=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At the end of this stage, we had seed sentences for each sub-task."},{"metadata":{"trusted":true},"cell_type":"code","source":"task_id = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\nwith open(f\"../input/covid19seedsentences/{task_id}.json\") as in_fp:\n    seed_sentences_json = json.load(in_fp)\n\nprint(seed_sentences_json['taskName'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Semantic Search for Relevant Sentences**\nAfter finding seed sentences, we wanted to expand our evidences by finding sentences with similar semantic meaning in the whole corpus (that haven't came up in the search engine from previous phase) in order to collect more information and evidence to support the research question.\n\nThere are many methods and techniques for sentence embedding in the NLP litretature and from our vast experience in the field we chose to use to implement the techniques from the paper [\"A Simple but Tough-to-Beat Baseline for Sentence Embeddings\"](https://openreview.net/forum?id=SyK00v5xx) (Sanjeev Arora, Yingyu Liang, Tengyu Ma). In their work, they use a pretrained word embedding model on unsupervised large corpus and in order to create embedding for the whole sentence, they use a smooth weighted average on the word embeddings of the the words in the sentence, and remove the 1st principal component from the vector after performing a dimension reduction technique (e.g. SVD or PCA). The latter improves to reduce noise from the sentence embeddings.\n\nWe developed a method that gets list of sentences and retrieves similar semantic sentences to them. Because the input is not just one sentence, we had to aggerage them in different manngers. We did research one some aggregation techniques:\n1. Union\n2. Mean\n3. 1st Principal Component (pc_1)\n4. 2nd Principal Component (pc_2)"},{"metadata":{},"cell_type":"markdown","source":"Credit to [Tal Almagor](https://github.com/talmago) and [Kevin Benassuly](https://github.com/KevinBenass) for helping to develop this module.\n\nImplementation of [\"A Simple but Tough-to-Beat Baseline for Sentence Embeddings\"](https://openreview.net/forum?id=SyK00v5xx) (Sanjeev Arora, Yingyu Liang, Tengyu Ma) below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_index_path = \"../input/covid19corpusindex/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import six\nimport io\n\n\nclass WordFreq:\n    \"\"\"\n    A dict-like object to hold word frequencies.\n\n    Usage example:\n\n    freqs = WordFreq.from_counts('/path/to/word_freq.txt')\n    freqs['the']\n    0.0505408583229405\n\n    Once created you can use it for weighted average sentence encoding:\n\n    encoder = SentenceEncoder(..., word_freq=freqs.__getitem__)\n    \"\"\"\n\n    def __init__(self, word_freq):\n        self.word_freq = word_freq\n\n    def __getitem__(self, arg):\n        return self.word_freq.get(arg, 0.0)\n\n    @classmethod\n    def from_counts(cls, fname):\n        total = 0\n        cnts = dict()\n        with open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') as fin:\n            for line in fin:\n                word, cnt = line.rstrip().split(' ')\n                cnt = int(cnt)\n                total += cnt\n                cnts[word] = cnt\n        word_freq = {word: cnt / total for word, cnt in cnts.items()}\n        return cls(word_freq)\n\n\nclass SimpleEncoder:\n    def __init__(self,\n                 word_embeddings: dict,\n                 word_embedding_dim: int = 100,\n                 preprocessor: callable = lambda s: s,\n                 tokenizer: callable = lambda s: s.split(),\n                 word_freq: callable = lambda w: 0.0,\n                 weighted: bool = True,\n                 alpha: float = 1e-3):\n        \"\"\"\n        Sentence encoder as a smooth average of word vectors.\n\n        Args:\n            word_embeddings (dict): map words to their vector representation.\n            word_embedding_dim (int): word embedding size. default is 200.\n            preprocessor (callable): optional, a callable to pre-process sentence before tokenizing into words.\n            tokenizer (callable): optional, a callable which splits a sentence into words.\n            word_freq (callable): optional, a callable which map a word to its frequency in range [0 - 1]\n            weighted (bool): optional, whether or not to use weighted average. default is True.\n            alpha (bool): smoothing alpha for Out-of-Vocab tokens.\n\n        Usage example (1 - bag-of-words average):\n        -----------------\n            w2v_path = '/path/to/vectors.txt'\n            encoder = SimpleEncoder.from_w2v(w2v_path)\n            encoder.encode('a sentence is here')\n\n        Usage example (2 - Smooth Inverse Frequency average):\n        -----------------\n            w2v_path = '/path/to/vectors.txt'\n            word_freq = WordFreq.from_counts('/path/to/word_freq.txt')\n            encoder = SimpleEncoder.from_w2v(w2v_path, weighted=True, word_freq=word_freq.__getitem__)\n            encoder.encode('a sentence is here')\n\n        Usage example (3 - Smooth Inverse Frequency average + removing 1st component):\n        -----------------\n            w2v_path = '/path/to/vectors.txt'\n            word_freq = WordFreq.from_counts('/path/to/word_freq.txt')\n            encoder = SimpleEncoder.from_w2v(w2v_path, weighted=True, word_freq=word_freq.__getitem__)\n            corpus = ['sentence a', 'sentence b']\n            emb = encoder.encode(corpus)\n            encoder.components_ = svd_components(emb, n_components=1)\n            emb = encoder.encode(corpus)  # re-calculate embeddings\n            encoder.encode('a sentence is here')\n\n        \"\"\"\n        # word embeddings (filename)\n        self.word_embeddings = word_embeddings\n\n        # word embedding dim (e.g 200)\n        self.word_embedding_dim = word_embedding_dim\n\n        # sentence tokenizer (callable)\n        self.tokenizer = tokenizer\n\n        # preprocessor (callable)\n        self.preprocessor = preprocessor\n\n        # word frequency (callable)\n        self.word_freq = word_freq\n\n        # yes/no: tf-idf weighted average\n        self.weighted = weighted\n\n        # smoothing alpha\n        self.alpha = alpha\n\n        # principal components (pre-calc)\n        self.components_ = None\n\n    def __str__(self):\n        components_dim = self.components_.shape if self.components_ is not None else None\n        return (f\"<SimpleEncoder(dim={self.word_embedding_dim}, \"\n                f\"weighted={self.weighted}, \"\n                f\"alpha={self.alpha}, \"\n                f\"components_dim={components_dim})>\")\n\n    @classmethod\n    def load(cls, w2v_path, word_count_path, principal_components_path):\n        \"\"\"Initialize an instance of `cls`.\n\n        Returns:\n            SimpleEncoder\n        \"\"\"\n        encoder = cls.from_w2v(w2v_path,\n                               tokenizer=lambda s: s.split(),\n                               preprocessor=lambda s: s)\n\n        encoder.load_word_counts(word_count_path)\n\n        encoder.load_components(principal_components_path)\n\n        return encoder\n\n    @classmethod\n    def from_w2v(cls, w2v_path, **init_kwargs):\n        \"\"\"Create a sentence encoder from word embeddings saved to disk.\n\n        Args:\n            w2v_path (str): filename of the word vectors.\n            init_kwargs: additional keyword arguments to ```init``` method.\n\n        Returns:\n            SimpleEncoder\n        \"\"\"\n        word_embeddings = {}\n        with open(w2v_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as fin:\n            _, dim = map(int, fin.readline().split())\n            for line in fin:\n                tokens = line.rstrip().split(' ')\n                word_embeddings[tokens[0]] = np.array(tokens[1:], np.float32)\n        return cls(word_embeddings=word_embeddings, word_embedding_dim=dim, **init_kwargs)\n\n    def load_word_counts(self, fname):\n        \"\"\"Load word count file and use it for td-idf weighted average.\n\n        Notice that ```weighted`` must be set to ```True`` in order to use it.\n\n        Args:\n            fname (str): filename.\n        \"\"\"\n        word_freq = WordFreq.from_counts(fname)\n        self.word_freq = word_freq.__getitem__\n\n    def load_components(self, fname):\n        \"\"\"Load pre-computed principal components from a file.\n\n        Args:\n            fname (str): filename (e.g 'components.npy').\n        \"\"\"\n        fd = io.open(fname, mode=\"rb\")\n        self.components_ = np.load(fd)\n\n    def encode(self, sentences) -> np.array:\n        if isinstance(sentences, six.string_types):\n            sentences = [sentences]\n        emb = np.stack([self._encode(sentence) for sentence in sentences])\n        if self.components_ is not None:\n            emb = emb - emb.dot(self.components_.transpose()).dot(self.components_)\n        return emb\n\n    def _encode(self, sent: str) -> np.array:\n        count = 0\n        sent_vec = np.zeros(self.word_embedding_dim, dtype=np.float32)\n        sent = self.preprocessor(sent)\n        words = self.tokenizer(sent)\n        for word in words:\n            word_vec = self.word_embeddings.get(word)\n            if word_vec is None:\n                continue\n            norm = np.linalg.norm(word_vec)\n            if norm > 0:\n                word_vec *= (1.0 / norm)\n            if self.weighted:\n                freq = self.word_freq(word)\n                word_vec *= self.alpha / (self.alpha + freq)\n            sent_vec += word_vec\n            count += 1\n        if count > 0:\n            sent_vec *= (1.0 / count)\n        return sent_vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We embedded all the sentences in our corpus and saved them in [nmslib](https://github.com/nmslib/nmslib) index for fast similarity comparison.\nWe chose [nmslib](https://github.com/nmslib/nmslib) as our indexing method, but one can implement their own indexing class."},{"metadata":{"trusted":true},"cell_type":"code","source":"import nmslib\nimport numpy as np\nfrom sklearn.decomposition import TruncatedSVD\n\n\ndef linalg_pca(X):\n    \"\"\"PCA transformation with ```np.linalg``` (Singular Value Decomposition).\n\n    Args:\n        X (np.array): 2d array.\n\n    Returns:\n        np.array (2d)\n    \"\"\"\n    # reduce mean\n    X -= np.mean(X, axis=0)\n    # compute covariance matrix\n    cov = np.cov(X, rowvar=False)\n    # compute eigen values & vectors\n    eigen_vals, eigen_vectors = np.linalg.eigh(cov)\n    # sort eigen vectors by eigen values\n    idx = np.argsort(eigen_vals)[::-1]\n    eigen_vectors = eigen_vectors[:, idx]\n    return np.dot(X, eigen_vectors)\n\n\nclass Aggregation:\n    UNION = 'union'\n    AVG = 'average'\n    MEAN = 'mean'\n    PC_1 = 'pc_1'\n    PC_2 = 'pc_2'\n\n    \ndef nn_iter(indices, distances, black=None):\n    \"\"\"Helper method to unpack nearest neighbors lists.\n\n    Args:\n        indices (list): neighbor indices.\n        distances (list): neighbor distances.\n        black (list): black list.\n\n    Returns:\n        list(tuple)\n    \"\"\"\n    for idx, dist in zip(indices, distances):\n        if dist <= 0.0:\n            continue\n        if black is not None and idx in black:\n            continue\n        yield int(idx), float(dist)\n\n\nclass NMSLibCorpusIndex:\n    def __init__(self, dim, metric='cosinesimil', **index_params):\n        \"\"\"Init ```nmslib.FloatIndex```.\n\n        References\n        -----------\n\n        1) Installation\n\n        https://github.com/nmslib/nmslib/tree/master/python_bindings#installation\n\n        2) Supported metrics\n\n        https://github.com/nmslib/nmslib/blob/master/manual/spaces.md\n\n        3) Index params\n\n        https://github.com/nmslib/nmslib/blob/master/manual/methods.md#graph-based-search-methods-sw-graph-and-hnsw\n        \"\"\"\n        self.dim = dim\n        self.index = nmslib.init(method='hnsw', space=metric)\n        self.index_params = index_params or {'post': 0}  # {'post': 2, 'efConstruction': 200, 'M': 25}\n        self._knn_batch_method = frozenset([Aggregation.UNION, Aggregation.AVG, Aggregation.MEAN, Aggregation.PC_1,Aggregation.PC_2])\n\n    def __len__(self):\n        return len(self.index)\n\n    def __repr__(self):\n        return f\"<NMSLibCorpusIndex(size={self.__len__()})>\"\n\n    def load(self, fname, **kwargs):\n        \"\"\"Load an index from disk.\n\n        Args:\n            fname (str): filename.\n            kwargs: additional keyword arguments.\n        \"\"\"\n        self.index.loadIndex(fname, **kwargs)\n\n    def save(self, fname, **kwargs):\n        \"\"\"Save index to disk.\n\n        Args:\n            fname (str): filename.\n            kwargs: additional keyword arguments.\n        \"\"\"\n        self.create_index()\n        self.index.saveIndex(fname, save_data=True)\n\n    def create_index(self):\n        \"\"\"Create ANN Index.\"\"\"\n        self.index.createIndex(self.index_params, print_progress=True)\n\n    def get_vector_by_id(self, idx):\n        \"\"\"Get vector from index by id.\n\n        Args:\n            idx (int): vector id.\n\n        Returns:\n            np.array.\n        \"\"\"\n        return np.array(self.index[idx], np.float32)\n\n    def add_dense(self, dense, ids=None):\n        \"\"\"Add a batch of vectors to the index.\n\n        Args:\n            dense (array-like): array like of vectors (each is a ``np.array``).\n            ids (array-like): array like of indices (each is a ``int``).\n        \"\"\"\n        self._check_dim(dense)\n\n        index_len = self.__len__()\n        self.index.addDataPointBatch(\n            data=dense,\n            ids=ids if ids is not None else np.arange(index_len, index_len + dense.shape[0]))\n\n    def knn_query(self, vec, ids=None, limit=10):\n        \"\"\"Find a set of approximate nearest neighbors to ``vec``.\n\n        Args:\n            vec (np.array): input vector.\n            ids (list): optional, list of indices to filter out from result.\n            limit (int): optional, limit result set size.\n\n        Returns:\n            list[tuple] = (neighbor_id, distance)\n        \"\"\"\n        self._check_dim(vec)\n\n        indices, distances = self.index.knnQuery(vec, k=limit * 2)\n        return sorted(nn_iter(indices, distances, black=ids), key=operator.itemgetter(1))[:limit]\n\n    def _check_batch_method(self, method):\n        assert method in self._knn_batch_method, f\"Invalid KNN batch method: {method}\"\n\n    def knn_query_batch(self, dense, ids=None, limit=10, method='union'):\n        \"\"\"Find a set of approximate nearest neighbors to ``dense``.\n\n        If ```method``` is 'union', than this set will be the top-slice of the union set of all nearest neighbors.\n\n        If ```method``` is 'mean', than this set equals to\n            ```self.knn_query(np.mean(dense, axis=0), ids=ids, limit=limit)```.\n\n        If ```method``` is 'pc_1', than this set equals to\n            ```self.knn_query(_linalg_pca(dense)[0], ids=ids, limit=limit)```.\n\n        If ```method``` is 'pc_2', than this set equals to\n            ```self.knn_query(_linalg_pca(dense)[1], ids=ids, limit=limit)```.\n\n        Args:\n            dense (array-like): array like of vectors (each is a ``np.array``).\n            ids (iterable): optional, list of indices to filter out from result.\n            limit (int): optional, limit result set size.\n            method (str): optional\n\n        Returns:\n            list[tuple] = (neighbor_id, distance)\n        \"\"\"\n        self._check_batch_method(method)\n\n        if method == 'mean':\n            return self.knn_query(np.mean(dense, axis=0), ids=ids, limit=limit)\n        elif method == 'pc_1':\n            return self.knn_query(linalg_pca(dense)[0], ids=ids, limit=limit)\n        elif method == 'pc_2':\n            return self._knn_query_batch(linalg_pca(dense)[:1], ids=ids, limit=limit)\n        elif method == 'union':\n            return self._knn_query_batch(dense, ids=ids, limit=limit)\n        else:  # union\n            return self._knn_query_batch(dense, ids=ids, limit=limit)\n\n    def _knn_query_batch(self, dense, ids=None, limit=10):\n        \"\"\"Find the union set of approximate nearest neighbors to ``dense``.\n\n        Args:\n            dense (array-like): array like of vectors (each is a ``np.array``).\n            ids (list): optional, list of indices to filter out from result.\n            limit (int): optional, limit result set size.\n\n        Returns:\n            list[tuple] = (neighbor_id, distance)\n        \"\"\"\n        self._check_dim(dense)\n\n        nearest_neighbors = []\n        for indices, distances in self.index.knnQueryBatch(dense, k=limit):\n            for idx, dist in nn_iter(indices, distances, black=ids):\n                nearest_neighbors.append((idx, dist))\n        return sorted(nearest_neighbors, key=operator.itemgetter(1))[:limit]\n\n    def _check_dim(self, dense):\n        dim = getattr(self, 'dim', None)\n        if dim:\n            if len(dense.shape) == 2:\n                dense_dim = dense.shape[1]\n            else:\n                dense_dim = dense.shape[0]\n\n            assert dim == dense_dim, f\"expected dense vectors shape to be {dim}, got {dense_dim} instead.\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def infer_dimension_from_corpus_name(fname):\n    match = re.search(r'(\\d+)d', fname)\n    if not match:\n        raise ValueError(f'Could not detect index dimension from {fname}.')\n    dim = int(match.group(1))\n    return dim\n\n\ndef load_corpus_index(fname, dim=None, **load_kwargs):\n    index = None\n    if dim is None:\n        dim = infer_dimension_from_corpus_name(fname)\n    if 'nmslib' in fname:\n        index = NMSLibCorpusIndex(dim=dim)\n    index.load(fname, **load_kwargs)\n    return index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We saved the sentences with their matching article metadata in [SQLite](https://www.sqlite.org/index.html) DB for querying purposes."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy import Column, Integer, String\n\nBase = declarative_base()\n\ndef get_session(conn):\n    \"\"\"Init DB Session.\n    \"\"\"\n    connect_args = {}\n    if conn.startswith('sqlite:///'):\n        connect_args.update({'check_same_thread': False})\n    engine = create_engine(conn, connect_args=connect_args)\n    Session = sessionmaker(bind=engine)\n    return Session()\n\nclass Sentence(Base):\n    __tablename__ = 'sentences'\n\n    id = Column(Integer, name='sentence_id', primary_key=True)\n    sentence = Column(String)\n    paper_id = Column(String, name='paper_id')\n    cord_uid = Column(String, name='cord_uid')\n    publish_time = Column(String, name='publish_time')\n\n    def __repr__(self):\n        return f\"<Sentence(id={self.id}, sentence=\\\"{self.sentence}\\\")>\"\n\n    def to_dict(self):\n        return {\n            'id': self.id,\n            'text': self.sentence,\n            'paper_id': self.paper_id,\n            'cord_uid': self.cord_uid,\n            'publish_time': self.publish_time\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is the class that responsible for our sentence similarity search, that integrates all the components from below together:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import murmurhash\nimport numpy as np\nimport scipy.spatial\n\n\nclass CovidSimilarity:\n    def __init__(self, corpus_index, sentence_encoder, db_session, bigram_model=None, trigram_model=None):\n        self.corpus_index = corpus_index\n        self.sentence_encoder = sentence_encoder\n        self.db_session = db_session\n        self.bigram_model = bigram_model\n        self.trigram_model = trigram_model\n\n    def similar_k(self, input_sentences, limit=10, method='union', group_by='cosine'):\n        \"\"\"Find similar sentences.\n\n        Args:\n            input_sentences (str/list[str]): one or more input sentences.\n            sentence_encoder  : encoder\n            limit (int): limit result set size to ``limit``.\n            corpus_index : type of corpus where to fetch the suggestions from\n            db_session  : Database to get neighbors from\n            method (str): aggregation method ('union', 'mean', 'pc1', 'pc2').\n            group_by (str): distance metric to use to group the result set. Default is 'cosine'.\n\n        Returns:\n            list<dict>\n        \"\"\"\n        res = []\n        nearest = dict()\n\n        cleaned_sentences = [clean_sentence(sentence) for sentence in input_sentences]\n        \n        if self.bigram_model and self.trigram_model:\n            tokenzied_sentences = [sentence.split(' ') for sentence in cleaned_sentences]\n            sentences_with_bigrams = self.bigram_model[tokenzied_sentences]\n            sentences_with_trigrams = self.trigram_model[sentences_with_bigrams]\n            cleaned_sentences = [' '.join(sentence) for sentence in sentences_with_trigrams]\n\n        embeddings = self.sentence_encoder.encode(cleaned_sentences)\n        indices = [murmurhash.hash(sent) for sent in cleaned_sentences]\n\n        for idx, dist in self.corpus_index.knn_query_batch(embeddings, ids=indices, limit=limit, method=method):\n            if idx not in nearest:\n                nearest[idx] = dist\n            else:\n                nearest[idx] = min(nearest[idx], dist)\n\n        for sentence in self.db_session.query(Sentence).filter(Sentence.id.in_(nearest.keys())).all():\n            sentence_dict = sentence.to_dict()\n            encoding = sentence_encoder.encode(sentence.sentence)\n            distances = scipy.spatial.distance.cdist(encoding, embeddings, group_by)\n            nearest_idx = int(np.argmax(distances))\n            sentence_dict['nearest'] = indices[nearest_idx]\n            sentence_dict['dist'] = nearest[sentence.id]\n            res.append(sentence_dict)\n\n        return {\n            'results': sorted(res, key=lambda x: x['dist']),\n            'sentences': [\n                {\n                    'id': sent_id,\n                    'text': sent\n                } for sent_id, sent in zip(indices, cleaned_sentences)\n            ]\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create the SQLite DB session, load our corpus index with `nmslib` and create an instance of our sentence encoder:"},{"metadata":{"trusted":true},"cell_type":"code","source":"db_session = get_session(conn=f\"sqlite:///{os.path.join(corpus_index_path, 'covid19.sqlite')}\")\ncorpus_index = load_corpus_index(os.path.join(corpus_index_path, 'simple-encoder-nmslib-100d.bin'))\nsentence_encoder = SimpleEncoder.load(\n    os.path.join(fasttext_model_dir, \"word-vectors-100d.txt\"),\n    os.path.join(fasttext_model_dir, \"word-counts.txt\"),\n    os.path.join(corpus_index_path, \"simple-encoder-100d-components.npy\")\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to create our COVID-19 sentence similarity object:"},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_similarity = CovidSimilarity(corpus_index, sentence_encoder, db_session, bigram_model, trigram_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check our sentence similarity model on a sample of 2sentence:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences= [\"Origination of 2019-nCoV from bats has been strongly supported but the presumed intermediate host remain to be identified initial reports that 2019-nCoV had an origin in snakes have not been verified\",\n           \"For example, farmed palm civets were suggested to be an intermediate host for SARS to be spilled over to humans although the details on how to link bat and farmed palm civets are unclear [15, 16, 17]\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_similarity.similar_k(sentences, limit=3, method=\"union\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Answer Summarization**\nThe final module in our project is the abstractive answer summarization. The goal is to build informative and consice answer for each sub-task using the relevant sentences we found from previous task. We chose to use Facebook's [BART](https://arxiv.org/abs/1910.13461) model. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. BART is really useful for text generation tasks, hence we chose it for this task. We use HuggingFace's great library [transformers](https://github.com/huggingface/transformers) for that end. A future improvement here can be to fine-tune BART model with COVID-19 articles. Due to lack of resources, we took the pretrained BART model."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom transformers import BartTokenizer, BartForConditionalGeneration\n\nclass BartSummarizer:\n    def __init__(self):\n        self.torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        model_name = 'bart-large-cnn'\n        print(f'Initializing BartTokenizer with model: {model_name} ...')\n        self.tokenizer_summarize = BartTokenizer.from_pretrained(model_name)\n        print(f'Finished initializing BartTokenizer with model: {model_name}')\n\n        print(f'Initializing BartForConditionalGeneration with model: {model_name} ...')\n        self.model_summarize = BartForConditionalGeneration.from_pretrained(model_name)\n        print(f'Finished initializing BartForConditionalGeneration with model: {model_name}')\n        self.model_summarize.to(self.torch_device)\n        self.model_summarize.eval()\n\n    def create_summary(self, text: str,\n                       repetition_penalty=1.0) -> str:\n        text_input_ids = self.tokenizer_summarize.batch_encode_plus(\n            [text], return_tensors='pt', max_length=1024)['input_ids'].to(self.torch_device)\n        summary_ids = self.model_summarize.generate(text_input_ids,\n                                                    num_beams=4,\n                                                    length_penalty=1.2,\n                                                    max_length=1024,\n                                                    min_length=124,\n                                                    no_repeat_ngram_size=4,\n                                                    repetition_penalty=repetition_penalty)\n        summary = self.tokenizer_summarize.decode(summary_ids.squeeze(), skip_special_tokens=True)\n        return summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This will take time on the first time since it downloads the model\nbart_summarizer = BartSummarizer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Putting it all together**\nNow that we have all the components ready, we can visualize the results for the task.\nWe will iterate the different sub-tasks, for each one we will find relevant sentences and create an abstractive summary."},{"metadata":{"trusted":true},"cell_type":"code","source":"answers_results = []\nfor sub_task_json in seed_sentences_json[\"subTasks\"]:\n    sub_task_description = sub_task_json[\"description\"]\n    print(f\"Working on task: {sub_task_description}\")\n    best_sentences = sub_task_json[\"bestSentences\"]\n    relevant_sentences = covid_similarity.similar_k(best_sentences)\n    relevant_sentences_texts = [result[\"text\"] for result in relevant_sentences[\"results\"]]\n    sub_task_summary = bart_summarizer.create_summary(\" \".join(best_sentences))\n    answers_results.append(dict(sub_task_description=sub_task_description, relevant_sentences=relevant_sentences, sub_task_summary=sub_task_summary))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Let's visualize the results:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display, HTML\npd.set_option('display.max_colwidth', 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_summary(summary: str):\n    return display(HTML(f\"<div>{summary}</div>\"))\n\ndef display_sub_task_description(sub_task_description):\n    return display(HTML(f\"<h2>{sub_task_description}</h2>\"))\n\ndef display_task_name(task_name):\n    return display(HTML(f\"<h1>{task_name}</h1>\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_output(seed_sentences, sub_task_json, sentence_id_to_metadata):\n    \"\"\"\n    Prints output for each sub-task\n    \"\"\"\n    # print description\n    display_sub_task_description(sub_task_json.get(\"sub_task_description\"))\n    display_summary(sub_task_json.get(\"sub_task_summary\"))\n    \n    # print output sentences\n    sentence_output = pd.DataFrame(sub_task_json.get('relevant_sentences').get('results'))\n    sentence_output.rename(columns={\"text\": \"Relevant Sentence\",\"cord_id\": \"CORD UID\",\n                                    \"publish_time\": \"Publish Time\", \"id\": \"row_id\"}, inplace=True)\n    sentence_output[\"URL\"] = sentence_output[\"row_id\"].apply(lambda row_id: sentence_id_to_metadata[row_id][\"url\"])\n    sentence_output[\"Source\"] = sentence_output[\"row_id\"].apply(lambda row_id: sentence_id_to_metadata[row_id][\"source\"])\n    \n    display(sentence_output[['cord_uid', 'Source', 'Publish Time', 'Relevant Sentence', 'URL']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize and save all relevant sentences to a dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"display_task_name(seed_sentences_json[\"taskName\"])\nfor idx, sub_task_json in enumerate(answers_results):\n    visualize_output(seed_sentences_json[\"subTasks\"][idx][\"bestSentences\"], sub_task_json, sentence_id_to_metadata)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aggregate all sentences into a dataframe to use LDA on relevant sentences to see if distincitve topics emerge\n\n##### LDA Model Code From:\n\nhttps://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_output(seed_sentences, sub_task_json, sentence_id_to_metadata):\n    \"\"\"\n    Saves output for each sub-task\n    \"\"\"\n    \n    # print output sentences\n    sentence_output = pd.DataFrame(sub_task_json.get('relevant_sentences').get('results'))\n    sentence_output.rename(columns={\"text\": \"Relevant Sentence\",\"cord_id\": \"CORD UID\",\n                                    \"publish_time\": \"Publish Time\", \"id\": \"row_id\"}, inplace=True)\n    sentence_output[\"URL\"] = sentence_output[\"row_id\"].apply(lambda row_id: sentence_id_to_metadata[row_id][\"url\"])\n    sentence_output[\"Source\"] = sentence_output[\"row_id\"].apply(lambda row_id: sentence_id_to_metadata[row_id][\"source\"])\n    \n    return sentence_output[['cord_uid', 'Source', 'Publish Time', 'Relevant Sentence', 'URL']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relevant_sentences = []\nfor idx, sub_task_json in enumerate(answers_results):\n    task_sentences = save_output(seed_sentences_json[\"subTasks\"][idx][\"bestSentences\"], sub_task_json, sentence_id_to_metadata)\n    relevant_sentences.append(task_sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_relevant_sentences = pd.concat(relevant_sentences).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_relevant_sentences.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_relevant_sentences.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_relevant_sentences['Relevant Sentence'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clean relevant sentences with the clean_sentence function"},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatized_sentences = []\n\nfor i in range(len(all_relevant_sentences['Relevant Sentence'])):\n    remove_stop_words = clean_sentence(all_relevant_sentences['Relevant Sentence'][i])\n    \n    lemmatized_sentences.append(bigram_model[remove_stop_words.split(' ')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatized_sentences[:1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Implement LDA Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim.corpora as corpora","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(lemmatized_sentences)\n\n# Create Corpus\ntexts = lemmatized_sentences\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(lemmatized_sentences)\n\n# Create Corpus\ntexts = lemmatized_sentences\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Human readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=15, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=lemmatized_sentences, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}