{"cells":[{"metadata":{},"cell_type":"markdown","source":"### By using interpretable ML techniques, this notebook gives you a taste of how to imputing missing data and process data accordingly","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Import all the libraries we need, some libraries are commented out since Kaggle doesn't support them**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#libraries we need\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom datetime import date\npd.options.mode.chained_assignment = None\nimport h2o\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n\n\n#libraries we need\n# !pip install h2o\n\nfrom scipy.special import expit\n\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\nfrom h2o.grid.grid_search import H2OGridSearch\n\nfrom sklearn.model_selection import train_test_split\nfrom h2o.estimators import H2OGradientBoostingEstimator\nSEED  = 1111   # global random seed for better reproducibility\n\nfrom sklearn.tree import export_graphviz\n# from sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \n# import pydotplus\n\nh2o.init(max_mem_size='24G', nthreads=4) # start h2o with plenty of memory and threads\nh2o.remove_all()                         # clears h2o memory\nh2o.no_progress() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add data by clicking **File** on the top left and find your data by using: \n\n* %cd ../input\n* %ls","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv') \ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic Data Info","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the id column from both test and training data\ntrain.drop(['Id'],axis=1, inplace=True)\ntest.drop(['Id'],axis=1, inplace=True)\n\nprint('The shape of train data is {}'.format(train.shape))\nprint('The shape of test data is {}'.format(test.shape))\n\n#concat both the datasets for easier cleaning \nfull = train.append(test, ignore_index=True)\nprint('The shape of full data is {}'.format(full.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting all the data with missing value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(full.isna().sum()*100/full.shape[0]).plot.bar(figsize=(20,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The above plot gives us a summary as percent values for all the variables in the training dataset.\n* For the variables with huge proportion of missing value: Alley, PoolQC, Fence and MiscFeature etc., it's proper to replace **NA** value with **None**. The **None** can also be a category, telling us something info. Say PoolQC with NA value means the house doesn't have a pool, whcih makes sense to most houses.\n* The area with NA value are imputed with the same logic","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#NA already existing category\nfull.update(full[['BsmtCond','BsmtFinType2','BsmtFinType1','BsmtExposure','BsmtQual',\n                  'GarageType','GarageQual','GarageFinish','GarageCond','FireplaceQu',\n                  'MiscFeature','Fence','PoolQC','Alley','Electrical','MasVnrType']].fillna('None'))\n\n#nan with zero as constant\nfull.update(full[['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','BsmtHalfBath',\n                  'BsmtFullBath','GarageArea','GarageCars','MasVnrArea','TotalBsmtSF']].fillna(0)) \n\n# Replacing the missing values with mode for the list of variables ['Exterior1st','Exterior2nd','Functional','KitchenQual','MSZoning','SaleType','Utilities']\nfull['Exterior1st']=full['Exterior1st'].fillna(full.Exterior1st.value_counts().index[0])\nfull['Exterior2nd']=full['Exterior2nd'].fillna(full.Exterior2nd.value_counts().index[0])\nfull['Functional']=full['Functional'].fillna(full.Functional.value_counts().index[0])\nfull['KitchenQual']=full['KitchenQual'].fillna(full.KitchenQual.value_counts().index[0])\nfull['MSZoning']=full['MSZoning'].fillna(full.MSZoning.value_counts().index[0])\nfull['SaleType']=full['SaleType'].fillna(full.SaleType.value_counts().index[0])\nfull['Utilities']=full['Utilities'].fillna(full.Utilities.value_counts().index[0])\n\n#Dropping irrelavent columns from the whole dataset based on the EDA on the training dataset\n#GarageQual is repetitive, which has the same meaning as Garage Cond\n#PoolQC is mostly NA and won't provide much info, and we've already have PoolArea\n#MSSubClass is a combination of dweiing and year\nfull= full.drop(['MoSold','GarageQual','PoolQC','MSSubClass'],axis=1)\n\n#filled missing garage years\n#It makes no sense to fill year with 0, so we assume the garage was built when the house was built\nfull['GarageYrBlt'] = full['GarageYrBlt'].fillna(full['YearBuilt'])\n\n#Create new features to make them more comprehensive to common sense\n#converting years into age \ncurrentYear = datetime.now().year\nfull['Age_House']=currentYear-full['YearBuilt']\nfull['Age_Renovation']=currentYear-full['YearRemodAdd']\nfull['Garage_age']=currentYear-full['GarageYrBlt']\nfull = full.drop(['YearBuilt','YearRemodAdd','GarageYrBlt'],axis=1)\n\n#Changing OverallCond into a categorical variable, they will be label encoded afterwards\n#These're ordinal variables\nfull['OverallCond'] = full['OverallCond'].astype(str)\nfull['YrSold'] = full['YrSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Label encoding some features and create a new variabel **TotalSF**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageCond', 'ExterQual', \n        'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'OverallCond', \n        'YrSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lb = LabelEncoder() \n    lb.fit(list(full[c].values)) \n    full[c] = lb.transform(list(full[c].values))\n    \n    \n    \n# Adding total sqfootage feature \nfull['TotalSF'] = full['TotalBsmtSF'] + full['1stFlrSF'] + full['2ndFlrSF']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the proccess above, take a look at the full data we have, there's still a feature with missing value, **LotFrontage** ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(full.isna().sum()*100/full.shape[0]).plot.bar(figsize=(20,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To imputing the missing value, we explore the relationship of LotFrontage with other features. It turns out it's related to LotArea, LotConfig, MSZoning and Neighborhood. We build a random forest mdoel to impute the missing value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#spillitng the data again\ntrain = full[full['SalePrice'].notnull()]\ntest = full[full['SalePrice'].isnull()]\ntrain_y = train['SalePrice']\ntrain_x = train.drop(['SalePrice'],axis=1)\ntest_x = test.drop(['SalePrice'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get train LotFrontage dummy variables\nLotFrontage_Dummies_df = pd.get_dummies(train_x[['LotFrontage', 'MSZoning', 'LotArea', 'LotConfig', 'Neighborhood']])\n\n# Get full dummy variables\n# Split the data into LotFrontage known and LotFrontage unknown\nLotFrontageKnown = LotFrontage_Dummies_df[LotFrontage_Dummies_df[\"LotFrontage\"].notnull()]\nLotFrontageUnknown = LotFrontage_Dummies_df[LotFrontage_Dummies_df[\"LotFrontage\"].isnull()]\n\n# Training data knowing LotFrontage\nLotFrontage_Known_X = LotFrontageKnown.drop([\"LotFrontage\"], axis = 1)\nLotFrontage_Known_y = LotFrontageKnown[\"LotFrontage\"]\n# Training data unknown LotFrontage\nLotFrontage_Unknown_X = LotFrontageUnknown.drop([\"LotFrontage\"], axis = 1)\n# Build model using random forest\nfrom sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor(random_state=1,n_estimators=500,n_jobs=-1)\nrfr.fit(LotFrontage_Known_X, LotFrontage_Known_y)\nrfr.score(LotFrontage_Known_X, LotFrontage_Known_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict training data unknown LotFrontage\nLotFrontage_Unknown_y = rfr.predict(LotFrontage_Unknown_X)\ntrain_x.loc[train_x[\"LotFrontage\"].isnull(), \"LotFrontage\"] = LotFrontage_Unknown_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Repeat same process for test data\n# Get train LotFrontage dummy variables\nLotFrontage_Dummies_df = pd.get_dummies(test_x[['LotFrontage', 'MSZoning', 'LotArea', 'LotConfig', 'Neighborhood']])\n\n# Get full dummy variables\n# Split the data into LotFrontage known and LotFrontage unknown\nLotFrontageKnown = LotFrontage_Dummies_df[LotFrontage_Dummies_df[\"LotFrontage\"].notnull()]\nLotFrontageUnknown = LotFrontage_Dummies_df[LotFrontage_Dummies_df[\"LotFrontage\"].isnull()]\n\n# Testing data knowing LotFrontage\nLotFrontage_Known_X = LotFrontageKnown.drop([\"LotFrontage\"], axis = 1)\nLotFrontage_Known_y = LotFrontageKnown[\"LotFrontage\"]\n# Testing data unknown LotFrontage\nLotFrontage_Unknown_X = LotFrontageUnknown.drop([\"LotFrontage\"], axis = 1)\n# Build model using random forest\nfrom sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor(random_state=1,n_estimators=500,n_jobs=-1)\nrfr.fit(LotFrontage_Known_X, LotFrontage_Known_y)\nrfr.score(LotFrontage_Known_X, LotFrontage_Known_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict testing data unknown LotFrontage\nLotFrontage_Unknown_y = rfr.predict(LotFrontage_Unknown_X)\ntest_x.loc[test_x[\"LotFrontage\"].isnull(), \"LotFrontage\"] = LotFrontage_Unknown_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['LotFrontage'] = train_x['LotFrontage']\ntest['LotFrontage'] = test_x['LotFrontage']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Take a quick look at the target column\n* Deviate from the normal distribution.\n* Have appreciable positive skewness.\nWe could take a log of the SalePrice to make it's distribution normal","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['LotFrontage'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.plot.scatter(x='Age_House', y='SalePrice', ylim=(0,800000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a new feature as the age of the house tells us that there's definately affect of age on the SalePrice as we can see a decreasing trend in Price as the age increases","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#box plot overallqual/saleprice\nvar = 'MSZoning'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows us that the mean of different categories of MSZoning, we can see there's a difference in the mean of the categories so keeping this variable in the model seems meaningful. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.concat([train_x, train_y], axis=1)\nCorr = result.corr().iloc[:-1,-1]\n\nfig, ax_ = plt.subplots(figsize=(8, 10))\n_ =  Corr.plot(kind='barh', ax=ax_, colormap='gnuplot')\n_ = ax_.set_xlabel('Pearson Correlation for continuous variables')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graph gives us the correlation between the numerical variables in the model:\n\n    Positively Correlated\n    - TotalSF\n    - OverallQual\n    - GrLivArea\n    - 1stFlrSF\n    \n    Negatively Correlated\n    - BsmtQual\n    - ExterQual\n    - Kitchenqual\n    - Agehouse","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## MODEL BUILDING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['SalePrice'] = np.log(train['SalePrice'])\ntest['SalePrice'] = np.log(test['SalePrice'])\n\ntrain_y = train['SalePrice']\ntrain_x = train.drop(['SalePrice'],axis=1)\n\ntest_y = test['SalePrice']\ntest_x = test.drop(['SalePrice'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Elastic Net GLM","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It's not proper to do GLM here but we build this modle as a benchmark","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.get_dummies(train)\ntest_df = pd.get_dummies(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y_df = train_df['SalePrice']\ntrain_x_df = train_df.drop('SalePrice', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separate the data into ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"r = 'SalePrice'\nx = list(train_x_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hf=h2o.H2OFrame(train_df)\ngf=h2o.H2OFrame(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_params = {'alpha': [0, .25, .5, .75, 1]\n                ,'lambda':[1, 0.5, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0]\n               }\n\nglm = H2OGeneralizedLinearEstimator(family = 'gaussian',standardize = True,lambda_search = True)\n\n# build grid search with previously made GLM and hyperparameters\ngrid = H2OGridSearch(model = glm, hyper_params = hyper_params,\n                     search_criteria = {'strategy': \"Cartesian\"})\n\n\ngrid.train(x = x, y = r, training_frame = hf,nfolds=5,seed=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_grid = grid.get_grid(sort_by='RMSLE', decreasing=False)\nbest_model = sorted_grid.models[0]\nbest_model.cross_validation_metrics_summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_glm_tr =  best_model.predict(h2o.H2OFrame(train_x_df))\npred_glm_tr = pred_glm_tr.as_data_frame()\nco = best_model.coef()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature importance for the continuous variables in elastic net glm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cc = [key for key in dict(train.dtypes) if dict(train.dtypes)[key] in ['float64', 'int64']]\ncc.remove('SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_coef = pd.DataFrame.from_dict(dict((k, co[k]) for k in cc),orient='index')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_coef = cont_coef.rename(columns={ 0: \"Beta\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_coef.plot.barh(figsize=(20, 20),color='orange')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above GLM model we can see that the most important numerical variables for the model:\nStreet and CentralAir followed by FullBath, Fireplaces and OverallQual - Positive Impact","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# GBM","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The next step in complexity from the penalized GLM will be a GBM model. The GBM model can fit the data using arbitrarily complex stair-step patterns, as opposed to being locked into the regression function form.\n\nThe goal is to compare the behavior of the monotonic GBM to the penalized GLM and Pearson correlation coefficients to make sure we trust and understand what the monotonic GBM is doing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(train_x, train_y, test_size=0.30, random_state=1111)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.concat([X_train, y_train], axis=1)\nX_valid = pd.concat([X_valid, y_valid], axis=1)\nX_train_hf = h2o.H2OFrame(X_train)\nX_valid_hf = h2o.H2OFrame(X_valid)\n\nSEED  = 1111   # global random seed for better reproducibility","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_name = 'SalePrice'\nx_names = list(train.columns.drop('SalePrice'))\n\npredictors = x_names\nresponse = \"SalePrice\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'learn_rate': [0.01, 0.05, 0.1], \n          'max_depth': list(range(2,13,2)),\n          'ntrees': [20, 50, 80, 110, 140, 170, 200],\n          'sample_rate': [0.5,0.6,0.7,0.9,1], \n          'col_sample_rate': [0.2,0.4,0.5,0.6,0.8,1]\n          }\n\n\n# Prepare the grid object\ngrid = H2OGridSearch(model=H2OGradientBoostingEstimator,   # Model to be trained\n                     grid_id='gbm_grid1',\n                     hyper_params=params,              # Dictionary of parameters\n                     search_criteria={\"strategy\": \"RandomDiscrete\", \"max_models\": 500}   # RandomDiscrete\n                     )\n\n# Train the Model\ngrid.train(x=predictors,y=response, \n           training_frame=X_train_hf, \n           validation_frame=X_valid_hf,\n           seed = SEED) # Grid Search ID","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify the best model generated with least error\nsorted_final_grid = grid.get_grid(sort_by='rmsle',decreasing = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model_id = sorted_final_grid.model_ids[0]\nbest_gbm_from_grid = h2o.get_model(best_model_id)\nbest_gbm_from_grid.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above is the summary of the best performing model based on the grid search.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_train = best_gbm_from_grid.predict(X_train_hf).exp().as_data_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_gbm_from_grid.model_performance(X_valid_hf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_hf = h2o.H2OFrame(test_x)\npreds = best_gbm_from_grid.predict(X_test_hf)\nfinal_preds = preds.exp()\nfinal_preds = final_preds.as_data_frame()\npred_pandas=final_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\nraw_id = raw_test['Id']\noutput = pd.concat([raw_id, final_preds], axis=1)\noutput = output.rename(columns={'exp(predict)': \"SalePrice\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The output is the final content for submission","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## INTERPRITIBILITY","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can see from a gradient boosting machine that the most important variable for our model is total square feet followed by overall quality followed by neighborhood excellent quality.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best_gbm_from_grid.varimp_plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SHAPLEY VALUES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"contributions = best_gbm_from_grid.predict_contributions(X_test_hf)\n#contributions.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\nshap.initjs()\ncontributions_matrix = contributions.as_data_frame().iloc[:,:].values\n\nX = list(train.columns)\nX.remove('SalePrice')\nlen(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values = contributions_matrix[:,:76]\nshap_values.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"expected_value = contributions_matrix[:,:76].min()\nexpected_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(expected_value, shap_values, X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above plot shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red and those pushing the prediction lower are in blue.\nValues pushing the model higher than the mean values:\n\n- GarageArea\n- GarageArea\n- OverallCond\n\nValues pushing the model lower than the mean values:\n- OverallQual\n- ToalSF\n- GrLivArea\n- Neighborhood\n- MSZoning\n\n\n\nThe shapley output value is based on the first row (X_test_hf[0,:]) is given below","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(expected_value, shap_values[0,:], X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, X, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above is the summary plotoftheshapley values, this gives us the importance od the features in our model\nWe can see that the most importnant variables in our model are:\n- TotalSF\n- OverallQual\n- Neighbourhood. \n\nThis shows that our model is not dependent heavily exclusively on just one variable.  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### PARTIAL DEPENDENCE","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Partial dependence can be interpreted as the estimated average output of a model across the values of some interesting input feature\n\nWe can see the PD for all the miportant variables in the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Continuous = [key for key in dict(train.dtypes) if dict(train.dtypes)[key] in ['float64', 'int64']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dd = ['TotalSF','OverallQual','1stFlrSF']\n\nfor i in dd:\n    print(best_gbm_from_grid.partial_plot(data = X_train_hf, cols = [i], server=True, plot = True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The partial dependence for the GBM show that it picks up on the values as the total square feet of the house increases more than 2000 and becomes constant after the daughter in square feet of the house is 4000 , we can also see a similar trend with the overall quality will we can see a steep jump in the main response of the sales price install quality of the house is greater than 5 and this trend continues until the quality of the house is equal to 8 and becomes constant after that. We can see that if the 1stFloorarea is greater than 500 there is a slight increase in the mean value for nothing in space and after becomes constant.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor,tree\ndt = DecisionTreeRegressor(max_depth=10, min_samples_leaf=0.04,\nrandom_state=SEED)\npred_pandas = h2o.as_list(preds)\ntest_x_dummies = pd.get_dummies(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = dt.fit(test_x_dummies,np.exp(pred_pandas))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.score(test_x_dummies,np.exp(pred_pandas))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"StringIO is not supported on Kaggle","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature_cols = list(test_x_dummies.columns.values)\n\n# dot_data = StringIO()\n# export_graphviz(dt, out_file=dot_data,  \n#                 filled=True, rounded=True,\n#                 special_characters=True,feature_names = feature_cols)\n# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n# Image(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SAVING THE MODEL\n* path = \"./house-prices-data/model\"\n* best_gbm_from_grid.save_mojo(path)\n* best_gbm = h2o.import_mojo(path)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### RESIDUAL ANALYSIS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"residual = np.exp(train['SalePrice']).sub(preds_train['exp(predict)'], axis = 0).abs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"residual = pd.DataFrame(residual,columns=['Residual'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"residual['SalePrice']= np.exp(train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"residual = residual.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([residual,train_x],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"residual.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nsns.set_style('whitegrid')\nfig, ax = plt.subplots(figsize=(20, 10))\nplt.scatter(residual['SalePrice'],residual['Residual'],color='r')\nplt.xlabel('SalePrice')\nplt.ylabel('Residual')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that our model doesn't show any general trend for residuals for sales prices lesser then 300,000 and we can see that the residence tend to increase as the sales prices go up this shows that our model get strained on the majority cluster which is between  100,000ùëéùëõùëë 300,000 this shows that our model is sensitive to the residuals.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set(font_scale=0.9)                                         \nsns.set_style('whitegrid') \n\ngroups = df.groupby(x_names)\n\nsorted_ = df.sort_values(by='Neighborhood') \n\ng=sns.FacetGrid(df, col=\"Neighborhood\",col_wrap=5)\ng= (g.map(plt.scatter, \"SalePrice\", \"Residual\").add_legend())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting up to see if there is a trend unless you do this for a specific neighborhood or in general there is a trend of residuals with respect to the neighborhood.\n\nWe can see for the neighborhood Edwards we have higher value of residuals as the sale price increases and same is followed for SWISU , other neighborhoods don't show a generic trend in The residuals with respect to the sales price.\n\nThese are one of the few neighborhoods where the GBM model is struggling to predict the sales price accurately","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(font_scale=0.9)                                         \nsns.set_style('whitegrid') \n\ngroups = df.groupby(x_names)\n\nsorted_ = df.sort_values(by='OverallCond') \n\ng=sns.FacetGrid(df, col=\"OverallCond\",col_wrap=3)\ng= (g.map(plt.scatter, \"SalePrice\", \"Residual\").add_legend())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But if you do this can also be plotted for another important input variable that is overall quality when plotted we can see when overall quality is equal to 7 dirty GBM is struggling to accurately predict the sales price.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## COMPARISON OF THE PERFORMANCE OF THE MODEL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10)) \nplt.plot(df['SalePrice'])\nplt.plot(np.exp(pred_pandas['predict']),color='orange')\nplt.plot(np.exp(pred_glm_tr['predict']),color='deeppink')\n_ = ax.set_xlabel('Ranked Row Index')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10)) \nplt.plot(df['SalePrice'],color='deeppink')\nplt.plot(np.exp(pred_pandas['predict']),color='orange')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10)) \nplt.plot(df['SalePrice'],color='deeppink')\nplt.plot(np.exp(pred_glm_tr['predict']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also compare the performance of our best elastic net model and GBM model, the first glove overlaps the actual sales price and the predictions by both of our models.\n\nThe second graph shows us a comparison between the actual sales price and the predictions normal gradient boosting machine we can see that the gradient boosting machine strongly trains around the majority values and is able to capture the effect of outliers in contrast with the elastic net model where we are not able to capture many outliers.\n\nOverall our gradient boosting machine seems to perform better and looks more reliable compared to the elastic net model.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}