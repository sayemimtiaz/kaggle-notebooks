{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import re\nimport string\nimport numpy as np\nimport pandas as pd\nimport random\nimport missingno\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import TransformerMixin\nfrom sklearn.metrics import accuracy_score, recall_score, plot_confusion_matrix\n\nfrom wordcloud import WordCloud\n\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking missing data in our dataframe.\nmissingno.matrix(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we can see their are a lot of null values in our dataset, so we need to figure out something later about it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.columns)\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From describing our data we get to know that their are 4 columns named as job_id, telecommuting, has_company_logo and has_questions features which have numerical data. So we can easily remove these columns as they are of no use in text classification problems.\n* We can also see one numerical feature 'fraudulent' is basically column on which our model will be trained and predicted.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets see how many jobs posted are fraud and real.\nsns.countplot(data.fraudulent)\ndata.groupby('fraudulent').count()['title'].reset_index().sort_values(by='title',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From the plot we can see their are very few fraud jobs posted.\n* Our data is very much imbalanced so its a hard work to make a good classifier, we will try best :-)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **Now let's fill the nan values and get rid of the columns which are of no use to make things simpler.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=['job_id', 'telecommuting', 'has_company_logo', 'has_questions', 'salary_range', 'employment_type']\nfor col in columns:\n    del data[col]\n\ndata.fillna(' ', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's check which country posts most number of jobs.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def split(location):\n    l = location.split(',')\n    return l[0]\n\ndata['country'] = data.location.apply(split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country = dict(data.country.value_counts()[:11])\ndel country[' ']\nplt.figure(figsize=(8,6))\nplt.title('No. of job postings country wise', size=20)\nplt.bar(country.keys(), country.values())\nplt.ylabel('No. of jobs', size=10)\nplt.xlabel('Countries', size=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most number of jobs are posted by US.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's check about which type of experience is required in most number of jobs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"experience = dict(data.required_experience.value_counts())\ndel experience[' ']\nplt.bar(experience.keys(), experience.values())\nplt.xlabel('Experience', size=10)\nplt.ylabel('no. of jobs', size=10)\nplt.xticks(rotation=35)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# title of jobs which are frequent.\nprint(data.title.value_counts()[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we should combine our text in a single column to start cleaning our data.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text']=data['title']+' '+data['location']+' '+data['company_profile']+' '+data['description']+' '+data['requirements']+' '+data['benefits']\ndel data['title']\ndel data['location']\ndel data['department']\ndel data['company_profile']\ndel data['description']\ndel data['requirements']\ndel data['benefits']\ndel data['required_experience']\ndel data['required_education']\ndel data['industry']\ndel data['function']\ndel data['country']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now lets see what type of words are frequent in fraud and actual jobs using wordclouds**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fraudjobs_text = data[data.fraudulent==1].text\nactualjobs_text = data[data.fraudulent==0].text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\nplt.figure(figsize = (16,14))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(\" \".join(fraudjobs_text)))\nplt.imshow(wc,interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,14))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(\" \".join(actualjobs_text)))\nplt.imshow(wc,interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Creating a function that accepts a sentence as input and processes the sentence into tokens, performing lemmatization, lowercasing, and removing stop words.\n* The function that i have used to do these work is found here https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/, i know that i cant write so neat so i just taken those functions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create our list of punctuation marks\npunctuations = string.punctuation\n\n# Create our list of stopwords\nnlp = spacy.load('en')\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nparser = English()\n\n# Creating our tokenizer function\ndef spacy_tokenizer(sentence):\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    mytokens = parser(sentence)\n\n    # Lemmatizing each token and converting each token into lowercase\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n\n    # Removing stop words\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n\n    # return preprocessed list of tokens\n    return mytokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Custom transformer using spaCy\nclass predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        # Cleaning Text\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\n# Basic function to clean the text\ndef clean_text(text):\n    # Removing spaces and converting text into lowercase\n    return text.strip().lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating our bag of words\nbow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* BoW converts text into the matrix of occurrence of words within a given document. It focuses on whether given words occurred or not in the document, and it generates a matrix that we might see referred to as a BoW matrix or a document term matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting our data in train and test\nX_train, X_test, y_train, y_test = train_test_split(data.text, data.fraudulent, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* We are creating a pipeline with three components: a cleaner, a vectorizer, and a classifier. The cleaner uses our predictors class object to clean and preprocess the text. The vectorizer uses countvector objects to create the bag of words matrix for our text. The classifier is an object that performs the logistic regression to classify the sentiments.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression()\n\n# Create pipeline using Bag of Words\npipe = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', bow_vector),\n                 ('classifier', clf)])\n\n# fitting our model.\npipe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting with a test dataset\npredicted = pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"Logistic Regression Accuracy:\", accuracy_score(y_test, predicted))\nprint(\"Logistic Regression Recall:\", recall_score(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(pipe, X_test, y_test, cmap='Blues', values_format=' ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier()\n\n# Create pipeline using Bag of Words\npipe = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', bow_vector),\n                 ('classifier', clf)])\n\n# fitting our model.\npipe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting with a test dataset\npredicted = pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"Random Forest Accuracy:\", accuracy_score(y_test, predicted))\nprint(\"Random Forest Recall:\", recall_score(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(pipe, X_test, y_test, cmap='Blues', values_format=' ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Support Vector Machine Classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SVC()\n\n# Create pipeline using Bag of Words\npipe = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', bow_vector),\n                 ('classifier', clf)])\n\n# fitting our model.\npipe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting with a test dataset\npredicted = pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"SVC Accuracy:\", accuracy_score(y_test, predicted))\nprint(\"SVC Recall:\", recall_score(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(pipe, X_test, y_test, cmap='Blues', values_format=' ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. XGBoost Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = XGBClassifier()\n\n# Create pipeline using Bag of Words\npipe = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', bow_vector),\n                 ('classifier', clf)])\n\n# fitting our model.\npipe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting with a test dataset\npredicted = pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"XGBoost Accuracy:\", accuracy_score(y_test, predicted))\nprint(\"XGBoost Recall:\", recall_score(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(pipe, X_test, y_test, cmap='Blues', values_format=' ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### * Sorry Guyz for creating Prediction section so long from next time i will do all classifiers in a loop and will try to implement tuning as i am still learning best way :-)\n### * If you like the notebook please Upvote it.\n### * Any kind of suggestion are appreciated.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}