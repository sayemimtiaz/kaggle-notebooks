{"cells":[{"metadata":{"_uuid":"cf3a5b12-dee7-4733-bfbd-cb33a5a1565e","_cell_guid":"6d0cfd7c-fe0a-4bd4-ba5d-6b38c11e6fc6","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5055217-e24f-486a-8224-439889566a8a","_cell_guid":"97ab2dda-3336-499b-b89d-4f5105e95c2b","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/insurance/insurance.csv\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a61acb1-bb91-4423-8aa5-b29116d131bd","_cell_guid":"9ef18afd-f842-4445-8a5a-50add2d4588d","trusted":true},"cell_type":"markdown","source":"# Data Analysis","execution_count":null},{"metadata":{"_uuid":"70dcdbda-32b6-43eb-b064-38fa96abb0da","_cell_guid":"0b80516e-020f-424f-88de-4b14304ad6c1","trusted":true},"cell_type":"markdown","source":"### 1) Missing Values","execution_count":null},{"metadata":{"_uuid":"f75d5878-2172-4f90-b99f-6d3fc30b9de8","_cell_guid":"f345ae9a-07b3-4cb5-aaa6-c67a5fac4397","trusted":true},"cell_type":"code","source":"f_na = [feature for feature in df.columns if df[feature].isna().sum()>0]\nf_na","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7392f9fb-1a8d-4199-b3c5-096ae37a4a0e","_cell_guid":"abb2db58-5b83-40ec-9347-492cbfdff84a","trusted":true},"cell_type":"markdown","source":"* No missing values in features","execution_count":null},{"metadata":{"_uuid":"074b9f92-f118-4b7f-913a-b54569101d01","_cell_guid":"6a2bbde8-7d21-4676-9f1f-2de4a70af8eb","trusted":true},"cell_type":"markdown","source":"### 2) Numerical Features","execution_count":null},{"metadata":{"_uuid":"073a39a8-87b2-4e4b-b759-08762baeb3d0","_cell_guid":"94c14f0c-3bdc-4a51-9d30-7f3a423c98b9","trusted":true},"cell_type":"code","source":"f_num = [feature for feature in df.columns if df[feature].dtype!='object']\nf_num","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff088beb-7127-4857-84a9-0d5b552d167a","_cell_guid":"a20e6726-3786-4505-88fb-2e4d0cfd5588","trusted":true},"cell_type":"code","source":"# Discrete Numerical Features\nf_dis = [feature for feature in f_num if len(df[feature].unique())<25]\nprint(f_dis)\n# Continuous Numerical Features\nf_cont = [feature for feature in f_num if feature not in f_dis]\nprint(f_cont)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d741734-54c8-4981-b57a-f5e86c695405","_cell_guid":"4e9885ad-6815-45c0-a783-631b3a599578","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfor feature in f_cont:\n    plt.hist(df[feature], bins=50)\n    plt.title(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f58b72d9-bcf5-4c98-abbf-00918adf02ce","_cell_guid":"aad68c46-8a9f-48a7-97ca-1626cdcc15fe","trusted":true},"cell_type":"code","source":"for feature in f_cont:\n    data = df.copy()\n    data[feature] = np.log(data[feature])\n    plt.hist(data[feature], bins=50)\n    plt.title(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7aa0f9a-90c1-4966-b6bb-02b6b6638e57","_cell_guid":"0002d661-7598-4432-9d0f-0588c0cc2f1e","trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nimport scipy.stats as stats\nimport pylab \nfig, axs = plt.subplots(len(f_cont), 2, figsize=(15,15))\n\nfor i in range(len(f_cont)):\n    data = df.copy()\n    data[f_cont[i]] = np.sqrt(data[f_cont[i]])\n    stats.probplot(df[f_cont[i]], dist=\"norm\", plot=axs[i,0])\n    axs[i,0].set_title(f_cont[i])\n    stats.probplot(data[f_cont[i]], dist=\"norm\", plot=axs[i,1])\n    axs[i,1].set_title(f_cont[i])\n    \n    #plt.title(feature)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89ccbc6a-3673-44f9-8cbc-ba66c37b6032","_cell_guid":"dc2b1168-ff34-4384-8bbc-1dd2ae0d4e3d","trusted":true},"cell_type":"code","source":"f_cont[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1fb571d-c6dc-4d5f-8620-5e0119cdf9b7","_cell_guid":"6f7e722e-58c4-4de7-a06c-810e609fec9c","trusted":true},"cell_type":"markdown","source":"* Taking sqrt of 'age' feature will make it gaussian distributed","execution_count":null},{"metadata":{"_uuid":"abd35d7a-bac0-4a8e-bf13-db8916221232","_cell_guid":"4d9dec5c-4193-46b8-b5d6-5a9558fe7296","trusted":true},"cell_type":"code","source":"df[f_dis]['children'].unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"034e8ee5-c232-45ad-8c73-0f5d57d30770","_cell_guid":"b6edd58f-4de6-493c-ad08-132b1a50d526","trusted":true},"cell_type":"markdown","source":"### 3) Categorical Features","execution_count":null},{"metadata":{"_uuid":"137d4990-b2d6-43c0-867c-a895ea87635d","_cell_guid":"7fd05803-483d-4d16-b256-27b2b7add8c8","trusted":true},"cell_type":"code","source":"f_cat = [feature for feature in df.columns if df[feature].dtype=='O']\nf_cat","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"507d8468-3959-4d34-9a2a-cd0b1a7f6d65","_cell_guid":"07606cfd-4c81-4d70-a02a-6c663e1f1497","trusted":true},"cell_type":"code","source":"for i in f_cat:\n    print(df[i].unique(),\"\\n\",df[i].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1649cca-746d-42e1-9bcb-12e04b3d86dc","_cell_guid":"f1f4cd9e-285c-4814-9c5d-7dec92918707","trusted":true},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"_uuid":"42a3eccd-3b6a-4814-bffa-e8467b175e20","_cell_guid":"514edea3-86c3-473c-a9b5-18452db36280","trusted":true},"cell_type":"markdown","source":"### 4) Outliers","execution_count":null},{"metadata":{"_uuid":"f89188f4-d640-4e06-a9df-3412052fc6a1","_cell_guid":"f2dc08f4-f60a-48ba-8ec5-a2b87d0a4736","trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set(style=\"whitegrid\")\nfor i in f_num:\n    ax = sns.boxplot(x=df[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"421be16f-a5b2-479b-80ce-593a7411dc21","_cell_guid":"d24ff298-710e-4385-84a3-536f784c5311","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5e8d8de-d1d6-4b58-b851-9981827d1b08","_cell_guid":"1ef1c669-a941-48a5-a282-1c1136bf5346","trusted":true},"cell_type":"code","source":"df[f_num]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"547d4b21-874c-4c48-805d-e3ac2aba2f87","_cell_guid":"d9ba04ec-2494-41ca-b39c-b6c8e5362cbe","trusted":true},"cell_type":"code","source":"from scipy import stats \nIQR=[]\nfor i in f_num:\n    IQR.append(stats.iqr(df[i], interpolation = 'midpoint'))\nIQR","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d78d467-bf3b-4045-a235-d6d72aeab407","_cell_guid":"9dd1ae01-d816-4a40-bca2-2b03ffc4a193","trusted":true},"cell_type":"code","source":"limits = dict()\nj=0\nfor i in f_num:\n    Q1 = np.percentile(df[i], 25, interpolation = 'midpoint')  \n    Q3 = np.percentile(df[i], 75, interpolation = 'midpoint')  \n    #print(Q1, Q3)\n    limits[i] = [Q1-(1.5*IQR[j]), Q3+(1.5*IQR[j])]\n    j+=1\n\nlimits","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e1a067a-de23-4829-afad-72b13d2d8d21","_cell_guid":"27075b03-3fa5-4384-8bea-066185b0fc07","trusted":true},"cell_type":"code","source":"df[df['bmi']>=47].index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abd3582f-e7b5-4beb-b750-354242cd101e","_cell_guid":"c672b225-746f-4b02-9d06-eeb6390835d2","trusted":true},"cell_type":"code","source":"outliers = dict()\nfor i in f_num:\n    outliers[i]=list()\n    for x in df[i]:\n        if(x<limits[i][0] or x>limits[i][1]):\n            outliers[i].append(x)\n   \noutliers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a52be51-18b1-42c8-a9e3-95a1051aa980","_cell_guid":"4f39a436-e852-44a7-a47c-2adb2543854e","trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53b4e283-ce43-4505-9b1f-587393e136bd","_cell_guid":"38285907-a0e9-4e26-9d43-0b58140662a2","trusted":true},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{"_uuid":"dce0abb4-038f-4b77-9725-eae96a66aa11","_cell_guid":"c34534a2-604b-4f1c-9e4d-7b8f75314f88","trusted":true},"cell_type":"code","source":"\"\"\"\n\n# One Hot Encoding for 'sex' and 'smoker' columns\ndf_new = df.copy()\npd.get_dummies(df_new, columns=['sex','smoker'], prefix=['sex', 'smoker'])\n\n\n# Binary Encoding for 'sex' and 'smoker' columns\nimport category_encoders as ce\ndf_new = df.copy()\nencoder = ce.BinaryEncoder(cols=['sex','smoker'])\ndf_new = encoder.fit_transform(df_new)\nprint(df_new.head())\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07d7d701-22ed-494b-b84c-0d6e39bbb4d7","_cell_guid":"93a2e65b-d358-4173-866b-5cabf41ab8cd","trusted":true},"cell_type":"markdown","source":"### Removing Outliers","execution_count":null},{"metadata":{"_uuid":"0bc13ea6-50e6-45ef-925e-4e16f4f3df7b","_cell_guid":"47e566a8-f2b4-4308-973c-35bff2916c68","trusted":true},"cell_type":"code","source":"#df_new = df.drop(df[df['bmi']>=47].index, axis=0).reset_index(drop=True)\n\ndf_new = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6677a436-38f9-435e-b6be-66a8bd4ce692","_cell_guid":"a6a3845e-6a9c-4e9a-9d09-14ce7b0b5d71","trusted":true},"cell_type":"markdown","source":"### Handling Categorical Features","execution_count":null},{"metadata":{"_uuid":"ea106a22-8efc-467b-8547-f1567c855ed5","_cell_guid":"3ba2756a-d1df-4493-bbf3-9e9eb6e11256","trusted":true},"cell_type":"code","source":"\n\n\n\n\"\"\"\n# Backward Difference Encoding for 'region' column\nimport category_encoders as ce\nencoder = ce.BackwardDifferenceEncoder(cols=['region'])\ndf_new = encoder.fit_transform(df_new)\nprint(df_new.head())\n\"\"\"\n\n# One Hot Encoding for 'region' column\nimport category_encoders as ce\nimport pandas as pd\n\n#Create object for one-hot encoding\nencoder=ce.OneHotEncoder(cols=['region'],handle_unknown='return_nan',return_df=True,use_cat_names=True)\n\n#Original Data\nprint(df_new.head())\n#Fit and transform Data\ndf_new = encoder.fit_transform(df_new)\nprint(df_new.head())\n\n\n# Label Encoding for 'sex' and 'smoker' columns\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf_new['sex'] = le.fit_transform(df_new['sex'])\ndf_new['smoker'] = le.fit_transform(df_new['smoker'])\nprint(list(le.classes_))\ndf_new.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c31d7cb1-0fa5-40da-bf10-3f285cd1c6a0","_cell_guid":"ba13776e-f5b4-438d-9d0d-de62d9acf2ca","trusted":true},"cell_type":"code","source":"#df_new.drop(['intercept'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"548c4b8d-3688-4e04-842e-941168c9a131","_cell_guid":"77821bac-dcd8-4787-a53d-482120c98d12","trusted":true},"cell_type":"code","source":"df_new.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3838ae0-a73f-47c6-af86-d232d119834e","_cell_guid":"159c7765-82ff-441f-beb6-afb9c9827fa4","trusted":true},"cell_type":"code","source":"df_new.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d0a9fb8-5b50-4234-bc73-b4381e2c9563","_cell_guid":"4a45b669-51bb-4b64-9458-acf78be3caa2","trusted":true},"cell_type":"markdown","source":"### SQRT Transformation","execution_count":null},{"metadata":{"_uuid":"690e3b3f-0ab1-45f5-8e6e-858727c55d29","_cell_guid":"d3a88611-43b9-4b61-aa19-9a76c60a676d","trusted":true},"cell_type":"code","source":"df_new['age'] = np.sqrt(df_new['age'])\ndf_new","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7682515-eede-4301-831c-6690a8482f3e","_cell_guid":"079a6996-89f5-4bfb-8434-cfa9b983a147","trusted":true},"cell_type":"markdown","source":"# Feature Scaling","execution_count":null},{"metadata":{"_uuid":"292643ff-a76a-4bb8-a02e-839732955cff","_cell_guid":"44c4198f-2dba-4f61-b81f-427b6cf88717","trusted":true},"cell_type":"code","source":"\"\"\"\n## StandardScaler\nfrom sklearn.preprocessing import StandardScaler\ndata = df_new.copy()\nscaler = StandardScaler()\ndata[data.drop(['charges'],axis=1).columns] = scaler.fit_transform(data.drop(['charges'],axis=1))\ndata = pd.DataFrame(data, columns=df_new.columns)\n#print(scaler.mean_)\n#print(scaler.transform([[2, 2]]))\n\n## MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\ndata = df_new.copy()\nscaler = MinMaxScaler()\ndata[data.drop(['charges'],axis=1).columns] = scaler.fit_transform(data.drop(['charges'],axis=1))\ndata = pd.DataFrame(data, columns=df_new.columns)\n\n## MaxAbsScaler\nfrom sklearn.preprocessing import MaxAbsScaler\nscaler = MaxAbsScaler()\ndata = df_new.copy()\ndata[data.drop(['charges'],axis=1).columns] = scaler.fit_transform(data.drop(['charges'],axis=1))\ndata = pd.DataFrame(data, columns=df_new.columns)\n\n## RobustScaler\nfrom sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\ndata = df_new.copy()\ndata[data.drop(['charges'],axis=1).columns] = scaler.fit_transform(data.drop(['charges'],axis=1))\ndata = pd.DataFrame(data, columns=df_new.columns)\n\n## QuantileTransformer\nfrom sklearn.preprocessing import QuantileTransformer\nscaler = QuantileTransformer()\ndata = df_new.copy()\ndata[data.drop(['charges'],axis=1).columns] = scaler.fit_transform(data.drop(['charges'],axis=1))\ndata = pd.DataFrame(data, columns=df_new.columns)\n\n## PowerTransformer using yeo-johnson\nfrom sklearn.preprocessing import PowerTransformer\nscaler = PowerTransformer(method='yeo-johnson')\ndata = df_new.copy()\ndata[data.drop(['charges'],axis=1).columns] = scaler.fit_transform(data.drop(['charges'],axis=1))\ndata = pd.DataFrame(data, columns=df_new.columns)\n\n## PowerTransformer using box-cox\nfrom sklearn.preprocessing import PowerTransformer\nscaler = PowerTransformer(method='box-cox')\ndata = df_new.copy()\ndata[data.drop(['charges'],axis=1).columns] = scaler.fit_transform(data.drop(['charges'],axis=1))\ndata = pd.DataFrame(data, columns=df_new.columns)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f397f76b-bba6-44cf-b8d7-572cda0c8746","_cell_guid":"1a23a839-beff-479c-8fc2-9dc1a206169d","trusted":true},"cell_type":"code","source":"data = df_new.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f53c0f58-926d-4c14-adda-deeb296d98e3","_cell_guid":"5d3a24c4-dffc-49ed-bf14-24c10bb64e7c","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.heatmap(data.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6971296b-6f8d-48ed-a355-59223a168247","_cell_guid":"805ec266-75de-4576-9726-e9fcb1d87652","trusted":true},"cell_type":"code","source":"X, y = data.iloc[:,:-1], data.iloc[:,-1]\nX","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a303affb-4a65-4dd1-b3df-e5ceba9f358f","_cell_guid":"c0d120a9-2ff1-4460-8bb7-24d42894cbeb","trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b71f54d-bc13-48ed-b9f5-09c5c39c8d62","_cell_guid":"a8aaaa76-86a6-40f9-9e57-b2e722e08c58","trusted":true},"cell_type":"markdown","source":"# Feature Selection","execution_count":null},{"metadata":{"_uuid":"75b8bc8a-fce2-493e-b729-50325e4acecb","_cell_guid":"452305c9-06f8-4566-93a7-6af6a5447816","trusted":true},"cell_type":"code","source":"\n\"\"\"\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\n\nfrom sklearn.feature_selection import RFE\nselector = RFE(reg, n_features_to_select=5, step=1)\nselector = selector.fit(X, y)\nprint(selector.support_)\nprint(selector.ranking_)\n\nf_selected = X.columns[selector.support_]\nf_selected\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd2df9dd-9235-4e16-87b7-d7052cce2caf","_cell_guid":"e8ae4a9f-0660-4f24-8393-c9f575d2823c","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"749e5780-3881-42a9-8fad-6fa777141caa","_cell_guid":"1433736f-cd81-4f0d-8f1d-845863a7f7bb","trusted":true},"cell_type":"code","source":"# Feature Importance with Extra Trees Classifier\nfrom pandas import read_csv\nfrom sklearn.ensemble import ExtraTreesRegressor\n\n# feature extraction\nmodel = ExtraTreesRegressor(n_estimators=10)\nmodel.fit(X, y)\nprint(model.feature_importances_)\ntop_features = np.array(model.feature_importances_)\n\nindices = (-top_features).argsort()[:3]\nprint(indices)\nf_selected = X.columns[indices]\nprint(X.columns)\nprint(f_selected)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffd26a77-bdb6-424b-9606-4a9c4e846355","_cell_guid":"dac84896-925f-4491-9696-2fda40dfc7d5","trusted":true},"cell_type":"markdown","source":"# Train-Test Split","execution_count":null},{"metadata":{"_uuid":"0ecaeef3-cfeb-4634-9663-0de61d884707","_cell_guid":"e8034c89-c9cd-4b18-8f48-8edef0949484","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=128)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2eb1b894-0d07-4d58-ac15-04ab459ad979","_cell_guid":"c9a93df6-1650-4191-9ca7-ac3fcdd25d2d","trusted":true},"cell_type":"code","source":"X_train = X_train[f_selected]\nX_test = X_test[f_selected]\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8c2987f-b5ad-4c23-85a9-5b137895bcad","_cell_guid":"d8e867dc-c8f4-43cd-b8ac-342fd2a98af3","trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b49c3ce7-26db-40fb-9ea2-b0df3b4b4203","_cell_guid":"0e4d4256-2773-458a-a729-bd3adf3aa9a3","trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94f60617-aec9-4e24-bde5-31f53bf09341","_cell_guid":"a9372c18-ab28-44ed-8aab-545c1aada435","trusted":true},"cell_type":"markdown","source":"# Model Building","execution_count":null},{"metadata":{"_uuid":"9b886b9d-5613-49b7-a6e3-61c0778d9c3d","_cell_guid":"288a7caf-1fc2-419b-a0d8-6e95da15c98f","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nreg = LinearRegression()\nscore = cross_val_score(reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41f671c2-ffd2-4ab3-b5c5-468aeae218a7","_cell_guid":"057033a1-40f8-4ed7-87a8-e6880ee5f3e8","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge\nridge_reg = Ridge(alpha=0.1)\n#ridge_reg.fit(X_train, y_train)\nscore = cross_val_score(ridge_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a48942ba-42d1-4db6-ad24-95cf992ed01c","_cell_guid":"38b18ed2-7c60-42bd-bb87-d13833262671","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlasso_reg = Lasso(alpha=0.1)\nscore = cross_val_score(lasso_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"455cf381-f57d-4d7b-97c1-88897bfcd6c4","_cell_guid":"ec9f55e6-0cc3-45eb-afe9-4eb4f20c9a8e","trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nimport numpy as np\nsvr_reg = SVR(C=1.0, epsilon=0.2)\nscore = cross_val_score(svr_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf14bc4d-dfb9-4181-bd70-c3522cf36de6","_cell_guid":"e03bf92e-3c44-4413-a0aa-e391f5d94385","trusted":true},"cell_type":"code","source":"X_train[f_selected].shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86fa88a2-6e3e-4609-9c57-0c3829ce18bb","_cell_guid":"1ff0ce0b-770a-4227-9d6f-bcb178300116","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\n\nelasticnet_reg = ElasticNet(l1_ratio=0.8, random_state=0)\nscore = cross_val_score(elasticnet_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0d152b9-7319-40d6-85d6-9c4d8bf0adf7","_cell_guid":"a84f965a-be64-4538-b4b7-88bf72a11b4e","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrandomfor_reg = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\nscore = cross_val_score(randomfor_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b515539-d7c4-4046-a62a-8d169c5dd3c2","_cell_guid":"be885a75-7eec-42cc-b5a5-f1547a95762d","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\n\ndecisiontree_reg = DecisionTreeRegressor(max_depth=4, random_state=0)\nscore = cross_val_score(decisiontree_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35b9b437-7bc4-4a67-a8eb-82fb7fda4a6d","_cell_guid":"a35f8d76-75a9-477b-82d8-1ec99dd80c8d","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\nadaboost_reg = AdaBoostRegressor(n_estimators=100, random_state=0)\nscore = cross_val_score(adaboost_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d44532f5-36b8-4b3a-9a8c-929756fef153","_cell_guid":"596b8fb8-32bb-485e-8b37-ed12fec0524f","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\ngradientboost_reg = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, max_depth=3, random_state=0)\nscore = cross_val_score(gradientboost_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5ce39d3-7d4c-4d52-b9d7-3ad4ea1480cc","_cell_guid":"62fdcfaf-7ea7-4ed2-95b9-4e139143bb16","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingRegressor\n\nbagging_reg = BaggingRegressor(n_estimators=100, max_features=3, max_samples=50, random_state=0)\nscore = cross_val_score(bagging_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8ed13ea-deac-4379-8ad7-c0742f43fb04","_cell_guid":"5a301ff0-97a9-4443-93d6-dbc45466e5d6","trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', max_depth=4, learning_rate = 0.1, alpha = 0.1, n_estimators = 200)\nscore = cross_val_score(xgb_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c78cdbf9-0cf0-42e7-93eb-dafd92d2ee86","_cell_guid":"218a7f51-d235-44fd-b35b-537c35d67ccb","trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error \nfrom matplotlib import pyplot as plt\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport warnings \nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a32704ad-09a2-415e-b6b7-32b108f0d702","_cell_guid":"ea13a7b9-42af-43c1-8ec6-76e27d87b90d","trusted":true},"cell_type":"code","source":"NN_model = Sequential()\nweight_init = 'random_normal'\n# The Input Layer :\nNN_model.add(Dense(256, kernel_initializer=weight_init,input_dim = X_train[f_selected].shape[1], activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(256, kernel_initializer=weight_init,activation='relu'))\nNN_model.add(Dense(256, kernel_initializer=weight_init,activation='relu'))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer=weight_init,activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='Nadam', metrics=[\"mean_absolute_error\"])\nNN_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28d2fd80-241b-4295-a8ef-5e5bc1a4179e","_cell_guid":"a825fabd-5ae0-46a2-8b16-0807a3de5a31","trusted":true},"cell_type":"code","source":"checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b37368d5-8540-48e7-b3d8-a5dd9bc71e99","_cell_guid":"ea3d3b26-f18c-4ea7-8574-de7dd4386855","trusted":true},"cell_type":"code","source":"#!rm ./*","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"481fed2f-0775-4b47-a483-99338ed9c713","_cell_guid":"d583a6e2-b732-4b0c-91bf-0740257ad274","trusted":true},"cell_type":"code","source":"NN_model.fit(X_train[f_selected], y_train, epochs=500, batch_size=64, validation_split = 0.2, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44d15a32-d78e-45a3-884d-37a7e5fdca1d","_cell_guid":"fe06e70a-3a13-40ca-a76c-d046f713598f","trusted":true},"cell_type":"code","source":"\nfrom sklearn.metrics import mean_absolute_error\ny_pred = NN_model.predict(X_train[f_selected])\ny_test_pred = NN_model.predict(X_test[f_selected])\nmae_train = mean_absolute_error(y_train, y_pred)\nmae_test = mean_absolute_error(y_test, y_test_pred)\nprint(\"MAE for train :\",mae_train)\nprint(\"MAE for test :\",mae_test)\n\nfrom sklearn.metrics import r2_score\nprint(\"R2 for train :\",r2_score(y_train, y_pred))\nprint(\"R2 for test :\",r2_score(y_test, y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06c8c33e-33fa-4de3-b9dc-21be2af96a12","_cell_guid":"73291b60-7289-48b6-91b8-76386ec3f4af","trusted":true},"cell_type":"code","source":"# from sklearn.ensemble import GradientBoostingRegressor\n\ngradientboost_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, max_features='sqrt', random_state=0)\ngradientboost_reg.fit(X_train[f_selected], y_train)\n\nfrom sklearn.metrics import mean_absolute_error\ny_pred = gradientboost_reg.predict(X_train[f_selected])\ny_test_pred = gradientboost_reg.predict(X_test[f_selected])\nmae_train = mean_absolute_error(y_train, y_pred)\nmae_test = mean_absolute_error(y_test, y_test_pred)\nprint(\"MAE for train :\",mae_train)\nprint(\"MAE for test :\",mae_test)\n\nfrom sklearn.metrics import r2_score\nprint(\"R2 for train :\",r2_score(y_train, y_pred))\nprint(\"R2 for test :\",r2_score(y_test, y_test_pred))\n#print(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5ddba7a-26a7-45b3-b93d-50074536013a","_cell_guid":"3573210c-a9fd-4269-9cc1-4a77c3d9b9fc","trusted":true},"cell_type":"code","source":"\"\"\"\n# Load wights file of the best model :\nwights_file = './Weights-365--1753.49939.hdf5' # choose the best checkpoint \nNN_model.load_weights(wights_file) # load it\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df5b2f5c-d224-48a0-8612-fe6df8638748","_cell_guid":"bf83f32c-89c2-4f6d-b8f7-d7b7759030d1","trusted":true},"cell_type":"code","source":"\"\"\"\ngradientboost_reg = GradientBoostingRegressor(random_state=0)\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\n\nsearch_space = {\n        \"max_depth\": Integer(2, 10),\n        \"max_features\": Categorical(['auto', 'sqrt','log2']), \n        \"min_samples_leaf\": Integer(2, 10),\n        \"min_samples_split\": Integer(2, 10),\n        \"n_estimators\": Integer(50, 300)\n    }\n\ndef on_step(optim_result):\n    score = forest_bayes_search.best_score_\n    print(\"best score: %s\" % score)\n    if score >= 0.98:\n        print('Interrupting!')\n        return True\n\nforest_bayes_search = BayesSearchCV(gradientboost_reg, search_space, n_iter=32, scoring=\"r2_score\", n_jobs=-1, cv=5)\n\nforest_bayes_search.fit(X[f_selected], y, callback=on_step) # callback=on_step will print score after each iteration\n\n# Just like in Scikit-Learn we can view the best parameters:\nforest_bayes_search.best_params_\n# And the best estimator:\nforest_bayes_search.best_estimator_\n# And the best score:\nforest_bayes_search.best_score_\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55618db6-4835-4f11-bb61-e6f97d430143","_cell_guid":"ccdd1b8f-f278-4195-bdfa-b23296d29a08","trusted":true},"cell_type":"markdown","source":"# Hyperparameter Optimization","execution_count":null},{"metadata":{"_uuid":"bbaadbeb-bc41-4871-957b-6a2978d85ebb","_cell_guid":"d418725e-5a78-4833-8933-7f40a6aea384","trusted":true},"cell_type":"markdown","source":"### 1) RandomizedSearchCV","execution_count":null},{"metadata":{"_uuid":"5943802c-153b-43b4-9ab7-f577f634ecbe","_cell_guid":"828e29d3-de8e-4133-a746-a5d9035f14d2","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 300)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(2, 10)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [int(x) for x in np.linspace(2, 10)]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [int(x) for x in np.linspace(2, 10)]\nloss = ['ls', 'lad', 'huber', 'quantile']\nlearning_rate = [0.01, 0.03, 0.1, 0.3]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'loss' : loss,\n               'learning_rate' : learning_rate,\n               }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e115a8e3-6c91-4a99-a6e2-c72a5dec14d1","_cell_guid":"201acf91-dda8-4f43-ae16-314a79c9653b","trusted":true},"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\ngradientboost_reg = GradientBoostingRegressor(random_state=0)\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\ngb_random = RandomizedSearchCV(estimator = gradientboost_reg, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=0, n_jobs = -1)\n# Fit the random search model\ngb_random.fit(X_train[f_selected], y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21d9eb2c-8e09-49c1-b3aa-1e5087cf4cab","_cell_guid":"2f9eed5b-c875-41d3-8315-c83e79af3273","trusted":true},"cell_type":"code","source":"print(gb_random.best_params_)\nprint(gb_random.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08158675-cbba-4958-aa55-6b6c3ddc7a26","_cell_guid":"3c4116d6-7bc0-4f20-9fa4-176fb3186732","trusted":true},"cell_type":"code","source":"best_random = gb_random.best_estimator_\n\n#gradientboost_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, max_features='sqrt', random_state=0)\nbest_random.fit(X_train[f_selected], y_train)\n\nfrom sklearn.metrics import mean_absolute_error\ny_pred = best_random.predict(X_train[f_selected])\ny_test_pred = best_random.predict(X_test[f_selected])\nmae_train = mean_absolute_error(y_train, y_pred)\nmae_test = mean_absolute_error(y_test, y_test_pred)\nprint(\"MAE for train :\",mae_train)\nprint(\"MAE for test :\",mae_test)\n\nfrom sklearn.metrics import r2_score\nprint(\"R2 for train :\",r2_score(y_train, y_pred))\nprint(\"R2 for test :\",r2_score(y_test, y_test_pred))\n#print(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2feae83b-b3bd-4a3f-a0dd-8cadaf053239","_cell_guid":"d3a17356-7d88-4e05-81b8-14209f2176ec","trusted":true},"cell_type":"markdown","source":"### 2) Bayesian Optimization","execution_count":null},{"metadata":{"_uuid":"8ff52cb3-0436-47e4-9186-bf94ce95eef2","_cell_guid":"d006200a-5908-4286-8599-28923a465aa9","trusted":true},"cell_type":"markdown","source":"### GradientBoostingRegressor","execution_count":null},{"metadata":{"_uuid":"008f1b31-17fc-419f-9dd4-13680e1e9480","_cell_guid":"f4abb86b-6604-43bf-81ab-4a35f0b6d327","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\n\ndef black_box_function(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n    \n    n_estimators = int(n_estimators*250 + 50)\n    max_depth = int(max_depth*8 + 2)\n    min_samples_split = int(min_samples_split*8 + 2)\n    min_samples_leaf = int(min_samples_leaf*8 + 2)\n    \n    gradientboost_reg = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth,\\\n                                                  min_samples_split=min_samples_split,\\\n                                                  min_samples_leaf=min_samples_leaf, random_state=0)\n    score = cross_val_score(gradientboost_reg, X_train[f_selected], y_train, cv=10)\n    r2 = score.mean()\n    n = len(X_train[f_selected])\n    p = X_train[f_selected].shape[1]\n    \n    adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n    \n    return adj_r2\n    \n# Number of trees in random forest\nn_estimators = (0,1)\n# Number of features to consider at every split\n#max_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = (0, 1)\n# Minimum number of samples required to split a node\nmin_samples_split = (0, 1)\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = (0, 1)\n#loss = ('ls', 'lad', 'huber', 'quantile')\n\n# Create the random grid\npbounds = {'n_estimators': n_estimators,\n           'max_depth': max_depth,\n           'min_samples_split': min_samples_split,\n           'min_samples_leaf': min_samples_leaf,\n           }\n\nfrom bayes_opt import BayesianOptimization\n\noptimizer = BayesianOptimization(\n    f=black_box_function,\n    pbounds=pbounds,\n    random_state=0,\n)\n\noptimizer.maximize(\n    init_points=40,\n    n_iter=50,\n)\n\nprint(optimizer.max)\n\ndef black_box_function_test(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n    \n    n_estimators = int(n_estimators*250 + 50)\n    max_depth = int(max_depth*8 + 2)\n    min_samples_split = int(min_samples_split*8 + 2)\n    min_samples_leaf = int(min_samples_leaf*8 + 2)\n    #print(n_estimators, max_depth, min_samples_split, min_samples_leaf)\n    \n    gradientboost_reg = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth,\\\n                                                  min_samples_split=min_samples_split,\\\n                                                  min_samples_leaf=min_samples_leaf, random_state=0)\n    #print(gradientboost_reg)\n    gradientboost_reg.fit(X_train[f_selected], y_train)\n\n    #from sklearn.metrics import mean_absolute_error\n    y_pred = gradientboost_reg.predict(X_train[f_selected])\n    y_test_pred = gradientboost_reg.predict(X_test[f_selected])\n    #mae_train = mean_absolute_error(y_train, y_pred)\n    #mae_test = mean_absolute_error(y_test, y_test_pred)\n    #print(\"MAE for train :\",mae_train)\n    #print(\"MAE for test :\",mae_test)\n\n    from sklearn.metrics import r2_score\n    #print(\"R2 for train :\",r2_score(y_train, y_pred))\n    #print(\"R2 for test :\",r2_score(y_test, y_test_pred))\n    r2_train = r2_score(y_train, y_pred)\n    n = len(X_train[f_selected])\n    p = X_train[f_selected].shape[1]\n    adj_r2_train = 1-(1-r2_train)*(n-1)/(n-p-1)\n    print(\"Train adjusted r2 score = \",adj_r2_train)\n    \n    #print(gradientboost_reg)\n    r2_test = r2_score(y_test, y_test_pred)\n    n = len(X_test[f_selected])\n    p = X_test[f_selected].shape[1]\n    adj_r2_test = 1-(1-r2_test)*(n-1)/(n-p-1)\n\n    print(\"Test adjusted r2 score = \",adj_r2_test)\n    \n    \nparams = optimizer.max['params']\nn_estimators = params['n_estimators']\nmax_depth = params['max_depth']\nmin_samples_split = params['min_samples_split']\nmin_samples_leaf = params['min_samples_leaf'\n                         ]\nprint(n_estimators, max_depth, min_samples_split, min_samples_leaf)\n\nblack_box_function_test(n_estimators, max_depth, min_samples_split, min_samples_leaf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"590b113d-f89f-4621-be4f-9cc89359bdb4","_cell_guid":"04d03ade-3a73-4558-ad12-6954f9c96f52","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrandomfor_reg = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\nscore = cross_val_score(randomfor_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\n\ndecisiontree_reg = DecisionTreeRegressor(max_depth=4, random_state=0)\nscore = cross_val_score(decisiontree_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())\n\n\nfrom sklearn.ensemble import AdaBoostRegressor\nadaboost_reg = AdaBoostRegressor(n_estimators=100, random_state=0)\nscore = cross_val_score(adaboost_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())\n\n\nbagging_reg = BaggingRegressor(n_estimators=100, max_features=3, max_samples=50, random_state=0)\nscore = cross_val_score(bagging_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())\n\nimport xgboost as xgb\n\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', max_depth=4, learning_rate = 0.1, alpha = 0.1, n_estimators = 200)\nscore = cross_val_score(xgb_reg, X_train[f_selected], y_train, cv=10)\nprint(score)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c2f0a33-7b9c-4efe-a9e1-51f14fb882e8","_cell_guid":"a5165e8c-bb46-4023-ad8c-5e9d4aa63a29","trusted":true},"cell_type":"markdown","source":"### RandomForestRegressor","execution_count":null},{"metadata":{"_uuid":"2e42a6b3-5bed-4be3-a62a-23ad1cacdc08","_cell_guid":"4942d4b2-1da5-4fb7-b492-4f37813b6837","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score \n\ndef randomfor_reg_bayesian_opt_function(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n    \n    n_estimators = int(n_estimators*250 + 50)\n    max_depth = int(max_depth*8 + 2)\n    min_samples_split = int(min_samples_split*8 + 2)\n    min_samples_leaf = int(min_samples_leaf*8 + 2)\n    \n\n    randomfor_reg = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth,\\\n                                                  min_samples_split=min_samples_split,\\\n                                                  min_samples_leaf=min_samples_leaf, random_state=0)\n    score = cross_val_score(randomfor_reg, X_train[f_selected], y_train, cv=10)\n    r2 = score.mean()\n    n = len(X_train[f_selected])\n    p = X_train[f_selected].shape[1]\n    \n    adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n    \n    return adj_r2\n\n# Number of trees in random forest\nn_estimators = (0,1)\n# Number of features to consider at every split\n#max_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = (0, 1)\n# Minimum number of samples required to split a node\nmin_samples_split = (0, 1)\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = (0, 1)\n#loss = ('ls', 'lad', 'huber', 'quantile')\n\n# Create the random grid\npbounds = {'n_estimators': n_estimators,\n           'max_depth': max_depth,\n           'min_samples_split': min_samples_split,\n           'min_samples_leaf': min_samples_leaf,\n           }\n\nfrom bayes_opt import BayesianOptimization\n\noptimizer = BayesianOptimization(\n    f=randomfor_reg_bayesian_opt_function,\n    pbounds=pbounds,\n    random_state=0,\n)\n\n\noptimizer.maximize(\n    init_points=40,\n    n_iter=50\n)\n\n\nparams = optimizer.max['params']\nn_estimators = params['n_estimators']\nmax_depth = params['max_depth']\nmin_samples_split = params['min_samples_split']\nmin_samples_leaf = params['min_samples_leaf']\nprint(n_estimators, max_depth, min_samples_split, min_samples_leaf)\n\ndef randomfor_reg_bayesian_opt_function_test(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n    \n    n_estimators = int(n_estimators*250 + 50)\n    max_depth = int(max_depth*8 + 2)\n    min_samples_split = int(min_samples_split*8 + 2)\n    min_samples_leaf = int(min_samples_leaf*8 + 2)\n    #print(n_estimators, max_depth, min_samples_split, min_samples_leaf)\n    randomfor_reg = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth,\\\n                                                  min_samples_split=min_samples_split,\\\n                                                  min_samples_leaf=min_samples_leaf, random_state=0)\n\n    \n    randomfor_reg.fit(X_train[f_selected], y_train)\n\n    y_pred = randomfor_reg.predict(X_train[f_selected])\n    y_test_pred = randomfor_reg.predict(X_test[f_selected])\n\n    from sklearn.metrics import r2_score\n    \n    r2_train = r2_score(y_train, y_pred)\n    n = len(X_train[f_selected])\n    p = X_train[f_selected].shape[1]\n    adj_r2_train = 1-(1-r2_train)*(n-1)/(n-p-1)\n    print(\"Train adjusted r2 score = \",adj_r2_train)\n    \n    r2_test = r2_score(y_test, y_test_pred)\n    n = len(X_test[f_selected])\n    p = X_test[f_selected].shape[1]\n    adj_r2_test = 1-(1-r2_test)*(n-1)/(n-p-1)\n\n    print(\"Test adjusted r2 score = \",adj_r2_test)\n    \n    \nrandomfor_reg_bayesian_opt_function_test(n_estimators, max_depth, min_samples_split, min_samples_leaf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0522932-6451-48cf-9652-61ef6af7f30a","_cell_guid":"48466b62-cb2b-4603-bbda-fad7380b2180","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d64d8a2c-d120-4316-9325-3b7d44e7012c","_cell_guid":"854d656e-52d0-4621-886a-6bb20b6babf2","trusted":true},"cell_type":"markdown","source":"### DecisionTreeRegressor","execution_count":null},{"metadata":{"_uuid":"88cb1b52-c902-477f-b121-5527db10e0f9","_cell_guid":"52325862-863b-498b-ae91-603dfa98551c","trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\n\ndef decisiontree_reg_bayesian_opt_function(criterion, max_depth, min_samples_split, min_samples_leaf):\n    \n    #n_estimators = int(n_estimators*250 + 50)\n    max_depth = int(max_depth*8 + 2)\n    min_samples_split = int(min_samples_split*8 + 2)\n    min_samples_leaf = int(min_samples_leaf*8 + 2)\n    criteria = [\"mse\", \"friedman_mse\", \"mae\"]\n    criterion = criteria[int(criterion)-1]\n    \n\n    decisiontree_reg = DecisionTreeRegressor(criterion=criterion, max_depth=max_depth,\\\n                                                  min_samples_split=min_samples_split,\\\n                                                  min_samples_leaf=min_samples_leaf, random_state=0)\n    score = cross_val_score(decisiontree_reg, X_train[f_selected], y_train, cv=10)\n    r2 = score.mean()\n    n = len(X_train[f_selected])\n    p = X_train[f_selected].shape[1]\n    \n    adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n    \n    return adj_r2\n\n\ncriterion = (1,3.99)\n# Number of trees in random forest\n#n_estimators = (0,1)\n# Number of features to consider at every split\n#max_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = (0, 1)\n# Minimum number of samples required to split a node\nmin_samples_split = (0, 1)\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = (0, 1)\n#loss = ('ls', 'lad', 'huber', 'quantile')\n\n# Create the random grid\npbounds = {'criterion': criterion,\n           'max_depth': max_depth,\n           'min_samples_split': min_samples_split,\n           'min_samples_leaf': min_samples_leaf,\n           }\n\nfrom bayes_opt import BayesianOptimization\n\noptimizer = BayesianOptimization(\n    f=decisiontree_reg_bayesian_opt_function,\n    pbounds=pbounds,\n    random_state=0,\n)\n\n\noptimizer.maximize(\n    init_points=40,\n    n_iter=50\n)\n\n\nparams = optimizer.max['params']\ncriterion = params['criterion']\nmax_depth = params['max_depth']\nmin_samples_split = params['min_samples_split']\nmin_samples_leaf = params['min_samples_leaf']\nprint(criterion, max_depth, min_samples_split, min_samples_leaf)\n\ndef decisiontree_reg_bayesian_opt_function_test(criterion, max_depth, min_samples_split, min_samples_leaf):\n    \n    #n_estimators = int(n_estimators*250 + 50)\n    max_depth = int(max_depth*8 + 2)\n    min_samples_split = int(min_samples_split*8 + 2)\n    min_samples_leaf = int(min_samples_leaf*8 + 2)\n    criteria = [\"mse\", \"friedman_mse\", \"mae\"]\n    criterion = criteria[int(criterion)]\n    #print(n_estimators, max_depth, min_samples_split, min_samples_leaf)\n    decisiontree_reg = DecisionTreeRegressor(criterion=criterion, max_depth=max_depth,\\\n                                                  min_samples_split=min_samples_split,\\\n                                                  min_samples_leaf=min_samples_leaf, random_state=0)\n\n    \n    decisiontree_reg.fit(X_train[f_selected], y_train)\n\n    y_pred = decisiontree_reg.predict(X_train[f_selected])\n    y_test_pred = decisiontree_reg.predict(X_test[f_selected])\n\n    from sklearn.metrics import r2_score\n    \n    r2_train = r2_score(y_train, y_pred)\n    n = len(X_train[f_selected])\n    p = X_train[f_selected].shape[1]\n    adj_r2_train = 1-(1-r2_train)*(n-1)/(n-p-1)\n    print(\"Train adjusted r2 score = \",adj_r2_train)\n    \n    r2_test = r2_score(y_test, y_test_pred)\n    n = len(X_test[f_selected])\n    p = X_test[f_selected].shape[1]\n    adj_r2_test = 1-(1-r2_test)*(n-1)/(n-p-1)\n\n    print(\"Test adjusted r2 score = \",adj_r2_test)\n    \n    \ndecisiontree_reg_bayesian_opt_function_test(criterion, max_depth, min_samples_split, min_samples_leaf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72c2d522-f9fa-4345-9ddd-f3b3a421e21e","_cell_guid":"a1f008b9-6ad0-449f-83e6-e5835a49421c","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}