{"cells":[{"metadata":{},"cell_type":"markdown","source":"# BM-25 and Query Similarity Ranking\n\nThis notebook uses code for BM-25 notebook https://www.kaggle.com/dgunning/browsing-research-papers-with-a-bm25-search-engine.\nIt uses relevant keywords from https://www.kaggle.com/d4v1d3/cord-19-lda-topic-modeling-reccomendation-system and performs a BM-25 search on the keywords.\n\nThen, the results are weighted against ranking of sentences from https://colab.research.google.com/drive/1DzjAfXIC8wQg6eqVq3QoMX12OuG4VtGW. This encourages the chosen sentences to be from documents most relevant to COVID-19."},{"metadata":{},"cell_type":"markdown","source":"### Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install rank_bm25 nltk","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path, PurePath\nimport requests\nfrom requests.exceptions import HTTPError, ConnectionError\nfrom ipywidgets import interact\nimport ipywidgets as widgets\nfrom rank_bm25 import BM25Okapi\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\"punkt\")\nimport re\nimport os\nimport json\nimport glob\nimport sys\nfrom tqdm import tqdm\nfrom rank_bm25 import BM25Okapi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sys.path.insert(0, \"../\")\n\nDATA_PATH = '/kaggle/input/CORD-19-research-challenge'\n\n# Walks all subdirectories in a directory, and their files. \n# Opens all json files we deem relevant, and append them to\n# a list that can be used as the \"data\" argument in a call to \n# pd.DataFrame.\ndef gather_jsons(dirName):\n    \n    # Get the list of all files in directory tree at given path\n    # include only json with encoded id (40-character SHA hash)\n    # Only length of filename is checked, but this should be sufficient\n    # given the task.\n    \n    listOfFiles = list()\n    for (dirpath, dirnames, filenames) in os.walk(dirName):\n        listOfFiles += [os.path.join(dirpath, file) for file in filenames\n                        if file.endswith(\"json\")\n                        and len(file) == 45]\n    jsons = []\n    \n    print(str(len(listOfFiles)) + \" jsons found! Attempting to gather.\")\n    \n    for file in tqdm(listOfFiles):\n        with open(file) as json_file:\n            jsons.append(json.load(json_file))\n    return jsons\n        \n        \n# Returns a dictionary object that's easy to parse in pandas.\ndef extract_from_json(json):\n    \n    # For text mining purposes, we're only interested in 4 columns:\n    # abstract, paper_id (for ease of indexing), title, and body text.\n    # In this particular dataset, some abstracts have multiple sections,\n    # with [\"abstract\"][1] or later representing keywords or extra info. \n    # We only want to keep [0][\"text\"] in these cases. \n    if len(json[\"abstract\"]) > 0:\n        json_dict = {\n            \"_id\": json[\"paper_id\"],\n            \"title\": json[\"metadata\"][\"title\"],\n            \"abstract\": json[\"abstract\"],\n            \"text\": \" \".join([i[\"text\"] for i in json[\"body_text\"]])\n        }\n        \n    # Else, [\"abstract\"] isn't a list and we can just grab the full text.\n    else:\n        json_dict = {\n            \"_id\": json[\"paper_id\"],\n            \"title\": json[\"metadata\"][\"title\"],\n            \"abstract\": json[\"abstract\"],\n            \"text\": \" \".join([i[\"text\"] for i in json[\"body_text\"]])\n        }\n\n    return json_dict\n\n# Combines gather_jsons and extract_from_json to create a\n# pandas DataFrame object.\ndef gather_data(dirName):\n    return(pd.DataFrame(data=[extract_from_json(json) for json in gather_jsons(dirName)]))\n\ncorona_df = gather_data(f\"{DATA_PATH}\")\ncorona_df['abstract'] = corona_df.abstract.apply(lambda x: ' '.join([r['text'] for r in x]))\ncorona_df.to_csv(\"covid_data_full.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Preprocessing\nTo prepare the text for the search index we perform the following steps\n1. Remove punctuations and special characters\n2. Convert to lowercase\n3. Tokenize into individual tokens (words mostly)\n4. Remove stopwords like (and, to))\n\nYou can tweak the code below to improve the search results"},{"metadata":{"trusted":true},"cell_type":"code","source":"english_stopwords = list(set(stopwords.words('english')))\n\ndef strip_characters(text):\n    t = re.sub('\\(|\\)|:|,|;|\\.|’|”|“|\\?|%|>|<', '', text)\n    t = re.sub('/', ' ', t)\n    t = t.replace(\"'\",'')\n    return t\n\ndef clean(text):\n    t = text.lower()\n    t = strip_characters(t)\n    return t\n\ndef tokenize(text):\n    words = nltk.word_tokenize(text)\n    return list(set([word for word in words \n                     if len(word) > 1\n                     and not word in english_stopwords\n                     and not (word.isnumeric() and len(word) is not 4)\n                     and (not word.isnumeric() or word.isalpha())] )\n               )\n\ndef preprocess(text):\n    t = clean(text)\n    tokens = tokenize(t)\n    return tokens\n\nclass SearchResults:\n    \n    def __init__(self, \n                 data: pd.DataFrame,\n                 columns = None):\n        self.results = data\n        if columns:\n            self.results = self.results[columns]\n            \n    def __getitem__(self, item):\n        return Paper(self.results.loc[item])\n    \n    def __len__(self):\n        return len(self.results)\n        \n    def _repr_html_(self):\n        return self.results._repr_html_()\n\nSEARCH_DISPLAY_COLUMNS = ['_id', 'title', 'abstract', 'text']\n\nclass WordTokenIndex:\n    \n    def __init__(self, \n                 corpus: pd.DataFrame, \n                 columns=SEARCH_DISPLAY_COLUMNS):\n        self.corpus = corpus\n        raw_search_str = self.corpus.abstract.fillna('') + ' ' + self.corpus.title.fillna('') + ' ' + self.corpus.text.fillna('')\n        self.index = raw_search_str.apply(preprocess).to_frame()\n        self.index.columns = ['terms']\n        self.index.index = self.corpus.index\n        self.columns = columns\n    \n    def search(self, search_string):\n        search_terms = preprocess(search_string)\n        result_index = self.index.terms.apply(lambda terms: any(i in terms for i in search_terms))\n        results = self.corpus[result_index].copy().reset_index().rename(columns={'index':'paper'})\n        return SearchResults(results, self.columns + ['paper'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using a RankBM25 Search Index\n\nRankBM25 is a python library that implements algorithms for a simple search index.\nhttps://pypi.org/project/rank-bm25/"},{"metadata":{"trusted":true},"cell_type":"code","source":"class RankBM25Index(WordTokenIndex):\n    \n    def __init__(self, corpus: pd.DataFrame, columns=SEARCH_DISPLAY_COLUMNS):\n        super().__init__(corpus, columns)\n        self.bm25 = BM25Okapi(self.index.terms.tolist())\n        \n    def search(self, search_string, n=4):\n        search_terms = preprocess(search_string)\n        doc_scores = self.bm25.get_scores(search_terms)\n        ind = np.argsort(doc_scores)[::-1][:n]\n        results = self.corpus.iloc[ind][self.columns]\n        results['Score'] = doc_scores[ind]\n        results['orig_ind'] = ind\n        results = results[results.Score > 0]\n        return SearchResults(results.reset_index(), self.columns + ['Score', 'orig_ind'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Search Index\n\nCreating a search index on the entire dataset takes over a minute. As a todo I will try to speed it up.\nFor now let's create the search index on the first 10000 records"},{"metadata":{"trusted":true},"cell_type":"code","source":"bm25_index = RankBM25Index(corona_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These keywords were retrieved from an LDA kernel and seem most relevant to COVID-19."},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords = ['sars-cov', 'sars', 'coronavirus', 'ace2', 'coronaviruses', 'ncov', 'covid-19', 'wuhan', 'spike', 'sars-cov-2']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The chosen rows are the union of 100 BM25 results for each keyword. We keep the scores for ranking later."},{"metadata":{"trusted":true},"cell_type":"code","source":"results = None\nadded = []\nfor word in keywords:\n    word_result = bm25_index.search(word, n=100).results\n    if results is None:\n        results = word_result\n        added += [r.orig_ind for i, r in word_result.iterrows()]\n        continue\n    for i, r in word_result.iterrows():\n        if r.orig_ind not in added:\n            results = results.append(r)\n            added.append(r.orig_ind)\ndf = results.sort_values(by='Score', ascending=False)\ndf.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ranking"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !tar -xvf /kaggle/input/spacy-covid19/covid-19-en_lg.tar.xz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install scispacy\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\nimport scispacy\nimport spacy\nimport en_core_sci_lg\nnlp = en_core_sci_lg.load()\n# nlp = spacy.load(\"covid-19-en_lg/\")\nnlp.max_length=2000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\nvector_list = []\nfor i in tqdm(df.index):\n    doc = nlp(df.iloc[i].text)\n    sents = [sent for sent in doc.sents]\n    vecs = [sent.vector for sent in sents]\n    for j in range(len(sents)):\n        vector_list.append(\n            {\"_id\": df.iloc[i]._id, \n             \"score\": df.iloc[i]['Score'],\n             \"sentence\": j, \n             \"vector\": vecs[j], \n             \"start_span\": sents[j].start_char,\n             \"end_span\": sents[j].end_char})\nvector_df = pd.DataFrame(data=vector_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"queries = \"\"\"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\nPrevalence of asymptomatic shedding and transmission. \nPrevalence of asymptomatic shedding and transmission in children, infants, and young people.\nSeasonality of transmission of the virus.\nPhysical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic or hydrophobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\nPersistence and stability on a multitude of substrates and sources (nasal discharge, sputum, urine, fecal matter, blood, bodily fluids and secretions).\nPersistence of virus on surfaces of different materials (copper, stainless steel, plastic).\nNatural history of the virus and shedding of it from an infected person.\nImplementation of diagnostics and products to improve clinical processes.\nDisease models, including animal models for infection, disease and transmission.\nTools and studies to monitor phenotypic change and potential adaptation of the virus.\nImmune response and immunity to the virus.\nEffectiveness of movement control strategies to prevent secondary transmission in health care and community settings.\nEffectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings.\nRole of the environment in transmission.\"\"\"\nqueries = queries.splitlines()\nqueries_df = pd.DataFrame(data=[{\"query\":query} for query in queries])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query_vector_list = []\nfor i in tqdm(range(len(queries))):\n    doc = nlp(queries[i])\n    vec = doc.vector\n    query_vector_list.append({\"_id\": f\"query_{i}\", \"vector\": vec})\n    \nquery_vector_df = pd.DataFrame(data=query_vector_list)\nquery_vector_df.to_csv(\"query_vecs.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we use a 0.9 RATIO to give weight to the relevance score. The final score is a linear combination of the cosine similarity score and the relevance score from BM25 search."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.spatial import distance\nRATIO = 0.9\ndistances = distance.cdist([value for value in query_vector_df[\"vector\"]], [value for value in vector_df[\"vector\"].values], \"cosine\")\nw2v_searchable_df = vector_df.drop(columns=[\"vector\"])\n# Create a column with cosine distances for each query vs the sentence\nfor i in range(len(queries)):\n    #w2v_searchable_df[f\"query_{i}_distance\"] = 1 - (np.power((1 - distances[i]), RATIO) * w2v_searchable_df['score'])\n    w2v_searchable_df[f\"query_{i}_distance\"] = RATIO * (1 - distances[i]) + (1-RATIO) * w2v_searchable_df['score']\nw2v_searchable_df.to_csv(\"covid_w2v_searchable.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(queries)):\n    columnName = f\"query_{i}_distance\"\n    context = w2v_searchable_df.sort_values(by=columnName, ascending=False)[[\"_id\",\"start_span\",\"end_span\"]][:20]\n    ix = context[\"_id\"].to_list()\n    spans1 = context[\"start_span\"].to_list()\n    spans2 = context[\"end_span\"].to_list()\n    print(\"Question: \" + queries[i] + \"\\n\")\n    for j in range(len(context.index)):\n        score = df[df[\"_id\"] == ix[j]].iloc[0]['Score']\n        print(f\"Rank {j+1} (Relevance Score {score}): \" + str(df[df[\"_id\"] == ix[j]].iloc[0][\"text\"])[spans1[j]:spans2[j]] + \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}