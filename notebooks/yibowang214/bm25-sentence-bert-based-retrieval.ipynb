{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"* Preprocess data\n* Queries\n* Rank document\n    * BM25\n    * Sentence-Bert\n* Summerization  \n    * Title (study)\n    * Main subject (Method)\n    * Most important sentence (result)\n    * #patients (measure of evidence)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Preprocess data\n * dataset: document_parses\n * use metadata.csv to extract non-repeated documents \n * extract meaningful contents \n          * pdf_json: extract title, abstract and body\n          * pmc_json: extract title and body\n          * metadata.csv: url, journal, date\n * clean data\n          * delete URLs, stopwords and punctuations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# # Extract meaningful fields and clean the data.\n# import json\n\n# import os\n\n# import re\n# import string\n# from nltk.tokenize import word_tokenize\n# from nltk.corpus import stopwords\n\n# import pandas as pd\n# import math\n\n# stop_words = stopwords.words('english')\n\n# path = \"/kaggle/input/CORD-19-research-challenge\"\n\n\n# # files = os.listdir(path)\n\n\n# def clean(content):\n#     # delete URL\n#     results = re.compile(r'http://[a-zA-Z0-9.?/&=:]*', re.S)\n#     content = results.sub(\"\", content)\n#     content = re.sub(r'(https|http)?://(\\w|\\.|/|\\?|=|&|%|-)*\\b', '', content, flags=re.MULTILINE)\n\n#     content = content.lower()\n#     # delete stopwords\n#     tokens = word_tokenize(content)\n#     content = [i for i in tokens if not i in stop_words]\n#     s = ' '\n#     content = s.join(content)\n\n#     # delete punctuations, but keep '-'\n#     del_estr = string.punctuation\n#     del_estr = list(del_estr)\n#     del_estr.remove('-')\n#     del_estr = ''.join(del_estr)\n#     replace = \" \" * len(del_estr)\n#     tran_tab = str.maketrans(del_estr, replace)\n#     content = content.translate(tran_tab)\n\n#     return content\n\n\n# data = pd.read_csv(path + '/metadata.csv')\n# print(data.head(5))\n# print(len(data))\n# print(data['pdf_json_files'][15])\n\n# i = 0\n\n# ppath = []\n# for file in range(len(data)):\n#     s = []\n#     # file = 61298\n#     if not isinstance(data['pdf_json_files'][file], float):\n#         # print(data['pdf_json_files'][file])\n#         pdf_path = data['pdf_json_files'][file].split('; ')\n#         filepath = path + \"/\" + pdf_path[0]\n#     elif not isinstance(data['pmc_json_files'][file], float):\n#         # print(data['pmc_json_files'][file])\n#         pmc_path = data['pmc_json_files'][file].split('; ')\n#         filepath = path + \"/\" + pmc_path[0]\n#     else:\n#         continue\n\n#     with open(filepath, 'r', encoding='utf-8') as f:\n#         temp = json.loads(f.read())\n#         # print(temp.keys())\n\n#         contents = []\n#         file_path = data['cord_uid'][file]\n#         if file_path in ppath:\n#             print(file_path)\n#             continue\n#         else:\n#             contents.append(file_path)\n#             contents.append('\\n')\n#             ppath.append(file_path)\n\n#         # print(file_path)\n\n#         metadata_dict = temp['metadata']\n#         metadata = []\n#         if 'title' in metadata_dict.keys():\n#             metadata.append(metadata_dict['title'])\n\n#         abstract = []\n#         if 'abstract' in temp.keys():\n#             abstract_list = temp['abstract']\n#             for content in abstract_list:\n#                 # print(content.keys())\n#                 if 'text' in content.keys():\n#                     abstract.append(content['text'])\n\n#         body_text_list = temp['body_text']\n#         body_text = []\n#         for content in body_text_list:\n#             # print(content.keys())\n#             if 'text' in content.keys():\n#                 body_text.append(content['text'])\n#         #\n#         # print(metadata)\n#         # print(\"___________________\")\n#         # print(body_text)\n#         # print(\"+++++++++++++++++++\")\n\n#         contents.append(filepath)\n#         contents.append('\\n')\n\n#         link = []\n#         if not isinstance(data['url'][file], float):\n#             link = data['url'][file].replace(' ', '')\n#             contents.append(link)\n#             # print(link)\n#         contents.append('\\n')\n\n#         journal = []\n#         if not isinstance(data['journal'][file], float):\n#             journal = data['journal'][file]\n#             contents.append(journal)\n#         contents.append('\\n')\n\n#         date = []\n#         if not isinstance(data['publish_time'][file], float):\n#             date = data['publish_time'][file]\n#             contents.append(date)\n#         contents.append('\\n')\n\n#         metadata = str(metadata)\n#         metadata = clean(metadata)\n#         contents.append(metadata)\n\n#         abstract = str(abstract)\n#         abstract = clean(abstract)\n#         contents.append(abstract)\n\n#         body_text = str(body_text)\n#         body_text = clean(body_text)\n#         contents.append(body_text)\n\n#         # print(contents)\n#         #\n#         f1 = open(path+\"/extract/%d.txt\" % (i + 1), 'w', encoding='utf-8')\n#         # print(contents)\n#         contents = \"\".join(contents)\n#         f1.write(contents)\n#         # print(i, contents)\n#         # print(\"!!!!!!!!!!!!!!!!!\")\n#         i += 1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Queries \n1 Human immune response to COVID-19  \n2 What is known about mutations of the virus?  \n3 Studies to monitor potential adaptations  \n4 Are there studies about phenotypic change?  \n5 Changes in COVID-19 as the virus evolves  \n6 What regional genetic variations (mutations) exist  \n7 What do models for transmission predict?  \n8 Serial Interval (for infector-infectee pair)  \n9 Efforts to develop qualitative assessment frameworks to systematically collect","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3. Rank documents\n * Use BM25 and sentence-bert to rank documents, and extract the top-5 relevant documents.\n \n     1. I used java Lucene BM25 to retrieve the top-1000 documents. Since kaggle does not support java, I attached github link here: https://github.com/YiboWANG214/COVID19/tree/master/lucenerank  \n        Then I built csv file according to doc_path to show rankings and document contents. The complete results with document contents are shown in Google docs:  \n        BM25: https://drive.google.com/file/d/1u5UQMTBIcT_kRddmCWZvggM8yUbDTl0g/view?usp=sharing  \n        \n     2. Then I used sententce-bert to rerank according to titles of documents and get the top 5 documents.  \n        The complete results with document contents are shown in Google docs:  \n        Bert: https://drive.google.com/file/d/1-27AONt0wEOShVPsooWVlMKoO_1IIKAK/view?usp=sharing  \n        \n     3. I also tried to rerank according to titles and abstracts to involve more information. But it is super slow, so I just reranked on the top-200 documents.    \n    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Collect useful information of the top-1000 documents to build a csv file.\n# import json\n# import pandas as pd\n# import csv\n\n# query = []\n# with open('/kaggle/input/cord19round2queries/queries2.txt', 'r', encoding='utf-8') as f3:\n#     for line in f3:\n#         # line = line.split(' ')\n#         line = line.strip('\\n')\n#         query.append(line[2:])\n# # print(query)\n\n# docid = []\n# docpath = []\n# url = []\n# journal = []\n# date = []\n# with open('/kaggle/input/cord19round2luceneresult/round2_BM25_1000.txt', 'r', encoding='utf-8') as f1:\n#     # i = 0\n#     # while i < len(f1):\n#     #     print(f1[i])\n#     for line in f1:\n#         line = line.split(' ')\n#         # print(line)\n#         docid.append(line[4])\n#         docpath.append(line[6])\n#         url.append(line[7])\n#         journal.append(line[8])\n#         date.append(line[9])\n\n# with open(\"/round2_BM25_1000_full.csv\", \"a\") as csvfile:\n#     writer_BM25 = csv.writer(csvfile)\n#     writer_BM25.writerow([\"query\", \"rank\", \"paper_id\", \"title\", \"abstract\", \"contents\", 'url', 'journal', 'date'])\n\n#     for i in range(9):\n#         k = 0\n\n#         query_curr = query[i]\n#         rank_curr = k\n#         for file in docpath[1000*i: 1000*i+1000]:\n#             # print(file)\n#             k += 1\n#             insert = []\n#             insert.append(query_curr)\n#             insert.append(k)\n#             with open(file, 'r', encoding='utf-8') as f:\n#                 temp = json.loads(f.read())\n\n#                 # paper_id_curr = temp['paper_id']\n#                 # insert.append(paper_id_curr)\n#                 insert.append(docid[1000*i+k-1])\n\n#                 metadata_dict = temp['metadata']\n\n#                 if 'title' in metadata_dict.keys():\n#                     title = metadata_dict['title']\n#                 insert.append(title)\n\n#                 if 'abstract' in temp.keys():\n#                     abstract_list = temp['abstract']\n#                     abstract = []\n#                     for content in abstract_list:\n#                         if 'text' in content.keys():\n#                             abstract.append(content['text'])\n#                 insert.append(' '.join(abstract))\n\n#                 if 'body_text' in temp.keys():\n#                     contents_list = temp['body_text']\n#                     contents = []\n#                     for content in contents_list:\n#                         if 'text' in content.keys():\n#                             contents.append(content['text'])\n#                 insert.append(' '.join(contents))\n\n#                 if len(url[1000*i+k-1]) == 1:\n#                     insert.append(' ')\n#                 else:\n#                     insert.append(url[1000*i+k-1][1:])\n#                 if len(journal[1000*i+k-1]) == 1:\n#                     insert.append(' ')\n#                 else:\n#                     insert.append(journal[1000*i+k-1][1:])\n#                 if len(date[1000*i+k-1]) == 1:\n#                     insert.append(' ')\n#                 else:\n#                     insert.append(date[1000*i+k-1][1:])\n\n#             # print([insert])\n#             writer_BM25.writerows([insert])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test GPU.\n\nimport tensorflow as tf\n\ndevice_name = tf.test.gpu_device_name()\n\nif device_name == '/device:GPU:0':\n    print('Found GPU at: {}'.format(device_name))\nelse:\n    raise SystemError('GPU device not found')\n\nimport torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Use bert to rerank.\n\n# import sys\n\n# !pip install -U sentence-transformers\n\n# from sentence_transformers import SentenceTransformer\n# model = SentenceTransformer('bert-large-nli-stsb-mean-tokens')\n\n# import pandas as pd\n# import numpy as np\n# np.set_printoptions(threshold=sys.maxsize)\n# pd.set_option('display.width',None)\n\n# df = pd.read_csv(\"/kaggle/input/cord19round2prepareforbert/round2_BM25_1000_full.csv\")\n# # print(df.head(5))\n# df['title'] = df['title'].astype(str)\n# df['encoding'] =\"\"\n# for rows,index in df.iterrows():\n#     title = index['title']\n#     print(title)\n#     search_phrase_vector = model.encode([title])[0]\n#     # print(search_phrase_vector)\n#     df.at[rows,'encoding'] = search_phrase_vector\n#     # print(df.loc[rows])\n# # df.to_csv('bert_encodings.csv')\n\n# query = []\n# with open('/kaggle/input/covid19round2queries/queries2.txt', 'r', encoding='utf-8') as f1:\n#     for line in f1:\n#         # line = line.split(' ')\n#         line = line.strip('\\n')\n#         query.append(line[2:]) \n        \n        \n# from sklearn.metrics.pairwise import cosine_similarity\n# import csv\n\n# with open(\"result_queries2_5_full.csv\", \"a\") as csvfile:\n#   writer = csv.writer(csvfile)\n#   writer.writerow([\"query\", \"rank\", \"paper_id\", \"title\", \"abstract\", \"contents\", \"url\", \"journal\", \"date\", \"value\"])\n#   for i in range(9):\n#     query_en = model.encode([query[i]])[0]\n#     query_en = query_en.reshape(-1,1024)\n    \n#     result_cur = []\n#     for j in range(i*1000, i*1000+1000):\n#         row = df.loc[j]\n#         doc = row['encoding']\n#         doc = doc.reshape(-1,1024)\n#         value = cosine_similarity(doc, query_en)\n#         query_cur = query[i]\n#         paper_id = row['paper_id']\n#         result_cur.append((paper_id, value))\n#     result_cur = sorted(result_cur, key=lambda x:x[1], reverse=True)\n#     print(result_cur[:100])\n\n#     t = 0\n#     for k in range(5):\n#         for j in range(i*1000, i*1000+1000):\n#             if df.loc[j]['paper_id'] == result_cur[k][0]:\n#                 title_cur = df.loc[j]['title']\n#                 abstract_cur = df.loc[j]['abstract']\n#                 contents_cur = df.loc[j]['contents']\n#                 encoding_cur = df.loc[j]['encoding']\n#                 url_cur = df.loc[j]['url']\n#                 journal_cur = df.loc[j]['journal']\n#                 date_cur = df.loc[j]['date']\n#                 break\n#         t += 1\n#         result = []\n#         result.append(query_cur)\n#         result.append(str(t))\n#         result.append(result_cur[k][0])\n#         result.append(title_cur)\n#         result.append(abstract_cur)\n#         result.append(contents_cur)\n#         result.append(url_cur)\n#         result.append(journal_cur)\n#         result.append(date_cur)\n#         result.append(str(result_cur[k][1][0][0]))\n# #         print(result)\n#         writer.writerows([result])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Summerization  \n\n1. Use title as 'study'; If there's no title, then use the most important sentence of the first 10 sentences as 'study'.\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/cord19round2bertresult/result_queries2_5_full.csv')\nprint(df.columns)\nprint(len(df))\n# date; sentence; url; journal; method; result;    measure of evidence\n# date; study   ; url; journal;       ; summerize; ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\n\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport pandas as pd\n\nstop_words = stopwords.words('english')\n\ndef clean(content):\n\n    def get_wordnet_pos(tag):\n        if tag.startswith('J'):\n            return wordnet.ADJ\n        elif tag.startswith('V'):\n            return wordnet.VERB\n        elif tag.startswith('N'):\n            return wordnet.NOUN\n        elif tag.startswith('R'):\n            return wordnet.ADV\n        else:\n            return None\n\n    if pd.isnull(content):\n        return ''\n\n    # delete URL\n    results = re.compile(r'http://[a-zA-Z0-9.?/&=:]*', re.S)\n    content = results.sub(\"\", content)\n    content = re.sub(r'(https|http)?://(\\w|\\.|/|\\?|=|&|%|-)*\\b', '', content, flags=re.MULTILINE)\n\n    content = content.lower()\n    # delete stopwords\n    tokens = word_tokenize(content)\n    content = [i for i in tokens if not i in stop_words]\n    # s = ' '\n    # content = s.join(content)\n\n    # lemmatization\n    tagged_sent = pos_tag(content)\n    wnl = WordNetLemmatizer()\n    lemmas_sent = []\n    for tag in tagged_sent:\n        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n        lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos))\n    # print(lemmas_sent)\n    s = ' '\n    content = s.join(lemmas_sent)\n\n    # delete punctuations, but keep '-'\n    del_estr = string.punctuation\n    del_estr = list(del_estr)\n    del_estr.remove('-')\n    del_estr = ''.join(del_estr)\n    replace = \" \" * len(del_estr)\n    tran_tab = str.maketrans(del_estr, replace)\n    content = content.translate(tran_tab)\n\n    return content","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words('english')\n\nstudy = []\nfor i in range(len(df)):\n    title = df['title'][i]\n    if not isinstance(title, float):\n        study.append(title)\n    else:\n        print(\"=====\")\n        word_frequencies = {}\n        text_original = ''\n        if not pd.isnull(df['title'][i]):\n            text_original = text_original + df['title'][i] + ' '\n        if not pd.isnull(df['abstract'][i]):\n            text_original = text_original + df['abstract'][i] + ' '\n        if not pd.isnull(df['contents'][i]):\n            text_original = text_original + df['contents'][i]\n        text = clean(text_original)\n\n        for word in nltk.word_tokenize(text):\n            if word not in stopwords:\n                if word not in word_frequencies.keys():\n                    word_frequencies[word] = 1\n                else:\n                    word_frequencies[word] += 1\n\n        maximum_frequncy = max(word_frequencies.values())\n\n        for word in word_frequencies.keys():\n            word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n\n        text_cur = df['contents'][i]\n        text_cur = text_cur.replace('e.g.', 'eg')\n        text_cur = text_cur.replace('et al.', 'et al')\n        sentence_list = nltk.sent_tokenize(text_cur)[:10]\n\n        sentence_scores = {}\n        for sent in sentence_list:\n            for word in nltk.word_tokenize(sent.lower()):\n                if word in word_frequencies.keys():\n                    if len(sent.split(' ')) < 30:\n                        if sent not in sentence_scores.keys():\n                            sentence_scores[sent] = word_frequencies[word]\n                        else:\n                            sentence_scores[sent] += word_frequencies[word]\n\n        study_sentences = heapq.nlargest(1, sentence_scores, key=sentence_scores.get)\n\n        studying = ' '.join(study_sentences)\n        study.append(studying)\n        # print(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['study'] = study\nprint(df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n2. Usually main subject is the most important terms of title and abstract. So I calculated scores of sentences in title and abstract, and extracted the subject of the most important sentence as 'method'.  \n\n    * calculated word frequencies of terms in preprocessed (deleted urls, stopwords, punctuations, and lemmatized) documents\n    * calculated score of each sentence\n    * retrieved the top 1 sentences\n    * extracted subject of the sentence as 'method'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import heapq\n\nstopwords = nltk.corpus.stopwords.words('english')\n\nmethods = []\nfor i in range(len(df)):\n    word_frequencies = {}\n    text_original = ''\n    if not pd.isnull(df['title'][i]):\n        text_original = text_original + df['title'][i] + ' '\n    if not pd.isnull(df['abstract'][i]):\n        text_original = text_original + df['abstract'][i] + ' '\n    if not pd.isnull(df['contents'][i]):\n        text_original = text_original + df['contents'][i]\n    text = clean(text_original)\n\n    # print(i, text[:10])\n\n    for word in nltk.word_tokenize(text):\n        if word not in stopwords:\n            if word not in word_frequencies.keys():\n                word_frequencies[word] = 1\n            else:\n                word_frequencies[word] += 1\n\n    maximum_frequncy = max(word_frequencies.values())\n\n    for word in word_frequencies.keys():\n        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n\n    text_cur = ' '\n    if not pd.isnull(df['title'][i]):\n        text_cur = text_cur + df['title'][i] + ' '\n    if not pd.isnull(df['abstract'][i]):\n        text_cur = text_cur + df['abstract'][i] + ' '\n    text_cur = text_cur.replace('e.g.', 'eg')\n    text_cur = text_cur.replace('et al.', 'et al')\n    sentence_list = nltk.sent_tokenize(text_cur)\n    # print(sentence_list)\n\n    sentence_scores = {}\n    for sent in sentence_list:\n        for word in nltk.word_tokenize(sent.lower()):\n            if word in word_frequencies.keys():\n                if len(sent.split(' ')) < 30:\n                    if sent not in sentence_scores.keys():\n                        sentence_scores[sent] = word_frequencies[word]\n                    else:\n                        sentence_scores[sent] += word_frequencies[word]\n\n    method_sentence = heapq.nlargest(1, sentence_scores, key=sentence_scores.get)\n\n    method = ' '.join(method_sentence)\n    methods.append(method)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(methods)\nprint(len(methods))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip\n!unzip stanford-corenlp-full-2018-02-27.zip\n!cd stanford-corenlp-full-2018-02-27\n\n!echo \"Downloading CoreNLP...\"\n!wget \"http://nlp.stanford.edu/software/stanford-corenlp-4.0.0.zip\" -O corenlp.zip\n!unzip corenlp.zip\n!mv ./stanford-corenlp-4.0.0 ./corenlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install stanza\n\nimport stanza\n\n\nimport os\nos.environ[\"CORENLP_HOME\"] = \"./corenlp\"\n\nfrom stanza.server import CoreNLPClient\n\nclient = CoreNLPClient(annotators=['tokenize','ssplit', 'pos', 'lemma', 'ner'], memory='4G', endpoint='http://localhost:9001')\nprint(client)\n\nclient.start()\nimport time; time.sleep(10)\n\n!pip install pycorenlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom pycorenlp import *\nimport collections\n\nsubjects = []\n\nfor sentence in methods:\n    if len(sentence) == 0:\n        subjects.append(' ')\n        continue\n    doc = client.annotate(sentence, properties={\"annotators\":\"tokenize,ssplit,pos,depparse,natlog,openie\",\n                                                \"outputFormat\": \"json\",\n                                                \"triple.strict\":\"true\"\n                                                #  \"openie.triple.strict\":\"true\",\n                                                # \"openie.max_entailments_per_clause\":\"2\"\n                                                })\n    result = [doc[\"sentences\"][0][\"openie\"] for item in doc]\n\n    length1 = 0\n    length2 = 0\n    subject = ''\n    object_ = ''\n    for i in result:\n        # print(i)\n        for rel in i:\n            relationSent1=rel['subject']\n            if len(relationSent1) > length1:\n                length1 = len(relationSent1)\n                subject = relationSent1\n#             relationSent2=rel['object']\n#             if len(relationSent2) > length2:\n#                 length2 = len(relationSent2)\n#                 object_ = relationSent2\n#         if length2 > length1:\n#             subjects.append(object_)\n#         else:\n#             subjects.append(subject)\n        if len(subject)>10:\n            subjects.append(subject)\n        else:\n            subjects.append(' ')\n#     print(subjects)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subjects = pd.Series(subjects)\n# print(subjects)\nprint(len(subjects))\ndf[\"method\"] = subjects\n# print(df.head(5))\nprint(df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. I calculated scores of sentences in 'title', 'abstract' and 'contents', and retrieved the two most important sentence as 'result'.  \n\n    * calculated word frequencies of terms in preprocessed (deleted urls, stopwords, punctuations, and lemmatized) documents\n    * calculated score of each sentence\n    * retrieved the top 2 sentences as 'result'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import heapq\n\nstopwords = nltk.corpus.stopwords.words('english')\n\nsummerize = []\nfor i in range(len(df)):\n    word_frequencies = {}\n    text_original = ''\n    if not pd.isnull(df['title'][i]):\n        text_original = text_original + df['title'][i] + ' '\n    if not pd.isnull(df['abstract'][i]):\n        text_original = text_original + df['abstract'][i] + ' '\n    if not pd.isnull(df['contents'][i]):\n        text_original = text_original + df['contents'][i]\n    text = clean(text_original)\n\n    # print(i, text[:10])\n\n    for word in nltk.word_tokenize(text):\n        if word not in stopwords:\n            if word not in word_frequencies.keys():\n                word_frequencies[word] = 1\n            else:\n                word_frequencies[word] += 1\n#     print(len(word_frequencies))\n\n    maximum_frequncy = max(word_frequencies.values())\n#     print(maximum_frequncy)\n#     print(max(word_frequencies,key=word_frequencies.get))\n\n    for word in word_frequencies.keys():\n        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n\n    text_cur = text_original.replace('e.g.', 'eg')\n    text_cur = text_cur.replace('et al.', 'et al')\n    sentence_list = nltk.sent_tokenize(text_cur)\n#     print(sentence_list)\n\n    sentence_scores = {}\n    for sent in sentence_list:\n        for word in nltk.word_tokenize(sent.lower()):\n            if word in word_frequencies.keys():\n                if len(sent.split(' ')) < 30:\n                    if sent not in sentence_scores.keys():\n                        sentence_scores[sent] = word_frequencies[word]\n                    else:\n                        sentence_scores[sent] += word_frequencies[word]\n\n    summary_sentences = heapq.nlargest(2, sentence_scores, key=sentence_scores.get)\n#     print(len(summary_sentences))\n\n    summary = ' '.join(summary_sentences)\n    summerize.append(summary)\n#     print(summary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(summerize[:2])\nprint(len(summerize))\ndf['result'] = summerize\nprint(df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Find out #patients as 'measure of evidence'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evidence = []\nfor i in range(len(df)):\n    word_frequencies = {}\n    text_original = ''\n    if not pd.isnull(df['title'][i]):\n        text_original = text_original + df['title'][i] + ' '\n    if not pd.isnull(df['abstract'][i]):\n        text_original = text_original + df['abstract'][i] + ' '\n    if not pd.isnull(df['contents'][i]):\n        text_original = text_original + df['contents'][i]\n    text = clean(text_original)\n\n    text = text.split()\n    num = 0\n    evidence_cur = ' '\n    for i in range(len(text)):\n        if text[i] == 'patient' and text[i-1].isdigit() and text[i-2] != 'fig' and text[i-2] != 'figure' and text[i-2] != 'table':\n            num = max(int(text[i-1]), num)\n    if num != 0:\n        evidence_cur += str(num)\n        evidence_cur += 'patients'\n#     print(evidence_cur)\n    evidence.append(evidence_cur)\nprint(len(evidence))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['measure of evidence'] = evidence\nprint(df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Output","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\nfrom pandas import Series,DataFrame\nimport pandas as pd\nimport re\n\ndata = {\n#         'Query':df['query'],\n#         'Rank':df['rank'],\n        \"Date\":df['date'],\n       \"Study\":df['study'],\n       \"Study Link\":df['url'],\n       \"Journal\":df['journal'],\n       \"Method\":df['method'], \n        \"Result\":df['result'], \n        \"Measure of Evidence\":df['measure of evidence']}\ndata = DataFrame(data)\nprint(data.columns)\nprint(data.head(5))\n# print(len(data))\n\n# data.to_csv('results.csv', index = False)\n\nfor i in range(9):\n    current = data[i*5:i*5+5]\n    csv_str = df['query'][i*5]\n    csv_str = re.sub(r'[^\\w\\s]','',csv_str) + '.csv'\n#     print(csv_str)\n    current.to_csv( csv_str, index = [0,1,2,3,4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}