{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center>\n    <h1>   \n        ==#== Bankruptcy Predictive Model ==#==\n    </h1>\n</center>\n \n<center><img src=\"https://cdn.corporatefinanceinstitute.com/assets/bankruptcy-1024x627.jpeg\" alt=\"Bankruptcy\" width=\"600\" ></center>\n\n<center> <h3> Please Upvote if you like the work ðŸ˜Š </h3></center>\n\n<center>Don't hesitate to give your comment on this notebook. It gives me motivation to improve the analysis in the future</center>\n\n# Introduction\nThis program is used to find the right model for the bankruptcy model. \n\n# Contents\nContents:\n\n* [1. Load Packages](#1)\n* [2. Load Dataset](#2)\n* [3. Drop NaN and duplicates values](#3)\n* [4. Do Explanatory Data Analysis](#4)\n* [5. Mapping Sentiment](#5) \n* [6. Modelling](#6)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n# ref: #https://www.kaggle.com/jaimebecerraguerrero/simple-yet-powerful-bankrupt-prediction-model\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>1. Load Packages</b></font><br>","metadata":{}},{"cell_type":"code","source":"# runtime\nimport timeit\n\n# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# preprocessing\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n\n# Ml model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE\nfrom matplotlib import pyplot\nfrom numpy import where\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\nnp.warnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>2. Load Dataset</b></font><br>\n","metadata":{}},{"cell_type":"code","source":"data_df=pd.read_csv(\"/kaggle/input/company-bankruptcy-prediction/data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3. Find Null and Duplicate Values</b></font><br>\n","metadata":{}},{"cell_type":"code","source":"display(data_df.isnull().sum())\nprint(\"====///====\")\ndisplay(data_df.duplicated().sum())\nprint(\"====///====\")\ndisplay(data_df.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> No null values detected","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4. Feature Selection</b></font><br>","metadata":{}},{"cell_type":"markdown","source":"We have a problem regarding the large number of features to choose from. Jaime in his [notebook](https://www.kaggle.com/jaimebecerraguerrero/simple-yet-powerful-bankrupt-prediction-model) describes the feature selection using the random forest classifier which is great for reducing computational load.\n\nRecursive feature elimination (RFE) is a feature selection process that suits a model and eliminates the weakest feature (or features) before the required number of features is achieved. Features are rated by the model's coef_ or feature importances_ attributes, and RFE aims to remove dependencies and collinearity by recursively deleting a small number of features per loop.","metadata":{}},{"cell_type":"code","source":"# training set\nX = data_df.iloc[:,1:].values\ny = data_df.iloc[:,0].values.reshape(-1, 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# determining optimal number of features\nn_features = [5, 10, 15, 20, 25, 30, 35, 40]\nfor i in n_features:\n    # Building the model based feature selection\n    select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=i)\n\n    select.fit(X_train, y_train)\n\n    mask = select.get_support()\n\n    X_train_rfe = select.transform(X_train)\n    X_test_rfe = select.transform(X_test)\n\n    score = RandomForestClassifier().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n    \n    print(\"Test score: {:.3f}\".format(score), \" number of features: {}\".format(i))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=15)\n\nselect.fit(X_train, y_train)\n\nmask = select.get_support()\n\nmask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_rfe = select.transform(X_train)\nX_test_rfe = select.transform(X_test)\n\nscore = RandomForestClassifier().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n\nprint(\"Test score: {:.3f}\".format(score), \" number of features: {}\".format(15))\n\nfeatures = pd.DataFrame({'features':list(data_df.iloc[:,1:].keys()), 'select':list(mask)})\nfeatures = list(features[features['select']==True]['features'])\nfeatures.append('Bankrupt?')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df = data_df[features]\n\n\ndata_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5. Exploratory Data Analysis</b></font><br>","metadata":{}},{"cell_type":"markdown","source":"## STEP 1: UNIVARIATE ANALYSIS","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split feature as categorical and continuous features\n\ncat_features=data_df.select_dtypes(exclude=np.number).columns\ncont_features=data_df.select_dtypes(include=[np.number,'float64','int64']).columns\n\nfeatures=print(data_df.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploring categorical data\n\nplt.figure(figsize=(20,8))\nfor index, col in enumerate(cat_features, start=1):\n    plt.subplot(4,4,index)\n    plt.title(col)\n    plt.pie(data_df[col].value_counts(), labels=data_df[col].value_counts().index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploring continuous data\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,12))\nfor index, col in enumerate(cont_features, start=1):\n    plt.subplot(4,5,index)\n    plt.title(col)\n    sns.distplot(data_df[col], kde_kws={'bw':0.5}) # bw is bandwith","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Boxplot\n\nimport numpy as np\nimport seaborn as sns\ncont_dummy=cont_features#.drop('store_and_fwd_flag')\nplt.figure(figsize=(20,16))\nfor index, col in enumerate(cont_dummy, start=1):\n    plt.subplot(4,5,index)\n    plt.title(col)\n    ax = sns.boxplot(data_df[col])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> We need to overcome the outliers problem","metadata":{}},{"cell_type":"markdown","source":"# STEP 2: BIVARIATE/MULTIVARIATE ANALYSIS","metadata":{}},{"cell_type":"code","source":"corrmat = data_df.corr(method='pearson')\nf, ax = plt.subplots(figsize=(12, 10))\nsns.heatmap(corrmat, ax=ax, cmap=\"YlGnBu\", linewidths=0.1, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>6. Handling Outliers</b></font><br>","metadata":{}},{"cell_type":"code","source":"def cap_outliers(series, zscore_threshold=3, iqr_threshold=1.5, verbose=False, method=\"IQR\"):\n    if method==\"IQR\":\n        #https://stackoverflow.com/questions/59489073/how-to-not-remove-but-handle-outliers-by-transforming-using-pandas\n        '''Caps outliers to closest existing value within threshold (IQR).'''\n        Q1 = series.quantile(0.25)\n        Q3 = series.quantile(0.75)\n        IQR = Q3 - Q1\n\n        lbound = Q1 - iqr_threshold * IQR\n        ubound = Q3 + iqr_threshold * IQR\n\n        outliers = (series < lbound) | (series > ubound)\n\n        series = series.copy()\n        series.loc[series < lbound] = series.loc[~outliers].min()\n        series.loc[series > ubound] = series.loc[~outliers].max()\n\n        # For comparison purposes.\n        if verbose:\n                print('\\n'.join(\n                    ['Capping outliers by the IQR method:',\n                     f'   IQR threshold: {iqr_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"mean\":\n        '''Caps outliers to closest existing value within threshold (Modified Z-score).'''\n        median_val = series.median()\n        mad_val = series.mad() # Median absolute deviation\n\n        z_score = (series - median_val) / mad_val\n        outliers = abs(z_score) > zscore_threshold\n\n        series = series.copy()\n        series.loc[z_score > zscore_threshold] = series.loc[~outliers].max()\n        series.loc[z_score < -zscore_threshold] = series.loc[~outliers].min() \n\n        # For comparison purposes.\n        if verbose:\n                lbound = median_val - zscore_threshold * mad_val\n                ubound = median_val + zscore_threshold * mad_val\n                print('\\n'.join(\n                    ['Capping outliers by the Modified Z-score method:',\n                     f'   Z-score threshold: {zscore_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"zscore\":\n        '''Caps outliers to closest existing value within threshold (Modified Z-score).'''\n        median_val = series.median()\n        mad_val = series.mad() # Median absolute deviation\n\n        z_score = (series - median_val) / mad_val\n        outliers = abs(z_score) > zscore_threshold\n\n        series = series.copy()\n        series.loc[z_score > zscore_threshold] = series.loc[~outliers].mean()\n        series.loc[z_score < -zscore_threshold] = series.loc[~outliers].mean() \n\n        # For comparison purposes.\n        if verbose:\n                lbound = median_val - zscore_threshold * mad_val\n                ubound = median_val + zscore_threshold * mad_val\n                print('\\n'.join(\n                    ['Capping outliers by the Modified Z-score method:',\n                     f'   Z-score threshold: {zscore_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"median\":\n        '''Caps outliers to closest existing value within threshold (Modified Z-score).'''\n        median_val = series.median()\n        mad_val = series.mad() # Median absolute deviation\n\n        z_score = (series - median_val) / mad_val\n        outliers = abs(z_score) > zscore_threshold\n\n        series = series.copy()\n        series.loc[z_score > zscore_threshold] = series.loc[~outliers].max()\n        series.loc[z_score < -zscore_threshold] = series.loc[~outliers].min() \n\n        # For comparison purposes.\n        if verbose:\n                lbound = median_val - zscore_threshold * mad_val\n                ubound = median_val + zscore_threshold * mad_val\n                print('\\n'.join(\n                    ['Capping outliers by the Modified Z-score method:',\n                     f'   Z-score threshold: {zscore_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"zscore\":\n        '''Caps outliers to closest existing value within threshold (Modified Z-score).'''\n        median_val = series.median()\n        mad_val = series.mad() # Median absolute deviation\n\n        z_score = (series - median_val) / mad_val\n        outliers = abs(z_score) > zscore_threshold\n\n        series = series.copy()\n        series.loc[z_score > zscore_threshold] = series.loc[~outliers].median()\n        series.loc[z_score < -zscore_threshold] = series.loc[~outliers].median() \n\n        # For comparison purposes.\n        if verbose:\n                lbound = median_val - zscore_threshold * mad_val\n                ubound = median_val + zscore_threshold * mad_val\n                print('\\n'.join(\n                    ['Capping outliers by the Modified Z-score method:',\n                     f'   Z-score threshold: {zscore_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"log\":\n        #https://stackoverflow.com/questions/37890849/pandas-series-log-normalize\n        series=series.map(lambda x: np.log(x))\n        \n\n    return series\n\n\nremove_list = [' Net Value Growth Rate', 'Bankrupt?']   \nfeatures_treating = list(data_df.columns.values)\n\nres = [i for i in features_treating if i not in remove_list]\n\nfeatures_treating = res\nfeatures_treating\nfor i, col in enumerate(features_treating):\n    data_df[col]=cap_outliers(data_df[col],method=\"IQR\", verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verification of Deleting Outliers\n\ncont_dummy=cont_features#.drop('store_and_fwd_flag')\nplt.figure(figsize=(20,16))\nfor index, col in enumerate(cont_dummy, start=1):\n    plt.subplot(4,5,index)\n    plt.title(col)\n    ax = sns.boxplot(data_df[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>7. Modelling</b></font><br>","metadata":{}},{"cell_type":"code","source":"X=data_df.drop('Bankrupt?',axis=1) #axis=1 drop the bulk of column\ny=data_df['Bankrupt?']\n\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)\n\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.25,random_state=0)\n\n#StSc = StandardScaler()\n#X_train  = StSc.fit_transform(X_train)\n#X_test  = StSc.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, col in enumerate([XGBClassifier(eval_metric=\"logloss\"),RandomForestClassifier(),LogisticRegression()]):\n    print(col)\n    pipeModel=Pipeline([('scaler', StandardScaler()), ('model', col)])\n    \n    # the benefit of using K-Fold is that we could calculate the cross validation value using some of the methods of scoring \n    kfold = KFold(n_splits=10, shuffle=True, random_state=0)\n    cv_results = cross_val_score(pipeModel, X_train, y_train, cv=kfold)\n\n    pipeModel.fit(X_train, y_train)\n    # this is the scaled LR\n    print('Score for',col,'method:', pipeModel.score(X_test, y_test))\n    # the mean result (10 data) of negative mean squared error\n    print('Score for',col,'method using cross_val_score:', cv_results.mean())\n    y_pred=pipeModel.predict(X_test)\n    #print(mean_squared_error(y_test, y_pred))\n    print(classification_report(y_test,y_pred))\n    roc_auc_score(y_test,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ngbrt = GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=1).fit(X_train, y_train)\ny_pred=gbrt.predict(X_test)\nprint(\"training set score : {:.2f}\".format(gbrt.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(gbrt.score(X_test, y_test)))\n\nconfusion_matrix(y_test, y_pred)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>8. Conclusion</b></font><br>","metadata":{}},{"cell_type":"markdown","source":"> We have trained a model with training set score 90% and test set score 90% using Gradient Boosting Classifier. Also, we have removed outliers using Interquartiles Caping Outliers Methods. ","metadata":{}},{"cell_type":"markdown","source":"<center> <h3> Please Upvote if you like the work ðŸ˜Š </h3></center>\n<center>Don't hesitate to give your comment on this notebook. It gives me motivation to improve the analysis in the future</center>","metadata":{}}]}