{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Effects of changing hyperperimeters\n## Caution: This notebook has too much maths, not for faint-hearted.\n\nI've seen people trying to achieve highly accurate models without any insight on how to do so. They either try to use hyperperimeters randomly or just don't bother changing them at all. They go for having more data and say nothing better can be done with what they have.\n\nIn this notebook I've tried to explain that what does the change in hyperperimeters of a model have on the accuracy of the model. First I've explained **from where and why the parameters are introduced** after that tried to show the **effect of those parameters** on the model.\n\nThis notebook has a lot more math, so I recommend going slowerly. It might give you headaches in the starting but by the end you'll see it's worth the trouble.\n\nBefore reading further think of **regularization** as a way to penalize large values of learned weights $\\theta$ and avoid overfitting."},{"metadata":{},"cell_type":"markdown","source":"# Contents:\n1. **Read data and see small portion of it**\n\n\n2. **Normalize data**\n\n\n3. **Create Train/Test split**\n\n\n4. **Basic notations**\n\n\n5.  **Logistic Regression**\n\n    5.1  Effect of value of C on L2 and L1 Regularization\n    \n    5.2  Weights for L2 regularization\n    \n    5.3  Weights for L1 Regularization\n    \n    5.4  Significance of type of regularization used\n    \n    5.5  Effect of solver on accuracy\n\n\n6.  **Naive Bayes**\n    \n    6.1  Basic Notations\n    \n    6.2  Laplace smoothing\n\n\n7.  **K Nearest Neighbors**\n    \n    7.1  Algorithm vs Accuracy\n\n\n8.  **Support Vector Machine**\n\n    8.1  Hyperperameters:\n\n    8.2  Effect of type of kernel on accuracy\n\n    8.3  Effect of value of C:\n    \n    8.4  Effect of value of ùõæ in RBF kernel\n\n\n9.  **Decision Trees**\n\n    9.1  Max Depth vs Accuracy\n\n    9.2  Min samples split vs Accuracy\n    \n    9.3  Min samples leaf vs Accuracy\n    \n    9.4  Max leaf Nodes vs Accuracy\n\n\n10.  **Random Forest (Ensemble method)**\n\n\n11.  **Boosting**\n\n\n12.  **Gradient Boosting Classifier**\n    \n    12.1  Learning rate vs Accuracy\n    \n    12.2  Number of estimators\n    \n    12.3  Subsampling vs Accuracy\n\n\n13.  **AdaBoostClassifier**\n    \n    13.1  Base Estimator vs Accuracy\n    \n    13.2  Number of estimators vs Accuracy\n    \n    13.3  Algorithm vs Accuracy\n\n\n14.  **Last trick in the arsenal: Voting classifier**\n    \n    14.1  Voting Strategy Vs Accuracy\n\n\n15.  **Conclusion**\n\n\n16. **Acknowledgement**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import VotingClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read data and see small portion of it\n* We read data as a pandas DataFrame object.\n* Then look at the fields in the data.\n* Then have a look at some records of data to get an idea on what we are dealing with.\n* If doing data analysis we might want to do more then just this but for our purpose that's enough information about the data."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"data = pd.read_csv(\"../input/heart.csv\")\ndata.info()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalize data\n* Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples. \n* For polynomial kernels in SVM if we don't have normalized data it takes forever to train.\n* Normaliztaion makes algorithms execute quickly and can boost accuracy by huge amounts.\n$$\nX_{changed} = \\frac{X-X_{min}}{X_{max}-X_{min}}\n$$\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data = data.drop('target', 1)\nx_data = x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Train/Test split\n* Split the data into two parts.\n* 80% to train the model and 20% to test the model.\n* The train_test_split function has `shuffle` parameter as `True` by default. Hence the data is shuffled automatically before spliting."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    x_data, data['target'], test_size=0.2, random_state=0)\nprint(\"Number of training examples: {0}\".format(X_train.shape[0]))\nprint(\"Number of features for a single example in the dataset: {0}\".format(X_train.shape[1]))\nprint(\"Number of test examples: {0}\".format(X_test.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic notations\n* {$x^{(i)}, y^{(i)}$} represent a training example in the dataset. **Note**: $x^{(i)}$ has nothing to do with exponentiation, it just represents the $i'th$ example in the dataset.\n* $x$ represent features of an example in the dataset (age, sex, cp etc.).\n* $ y \\in \\{0,1\\} $. Where $0, 1$ corresponds to having or not having heart disease respectivaly (target).\n* $m$ be number of training examples in the dataset ($242$ in our case).\n* $h_\\theta(x)$ be the hypothesis function.\n* $n$ be the number of input features ($13$ in our case).\n* $h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n$.\n* $h_\\theta(x) = \\sum\\limits_{i=0}^{n} \\theta_ix_i$, Where we take $x_0$ = 1.\n* $X$ represents all training examples stacked column wise.\n\n$\nX = \\begin{bmatrix}x^{(1)} \\\\ x^{(2)} \\\\ x^{(3)} \\\\ ... \\\\ x^{(m)}\\end{bmatrix}\n$\n\n* $\\overrightarrow y$ represents the labels of the training examples.\n\n$\n\\overrightarrow y = \\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ y^{(3)} \\\\ ... \\\\ y^{(m)}\\end{bmatrix}\n$"},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression\nAs an optimization problem, binary class L2 penalized logistic regression minimizes the following cost function:\n\n$\n\\min_{w, c} \\frac{1}{2}w^T w + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1) .\n$\n\nSimilarly, L1 regularized logistic regression solves the following optimization problem\n\n$\n\\min_{w, c} \\|w\\|_1 + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1).\n$\n\nNote that, in this notation, it‚Äôs assumed that the observation $y_i$ takes values in the set $1,-1$ at trial $i$.\n"},{"metadata":{},"cell_type":"markdown","source":"## Effect of value of C on L2 and L1 Regularization\n* **C** is the inverse of regularization strength; must be a positive float. smaller values specify stronger regularization.\n* If you don't want regularization you need to put a very large value of $C$."},{"metadata":{"trusted":true},"cell_type":"code","source":"C_values = [0.01, 0.1, 0.5, 1, 5, 10, 100, 1000, 1e42]\naccuracies = []\nweights_l2 = []\n\nfor C in C_values:\n    clf_l2 = LogisticRegression(penalty='l2',\n                             tol=0.0001,\n                             C=C,\n                             fit_intercept=True,\n                             solver='liblinear',\n    )\n\n    # Train\n    clf_l2.fit(X_train, y_train)\n    \n    # Store wieghts for further analysis\n    weights_l2.append(clf_l2.coef_[0])\n    \n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf_l2.score(X_test, y_test))\n\nfig, ax = plt.subplots(ncols=2)\n\nfig.set_size_inches(14, 4)\n\nax[0].set_xlabel(\"Value of C\", fontsize=16)\nax[0].set_ylabel(\"Accuracy\", fontsize=16)\nax[0].set_title(\"L2 Regularization\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax[0].text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=C_values, y=accuracies, ax=ax[0])\n\n\n# For L1 regularization\naccuracies = []\nweights_l1 = []\nfor C in C_values:\n    clf_l1 = LogisticRegression(penalty='l1',\n                             C=C,\n                             fit_intercept=True,\n                             solver='liblinear',\n    )\n\n    # Train\n    clf_l1.fit(X_train, y_train)\n\n    # Store wieghts for further analysis\n    weights_l1.append(clf_l1.coef_[0])\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf_l1.score(X_test, y_test))\n\nax[1].set_xlabel(\"Value of C\", fontsize=16)\nax[1].set_ylabel(\"Accuracy\", fontsize=16)\nax[1].set_title(\"L1 Regularization\", fontsize=20)\n\nfor i, accuracy in enumerate(accuracies):\n    ax[1].text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=C_values, y=accuracies, ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* If we choose $C$ to be too small that actually turns out to be worst then using no regularization at all.\n\nNow, Lets have a look at the values of weights for no regularization, too much regularization, and the values for which we got most accuracy."},{"metadata":{},"cell_type":"markdown","source":"## For L2 regularization"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Value of C: %s\" % C_values[0])\nprint(\"Weights:\")\nprint(weights_l2[0])\n\nprint(\"Value of C: %s\" % C_values[2])\nprint(\"Weights:\")\nprint(weights_l2[2])\n\nprint(\"Value of C: %s\" % C_values[-1])\nprint(\"Weights:\")\nprint(weights_l2[-1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## For L1 Regularization"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Value of C: %s\" % C_values[0])\nprint(\"Weights:\")\nprint(weights_l1[0])\n\nprint(\"Value of C: %s\" % C_values[2])\nprint(\"Weights:\")\nprint(weights_l1[2])\n\nprint(\"Value of C: %s\" % C_values[-1])\nprint(\"Weights:\")\nprint(weights_l1[-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Significance of type of regularization used\n* The intresting part above is that in case of L1 regularization with too strong regularization we get sparse weights. i.e. using just 8 non zero values we got an accuracy of 82% which can be seen as a way of feature selection.\n* For C = $1e+42$ We get almost identical values for both regularization.\n* For C around 1.0 we get the most out of regularization. And the values of weights turns out to be too small in case of L1 regularization where as for L2 regularization weights are somewhat higher but still significantly lower then no regularization case."},{"metadata":{},"cell_type":"markdown","source":"## Effect of solver on accuracy\n* Let's fix value of C to 1 and see how changing solver affects the accuracy.\n* **Note:** **‚Äònewton-cg‚Äô, ‚Äòlbfgs‚Äô and ‚Äòsag‚Äô** only handle **L2** penalty, whereas **‚Äòliblinear‚Äô and ‚Äòsaga‚Äô** handle **L1** penalty.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"C = 1.0\nsolvers = ['newton-cg', 'lbfgs', 'sag', 'saga', 'liblinear']\n\n# For L2 regularization\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\naccuracies = []\nfor solver in solvers:\n    clf = LogisticRegression(penalty='l2',\n                             C=C,\n                             fit_intercept=True,\n                             solver=solver,\n                             max_iter=500,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nax.set_title('L2 Regularization', fontsize=20)\nfor solver in solvers:\n    ax.set_xlabel(\"Solver\", fontsize=16)\n    ax.set_ylabel(\"Accuracy\", fontsize=16)\n    for i, accuracy in enumerate(accuracies):\n        ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=solvers, y=accuracies, ax=ax)\n\n# For L1 regularization\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\naccuracies = []\nfor solver in solvers[3:]:\n    clf = LogisticRegression(penalty='l1',\n                             C=C,\n                             fit_intercept=True,\n                             solver=solver,\n                             max_iter=200,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nax.set_title('L1 Regularization', fontsize=20)\nfor solver in solvers[3:]:\n    ax.set_xlabel(\"Solver\", fontsize=16)\n    ax.set_ylabel(\"Accuracy\", fontsize=16)\n    for i, accuracy in enumerate(accuracies):\n        ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=solvers[3:], y=accuracies, ax=ax)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can't infer a lot from this apart from the fact that **liblinear** does slightly better then others. That may not be the case always.\n* The reason why we don't have a lot of choices for L1 regularization is because $||w||_1$ term in L1 regularization is not differentiable due to which solvers which rely on derivatives don't support L1 regularization. Some which support have to use some kind of tricks like [Subgradient method](https://en.m.wikipedia.org/wiki/Subgradient_method?wprov=sfla1) to make it work.\n* **'lbfgs'** takes too many iterations to converge which can be timetaking on large datasets. "},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes\n* Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes‚Äô theorem with the ‚Äúnaive‚Äù assumption of conditional independence between every pair of features given the value of the class variable.\n\n* As we have only two possible values of $y$, we will use Bernoulli Naive Bayes for our case.\n* The decision rule for Bernoulli naive Bayes is based on:\n$\n\\: P(x_i \\mid y) = P(i \\mid y) x_i + (1 - P(i \\mid y)) (1 - x_i)\n$\n. It explicitly penalizes the non-occurrence of a feature  that is an indicator for class $y$.\n\n* Naive Bayes is also Naive in terms of hyperperameters, we can only experiment with smoothing. Let's first see from where this parameter alpha comes.\n\n## Basic Notations:\n* Naive Bayes is a generative learning algorithm. i.e. instead of modeling $p(y|x)$ directly (such as logistic regression), here we try to model $p(x|y)$ (and $p(y)$).\n* For instance, if $y$ indicates whether an example is a dog(0) or an elephant(1), then $p(x|y=0)$ models the distribution of dogs' features, and p(x|y=1) models the distributions of elephants' features.\n* After modeling $p(y)$ (called class priors) and $p(x|y)$, our algorithm can use Bayes rule to derive $p(y|x)$\n$$\n    \\: p(y|x) = \\frac{p(x|y)p(y)}{p(x)}\n$$\n* Here $p(x) = p(x|y=1)p(y=1) + p(x|y=0)p(y=0)$. Thus this can be expressed in terms of the quantities we have learned.\n* In order to make predictions we don't actually need the $p(x)$ term.\n$$\nargmax_y(p(y|x)) = argmax_y \\frac{p(x|y)p(y)}{p(x)}\n$$\n$$\n= argmax_y p(x|y)p(y)\n$$\n\n* Let, \n\n$\ny \\sim Bernoulli(\\phi)\n$\n\n$\np(x_i = 1| y=1) = \\phi_{i|y=1}\n$\n\n$\np(x_i = 1 | y=0) = \\phi_{i|y=0}\n$\n\n$\np(y=1) = \\phi_y\n$\n\n* So parameters we want to learn are $\\phi_{i|y=1}$, $\\phi_{i|y=0}$, $\\phi_y$\n\n* We can write down the joint likelihood of the data as:\n $$\n \\mathcal{L}(\\phi_y, \\phi_{i|y=0}, \\phi_{i|y=1}) = \\prod\\limits_{i=1}^m{p(x^{(i)}, y^{(i)})}\n $$\n* Maximizing the above equation w.r.t  $\\phi_y, \\phi_{i|y=0}, \\phi_{i|y=1}$ we get our trained parameters.\n* To predict new example\n$$\np(y=1|x) = \\frac{p(x|y=1)p(y=1)}{p(x)}\n$$\n$$\n= \\frac{(\\prod\\limits_{i=1}^n{p(x_i|y=1)})\\ p(y=1)}{(\\prod\\limits_{i=1}^n{p(x_i|y=1)})\\ p(y=1)\\ +\\ (\\prod\\limits_{i=1}^n{p(x_i|y=0)})\\ p(y=0)}\n$$\n\n## Laplace smoothing\n* Lets take a look at the best explanation I've seen so far: This has been taken from [CS229 Lecture Notes](https://see.stanford.edu/materials/aimlcs229/cs229-notes2.pdf).\n* Consider spam/email classification, and lets suppose that, after completing CS229 and having done excellent work on the project, you decide around June 2003 to submit the work you did to the NIPS conference for publication.\n(NIPS is one of the top machine learning conferences, and the deadline for submitting a paper is typically in late June or early July.) Because you end up discussing the conference in your emails, you also start getting messages\nwith the word ‚Äúnips‚Äù in it. But this is your first NIPS paper, and until this time, you had not previously seen any emails containing the word ‚Äúnips‚Äù; in particular ‚Äúnips‚Äù did not ever appear in your training set of spam/non-\nspam emails. Assuming that ‚Äúnips‚Äù was the 35000th word in the dictionary, your Naive Bayes spam filter therefore had picked its maximum likelihood estimates of the parameters $\\phi_{35000|y}$ to be\n$$\n\\phi_{35000|y=1} = \\frac{\\sum\\limits_{i=1}^{m} 1\\{x_{35000}^{(i)} = 1 \\wedge y^{(i)}=1\\}}{\\sum\\limits_{i=1}^{m}\\{ y^{(i)}=1\\}} = 0\n$$\n$$\n\\phi_{35000|y=0} = \\frac{\\sum\\limits_{i=1}^{m} 1\\{x_{35000}^{(i)} = 1 \\wedge y^{(i)}=0\\}}{\\sum\\limits_{i=1}^{m}\\{ y^{(i)}=0\\}} = 0\n$$\n\n* I.e. because we have never seen \"npis\" before in either spam or not spam training examples, it thinks the probability of seeing it in either type of email is zero. Hence, when trying to decide if one of these messages containing ‚Äúnips‚Äù is spam, it calculates the class posterior probabilities, and obtain\n$$\np(y=1|x) = \\frac{(\\prod\\limits_{i=1}^n{p(x_i|y=1)})\\ p(y=1)}{(\\prod\\limits_{i=1}^n{p(x_i|y=1)})\\ p(y=1)\\ +\\ (\\prod\\limits_{i=1}^n{p(x_i|y=0)})\\ p(y=0)} = \\frac{0}{0}\n$$\n* Hence our algorithm doesnot know how to make a prediction in this case.\n\n* Stating the problem more broadly, it is statistically a bad idea to estimate the probability of some event to be zero just because you haven‚Äôt seen it before in your finite training set.\n\n* Take the problem of estimating the mean of a multinomial random variable z taking values in $\\{1, . . . , k\\}$. We can parameterize our multinomial with $\\phi_i = p(z = i)$. Given a set of m independent observations $\\{z (1) , . . . , z (m) \\}$, the maximum likelihood estimates are given by\n\n$$\n\\phi_j = \\frac{\\sum\\limits_{i=1}^{m} 1\\{z^{(i)} = j \\}}{m}\n$$\n\n* As we saw previously, if we were to use these maximum likelihood estimates, then some of the $\\phi_j$‚Äôs might end up as zero, which was a problem. To avoid this, we can use Laplace smoothing, which replaces the above estimate with\n$$\n\\phi_j = \\frac{\\sum\\limits_{i=1}^{m} 1\\{z^{(i)} = j \\} + \\alpha}{m+k}\n$$\n\n* Now we have:\n$$\n\\phi_{j|y=1} = \\frac{\\sum\\limits_{i=1}^{m} 1\\{x_{j}^{(i)} = 1 \\wedge y^{(i)}=1\\} + \\alpha}{\\sum\\limits_{i=1}^{m}\\{ y^{(i)}=1\\} + 2}\n$$\n$$\n\\phi_{j|y=0} = \\frac{\\sum\\limits_{i=1}^{m} 1\\{x_{j}^{(i)} = 1 \\wedge y^{(i)}=0\\} + \\alpha}{\\sum\\limits_{i=1}^{m}\\{ y^{(i)}=0\\} + 2}\n$$\n\n* The parameter $\\alpha$ in the above equation is the one we are going to vary and see how our results vary."},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha_values = [0, 0.8, 1.0, 5.0, 100.0, 200.0, 230.0, 300.0, 500.0, 1000.0, 10000.0]\n\naccuracies = []\nfor alpha in alpha_values:\n    clf = BernoulliNB(alpha=alpha)\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Value of $\\\\alpha$\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"Naive Bayes\", fontsize=20)\n\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=alpha_values, y=accuracies, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* If we choose high values of smoothing we will loose a lot of accuracy. And also we saw that no smoothing may result in numerical errors (divide by 0)."},{"metadata":{},"cell_type":"markdown","source":"# K Nearest Neighbors\n* Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n\n* Here hyperperameters are:\n* **K**: The number of nearest neighbors to compare the new point to.\n* **Algorithm**: The algorithm to be used to calculate nearest neighbors ('ball_tree', 'kd_tree', 'brute'). \n* **Note**: I'm not considering 'leaf_size' as a hyperparameter because it benifits computation not accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"neighbors = [1, 2, 5, 10, 14, 16, 20, 25, 32, 50, 60]\n\naccuracies = []\nfor neighbor in neighbors:\n    clf = KNeighborsClassifier(\n        n_neighbors=neighbor,\n        algorithm='brute',\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Value of $K$\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"K Nearest Neighbors\", fontsize=20)\n\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=neighbors, y=accuracies, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Algorithm vs Accuracy\n* Let's set K = 5 as that gave us the best accuracy.\n* compare the three algorithms.\n* **Brute force** algorithm does N comparisions to predict a new point where N in number of points.\n* **KD-Tree** exploits tree data structures, it generalizes two-dimensional Quad-trees and 3-dimensional Oct-trees to an arbitrary number of dimensions. Requires $O(log(N))$ time complexity to predict a new point. \n* **Ball trees** partition data in a series of nesting hyper-spheres. And a single comparision is needed to predict a point."},{"metadata":{"trusted":true},"cell_type":"code","source":"algorithms = ['ball_tree', 'kd_tree', 'brute']\n\naccuracies = []\nfor algorithm in algorithms:\n    clf = KNeighborsClassifier(\n        n_neighbors=5,\n        algorithm=algorithm,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Algorithm\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"K Nearest Neighbors\", fontsize=20)\n\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=algorithms, y=accuracies, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see that it makes no difference on how the nearest neighbours are found we get the same results.\n* It turns out the **algorithms** help in **reducing computational time** not the accuracy of the algorithm."},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machine\n* A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.\n* Given training vectors $x_i \\in R^p$, $i=1,‚Ä¶, n$, in two classes, and a vector $y \\in \\{1,-1\\}$, SVC solves the following primal problem:\n$$\n\\begin{align}\\begin{aligned}\\min_ {w, b, \\zeta} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} \\zeta_i\\\\\\begin{split}\\textrm {subject to } & y_i (w^T \\phi (x_i) + b) \\geq 1 - \\zeta_i,\\\\\n& \\zeta_i \\geq 0, i=1, ..., n\\end{split}\\end{aligned}\\end{align}\n$$\n\n* The decision function is:\n$$\n\\operatorname{sgn}(\\sum_{i=1}^n y_i \\alpha_i K(x_i, x) + \\rho)\n$$\n\n## Hyperperameters:\n* **C**: Same as we described in logistic regression.  **C** is the inverse of regularization strength; must be a positive float. smaller values specify stronger regularization.\n* **Kernal**: The kernel to use (linear, polynomial, rbf, sigmoid).\n* **gamma**: Kernel coefficient for rbf, poly and sigmoid.\n* **degree**: Degree of the polynomial kernel function (‚Äòpoly‚Äô)."},{"metadata":{},"cell_type":"markdown","source":"## Effect of type of kernel on accuracy\n* **linear**: $\\langle x, x'\\rangle$.\n* **polynomial**: $(\\gamma \\langle x, x'\\rangle + r)^d$. $d$ is specified by keyword `degree`, $r$ by `coef0`. In practice these kernels turns out to be too slow if we don't normalize data.\n* **rbf (Radial Basis Function)**: $\\exp(-\\gamma \\|x-x'\\|^2)$. $\\gamma$ is specified by keyword `gamma`, must be greater than 0. This is an example of infinite dimensional kernel. I.e. it represents the data in an infinite dimensional space.\n* **sigmoid**: $\\tanh(\\gamma \\langle x,x'\\rangle + r)$, where $r$ is specified by `coef0`."},{"metadata":{"trusted":true},"cell_type":"code","source":"kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n\naccuracies = []\nfor kernel in kernels:\n    clf = clf = svm.SVC(\n        kernel=kernel,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Kernel\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"SVM Classifier\", fontsize=20)\n\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=kernels, y=accuracies, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The *linear kernel* outperforms other two by a very significant margin. Which is not usually true in most of the classification tasks. rbf turns out to be better most of the times."},{"metadata":{},"cell_type":"markdown","source":"## Effect of value of C:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"C_values = [0.01, 0.1, 0.2, 0.5, 1, 5, 10, 20]\n\naccuracies = []\n\nfor C in C_values:\n    clf = clf = svm.SVC(\n        C=C,\n        kernel='linear',\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\n    # Train\n    clf.fit(X_train, y_train)\n\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Value of C\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"Effect of C SVM\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=C_values, y=accuracies, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Effect of value of $\\gamma$ in RBF kernel\n* It's kernel coefficient for rbf, poly and sigmoid.\n* When training an SVM with the Radial Basis Function (RBF) kernel, gamma defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected."},{"metadata":{"trusted":true},"cell_type":"code","source":"gammas = [0.01, 0.1, 0.2, 0.5, 1, 5]\n\naccuracies = []\n\nfor gamma in gammas:\n    clf = clf = svm.SVC(\n        C=5,\n        kernel='rbf',\n        gamma=gamma,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\n    # Train\n    clf.fit(X_train, y_train)\n\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Value of $\\\\gamma$\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"Effect of $\\\\gamma$ SVM\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=gammas, y=accuracies, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Value of $\\gamma$ can dramatically increase accuracy as we see above."},{"metadata":{},"cell_type":"markdown","source":"# Decision Trees\n\n* Decision trees are non-linear classifiers, capable of performing multi-class classification on a dataset.\n* The bases of this classifier is to divide input space $\\chi$ into disjoint subsets (or regions) $R_i$.\n\n$$\n\\chi = \\bigcup\\limits_{i=0}^{n} R_i \\\\\ns.t. R_i \\cap R_j = \\phi\\ for\\ i\\ \\neq j\n$$\n\n* To select regions we use a greedy, top-down, recursive partitioning. Split is done into two child reions by thresholding a single feature. Then take it's childern in a recursive manner, always selecting a leaf node, a feature, and a threshold to form a new split. Given a parent region $R_p$, a feature index $j$, and a threshold $t \\in R$, we obtain two child regions $R_1$ and $R_2$ as follows:\n$$\n    R_1 = \\{X|X_j < t, X \\in R_p \\} \\\\\n    R_2 = \\{X|X_j \\ge t, X \\in R_p \\}\n$$\n\n* In order to choose our splits we need to define a loss function. For a classification problem, we are intrested in the misclassification loss $L_{misclass}$. For region $R$ let $\\hat{p_c}$ be the proportion of examples in R that are of class c. Misclassification loss on $R$ can be written as:\n$$\nL_{misclass}(R) = 1- max_c(\\hat{p_c})\n$$\n* A more senstive loss is cross-entropy loss.\n$$\nL_{cross}(R) = - \\sum\\limits_{c} \\hat{p_c} log_2 \\hat{p_c}\n$$\n* Or you can use [Gini impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity).\n* Decision trees have a simple interpratation but have too many hyperperemeters.\n* **criterion** : The loss function to measure the quality of a split. ('crossentropy' or 'gini').\n\n* **max_depth** : The maximum allowed depth of the tree.\n\n* **min_samples_split** : The minimum number of samples required to split an internal node.\n\n* **min_samples_leaf** : The minimum number of samples required to be at a leaf node.\n\n* **max_leaf_nodes** : The maximum number of leaf nodes.\n\n* **min_impurity_decrease** : A node will be split if this split induces a decrease of the impurity greater than or equal to this value. **Note** : This is problematic approach as the greedy, single feature at a time approach of decision tree could mean missing higher order interactions."},{"metadata":{},"cell_type":"markdown","source":"## Max Depth vs Accuracy\n* The maximum allowed depth of the tree."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depths = [2, 3, 4, 5, 10, 20, 50, 100]\naccuracies = []\n\n# For cross entropy loss\nfor max_depth in max_depths:\n    clf = DecisionTreeClassifier(criterion='entropy',\n                                max_depth=max_depth,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n    \n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots(ncols=2)\nfig.set_size_inches(14, 4)\n\nax[0].set_xlabel(\"Max Depth\", fontsize=16)\nax[0].set_ylabel(\"Accuracy\", fontsize=16)\nax[0].set_title(\"Cross Entropy\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax[0].text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=max_depths, y=accuracies, ax=ax[0])\n\naccuracies = []\n\n# For Gini loss\nfor max_depth in max_depths:\n    clf = DecisionTreeClassifier(criterion='entropy',\n                                max_depth=max_depth,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n    \n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nax[1].set_xlabel(\"Max Depth\", fontsize=16)\nax[1].set_ylabel(\"Accuracy\", fontsize=16)\nax[1].set_title(\"Gini impurity\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax[1].text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=max_depths, y=accuracies, ax=ax[1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Min samples split vs Accuracy\n* The minimum number of samples required to split an internal node."},{"metadata":{"trusted":true},"cell_type":"code","source":"min_samples_splits = [2, 3, 4, 5, 10, 20, 50, 100]\n\naccuracies = []\n\nfor min_samples_split in min_samples_splits:\n    clf = DecisionTreeClassifier(criterion='gini',\n                                 max_depth=3,\n                                 min_samples_split=min_samples_split,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n    \n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Min Samples Split\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"Min Samples Split vs Accuracy\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=min_samples_splits, y=accuracies, ax=ax)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Min samples leaf vs Accuracy\n* The minimum number of samples required to be at a leaf node."},{"metadata":{"trusted":true},"cell_type":"code","source":"min_samples_leaves = [2, 3, 4, 5, 10, 20, 50, 100]\n\naccuracies = []\n\nfor min_samples_leaf in min_samples_leaves:\n    clf = DecisionTreeClassifier(criterion='gini',\n                                 max_depth=3,\n                                 min_samples_leaf=min_samples_leaf,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n    \n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Min Samples Leaf\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"Min Samples Leaf vs Accuracy\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=min_samples_leaves, y=accuracies, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Max leaf Nodes vs Accuracy\n* The maximum number of leaf nodes."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_leaves_nodes = [2, 3, 4, 5, 6, 7, 10, 20, 50, 100]\n\naccuracies = []\n\nfor max_leaf_nodes in max_leaves_nodes:\n    clf = DecisionTreeClassifier(criterion='gini',\n                                 max_depth=3,\n                                 max_leaf_nodes=max_leaf_nodes,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n    \n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Max Leaf Nodes\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"Max Leaf Nodes vs Accuracy\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=max_leaves_nodes, y=accuracies, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest (Ensemble method)\n* A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if `bootstrap=True`.\n* Bootstrap is a method from statistics traditionally used to measure uncertainty of some extimator (e.g. mean).\n* This is an ensemble of number of decision trees and hence will also contain the hyperperemeters from **decision tree** classifiers. In addition to those we only have two more hyperperemeters.\n* **Number of estimators** : Number of trees to use for the estimation.\n* **Bootstrap**: Wheather to use bootstraping or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_estimators = [2, 3, 4, 5, 10, 20, 50, 100, 200]\naccuracies = []\n\n# With bootstraping\nfor n_estimators in num_estimators:\n    clf = RandomForestClassifier(\n        criterion='gini',\n        max_depth=3,\n        min_samples_leaf=4,\n        max_leaf_nodes=5,\n        n_estimators=n_estimators,\n        bootstrap=True,\n        random_state=10,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots(ncols=2)\nfig.set_size_inches(14, 4)\n\nax[0].set_xlabel(\"Number of estimators\", fontsize=16)\nax[0].set_ylabel(\"Accuracy\", fontsize=16)\nax[0].set_title(\"With Bootstraping\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax[0].text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=num_estimators, y=accuracies, ax=ax[0])\n\naccuracies = []\n\n# Without bootstraping\nfor n_estimators in num_estimators:\n    clf = RandomForestClassifier(\n        criterion='gini',\n        max_depth=3,\n        min_samples_leaf=4,\n        max_leaf_nodes=5,\n        n_estimators=n_estimators,\n        bootstrap=False,\n        random_state=10,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n    \n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nax[1].set_xlabel(\"Number of estimators\", fontsize=16)\nax[1].set_ylabel(\"Accuracy\", fontsize=16)\nax[1].set_title(\"Without Bootstraping\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax[1].text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=num_estimators, y=accuracies, ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Boosting\n* Boosting is used for bias-reduction. We therefore want high bias, low varience models, also know as weak learners.\n* The general idea of boosting is to draw samples randomly while increasing probability of a sample which has been predicted incorrectly and decreasing probability of other samples which were correctly classified.\n* Here we will Exploit two boosting classifiers **GradientBoostingClassifier** and **AdaBoostClassifier**."},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting Classifier\n* Gradient Tree Boosting or Gradient Boosted Regression Trees (GBRT) is a generalization of boosting to arbitrary differentiable loss functions. GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. Gradient Tree Boosting models are used in a variety of areas including Web search ranking and ecology.\n\n* The advantages of GBRT are:\n* Natural handling of data of mixed type (= heterogeneous features)\n* Predictive power\n* Robustness to outliers in output space (via robust loss functions)\n\n* The disadvantages of GBRT are:\n* Scalability, due to the sequential nature of boosting it can hardly be parallelized.\n\n* GBRT considers additive models of the following form:\n$$\nF(x) = \\sum_{m=1}^{M} \\gamma_m h_m(x)\n$$\n\n* Gradient Tree Boosting uses decision trees of fixed size as weak learners. Decision trees have a number of abilities that make them valuable for boosting, namely the ability to handle data of mixed type and the ability to model complex functions.\n* Similar to other boosting algorithms, GBRT builds the additive model in a greedy fashion:\n$$\nF_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\n$$\n* Where the newly added tree $h_m$ tries to minimize the loss L, given the previous ensemble $F_{m-1}$:\n$$\nh_m =  \\arg\\min_{h} \\sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) + h(x_i)).\n$$\n\n* Gradient Boosting attempts to solve this minimization problem numerically via steepest descent: The steepest descent direction is the negative gradient of the loss function evaluated at the current model $F_{m-1}$ which can be calculated for any differentiable loss function:\n$$\nF_m(x) = F_{m-1}(x) - \\gamma_m \\sum_{i=1}^{n} \\nabla_F L(y_i, F_{m-1}(x_i))\n$$\n\n* Where the step length $\\gamma_m$ is chosen using line search:\n$$\n\\gamma_m = \\arg\\min_{\\gamma} \\sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) - \\gamma \\frac{\\partial L(y_i, F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)})\n\\\\\n$$\n\n* Apart from the decision tree hyperperemeters. The new hyperperemeters are:\n* **loss** : Loss function ($L$ in above equations) to be optimized (deviance, exponential).\n* **Learning rate**: Learning rate ($\\gamma_m$) shrinks the contribution of each tree by learning_rate. There is a trade-off between learning rate and number of estimators.\n* **Number of estimators** : The number of boosting stages($M$) to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n* **subsample** : The fraction of samples to be used for fitting the individual base learners.\n* **criterion** : The function to measure the quality of a split (friedman_mse(mean squared error with improvement score by Friedman), mse(mean squared error), mae( mean absolute error))."},{"metadata":{},"cell_type":"markdown","source":"## Learning rate vs Accuracy\n* Learning rate ($\\gamma_m$) shrinks the contribution of each tree by learning_rate. There is a trade-off between learning rate and number of estimators."},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n\naccuracies = []\n\n# With deviance loss\nfor learning_rate in learning_rates:\n    clf = GradientBoostingClassifier(\n        loss='deviance',\n        learning_rate=learning_rate,\n        max_depth=3,\n        min_samples_leaf=4,\n        max_leaf_nodes=5,\n        random_state=10,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots(ncols=2)\nfig.set_size_inches(14, 4)\n\nax[0].set_xlabel(\"Learning rate\", fontsize=16)\nax[0].set_ylabel(\"Accuracy\", fontsize=16)\nax[0].set_title(\"With Deviance\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax[0].text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=learning_rates, y=accuracies, ax=ax[0])\n\naccuracies = []\n\n# With exponential loss\nfor learning_rate in learning_rates:\n    clf = GradientBoostingClassifier(\n        loss='exponential',\n        learning_rate=learning_rate,\n        max_depth=3,\n        min_samples_leaf=4,\n        max_leaf_nodes=5,\n        random_state=10,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n    \n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nax[1].set_xlabel(\"Learning rate\", fontsize=16)\nax[1].set_ylabel(\"Accuracy\", fontsize=16)\nax[1].set_title(\"Exponential Loss\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax[1].text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=learning_rates, y=accuracies, ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We see that learning rate has a lot of impace on accuracy. Too slow learning and too fast learning both are problematic."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Number of estimators\n* The number of boosting stages($M$) to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_estimators = [2, 3, 4, 5, 10, 20, 50, 100, 200]\naccuracies = []\n\n# With bootstraping\nfor n_estimators in num_estimators:\n    clf = GradientBoostingClassifier(\n        n_estimators=n_estimators,\n        loss='deviance',\n        learning_rate=0.1,\n        max_depth=3,\n        min_samples_leaf=4,\n        max_leaf_nodes=5,\n        random_state=10,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Number of estimators\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"Number of estimators Vs accuracy\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=num_estimators, y=accuracies, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Subsampling vs Accuracy\n* The fraction of samples to be used for fitting the individual base learners."},{"metadata":{"trusted":true},"cell_type":"code","source":"subsampling_values = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\naccuracies = []\n\n# With bootstraping\nfor subsample in subsampling_values:\n    clf = GradientBoostingClassifier(\n        subsample=subsample,\n        n_estimators=50,\n        loss='deviance',\n        learning_rate=0.1,\n        max_depth=3,\n        min_samples_leaf=4,\n        max_leaf_nodes=5,\n        random_state=10,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Subsampling\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"Subsampling Vs accuracy\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 3), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=subsampling_values, y=accuracies, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AdaBoostClassifier\n* The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights $w_1, w_2, ..., w_n$ to each of the training samples. Initially, those weights are all set to $w_i = 1/N$, so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence.\n* There is not much math to explain on where hyperparameters come from in this case. Learning rate is analogous to learning rate in gradient boosting classifier.\n* **base_estimator**: The classifier to be used as weak learner.\n* **n_estimators**: Number of estimators to be used for prediction.\n* **algorithm**: (SAMME, SAMME.R) If ‚ÄòSAMME.R‚Äô then use the SAMME.R real boosting algorithm. base_estimator must support calculation of class probabilities. If ‚ÄòSAMME‚Äô then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations."},{"metadata":{},"cell_type":"markdown","source":"## Base Estimator vs Accuracy\n* The classifier to be used as weak learner."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclassifiers = [\n    svm.SVC(probability=True, C=5, kernel='rbf', gamma=0.5,),\n    LogisticRegression(penalty='l2', tol=0.0001, C=1.0, fit_intercept=True, solver='liblinear'),\n    RandomForestClassifier(criterion='gini', max_depth=3, min_samples_leaf=4, max_leaf_nodes=5,\n                           n_estimators=10, bootstrap=True,random_state=10\n    ),\n    DecisionTreeClassifier(criterion='gini', max_depth=3, max_leaf_nodes=5),\n    GradientBoostingClassifier(n_estimators=50, subsample=0.6, loss='deviance', learning_rate=0.1, max_depth=3,\n                               min_samples_leaf=4, max_leaf_nodes=5, random_state=10,\n    ),\n]\n\nclassifier_names=['SVM', 'LogisticRegression', 'RandomForest', 'DecisionTree', 'GradientBoosting']\n\naccuracies = []\n\n# With bootstraping\nfor classifier in classifiers:\n    clf = AdaBoostClassifier(\n        base_estimator=classifier,\n        learning_rate=0.1,\n        random_state=10,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Number of estimators\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"Number of estimators Vs accuracy\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=classifier_names, y=accuracies, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here we see that it's not the case that we get better accuracies by using ensemble instead of using a single classifier. The SVM used above has an individual accuracy of 89% but with AdaBoost it's accuracy is decreased by 4%."},{"metadata":{},"cell_type":"markdown","source":"## Number of estimators vs Accuracy\n* Number of estimators to be used for prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_estimators = [2, 3, 4, 5, 10, 20, 50, 100, 200]\naccuracies = []\n\n# With bootstraping\nfor n_estimators in num_estimators:\n    clf = AdaBoostClassifier(\n        n_estimators=n_estimators,\n        learning_rate=0.1,\n        random_state=10,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Number of estimators\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"Number of estimators Vs accuracy\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=num_estimators, y=accuracies, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Can't predict any pattern from this. We see that it varies a lot."},{"metadata":{},"cell_type":"markdown","source":"## Algorithm vs Accuracy\n* If ‚ÄòSAMME.R‚Äô then use the SAMME.R real boosting algorithm. base_estimator must support calculation of class probabilities. If ‚ÄòSAMME‚Äô then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations."},{"metadata":{"trusted":true},"cell_type":"code","source":"algorithms = ['SAMME', 'SAMME.R']\naccuracies = []\n\nfor algorithm in algorithms:\n    clf = AdaBoostClassifier(\n        n_estimators=5,\n        learning_rate=0.1,\n        random_state=10,\n        algorithm=algorithm,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Algorithms\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"Agorithm Vs accuracy\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 2), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=algorithms, y=accuracies, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Last trick in the arsenal: Voting classifier\n* Voting classifier is an ensemble of classifiers which can be used to get a little more out the best calibrated classifiers."},{"metadata":{},"cell_type":"markdown","source":"## Voting Strategy Vs Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"votings = ['hard', 'soft']\n\naccuracies = []\n\nclassifiers = [\n    ('SVM',svm.SVC(probability=True, C=5, kernel='rbf', gamma=0.5)),\n    ('LR',LogisticRegression(penalty='l2', tol=0.0001, C=1.0, fit_intercept=True, solver='liblinear')),\n    ('RF',RandomForestClassifier(criterion='gini', max_depth=3, min_samples_leaf=4, max_leaf_nodes=5,\n                           n_estimators=10, bootstrap=True,random_state=10\n    )),\n    ('DT',DecisionTreeClassifier(criterion='gini', max_depth=3, max_leaf_nodes=5)),\n    ('GB',GradientBoostingClassifier(n_estimators=50, subsample=0.6, loss='deviance', learning_rate=0.1, max_depth=3,\n                               min_samples_leaf=4, max_leaf_nodes=5, random_state=10,\n    )),\n    ('AB',AdaBoostClassifier(n_estimators=5, learning_rate=0.1, random_state=10)),\n]\n\nfor voting in votings:\n    clf = VotingClassifier(\n        estimators=classifiers,\n        voting=voting,\n    )\n\n    # Train\n    clf.fit(X_train, y_train)\n\n    # Calculate the mean accuracy on the given test data and labels.\n    accuracies.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 4)\n\nax.set_xlabel(\"Voting Strategy\", fontsize=16)\nax.set_ylabel(\"Accuracy\", fontsize=16)\nax.set_title(\"Voting Strategy Vs Accuracy\", fontsize=20)\nfor i, accuracy in enumerate(accuracies):\n    ax.text(i, accuracy, np.round(accuracies[i], 3), color='black', ha=\"center\", fontsize=14)\n\nsns.barplot(x=votings, y=accuracies, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Voting strategy in itself can give drastic changes in the accuracy."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n## Final accuracy: 90.2%\n* We saw how tuning hyperperimeters can be used to gain 5-10% accuracy in classifiers.\n* This 5-10% increase can win you competitions.\n* We can never say which set of hyperperameters will work best, but we can surely say it's worth tuning those.\n* Some hyperperameters may be less effective then others. Knowing this we can save ourself some time.\n* For **large datasets** we can't do such a through analysis but still we can change a few hyperperemeters which usually make a lot of difference.\n* Some of the hyperperameters are more intutive then others, while others just happen to exist.\n* Voting classifiers can give you a slight edge in the end if you plan to use multiple classifiers.\n* **Tree classifiers** tend to have more hyperperemeters then other classifiers and the hyperperemeter values that works tend to be very unstable in the sense that we are not able to predict any definite pattern for those.\n* If we have **large datasets** and can't play around with hyperparameters a lot then **SVM** seems to be a better choice. Trees even though give comparable accuracy tend to be very complicated when it comes to tuning hyperparameters.\n* **Note: The hyperperemeters I've tuned isn't a complete set and hence we never know what's yet to come.**"},{"metadata":{},"cell_type":"markdown","source":"# Acknowledgement\n\n#### The content for this notebook is taken mostly from two resources:\n* [Sklearn User Guide](https://scikit-learn.org/stable/user_guide.html).\n* [CS229 Leacture Notes](https://see.stanford.edu/course/cs229).\n"},{"metadata":{},"cell_type":"markdown","source":"### Thanks for reading."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}