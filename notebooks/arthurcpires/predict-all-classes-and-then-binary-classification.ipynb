{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"id":"koLb9XdoALPx"},"cell_type":"markdown","source":"# 1.0 Importing the dataset directly from Kaggle \n\n\n"},{"metadata":{"id":"Jf_3i9tNFNI5"},"cell_type":"markdown","source":"## 1.1 Importing packages and dataset\n\n"},{"metadata":{"id":"riq-q5XBARmm","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"id":"zKpfIVlTA5OR","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\nfrom scipy.stats import pearsonr, skew, boxcox, chi2\nfrom scipy.stats.stats import pearsonr\nfrom scipy import stats\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import plot_confusion_matrix,classification_report,mean_absolute_error\nfrom sklearn.svm import SVC\n\nfrom xgboost import XGBClassifier\n\nfrom pprint import pprint\n\nfrom imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"id":"DNPrK_-vBHh8","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"zhGUQL3_IqLu","trusted":true},"cell_type":"code","source":"# Plot configurations\nplt.rcParams.update({\n    'font.size': 12,\n    'axes.titlesize': 20,\n    'figure.figsize': (24,15)\n})","execution_count":null,"outputs":[]},{"metadata":{"id":"10Qgt8uwfrZZ"},"cell_type":"markdown","source":"## 1.2 Custom functions"},{"metadata":{"id":"wn5QhcL4fv_R","trusted":true},"cell_type":"code","source":"def confusion_matrix(model,X,y):\n  matrix = plot_confusion_matrix(model,X,y,normalize='true',cmap=plt.cm.Reds)\n  matrix.ax_.set_title('Confusion Matrix')\n  plt.xlabel('Predicted Label')\n  plt.ylabel('True Label')\n  plt.gcf().set_size_inches(15,8)\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"FeQCUoF0Fy0_"},"cell_type":"markdown","source":"# 2.0 Exploratory Data Analysis (EDA) \n"},{"metadata":{"id":"EZGNbKjgvq6H"},"cell_type":"markdown","source":"## 2.1 Missing data and data info\n"},{"metadata":{"id":"8sQ1MMROIfWP","outputId":"37dc743d-35ce-48ed-f4f6-ccdea8b69fcd","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"rBSVQpYJGy55","outputId":"346f0ec0-73b1-4e91-dbd5-298f74929b97","trusted":true},"cell_type":"code","source":"# Lets see if there is missing data first\nis_missing = df.isnull()\nsns.heatmap(is_missing)","execution_count":null,"outputs":[]},{"metadata":{"id":"t5DWlKRcHnpw","outputId":"a2c85633-27a1-47db-8d57-3ef11dceb26b","trusted":true},"cell_type":"code","source":"df.info() ","execution_count":null,"outputs":[]},{"metadata":{"id":"w2Hqg41aImrt","outputId":"114d435d-6f01-4221-ca18-fb0deb43ad45","trusted":true},"cell_type":"code","source":"# Lets see if the variable 'quality' has an even distribution of data\nsns.countplot(df['quality'])","execution_count":null,"outputs":[]},{"metadata":{"id":"T3dWOOnLJJW7"},"cell_type":"markdown","source":"The classes 3,4 and 8 are much lower than 5 6 and 7. Lets see how much we have in each class\n\n"},{"metadata":{"id":"x2uyQxh_JQzg","outputId":"7d719b49-6f1d-4612-c8f0-35031c0ab9c3","trusted":true},"cell_type":"code","source":"df['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"aPzPINZblDIo"},"cell_type":"markdown","source":"We will have to correct class imbalance later"},{"metadata":{"id":"OWV97KDQvzHe"},"cell_type":"markdown","source":"## 2.2 Plotting the variables "},{"metadata":{"id":"_4lgEFwi8W_5","trusted":true},"cell_type":"code","source":"y = df['quality']\nX = df.drop(['quality'],axis=1)\nX_cols = X.columns\n\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.80,random_state=0) ","execution_count":null,"outputs":[]},{"metadata":{"id":"Rb6MAL2iMO-Q"},"cell_type":"markdown","source":"Lets boxplot all variables and see if our variables are skewed and have lots of outliers"},{"metadata":{"id":"S6ZNJA7MMQLd","outputId":"54f4c4e8-7088-43f0-c059-057479388412","trusted":true},"cell_type":"code","source":"for i in range(11):\n  plt.subplot(3,4,i+1)\n  sns.boxplot(y=X[X.columns[i]],x=y)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"opxXlakrYDOQ"},"cell_type":"markdown","source":"The most weird outliers due to their large range are given by the following variables:\n\n- chlorides\n- sulphates\n- residual sugar \n\nIs there a reason to eliminate them from our prediction model? I don't know since I´m not an expert it the wine industry to see something bizarre. However, we can test the model accuracy with and without outliers to see the accuracy and decide. \n\nAnother option is to transform the data to treat these outliers\n\n\n"},{"metadata":{"id":"HEUJ-zPOF__F","outputId":"53b325b0-133e-47a1-eebb-7dfc80be72a9","trusted":true},"cell_type":"code","source":"# Since no data is missing, lets look at the summary of the dataset and see if we find something weird \ndisplay(df.describe())\nprint(df.shape)\nprint(df.skew())","execution_count":null,"outputs":[]},{"metadata":{"id":"iZhIg3y9kQkB"},"cell_type":"markdown","source":"It seems that most of our data is skewed, which is bad for the ML algorithm. Lets see if we can put a Gaussian distribution on top of them\n"},{"metadata":{"id":"SZmN_CBokcYa","outputId":"b1b4310d-e085-4134-bbae-816a21a4a3c7","trusted":true},"cell_type":"code","source":"for i in range(11):\n  plt.subplot(3,4,i+1)\n  sns.distplot(df[df.columns[i]])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"6qTHLD5eGY3D"},"cell_type":"markdown","source":"It is important for our data to have a Gaussian Distribution, which is not the case for most of the variables seen above.\n\nThese variables must be scaled while also dealing with the outliers present in the dataset\n"},{"metadata":{"id":"MQu0y0R3rtzh","outputId":"a1ff1899-6751-42ac-d0c1-965e0c69b9ab","trusted":true},"cell_type":"code","source":"corrMatrix = df.corr(method='spearman') # spearman coefficient can be used on numerical and ordinal variables (quality is ordinal). Pearson only has meaning for numerical values. \ng = sns.clustermap(corrMatrix, cmap=\"coolwarm\", linewidths=1,annot=True,vmin = -1.0 , vmax=1.0, cbar_kws={\"ticks\":[-1,-0.5,0,0.5,1]}) \n\ng.fig.set_figwidth(15)\ng.fig.set_figheight(11)\n\n# Clustermaps do hierarchical clustering and orders the rows and columns based on similarity, making it easier to see correlations","execution_count":null,"outputs":[]},{"metadata":{"id":"s46yu4ITr3oz"},"cell_type":"markdown","source":"Correlation maps only has meaning when looking at numerical values, so lets ignore to variable 'quality' and look for correlations between the independent variables\n\nFor the dependent variable, we can see that the most important variables for the model are likely to be:\n- alcohol (0.48)\n- sulphates (0.38)\n- volatile acidity (-0.38)\n\nThere is also the chance that the below variables will be important if we were more linient on the choices:\n- citric acid (0.21)\n- total sulfur dioxide (0.2)\n- chlorides (-0.19)\n- density (-0.18)\n\nNo analysis on the independent variables was done. \n\n\n"},{"metadata":{"id":"ZSUErsaqbbpd"},"cell_type":"markdown","source":"# 3.0 Reference Model\n\nWe will create a model without any preprocessing to have a reference of the precision\n"},{"metadata":{"id":"L1xY78tQcRU3","outputId":"99286ebe-8a65-4e05-f88c-02214a6b8b44","trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100, random_state=0)\nrandom_forest.fit(X_train, y_train)\npred = random_forest.predict(X_test)\n\ncross_val = cross_val_score(random_forest,X_train,y_train,cv=2,scoring=\"accuracy\")\n\nprint('Test MAE :\\n', mean_absolute_error(y_test,pred))\nprint('Train accuracy score:\\n',random_forest.score(X_train, y_train))\nprint('Test accuracy score:\\n',random_forest.score(X_test,y_test))\nprint('Cross validation accuracy:\\n',cross_val.mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"pTDIJUtLc1VO","outputId":"8a15fa17-9c8b-441a-b3f5-77faa07cadd6","trusted":true},"cell_type":"code","source":"print(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"EHRzntqResTi","outputId":"495de904-a95d-408e-aab2-4cabbe92465c","trusted":true},"cell_type":"code","source":"confusion_matrix(random_forest,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"hliM_zsAhDXv"},"cell_type":"markdown","source":"# 4.0 Preprocessing the data for classification problem\n\n\nWe shall apply transforms to the variables to see how it improves the model. In this case, we shall try 3 things:\n\n- Scale the outliers and run the model\n- Remove the more extreme outliers and run the model\n- Transform the data to a more Gaussian distribution and run the model\n- Do all of the above \n\n"},{"metadata":{"id":"SCbo0JFihk5c"},"cell_type":"markdown","source":"## 4.1 Scaling the outliers \n\nA scaler was chosen to scale the distribution in such a way that it is robust to outliers"},{"metadata":{"id":"uqSc5qYKXdMM","trusted":true},"cell_type":"code","source":"robust_scaler = RobustScaler(quantile_range=(25,100))\nX_train_scaled = robust_scaler.fit_transform(X_train)\nX_test_scaled = robust_scaler.transform(X_test)\n\nX_train_scaled_aux = pd.DataFrame(X_train_scaled,columns=X_cols)","execution_count":null,"outputs":[]},{"metadata":{"id":"YIDIkwek7nCo","outputId":"37bf50b0-d766-458a-8f5d-750b70b96561","trusted":true},"cell_type":"code","source":"for i in range(11):\n  plt.subplot(4,3,i+1)\n  sns.boxplot(y=X_train_scaled_aux[X_train_scaled_aux.columns[i]],x=y_train)\nplt.tight_layout()\n\n# Observe the y axis values","execution_count":null,"outputs":[]},{"metadata":{"id":"jyBMojWSrk1O","outputId":"9acc69a0-6f53-417b-d694-67c6afcc89b4","trusted":true},"cell_type":"code","source":"for i in range(11):\n  plt.subplot(4,3,i+1)\n  sns.distplot(X_train_scaled_aux[X_train_scaled_aux.columns[i]])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"jCXXsYUNzGh8","outputId":"f4d65063-b7f1-4c60-90d8-f9dbea35aa50","trusted":true},"cell_type":"code","source":"display(X_train.skew())                 # before tranformation\ndisplay(X_train_scaled_aux.skew())      # after transformation","execution_count":null,"outputs":[]},{"metadata":{"id":"wBg8UKGIqV9K","outputId":"cdf640a1-cd20-4c83-e306-f002b00893c9","trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100, random_state=0)\nrandom_forest.fit(X_train_scaled, y_train)\npred = random_forest.predict(X_test_scaled)\n\ncross_val = cross_val_score(random_forest,X_train_scaled, y_train,cv=2,scoring=\"accuracy\")\n\nprint('Test MAE :\\n', mean_absolute_error(y_test,pred))\nprint('Train accuracy score:\\n',random_forest.score(X_train_scaled, y_train))\nprint('Test accuracy score:\\n',random_forest.score(X_test_scaled,y_test))\nprint('Cross validation accuracy:\\n',cross_val.mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"Y-rJGKb6q5LG","outputId":"bdd4335f-661a-433b-c798-b4057795a84a","trusted":true},"cell_type":"code","source":"print(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"6Z8TxNfPq5LY","outputId":"6deab57e-4325-43e3-9dfc-42b801f2d4b1","trusted":true},"cell_type":"code","source":"confusion_matrix(random_forest,X_test_scaled,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"uJg--BfQrMxv"},"cell_type":"markdown","source":"No considerable improvement was shown"},{"metadata":{"id":"oXORIujLrWQu"},"cell_type":"markdown","source":"## 4.2 Removing  some outliers"},{"metadata":{"id":"fMcSFba1rzXn","outputId":"37e9095a-d7d4-4de7-ea2d-b63e5410c136","trusted":true},"cell_type":"code","source":"Q1 = df.quantile(q=.25)\nQ3 = df.quantile(q=.75)\nIQR = df.loc[ : , df.columns != 'quality'].apply(stats.iqr)\n\n\n#only keep rows in dataframe that have values within 2.5*IQR of Q1 and Q3\ndf_cleaned = df[~((df < (Q1-2.5*IQR)) | (df > (Q3+2.5*IQR))).any(axis=1)]\n\n#find how many rows are left in the dataframe \nprint(df_cleaned.shape)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"MtR7JDfetpih","trusted":true},"cell_type":"code","source":"y_cleaned = df_cleaned['quality']\nX_cleaned = df_cleaned.drop(['quality'],axis=1)\nX_cols = X.columns\n\n\nX_train,X_test,y_train,y_test = train_test_split(X_cleaned,y_cleaned,train_size=0.80,random_state=0) ","execution_count":null,"outputs":[]},{"metadata":{"id":"ux0TVT1UsGET","outputId":"792b432f-991e-4bfa-bb27-5dc5cb62eb94","trusted":true},"cell_type":"code","source":"for i in range(11):\n  plt.subplot(4,3,i+1)\n  sns.boxplot(y=X_train[X_train.columns[i]],x=y_train)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"uEjzCNy8uXNy","outputId":"0777c6ba-cb47-4e14-82ac-a901bf073e31","trusted":true},"cell_type":"code","source":"for i in range(11):\n  plt.subplot(4,3,i+1)\n  sns.distplot(X_train[X_train.columns[i]])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"AZFufhQruhkL"},"cell_type":"markdown","source":"By removing only the more extreme outliers, the data distribution is a lot more gaussian for most of the variables and the skew has decreased\n"},{"metadata":{"id":"PSm_G-WQu_kZ","outputId":"44dc6e8b-c757-49b1-be1f-2189cd85db2f","trusted":true},"cell_type":"code","source":"display(X_train.skew())  # Considerable reduced skew compared to using all the outliers","execution_count":null,"outputs":[]},{"metadata":{"id":"lpi7lqaIu_ki","outputId":"aeaf9b3a-2862-43ab-8f2d-3770e92c7c74","trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100, random_state=0)\nrandom_forest.fit(X_train, y_train)\npred = random_forest.predict(X_test)\n\ncross_val = cross_val_score(random_forest,X_train, y_train,cv=2,scoring=\"accuracy\")\n\nprint('Test MAE :\\n', mean_absolute_error(y_test,pred))\nprint('Train accuracy score:\\n',random_forest.score(X_train, y_train))\nprint('Test accuracy score:\\n',random_forest.score(X_test,y_test))\nprint('Cross validation accuracy:\\n',cross_val.mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"eYDbjbXxu_ko","outputId":"0d846e26-2504-4ff6-8c2d-3d06307c2537","trusted":true},"cell_type":"code","source":"print(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"kcZlbrviu_ks","outputId":"b9da3e51-3061-4065-cd6c-9cff7a3a6e5f","trusted":true},"cell_type":"code","source":"confusion_matrix(random_forest,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"3qAvh4F2wVkI"},"cell_type":"markdown","source":"Slight increase in test and training accuracy and improvement for the least present labels. Therefore we will keep this way of dealing with outliers.\n\nThere is no need to do robust scaling since the extreme outliers were removed. "},{"metadata":{"id":"IUC43ewm-GIX"},"cell_type":"markdown","source":"## 4.3 Fixing the skew of the data"},{"metadata":{"id":"1j9j0ABT7emK","trusted":true},"cell_type":"code","source":"power = PowerTransformer(method='yeo-johnson', standardize=True)\nX_train_power = power.fit_transform(X_train)\nX_test_power = power.transform(X_test)\n\nX_train_power_aux = pd.DataFrame(X_test_power,columns=X_cols)","execution_count":null,"outputs":[]},{"metadata":{"id":"2ePqxqxSBI89","outputId":"5c398f60-86c3-45ec-f641-dd232700874e","trusted":true},"cell_type":"code","source":"for i in range(11):\n  plt.subplot(4,3,i+1)\n  sns.distplot(X_train_power_aux[X_train_power_aux.columns[i]],bins=10)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"ceKm7-YXBMJG","outputId":"be7213b9-9ef0-42b8-e486-0261cd72d62c","trusted":true},"cell_type":"code","source":"display(X_train.skew())                 # before tranformation\ndisplay(X_train_power_aux.skew())       # after transformation","execution_count":null,"outputs":[]},{"metadata":{"id":"pHdyl1_0yb59"},"cell_type":"markdown","source":"With the exception of alcohol and citric acid, we can consider the data to be relatively gaussian with enough reduced skew to use in the model"},{"metadata":{"id":"teQKD_B7ytT8","outputId":"13116d77-8b48-4e5d-c3c2-aa27b4630f58","trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100, random_state=0)\nrandom_forest.fit(X_train_power, y_train)\npred = random_forest.predict(X_test_power)\n\ncross_val = cross_val_score(random_forest,X_train_power, y_train,cv=2,scoring=\"accuracy\")\n\nprint('Test MAE :\\n', mean_absolute_error(y_test,pred))\nprint('Train accuracy score:\\n',random_forest.score(X_train_power, y_train))\nprint('Test accuracy score:\\n',random_forest.score(X_test_power,y_test))\nprint('Cross validation accuracy:\\n',cross_val.mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"6zGqR-GtytUE","outputId":"881531b5-45af-4d3e-fcb9-ca26fb6b5e5c","trusted":true},"cell_type":"code","source":"print(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"dDM_bOtwytUK","outputId":"fbc944b2-55e2-43d3-fe14-637b6f4ebc8e","trusted":true},"cell_type":"code","source":"confusion_matrix(random_forest,X_test_power,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"G7YTNmcfzDxD"},"cell_type":"markdown","source":"No considerable improvement by decreasing the skew"},{"metadata":{"id":"uJ3Hquir69O_"},"cell_type":"markdown","source":"## 4.4 Fixing class imbalance"},{"metadata":{"id":"7XmrpIYXC7WI"},"cell_type":"markdown","source":"Since, theoretically, we must test the model with real data values, SMOTE should only applied to the training dataset. \n\nThe test dataset should maintain the proportions of the classes since it is more realistic"},{"metadata":{"id":"QEgd5Q4P0d1R","outputId":"674199f5-2f56-4eaf-b4f8-ace51ce8253d","trusted":true},"cell_type":"code","source":"y_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"OqNuDfa301dI","trusted":true},"cell_type":"code","source":"#sampling_strategy={3: 250,\n#                   4: 300,\n#                   8: 400,\n#                   }","execution_count":null,"outputs":[]},{"metadata":{"id":"mKrJjVRxk5L5","trusted":true},"cell_type":"code","source":"oversample = SMOTE(random_state=0,\n\n                   k_neighbors=2\n                   )\nX_resampled,y_resampled = oversample.fit_resample(X_train_power,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"iO28hypMZmKk","outputId":"33d346d4-1297-4313-91dd-9dc098ab48d2","trusted":true},"cell_type":"code","source":"sns.countplot(y_resampled)","execution_count":null,"outputs":[]},{"metadata":{"id":"Annqk4dwUYjE","outputId":"825a5d8a-51f4-4a44-a61f-1e191dc98cd1","trusted":true},"cell_type":"code","source":"pd.DataFrame(y_resampled).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"N0vane_e4LOe","outputId":"b5418f58-1419-4bbb-f095-42012adff448","trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100, random_state=0)\nrandom_forest.fit(X_resampled, y_resampled)\npred = random_forest.predict(X_test_power)\n\ncross_val = cross_val_score(random_forest,X_resampled, y_resampled,cv=5,scoring=\"accuracy\")\n\nprint('Test MAE :\\n', mean_absolute_error(y_test,pred))\nprint('Train accuracy score:\\n',random_forest.score(X_resampled, y_resampled))\nprint('Test accuracy score:\\n',random_forest.score(X_test_power,y_test))\nprint('Cross validation accuracy:\\n',cross_val.mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"zqEHm9wV4LOm","outputId":"6dfb4c3b-693d-432a-f198-a1a4922ce9d5","trusted":true},"cell_type":"code","source":"print(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"xZOwHNbT4LOp","outputId":"ab54300a-687f-4d99-aed6-0643ebf44463","trusted":true},"cell_type":"code","source":"confusion_matrix(random_forest,X_test_power,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"n2Uxh5BX8RdW"},"cell_type":"markdown","source":"- Precision of class 8 has reduced considerably \n- Overall accuracy is lower\n- Cross validation accuracy is higher due to the oversampling of the least represented categories \n\n\nConclusion: SMOTE does not help this type of classification problem due to the lack of dataset points for the least represented classes. SMOTE needs to use the 'k_neighbors' parameter lower than the number of the least represented classes. In this case, the lowest number is 3 from class 8, meaning my k_neighbors can only be 2 or lower. If these points are very far apart from each other, the artificial data doesn´t represent the class, which explains the reduced accuracy.\n\nCros validation accuracy increased because of the more present classes"},{"metadata":{"id":"ERBGz3I2pd2o"},"cell_type":"markdown","source":"# 5.0 Processing the data for binary classification problem"},{"metadata":{"id":"n_NWdBD7pu-I","trusted":true},"cell_type":"code","source":"bins = (2, 6.5, 8) # Suggested by the main page of the problem\ntarget_groups = ['Bad', 'Good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = target_groups)","execution_count":null,"outputs":[]},{"metadata":{"id":"wfIioMNkHuTZ","trusted":true},"cell_type":"code","source":"label_quality = LabelEncoder()\ndf['quality'] = label_quality.fit_transform(df['quality'])","execution_count":null,"outputs":[]},{"metadata":{"id":"DE93mxpuU0rk","outputId":"ec39168d-c183-4a30-9fd7-17e58406c4d8","trusted":true},"cell_type":"code","source":"df['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"StjbsfPEJEpr","outputId":"63036239-4166-4e92-93fa-5720ee4ace5d","trusted":true},"cell_type":"code","source":"sns.countplot(df['quality'])","execution_count":null,"outputs":[]},{"metadata":{"id":"DJvl5SYrXsBw","trusted":true},"cell_type":"code","source":"# Plot configurations\nplt.rcParams.update({\n    'font.size': 12,\n    'axes.titlesize': 20,\n    'figure.figsize': (24,15)\n})","execution_count":null,"outputs":[]},{"metadata":{"id":"gFGPcZxoVr6R"},"cell_type":"markdown","source":"## 5.1 Reference Model\n"},{"metadata":{"id":"ggzW-Am1MXAE","trusted":true},"cell_type":"code","source":"y = df['quality']\nX = df.drop(['quality'],axis=1)\nX_cols = X.columns\n\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.80,random_state=0) ","execution_count":null,"outputs":[]},{"metadata":{"id":"w5CtungSMLMj","outputId":"2abe8c69-66e1-4fc3-b5a3-325a848fcb9c","trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100, random_state=0)\nrandom_forest.fit(X_train, y_train)\npred = random_forest.predict(X_test)\n\ncross_val = cross_val_score(random_forest,X_train,y_train,cv=5,scoring=\"accuracy\")\n\nprint('Test MAE :\\n', mean_absolute_error(y_test,pred))\nprint('Train accuracy score:\\n',random_forest.score(X_train, y_train))\nprint('Test accuracy score:\\n',random_forest.score(X_test,y_test))\nprint('Cross validation accuracy:\\n',cross_val.mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"dCr33NOpMhvr","outputId":"75055a1e-46aa-4c85-caca-3f7f7fce7fef","trusted":true},"cell_type":"code","source":"print(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"kKosMs5QMhvv","outputId":"a5a9e41a-a1e9-4719-c1f0-852b0dd2a55e","trusted":true},"cell_type":"code","source":"confusion_matrix(random_forest,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"5m-oN8WDMKpC"},"cell_type":"markdown","source":"## 5.2 Removing outliers\n"},{"metadata":{"id":"5rbKiYeUJwYW","outputId":"c5166231-d20d-43c9-f8ef-05fd9a2c04ae","trusted":true},"cell_type":"code","source":"Q1 = df.quantile(q=.25)\nQ3 = df.quantile(q=.75)\nIQR = df.loc[ : , df.columns != 'quality'].apply(stats.iqr)\n\n\n#only keep rows in dataframe that have values within 2*IQR of Q1 and Q3\ndf_cleaned = df[~((df < (Q1-1.5*IQR)) | (df > (Q3+1.5*IQR))).any(axis=1)]\n\n#find how many rows are left in the dataframe \nprint(df_cleaned.shape)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"vgg2vkmT-aXe","trusted":true},"cell_type":"code","source":"y_cleaned = df_cleaned['quality']\nX_cleaned = df_cleaned.drop(['quality'],axis=1)\n\nX_train,X_test,y_train,y_test = train_test_split(X_cleaned,y_cleaned,train_size=0.80,random_state=24) ","execution_count":null,"outputs":[]},{"metadata":{"id":"IxXNNm03-aXh","outputId":"946615a2-d9da-405a-c558-c337521c9ff8","trusted":true},"cell_type":"code","source":"for i in range(11):\n  plt.subplot(4,3,i+1)\n  sns.boxplot(y=X_train[X_train.columns[i]],x=y_train)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"9sUMCCGx-aXm","outputId":"59d8dcd4-d992-45f1-906f-c08c01e0cb28","trusted":true},"cell_type":"code","source":"for i in range(11):\n  plt.subplot(4,3,i+1)\n  sns.distplot(X_train[X_train.columns[i]])\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"IeDqY7KK-aXp","outputId":"752d6a34-9cf7-4297-c2aa-27d71133fa18","trusted":true},"cell_type":"code","source":"display(X_train.skew())  ","execution_count":null,"outputs":[]},{"metadata":{"id":"tf9LI2mV-aXt","outputId":"878aeace-1a5d-467c-8a9d-2697ab527127","trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100, random_state=0)\nrandom_forest.fit(X_train, y_train)\npred = random_forest.predict(X_test)\n\ncross_val = cross_val_score(random_forest,X_train, y_train,cv=5,scoring=\"accuracy\")\n\nprint('Test MAE :\\n', mean_absolute_error(y_test,pred))\nprint('Train accuracy score:\\n',random_forest.score(X_train, y_train))\nprint('Test accuracy score:\\n',random_forest.score(X_test,y_test))\nprint('Cross validation accuracy:\\n',cross_val.mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"h-RS5JVX-aXz","outputId":"12ba8ff0-0630-4890-bb7d-fc33a31e7c12","trusted":true},"cell_type":"code","source":"print(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"6j8O8FIf-aX5","outputId":"9b657fac-80df-45fd-dbde-e18ac5599cac","trusted":true},"cell_type":"code","source":"confusion_matrix(random_forest,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"aZPilfrTNcVq"},"cell_type":"markdown","source":"High precision but pretty bad recall for the 1 (Good) label\n"},{"metadata":{"id":"Glk5ej6fNono"},"cell_type":"markdown","source":"## 5.3 Fixing the skew of the data "},{"metadata":{"id":"ufsNMjc3N1Ws","trusted":true},"cell_type":"code","source":"power = PowerTransformer(method='yeo-johnson', standardize=True)\nX_train_power = power.fit_transform(X_train)\nX_test_power = power.transform(X_test)\n\nX_train_power_aux = pd.DataFrame(X_test_power,columns=X_cols)","execution_count":null,"outputs":[]},{"metadata":{"id":"mqnxMlk3N1Wy","outputId":"64b5466b-188f-4416-d385-c79b389aca47","trusted":true},"cell_type":"code","source":"for i in range(11):\n  plt.subplot(4,3,i+1)\n  sns.distplot(X_train_power_aux[X_train_power_aux.columns[i]],bins=10)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"Srv1hrmFN1W1","outputId":"2d5f38b7-9627-410d-9e12-5184b7a4645c","trusted":true},"cell_type":"code","source":"display(X_train.skew())                 # before tranformation\ndisplay(X_train_power_aux.skew())       # after transformation","execution_count":null,"outputs":[]},{"metadata":{"id":"r0AMKWT0N1W3","outputId":"57300c7e-59e0-4254-9293-0bb17db9ca19","trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100, random_state=0)\nrandom_forest.fit(X_train_power, y_train)\npred = random_forest.predict(X_test_power)\n\ncross_val = cross_val_score(random_forest,X_train_power, y_train,cv=5,scoring=\"accuracy\")\n\nprint('Test MAE :\\n', mean_absolute_error(y_test,pred))\nprint('Train accuracy score:\\n',random_forest.score(X_train_power, y_train))\nprint('Test accuracy score:\\n',random_forest.score(X_test_power,y_test))\nprint('Cross validation accuracy:\\n',cross_val.mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"-tYkw33qN1W5","outputId":"5a2cf752-6f54-4281-9f0e-31927268adcb","trusted":true},"cell_type":"code","source":"print(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"kfxl5Il3N1W6","outputId":"90da07ab-c7c1-4265-99dc-b04d03727106","trusted":true},"cell_type":"code","source":"confusion_matrix(random_forest,X_test_power,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"w68PTt16OmUn"},"cell_type":"markdown","source":"Decreasing the skew had no impact on model precision"},{"metadata":{"id":"fOdi0ipVOsHF"},"cell_type":"markdown","source":"## 5.4 Fixing class imbalance"},{"metadata":{"id":"1CtJ801VO0AV","outputId":"7eb54c54-9f78-42c0-8a25-8e787fd10688","trusted":true},"cell_type":"code","source":"y_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"jAZ0HHiBQNcY","trusted":true},"cell_type":"code","source":"sampling_strategy={1: 800\n                   }","execution_count":null,"outputs":[]},{"metadata":{"id":"grPOyP1rO0Ag","trusted":true},"cell_type":"code","source":"oversample = SMOTE(random_state=0,sampling_strategy=sampling_strategy,k_neighbors=5)\nX_resampled,y_resampled = oversample.fit_resample(X_train_power,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"LAGTbco6O0Ak","outputId":"0cb6b33b-4b01-4d11-b792-a26ed012f456","trusted":true},"cell_type":"code","source":"sns.countplot(y_resampled)","execution_count":null,"outputs":[]},{"metadata":{"id":"AUu2Jma8O0An","outputId":"ee639beb-08ac-4f1d-d0ee-ec02cdb05418","trusted":true},"cell_type":"code","source":"pd.DataFrame(y_resampled).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"ekL4esZhO0Aq","outputId":"4b449bb6-4aad-4392-e70c-93b65a9824e9","trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100, random_state=0)\nrandom_forest.fit(X_resampled, y_resampled)\npred = random_forest.predict(X_test_power)\n\ncross_val = cross_val_score(random_forest,X_resampled, y_resampled,cv=5,scoring=\"accuracy\")\n\nprint('Test MAE :\\n', mean_absolute_error(y_test,pred))\nprint('Train accuracy score:\\n',random_forest.score(X_resampled, y_resampled))\nprint('Test accuracy score:\\n',random_forest.score(X_test_power,y_test))\nprint('Cross validation accuracy:\\n',cross_val.mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"LTzbX7dgO0Au","outputId":"620e86ca-5f9d-4bb0-9c8f-44ae37ffa297","trusted":true},"cell_type":"code","source":"print(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"j02reOK_O0Aw","outputId":"7ca5019b-10bf-4589-8558-0086a64113fe","trusted":true},"cell_type":"code","source":"confusion_matrix(random_forest,X_test_power,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"eKubFurnRbMu"},"cell_type":"markdown","source":" SMOTE has shown to be sucessful in increasing model performance for the recall metric\n\n"},{"metadata":{"id":"XuC12uG6pbMz"},"cell_type":"markdown","source":"## 5.5 Model optimization\n"},{"metadata":{"id":"AFPYCO1ojcwq","outputId":"e7aaaed3-3818-4a50-9c05-52fe4e0675a9","trusted":true},"cell_type":"code","source":"print('Parameters currently in use:\\n')\npprint(random_forest.get_params())","execution_count":null,"outputs":[]},{"metadata":{"id":"5tTepdiAkIXx","outputId":"41ab07de-ae70-4584-c0c4-9ba6e2f54400","trusted":true},"cell_type":"code","source":"random_grid = {\"max_depth\": [None],\n              \"max_features\": [3, 5, 10],\n              \"min_samples_split\": [2, 5, 10],\n              \"min_samples_leaf\": [1, 5, 10],\n              \"bootstrap\": [False, True],\n              \"n_estimators\" :[100,500],\n              \"criterion\": [\"gini\"]}\n\n\npprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"id":"P73UunkKjM9D","outputId":"a0c68364-1cc3-44ab-dee2-1ddd2e576d18","trusted":true},"cell_type":"code","source":"%%time \nsearch_param_forest = RandomizedSearchCV(random_forest,random_grid,cv=5,verbose=3,n_jobs=-1,n_iter=5)\nsearch_param_forest.fit(X_resampled,y_resampled)","execution_count":null,"outputs":[]},{"metadata":{"id":"nLwlNOwAmdna","outputId":"ae35de1f-2fec-41de-f79d-a64f0063a36c","trusted":true},"cell_type":"code","source":"search_param_forest.best_score_","execution_count":null,"outputs":[]},{"metadata":{"id":"AvwDqOpZnl0k","outputId":"60c2a76c-3810-4dd7-f301-707b38931feb","trusted":true},"cell_type":"code","source":"search_param_forest.best_params_","execution_count":null,"outputs":[]},{"metadata":{"id":"5OGhI8Noz_aw","outputId":"e34ea07a-f906-47fd-a4f7-47a54e19b230","trusted":true},"cell_type":"code","source":"search_param_forest.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"id":"v3bN051nSYOq","outputId":"16a1a11e-63f8-4a29-fede-02b0af133210","trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features=3,\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=5,\n                       min_weight_fraction_leaf=0.0, n_estimators=500,\n                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n                       warm_start=False)\n\nrandom_forest.fit(X_resampled, y_resampled)\npred = random_forest.predict(X_test_power)\n\ncross_val = cross_val_score(random_forest,X_resampled, y_resampled,cv=5,scoring=\"accuracy\")\n\nprint('Test MAE :\\n', mean_absolute_error(y_test,pred))\nprint('Train accuracy score:\\n',random_forest.score(X_resampled, y_resampled))\nprint('Test accuracy score:\\n',random_forest.score(X_test_power,y_test))\nprint('Cross validation accuracy:\\n',cross_val.mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"5-AC_YnjSYOx","outputId":"ed2ca1c2-ce85-47fe-bd89-18d5ae755836","trusted":true},"cell_type":"code","source":"print(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"U4Ctzn8JSYOz","outputId":"f9d72b77-7630-4aff-a592-800039d105d8","trusted":true},"cell_type":"code","source":"confusion_matrix(random_forest,X_test_power,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"G7mUVVuvZtyY"},"cell_type":"markdown","source":"# 6.0 Selecting most important variables for the model"},{"metadata":{"id":"5pHkA-ohp90A"},"cell_type":"markdown","source":"From the correlation heatmap, we have the expectation of chosen variables below:\n\nFor the dependent variable, we can see that the most important variables for the model are likely to be:\n- alcohol (0.48)\n- sulphates (0.38)\n- volatile acidity (-0.38)\n\nThere is also the chance that the below variables will be important if we were more linient on the choices:\n- citric acid (0.21)\n- total sulfur dioxide (0.2)\n- chlorides (-0.19)\n- density (-0.18)\n"},{"metadata":{"id":"Ky3La3nVskkD"},"cell_type":"markdown","source":"## 6.1 Model feature importance\n"},{"metadata":{"id":"MR6o6UiDsnVg","outputId":"b6bc33a7-a186-48a5-c591-3be9218d1a68","trusted":true},"cell_type":"code","source":"random_forest.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"id":"BMoezqylvZFw","trusted":true},"cell_type":"code","source":"#The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.","execution_count":null,"outputs":[]},{"metadata":{"id":"yzctGGKCsvw_","outputId":"e77faaaf-f92c-48dd-8209-a4cb71cc5250","trusted":true},"cell_type":"code","source":"feat_importances = pd.Series(random_forest.feature_importances_,index=X.columns)\nfeat_importances.nlargest(5).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"0kYq42nqALys"},"cell_type":"markdown","source":"The expectation from the correlation map is confirmed by the model"},{"metadata":{"id":"ADNmU3zbCcbp"},"cell_type":"markdown","source":"# 7.0 Conclusion\n\n- Scaling the variables using RobustScaler was ineffective\n- Removing some of the outliers showed slight improvement in the model\n- The data was not skewed enough after removing the more critical outliers to make yeo-johnson scaling show considerable model improvement.\n\nFinal procedure: remove most critical outliers -> apply yeo-johnson -> apply SMOTE\n\n\n### Classification problem model improvement\n- Train accuracy score: 1.0  -> 1.0 \n- Test accuracy score: 0.7125 ->  0.682\n- Cross validation accuracy: 0.652 -> 0.894\n\n- The test score got worse while the cross validation score got better do to SMOTE. however, due to the few number of member of the least present classes, this method was inefective, as discussed in section 4.4. \n\n- The reason the cross validation got better was due to the presence of the artificial terms in the least represented classes. The most likely reason why the artificial data caused a decrease in test accuracy is because the original data points are not close to one another, therefore, the artifical data points are not in a clustered area. \n\n## Binary classification model improvement\n- Train accuracy score: 1.0 -> 1.0\n- Test accuracy score:  0.93125 -> 0.954\n- Cross validation accuracy: 0.899 ->  0.9504\n- SMOTE was effective in this case due to the higher number of classes 'Good' and it is likely that their location is more close to one another.\n\n\n\n## Top 5 most important features:\n- alcohol                 0.234330\n- sulphates               0.164203\n- volatile acidity        0.109635\n- citric acid             0.088512\n- density                 0.074837\n"},{"metadata":{"id":"xt3PnV8ICegq","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}