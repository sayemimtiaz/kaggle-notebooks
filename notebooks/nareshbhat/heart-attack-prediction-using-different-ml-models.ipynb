{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **ABOUT HEART ATTACK**"},{"metadata":{},"cell_type":"markdown","source":"> A heart attack (Cardiovascular diseases) occurs when the flow of blood to the heart muscle suddenly becomes blocked. From WHO statistics every year 17.9 million dying from heart attack. The medical study says that human life style is the main reason behind this heart problem. Apart from this there are many key factors which warns that the person may/maynot getting chance of heart attack.\n\n> <img style=\"float: centre;\" src=\"https://img.lovepik.com/photo/50074/6189.jpg_wh860.jpg\" width=\"600px\"/>\n\n> This dataset contain some medical information of patients which tells whether that person getting a heart attack chance is less or more. Using the information explore the dataset and classify the target variable using different Machine Learning models and findout which algorithm suitable for this dataset."},{"metadata":{},"cell_type":"markdown","source":"## Table of Contents\n1) Import Packages\n\n2) EDA\n\n3) Preparing ML models\n\n4) Models evaluation\n\n5) Ensembling\n\n6) Conclusion"},{"metadata":{},"cell_type":"markdown","source":"## Packages Required"},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading dataset\nimport pandas as pd\nimport numpy as np\n#visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n#EDA\nfrom collections import Counter\nimport pandas_profiling as pp\n# data preprocessing\nfrom sklearn.preprocessing import StandardScaler\n# data splitting\nfrom sklearn.model_selection import train_test_split\n# data modeling\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n#ensembling\nfrom mlxtend.classifier import StackingCVClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/health-care-data-set-on-heart-attack-possibility/heart.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **EDA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pp.ProfileReport(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Model prepration**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"target\"]\nX = data.drop('target',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Before applying algorithm we should check whether the data is equally splitted or not, because if data is not splitted equally it will cause for data imbalacing problem**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_test.unique())\nCounter(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **ML models**\n\nHere I take different machine learning algorithm and try to find algorithm which predict accurately.\n\n1. Logistic Regression\n2. Naive Bayes\n3. Random Forest Classifier\n4. Extreme Gradient Boost\n5. K-Nearest Neighbour\n6. Decision Tree\n7. Support Vector Machine\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"m1 = 'Logistic Regression'\nlr = LogisticRegression()\nmodel = lr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\nlr_conf_matrix = confusion_matrix(y_test, lr_predict)\nlr_acc_score = accuracy_score(y_test, lr_predict)\nprint(\"confussion matrix\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,lr_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m2 = 'Naive Bayes'\nnb = GaussianNB()\nnb.fit(X_train,y_train)\nnbpred = nb.predict(X_test)\nnb_conf_matrix = confusion_matrix(y_test, nbpred)\nnb_acc_score = accuracy_score(y_test, nbpred)\nprint(\"confussion matrix\")\nprint(nb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Naive Bayes model:\",nb_acc_score*100,'\\n')\nprint(classification_report(y_test,nbpred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m3 = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators=20, random_state=12,max_depth=5)\nrf.fit(X_train,y_train)\nrf_predicted = rf.predict(X_test)\nrf_conf_matrix = confusion_matrix(y_test, rf_predicted)\nrf_acc_score = accuracy_score(y_test, rf_predicted)\nprint(\"confussion matrix\")\nprint(rf_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')\nprint(classification_report(y_test,rf_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m4 = 'Extreme Gradient Boost'\nxgb = XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, \n                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\nxgb.fit(X_train, y_train)\nxgb_predicted = xgb.predict(X_test)\nxgb_conf_matrix = confusion_matrix(y_test, xgb_predicted)\nxgb_acc_score = accuracy_score(y_test, xgb_predicted)\nprint(\"confussion matrix\")\nprint(xgb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Extreme Gradient Boost:\",xgb_acc_score*100,'\\n')\nprint(classification_report(y_test,xgb_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m5 = 'K-NeighborsClassifier'\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nknn_predicted = knn.predict(X_test)\nknn_conf_matrix = confusion_matrix(y_test, knn_predicted)\nknn_acc_score = accuracy_score(y_test, knn_predicted)\nprint(\"confussion matrix\")\nprint(knn_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of K-NeighborsClassifier:\",knn_acc_score*100,'\\n')\nprint(classification_report(y_test,knn_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m6 = 'DecisionTreeClassifier'\ndt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 6)\ndt.fit(X_train, y_train)\ndt_predicted = dt.predict(X_test)\ndt_conf_matrix = confusion_matrix(y_test, dt_predicted)\ndt_acc_score = accuracy_score(y_test, dt_predicted)\nprint(\"confussion matrix\")\nprint(dt_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of DecisionTreeClassifier:\",dt_acc_score*100,'\\n')\nprint(classification_report(y_test,dt_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m7 = 'Support Vector Classifier'\nsvc =  SVC(kernel='rbf', C=2)\nsvc.fit(X_train, y_train)\nsvc_predicted = svc.predict(X_test)\nsvc_conf_matrix = confusion_matrix(y_test, svc_predicted)\nsvc_acc_score = accuracy_score(y_test, svc_predicted)\nprint(\"confussion matrix\")\nprint(svc_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Support Vector Classifier:\",svc_acc_score*100,'\\n')\nprint(classification_report(y_test,svc_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_feature = pd.DataFrame({'Feature': ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal'], 'Importance': xgb.feature_importances_})\nplt.figure(figsize=(10,4))\nplt.title(\"barplot Represent feature importance \")\nplt.xlabel(\"importance \")\nplt.ylabel(\"features\")\nplt.barh(imp_feature['Feature'],imp_feature['Importance'],color = 'rgbkymc')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_false_positive_rate,lr_true_positive_rate,lr_threshold = roc_curve(y_test,lr_predict)\nnb_false_positive_rate,nb_true_positive_rate,nb_threshold = roc_curve(y_test,nbpred)\nrf_false_positive_rate,rf_true_positive_rate,rf_threshold = roc_curve(y_test,rf_predicted)                                                             \nxgb_false_positive_rate,xgb_true_positive_rate,xgb_threshold = roc_curve(y_test,xgb_predicted)\nknn_false_positive_rate,knn_true_positive_rate,knn_threshold = roc_curve(y_test,knn_predicted)\ndt_false_positive_rate,dt_true_positive_rate,dt_threshold = roc_curve(y_test,dt_predicted)\nsvc_false_positive_rate,svc_true_positive_rate,svc_threshold = roc_curve(y_test,svc_predicted)\n\n\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,5))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(lr_false_positive_rate,lr_true_positive_rate,label='Logistic Regression')\nplt.plot(nb_false_positive_rate,nb_true_positive_rate,label='Naive Bayes')\nplt.plot(rf_false_positive_rate,rf_true_positive_rate,label='Random Forest')\nplt.plot(xgb_false_positive_rate,xgb_true_positive_rate,label='Extreme Gradient Boost')\nplt.plot(knn_false_positive_rate,knn_true_positive_rate,label='K-Nearest Neighbor')\nplt.plot(dt_false_positive_rate,dt_true_positive_rate,label='Desion Tree')\nplt.plot(svc_false_positive_rate,svc_true_positive_rate,label='Support Vector Classifier')\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model Evaluation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_ev = pd.DataFrame({'Model': ['Logistic Regression','Naive Bayes','Random Forest','Extreme Gradient Boost',\n                    'K-Nearest Neighbour','Decision Tree','Support Vector Machine'], 'Accuracy': [lr_acc_score*100,\n                    nb_acc_score*100,rf_acc_score*100,xgb_acc_score*100,knn_acc_score*100,dt_acc_score*100,svc_acc_score*100]})\nmodel_ev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['red','green','blue','gold','silver','yellow','orange',]\nplt.figure(figsize=(12,5))\nplt.title(\"barplot Represent Accuracy of different models\")\nplt.xlabel(\"Accuracy %\")\nplt.ylabel(\"Algorithms\")\nplt.bar(model_ev['Model'],model_ev['Accuracy'],color = colors)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Ensembling**\n\n> **In order to increase the accuracy of the model we use ensembling. Here we use stacking technique.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"scv=StackingCVClassifier(classifiers=[xgb,knn,svc],meta_classifier= svc,random_state=42)\nscv.fit(X_train,y_train)\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Conclusion**\n\n1) Extreme Gradient Boost gives the best Accuracy compared to other models.\n\n2) Exercise induced angina,Chest pain is major symptoms of heart attack.\n\n3) Ensembling technique increase the accuracy of the model.\n\n**Feel free to ask any question related to this topic. I'm happy to answer. If you like my work don't hesitate to upvote.**\n\n**HAPPY LEARNING :-)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}