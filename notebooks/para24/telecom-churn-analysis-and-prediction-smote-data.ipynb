{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Telecom Churn Analysis and Prediction using SMOTE data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* [1.  Import and Clean data](#import-and-clean-data)\n    * [1.1.  Delete `customerid` column](#delete-%60customerid%60-column)\n    * [1.2.  Data Munging](#data-munging)\n        * [1.2.1.  Checking for null values in the dataset](#checking-for-null-values-in-the-dataset)\n        * [1.2.2.  Making labels concise](#making-labels-concise)\n        * [1.2.3.  Column Type Casting and Imputation](#column-type-casting-and-imputation)\n            * [1.2.3.1.  Cast `TotalCharges` column to `float`](#cast-%60totalcharges%60-column-to-%60float%60)\n            * [1.2.3.2.  Search for categorical columns and cast them to `pd.Categorical`](#search-for-categorical-columns-and-cast-them-to-%60pd.categorical%60)\n        * [1.2.4.  Reordering Columns](#reordering-columns)\n* [2.  Correlations in the data](#correlations-in-the-data)\n    * [2.1.  Correlation between Quantitative variables](#correlation-between-quantitative-variables)\n    * [2.2.  Correlation between Qualitative/ Categorical variables](#correlation-between-qualitative/-categorical-variables)\n* [3.  Data Preprocessing](#data-preprocessing)\n    * [3.1.  Train-Test split](#train-test-split)\n    * [3.2. Oversample Training Data (SMOTE-NC)](#oversample-training-data-%28smote-nc%29)\n    * [3.3.  One-hot Encoding and Standardization](#one-hot-encoding-and-standardization)\n* [4.  Data Modeling](#data-modeling)\n    * [4.1.  Utility Functions](#utility-functions)\n    * [4.2. Naive Bayes](#naive-bayes)\n    * [4.3.  Logistic Regression](#logistic-regression)\n    * [4.4.  K-Nearest Neighbors](#k-nearest-neighbors)\n    * [4.5.  Decision Tree](#decision-tree)\n    * [4.6.  Decision Trees with Bagging](#decision-trees-with-bagging)\n    * [4.7.  Random Forests](#random-forests)\n    * [4.8.  Decision Trees with AdaBoost](#decision-trees-with-adaboost)\n    * [4.9.  Linear SVC](#linear-svc)\n    * [4.10.  SVM with RBF kernel](#svm-with-rbf-kernel)\n    * [4.11.  XGBoost](#xgboost)\n    * [4.12.  CatBoost](#catboost)\n* [5.  Model Comparison](#model-comparison)\n    * [5.1.  Evaluation Metrics](#evaluation-metrics)\n    * [5.2. 2 ROC and PR Curves](#2-roc-and-pr-curves)\n* [6.  Further Analysis](#further-analysis)","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"import os, sys\n\nimport numpy as np\nfrom scipy.stats import chi2_contingency\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# Included changes to make the kernel run as a jupyter notebook on windows without the need to make any changes\nkaggle_data_folder = os.path.join('kaggle', 'input') if sys.platform == 'win32' else os.path.join(os.path.sep, 'kaggle', 'input')\nfile_ext = \".csv\"\nfiles = []\nfor dirname, _, filenames in os.walk(kaggle_data_folder):\n    for filename in filenames:\n        if filename.endswith(file_ext):\n            files.append(os.path.join(dirname, filename))\n\nprint(files)\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"import-and-clean-data\"></a>\n# 1.  Import and Clean data","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"df = pd.read_csv(files[0])\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"delete-%60customerid%60-column\"></a>\n## 1.1.  Delete `customerid` column\nSince 'customerid' column does not provide any relevant information in predicting the customer churn, we can delete the column.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"df.drop(labels=['customerID'], axis=1, inplace=True)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"data-munging\"></a>\n## 1.2.  Data Munging","execution_count":null},{"metadata":{"Collapsed":"false","toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"checking-for-null-values-in-the-dataset\"></a>\n### 1.2.1.  Checking for null values in the dataset","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"As of now we don't see any null values. However, we will find a few in the `TotalCharges` column after casting it to `float64`","execution_count":null},{"metadata":{"Collapsed":"false","toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"making-labels-concise\"></a>\n### 1.2.2.  Making labels concise\nLet's make the categorical labels more concise. For instance, we will convert the categorical label `'Bank transfer (automatic)'` to `'Bank transfer'` to make it easier to access (and display) during visualization.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"## Shorten the Labels\nvalue_mapper = {'Female': 'F', 'Male': 'M', 'Yes': 'Y', 'No': 'N',\n                'No phone service': 'No phone', 'Fiber optic': 'Fiber',\n                'No internet service': 'No internet', 'Month-to-month': 'Monthly',\n                'Bank transfer (automatic)': 'Bank transfer',\n                'Credit card (automatic)': 'Credit card',\n                'One year': '1 yr', 'Two year': '2 yr'}\ndf.replace(to_replace=value_mapper, inplace=True)\n# Another method\n# df = df.applymap(lambda v: value_mapper[v] if v in value_mapper.keys() else v)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Let's also change column labels from `TitleCase` to `lowercase` to ease access.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"df.columns = [label.lower() for label in df.columns]\ndf.head(10).T","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"column-type-casting-and-imputation\"></a>\n### 1.2.3.  Column Type Casting and Imputation\nPandas couldn't properly cast the data type of several columns. For instance, the `TotalCharges` column is recognized as `object` instead of `float`. Similarly, all the categorical columns were casted as `object` type instead of `pd.Categorical`.","execution_count":null},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"cast-%60totalcharges%60-column-to-%60float%60\"></a>\n#### 1.2.3.1.  Cast `TotalCharges` column to `float`","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"df['totalcharges'] = pd.to_numeric(df['totalcharges'], errors='coerce')\ndf['totalcharges'].head()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"Here we see that the `totalcharges` has 11 missing values. Let's see the complete data corresponding to these customers.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"df[np.isnan(df['totalcharges'])]","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"It can also be noted that the `Tenure` column is 0 for these entries even though the `monthlycharges` column is not empty. Let's see if there are any other 0 values in the `tenure` column.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"df[df['tenure'] == 0].index","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"There are no additional missing values in the `Tenure` column. Let's delete the rows with missing values in `monthlycharges` and `tenure` columns.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"df.drop(labels=df[df['tenure'] == 0].index, axis=0, inplace=True)\ndf[df['tenure'] == 0].index","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"search-for-categorical-columns-and-cast-them-to-%60pd.categorical%60\"></a>\n#### 1.2.3.2.  Search for categorical columns and cast them to `pd.Categorical`\nWe need to manually identify categorical columns in the data before casting them to `pd.Categorical`. Casting categorical columns from the detected *object* type to *categorical* will ease visualization.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"def summarize_categoricals(df, show_levels=False):\n    \"\"\"\n        Display uniqueness in each column\n    \"\"\"\n    data = [[df[c].unique(), len(df[c].unique()), df[c].isnull().sum()] for c in df.columns]\n    df_temp = pd.DataFrame(data, index=df.columns,\n                           columns=['Levels', 'No. of Levels', 'No. of Missing Values'])\n    return df_temp.iloc[:, 0 if show_levels else 1:]\n\n\ndef find_categorical(df, cutoff=10):\n    \"\"\"\n        Function to find categorical columns in the dataframe.\n    \"\"\"\n    cat_cols = []\n    for col in df.columns:\n        if len(df[col].unique()) <= cutoff:\n            cat_cols.append(col)\n    return cat_cols\n\n\ndef to_categorical(columns, df):\n    \"\"\"\n        Converts the columns passed in `columns` to categorical datatype\n    \"\"\"\n    for col in columns:\n        df[col] = df[col].astype('category')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"summarize_categoricals(df, show_levels=True)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"df = to_categorical(find_categorical(df), df)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"reordering-columns\"></a>\n### 1.2.4.  Reordering Columns","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"new_order = list(df.columns)\nnew_order.insert(16, new_order.pop(4))\ndf = df[new_order]\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"correlations-in-the-data\"></a>\n# 2.  Correlations in the data","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"correlation-between-quantitative-variables\"></a>\n## 2.1.  Correlation between Quantitative variables","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"sns.heatmap(data=df[['tenure', 'monthlycharges', 'totalcharges']].corr(),\n            annot=True, cmap='coolwarm');","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"***Inference:*** As evident from the correlation matrix and regplots, since ***'totalcharges'*** is the total monthly charges over the tenure of a customer, ***'totalcharges'*** is highly correlated with ***'monthlycharges'*** and ***'tenure'***.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"sns.lmplot('monthlycharges', 'totalcharges', data=df, hue='churn',\n           scatter_kws={'alpha': 0.1})\nfig = sns.lmplot('tenure', 'totalcharges', data=df, hue='churn',\n                 scatter_kws={'alpha': 0.1})\nfig.set_xlabels('tenure (in months)');","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"correlation-between-qualitative/-categorical-variables\"></a>\n## 2.2.  Correlation between Qualitative/ Categorical variables\n`Cramer's V` is more appropriate than Pearson correlation to find correlation between two nominal variables. Here, the `Cramer's V` metric is implemented.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"def cramers_corrected_stat(contingency_table):\n    \"\"\"\n        Computes corrected Cramer's V statistic for categorial-categorial association\n    \"\"\"\n    chi2 = chi2_contingency(contingency_table)[0]\n    n = contingency_table.sum().sum()\n    phi2 = chi2/n\n    \n    r, k = contingency_table.shape\n    r_corrected = r - (((r-1)**2)/(n-1))\n    k_corrected = k - (((k-1)**2)/(n-1))\n    phi2_corrected = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n    \n    return (phi2_corrected / min( (k_corrected-1), (r_corrected-1)))**0.5","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"def categorical_corr_matrix(df):\n    \"\"\"\n        Computes corrected Cramer's V statistic between\n        all the categorical variables in the dataframe\n    \"\"\"\n    df = df.select_dtypes(include='category')\n    cols = df.columns\n    n = len(cols)\n    corr_matrix = pd.DataFrame(np.zeros(shape=(n, n)), index=cols, columns=cols)\n    \n    for col1 in cols:\n        for col2 in cols:\n            if col1 == col2:\n                corr_matrix.loc[col1, col2] = 1\n                break\n            df_crosstab = pd.crosstab(df[col1], df[col2], dropna=False)\n            corr_matrix.loc[col1, col2] = cramers_corrected_stat(df_crosstab)\n    \n    # Flip and add to get full correlation matrix\n    corr_matrix += np.tril(corr_matrix, k=-1).T\n    return corr_matrix","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(categorical_corr_matrix(df), annot=True, cmap='coolwarm', \n            cbar_kws={'aspect': 50}, square=True, ax=ax)\nplt.xticks(rotation=60);","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"***Inference:*** There is some correlation between *'phone service'* and *'multiple lines'* since those who don't have a phone service cannot have multiple lines. So, knowing that a particular customer is not subscribed to phone service we can infer that the customer doesn't have multiple lines. Similarly, there is also a correlation between *'internet service'* and *'online security', 'online backup', 'device protection', 'streaming tv'* and *'streaming movies'*","execution_count":null},{"metadata":{"Collapsed":"false","toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"data-preprocessing\"></a>\n# 3.  Data Preprocessing\nData needs to be one-hot-encoded before applying machine learning models.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"x = df.iloc[:, :-1]\ny = df['churn']\n\ncategorical_columns = list(x.select_dtypes(include='category').columns)\nnumeric_columns = list(x.select_dtypes(exclude='category').columns)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"train-test-split\"></a>\n## 3.1.  Train-Test split\nCatBoost classifier does not require any knd of preprocessing while Naive bayes requires a different kind of preprocesing. Therefore, we will use raw/ unmodified data (`x_train_cat, x_test_cat, y_train_cat, y_test_cat`) for CatBoost and preprocessed data (`x_train, x_test, y_train, y_test`) for all other classifiers. For Naive Bayes, we will use the raw data (`x_train_cat, x_test_cat, y_train_cat, y_test_cat`) and preprocess it as required in the Naive Bayes section.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata_splits = train_test_split(x, y, test_size=0.25, random_state=0,\n                               shuffle=True, stratify=y)\nx_train, x_test, y_train, y_test = data_splits\n\n\n# For CatBoost and Naive Bayes\ndata_splits = train_test_split(x, y, test_size=0.25, random_state=0,\n                               shuffle=True, stratify=y)\nx_train_cat, x_test_cat, y_train_cat, y_test_cat = data_splits\n\n\n# Save the non-scaled version of monthlycharges and totalcharges to compare classifiers\nx_test_charges = np.array(x_test[['monthlycharges', 'totalcharges']], copy=True)\n\nlist(map(lambda x: x.shape, [x, y, x_train, x_test, y_train, y_test]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"oversample-training-data-%28smote-nc%29\"></a>\n## 3.2. Oversample Training Data (SMOTE-NC)\nSMOTE is an oversampling method that balances imbalanced datasets by sampling (with replacement) minority class. SMOTE-NC stands for Synthetic Minority Over-sampling TEchnique for data with Numerical-Categorical features. Note that only training data is oversampled. The testing data is untouched.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from imblearn.over_sampling import SMOTENC\n\nsmote = SMOTENC(categorical_features=(x_train.dtypes == \"category\").values,\n                random_state=42)\n\nx_train, y_train = smote.fit_resample(x_train, y_train)\n\nx_train_cat, y_train_cat = smote.fit_resample(x_train_cat, y_train_cat)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"pd.Series(y_train).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"sns.countplot(x=y_train);","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"one-hot-encoding-and-standardization\"></a>\n## 3.3.  One-hot Encoding and Standardization\nWe need to standardize the continuous or quantitative variables/ features before applying Machine Learning models. This is important because if we don't standardize the features, features with high variance that are orders of magnitude larger that others might dominate the model fitting process and causing the model unable to learn from other features (with lower variance) correctly as expected. <br/>\nThere is no need to standardize categorical variables.\n\n***Also we need to standardize the data only after performing train-test split because if we standardize before splitting then there is a chance for some information leak from the test set into the train set. We always want the test set to be completely new to the ML models. [Read more](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data)***","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n\ncategorical_columns = list(x.select_dtypes(include='category').columns)\n\n\n## Column Transformer\ntransformers = [('one_hot_encoder',\n                  OneHotEncoder(drop='first',dtype='int'),\n                  categorical_columns),\n                ('standard_scaler', StandardScaler(), numeric_columns)]\nx_trans = ColumnTransformer(transformers, remainder='passthrough')\n\n## Applying Column Transformer\nx_train = x_trans.fit_transform(x_train)\nx_test = x_trans.transform(x_test)\n\n## Label encoding\ny_trans = LabelEncoder()\ny_train = y_trans.fit_transform(y_train)\ny_test = y_trans.transform(y_test)\n\n\n## Save feature names after one-hot encoding for feature importances plots\nfeature_names = list(x_trans.named_transformers_['one_hot_encoder'] \\\n                            .get_feature_names(input_features=categorical_columns))\nfeature_names = feature_names + numeric_columns","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"data-modeling\"></a>\n# 4.  Data Modeling\nSince the dataset is imbalanced we will be using class-weighted/ cost-sensitive learning. In cost-sensitive learning, a weighted cost function is used. Therefore, misclassifying a sample from the minority class will cost the classifiers more than misclassifying a sample from the majority class. In most of the Sklearn classifiers, cost-sensitive learning can be enabled by setting `class_weight='balanced'`.","execution_count":null},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"utility-functions\"></a>\n## 4.1.  Utility Functions","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"import timeit\nimport pickle\nimport sys\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, \\\n                            precision_recall_curve, roc_curve, accuracy_score\nfrom sklearn.exceptions import NotFittedError","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"def confusion_plot(matrix, labels=None):\n    \"\"\" Display binary confusion matrix as a Seaborn heatmap \"\"\"\n    \n    labels = labels if labels else ['Negative (0)', 'Positive (1)']\n    \n    fig, ax = plt.subplots(nrows=1, ncols=1)\n    sns.heatmap(data=matrix, cmap='Blues', annot=True, fmt='d',\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_xlabel('PREDICTED')\n    ax.set_ylabel('ACTUAL')\n    ax.set_title('Confusion Matrix')\n    plt.close()\n    \n    return fig","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"def roc_plot(y_true, y_probs, label, compare=False, ax=None):\n    \"\"\" Plot Receiver Operating Characteristic (ROC) curve \n        Set `compare=True` to use this function to compare classifiers. \"\"\"\n    \n    fpr, tpr, thresh = roc_curve(y_true, y_probs)\n    auc = round(roc_auc_score(y_true, y_probs), 2)\n    \n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1)\n    label = ' '.join([label, f'({auc})']) if compare else None\n    sns.lineplot(x=fpr, y=tpr, ax=axis, label=label)\n    \n    if compare:\n        axis.legend(title='Classifier (AUC)', loc='lower right')\n    else:\n        axis.text(0.72, 0.05, f'AUC = { auc }', fontsize=12,\n                  bbox=dict(facecolor='green', alpha=0.4, pad=5))\n            \n        # Plot No-Info classifier\n        axis.fill_between(fpr, fpr, tpr, alpha=0.3, edgecolor='g',\n                          linestyle='--', linewidth=2)\n        \n    axis.set_xlim(0, 1)\n    axis.set_ylim(0, 1)\n    axis.set_title('ROC Curve')\n    axis.set_xlabel('False Positive Rate [FPR]\\n(1 - Specificity)')\n    axis.set_ylabel('True Positive Rate [TPR]\\n(Sensitivity or Recall)')\n    \n    plt.close()\n    \n    return axis if ax else fig","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"def precision_recall_plot(y_true, y_probs, label, compare=False, ax=None):\n    \"\"\" Plot Precision-Recall curve.\n        Set `compare=True` to use this function to compare classifiers. \"\"\"\n    \n    p, r, thresh = precision_recall_curve(y_true, y_probs)\n    p, r, thresh = list(p), list(r), list(thresh)\n    p.pop()\n    r.pop()\n    \n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1)\n    \n    if compare:\n        sns.lineplot(r, p, ax=axis, label=label)\n        axis.set_xlabel('Recall')\n        axis.set_ylabel('Precision')\n        axis.legend(loc='lower left')\n    else:\n        sns.lineplot(thresh, p, label='Precision', ax=axis)\n        axis.set_xlabel('Threshold')\n        axis.set_ylabel('Precision')\n        axis.legend(loc='lower left')\n\n        axis_twin = axis.twinx()\n        sns.lineplot(thresh, r, color='limegreen', label='Recall', ax=axis_twin)\n        axis_twin.set_ylabel('Recall')\n        axis_twin.set_ylim(0, 1)\n        axis_twin.legend(bbox_to_anchor=(0.24, 0.18))\n    \n    axis.set_xlim(0, 1)\n    axis.set_ylim(0, 1)\n    axis.set_title('Precision Vs Recall')\n    \n    plt.close()\n    \n    return axis if ax else fig","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"def feature_importance_plot(importances, feature_labels, ax=None):\n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1, figsize=(5, 10))\n    sns.barplot(x=importances, y=feature_labels, ax=axis)\n    axis.set_title('Feature Importance Measures')\n    \n    plt.close()\n    \n    return axis if ax else fig","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"def train_clf(clf, x_train, y_train, sample_weight=None, refit=False):\n    train_time = 0\n    \n    try:\n        if refit:\n            raise NotFittedError\n        y_pred_train = clf.predict(x_train)\n    except NotFittedError:\n        start = timeit.default_timer()\n        \n        if sample_weight is not None:\n            clf.fit(x_train, y_train, sample_weight=sample_weight)\n        else:\n            clf.fit(x_train, y_train)\n        \n        end = timeit.default_timer()\n        train_time = end - start\n        \n        y_pred_train = clf.predict(x_train)\n    \n    train_acc = accuracy_score(y_train, y_pred_train)\n    return clf, y_pred_train, train_acc, train_time","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"def model_memory_size(clf):\n    return sys.getsizeof(pickle.dumps(clf))","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"def report(clf, x_train, y_train, x_test, y_test, sample_weight=None,\n           refit=False, importance_plot=False, confusion_labels=None,\n           feature_labels=None, verbose=True):\n    \"\"\" Trains the passed classifier if not already trained and reports\n        various metrics of the trained classifier \"\"\"\n    \n    dump = dict()\n    \n    ## Train if not already trained\n    clf, train_predictions, \\\n    train_acc, train_time = train_clf(clf, x_train, y_train,\n                                                     sample_weight=sample_weight,\n                                                     refit=refit)\n    ## Testing\n    start = timeit.default_timer()\n    test_predictions = clf.predict(x_test)\n    end = timeit.default_timer()\n    test_time = end - start\n    \n    test_acc = accuracy_score(y_test, test_predictions)\n    y_probs = clf.predict_proba(x_test)[:, 1]\n    \n    roc_auc = roc_auc_score(y_test, y_probs)\n    \n    \n    ## Model Memory\n    model_mem = round(model_memory_size(clf) / 1024, 2)\n    \n    print(clf)\n    print(\"\\n=============================> TRAIN-TEST DETAILS <======================================\")\n    \n    ## Metrics\n    print(f\"Train Size: {x_train.shape[0]} samples\")\n    print(f\" Test Size: {x_test.shape[0]} samples\")\n    print(\"------------------------------------------\")\n    print(f\"Training Time: {round(train_time, 3)} seconds\")\n    print(f\" Testing Time: {round(test_time, 3)} seconds\")\n    print(\"------------------------------------------\")\n    print(\"Train Accuracy: \", train_acc)\n    print(\" Test Accuracy: \", test_acc)\n    print(\"------------------------------------------\")\n    print(\" Area Under ROC: \", roc_auc)\n    print(\"------------------------------------------\")\n    print(f\"Model Memory Size: {model_mem} kB\")\n    print(\"\\n=============================> CLASSIFICATION REPORT <===================================\")\n    \n    ## Classification Report\n    clf_rep = classification_report(y_test, test_predictions, output_dict=True)\n    \n    print(classification_report(y_test, test_predictions,\n                                target_names=confusion_labels))\n    \n    \n    if verbose:\n        print(\"\\n================================> CONFUSION MATRIX <=====================================\")\n    \n        ## Confusion Matrix HeatMap\n        display(confusion_plot(confusion_matrix(y_test, test_predictions),\n                               labels=confusion_labels))\n        print(\"\\n=======================================> PLOTS <=========================================\")\n\n\n        ## Variable importance plot\n        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\n        roc_axes = axes[0, 0]\n        pr_axes = axes[0, 1]\n        importances = None\n\n        if importance_plot:\n            if not feature_labels:\n                raise RuntimeError(\"'feature_labels' argument not passed \"\n                                   \"when 'importance_plot' is True\")\n\n            try:\n                importances = pd.Series(clf.feature_importances_,\n                                        index=feature_labels) \\\n                                .sort_values(ascending=False)\n            except AttributeError:\n                try:\n                    importances = pd.Series(clf.coef_.ravel(),\n                                            index=feature_labels) \\\n                                    .sort_values(ascending=False)\n                except AttributeError:\n                    pass\n\n            if importances is not None:\n                # Modifying grid\n                grid_spec = axes[0, 0].get_gridspec()\n                for ax in axes[:, 0]:\n                    ax.remove()   # remove first column axes\n                large_axs = fig.add_subplot(grid_spec[0:, 0])\n\n                # Plot importance curve\n                feature_importance_plot(importances=importances.values,\n                                        feature_labels=importances.index,\n                                        ax=large_axs)\n                large_axs.axvline(x=0)\n\n                # Axis for ROC and PR curve\n                roc_axes = axes[0, 1]\n                pr_axes = axes[1, 1]\n            else:\n                # remove second row axes\n                for ax in axes[1, :]:\n                    ax.remove()\n        else:\n            # remove second row axes\n            for ax in axes[1, :]:\n                ax.remove()\n\n\n        ## ROC and Precision-Recall curves\n        clf_name = clf.__class__.__name__\n        roc_plot(y_test, y_probs, clf_name, ax=roc_axes)\n        precision_recall_plot(y_test, y_probs, clf_name, ax=pr_axes)\n\n        fig.subplots_adjust(wspace=5)\n        fig.tight_layout()\n        display(fig)\n    \n    ## Dump to report_dict\n    dump = dict(clf=clf, train_acc=train_acc, train_time=train_time,\n                train_predictions=train_predictions, test_acc=test_acc,\n                test_time=test_time, test_predictions=test_predictions,\n                test_probs=y_probs, report=clf_rep, roc_auc=roc_auc,\n                model_memory=model_mem)\n    \n    return clf, dump","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"def compare_models(y_test=None, clf_reports=[], labels=[]):\n    \"\"\" Compare evaluation metrics for the True Positive class [1] of \n        binary classifiers passed in the argument and plot ROC and PR curves.\n        \n        Arguments:\n        ---------\n        y_test: to plot ROC and Precision-Recall curves\n        \n        Returns:\n        -------\n        compare_table: pandas DataFrame containing evaluated metrics\n                  fig: `matplotlib` figure object with ROC and PR curves \"\"\"\n\n    \n    ## Classifier Labels\n    default_names = [rep['clf'].__class__.__name__ for rep in clf_reports]\n    clf_names =  labels if len(labels) == len(clf_reports) else default_names\n    \n    \n    ## Compare Table\n    table = dict()\n    index = ['Train Accuracy', 'Test Accuracy', 'Overfitting', 'ROC Area',\n             'Precision', 'Recall', 'F1-score', 'Support']\n    for i in range(len(clf_reports)):\n        train_acc = round(clf_reports[i]['train_acc'], 3)\n        test_acc = round(clf_reports[i]['test_acc'], 3)\n        clf_probs = clf_reports[i]['test_probs']\n        roc_auc = clf_reports[i]['roc_auc']\n        \n        # Get metrics of True Positive class from sklearn classification_report\n        true_positive_metrics = list(clf_reports[i]['report'][\"1\"].values())\n        \n        table[clf_names[i]] = [train_acc, test_acc,\n                               test_acc < train_acc, roc_auc] + true_positive_metrics\n    \n    table = pd.DataFrame(data=table, index=index)\n    \n    \n    ## Compare Plots\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n    \n    # ROC and Precision-Recall\n    for i in range(len(clf_reports)):\n        clf_probs = clf_reports[i]['test_probs']\n        roc_plot(y_test, clf_probs, label=clf_names[i],\n                 compare=True, ax=axes[0])\n        precision_recall_plot(y_test, clf_probs, label=clf_names[i],\n                              compare=True, ax=axes[1])\n    # Plot No-Info classifier\n    axes[0].plot([0,1], [0,1], linestyle='--', color='green')\n        \n    fig.tight_layout()\n    plt.close()\n    \n    return table.T, fig","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"naive-bayes\"></a>\n## 4.2. Naive Bayes\nThe fundamental assumption made by Naive Bayes regarding the data is ***class conditional independence of features***. Sklearn provides different variants of Naive Bayes depending on whether the features follow a categorical distribution (CategoricalNB), normal distribution (GaussianNB), bernoulli distribution (BernoulliNB), multinomial distribution (MultinomialNB)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.naive_bayes import CategoricalNB, GaussianNB \nfrom sklearn.preprocessing import KBinsDiscretizer, OrdinalEncoder\n\nconfusion_lbs = ['No Churn', 'Churn']\n\n## Discretize 'monthlycharges' and 'totalcharges' into 3bins\nkbn = KBinsDiscretizer(n_bins=12, encode='ordinal')\node = OrdinalEncoder(dtype=np.int64)\nnb_trans = [('ordinal', ode, categorical_columns),\n            ('kbn', kbn, numeric_columns[1:])]\nnb_col_trans = ColumnTransformer(nb_trans, remainder='passthrough')\n\n## Applying Column Transformer\nx_train_nb = nb_col_trans.fit_transform(x_train_cat)\nx_test_nb = nb_col_trans.transform(x_test_cat)\n\nnb_clf = CategoricalNB()\n\nnb_clf, nb_report = report(nb_clf, x_train_nb, y_train,\n                           x_test_nb, y_test, refit=True,\n                           confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"logistic-regression\"></a>\n## 4.3.  Logistic Regression","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegressionCV\n\nlogit_cv = LogisticRegressionCV(Cs=10, class_weight='balanced', cv=5, dual=False,\n                                fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n                                max_iter=500, multi_class='auto', n_jobs=None,\n                                penalty='l1', random_state=0, refit=True,\n                                scoring='f1', solver='liblinear', tol=0.0001,\n                                verbose=0)\n\nlogit_cv, logit_report = report(logit_cv, x_train, y_train,\n                                x_test, y_test, refit=True,\n                                importance_plot=True,\n                                feature_labels=feature_names,\n                                confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"k-nearest-neighbors\"></a>\n## 4.4.  K-Nearest Neighbors\nKNN estimator in Scikit-learn does not provide a way to pass class-weights to enable cost-sensitive/ class-weighted learning.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=91, p=1,\n                           weights='uniform', n_jobs=-1)\n\nknn, knn_report = report(knn, x_train, y_train,\n                         x_test, y_test,\n                         importance_plot=True,\n                         feature_labels=feature_names,\n                         confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"decision-tree\"></a>\n## 4.5.  Decision Tree","execution_count":null},{"metadata":{"Collapsed":"false","_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier(class_weight='balanced',\n                                       criterion='entropy',\n                                       max_depth=3,\n                                       random_state=0)\n\ndecision_tree, decision_tree_report = report(decision_tree, x_train, y_train,\n                                             x_test, y_test,\n                                             importance_plot=True,\n                                             feature_labels=feature_names,\n                                             confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"decision-trees-with-bagging\"></a>\n## 4.6.  Decision Trees with Bagging","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\n\nbagging_dtree = DecisionTreeClassifier(max_depth=2, class_weight='balanced',\n                                       criterion='entropy', random_state=0)\n\nbagging_clf = BaggingClassifier(base_estimator=bagging_dtree,\n                                max_samples=110, n_estimators=80,\n                                max_features=15, n_jobs=-1,\n                                random_state=0)\n\nbagging_clf, bagging_clf_report = report(bagging_clf, x_train, y_train,\n                                         x_test, y_test,\n                                         feature_labels=feature_names,\n                                         confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"random-forests\"></a>\n## 4.7.  Random Forests","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(class_weight='balanced', criterion='entropy',\n                                       max_depth=1, max_samples=2000, n_estimators=100,\n                                       n_jobs=-1, random_state=0)\n\nrandom_forest, random_forest_report = report(random_forest, x_train, y_train,\n                                             x_test, y_test,\n                                             importance_plot=True,\n                                             feature_labels=feature_names,\n                                             confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"decision-trees-with-adaboost\"></a>\n## 4.8.  Decision Trees with AdaBoost\nThe default base estimator for `AdaBoostClassifier` is `DecisionTreeClassifier(max_depth=1)`","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nboosting_dtree = DecisionTreeClassifier(class_weight='balanced',\n                                        criterion='entropy',\n                                        max_depth=1, random_state=0)\nadaboot = AdaBoostClassifier(base_estimator=boosting_dtree,\n                             n_estimators=285, learning_rate=0.1,\n                             random_state=0)\n\nadaboot, adaboot_report = report(adaboot, x_train, y_train,\n                                 x_test, y_test,\n                                 importance_plot=True,\n                                 feature_labels=feature_names,\n                                 confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"linear-svc\"></a>\n## 4.9.  Linear SVC","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from sklearn.svm import SVC\n\nlinear_svc = SVC(kernel='linear', probability=True,\n                 class_weight='balanced', random_state=0)\n\nlinear_svc, linear_svc_report = report(linear_svc, x_train, y_train,\n                                       x_test, y_test,\n                                       importance_plot=True,\n                                       feature_labels=feature_names,\n                                       confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"svm-with-rbf-kernel\"></a>\n## 4.10.  SVM with RBF kernel","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"rbf_svc = SVC(C=0.3, kernel='rbf', probability=True,\n              class_weight='balanced', random_state=0)\n\nrbf_svc, rbf_svc_report = report(rbf_svc, x_train, y_train,\n                                 x_test, y_test,\n                                 importance_plot=True,\n                                 feature_labels=feature_names,\n                                 confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"xgboost\"></a>\n## 4.11.  XGBoost","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.utils import class_weight\n\n## Compute `class_weights` using sklearn\ncls_weight = (y_train.shape[0] - np.sum(y_train)) / np.sum(y_train)\n\nxgb_clf = XGBClassifier(learning_rate=0.01, random_state=0,\n                        scale_pos_weight=cls_weight, n_jobs=-1)\nxgb_clf.fit(x_train, y_train);\n\nxgb_clf, xgb_report = report(xgb_clf, x_train, y_train,\n                             x_test, y_test,\n                             importance_plot=True,\n                             feature_labels=feature_names,\n                             confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"catboost\"></a>\n## 4.12.  CatBoost\nCat boost performs better without One-hot encoding because it performs an internal categorical encoding that is similar to Leave One Out Encoding (LOOE). So, we can give the dataframe as input to the catboost classifier.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"from catboost import CatBoostClassifier\n\n# Basic working\n\ncatboost_clf = CatBoostClassifier(cat_features=categorical_columns,\n                                  l2_leaf_reg=120, depth=6,\n                                  auto_class_weights='Balanced',\n                                  iterations=200, learning_rate=0.16,\n                                  use_best_model=True,\n                                  early_stopping_rounds=150,\n                                  eval_metric='F1', random_state=0)\n\ncatboost_clf.fit(x_train_cat, y_train, \n                 eval_set=(x_train_cat, y_train),\n                 verbose=False)\n\n\nf_labels = categorical_columns+numeric_columns\ncatboost_clf, catboost_report = report(catboost_clf, x_train_cat, y_train,\n                                       x_test_cat, y_test,\n                                       importance_plot=True,\n                                       feature_labels=f_labels,\n                                       confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"model-comparison\"></a>\n# 5.  Model Comparison\nSince input data format for Naive Bayes and CatBoost are different, we will add them to the comparison manually.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"report_list = [nb_report, logit_report, knn_report, decision_tree_report, \n               bagging_clf_report, random_forest_report, adaboot_report,\n               xgb_report, linear_svc_report, rbf_svc_report, catboost_report]\nclf_labels = [rep['clf'].__class__.__name__ for rep in report_list]\nclf_labels[-3], clf_labels[-2] = 'Linear SVC', 'RBF SVC'","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"evaluation-metrics\"></a>\n## 5.1.  Evaluation Metrics","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"compare_table, compare_plot = compare_models(y_test, clf_reports=report_list, labels=clf_labels)\n\ncompare_table.sort_values(by=['Overfitting'])","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"***Inference:*** We can see that among the classifiers that do not overfit, Random Forests classifier has the highest recall while Logistic Regression has the highest F1-score. In terms of Revenue Retained, Random Forests are the best. However, Random Forests suffer from low precision.","execution_count":null},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"2-roc-and-pr-curves\"></a>\n## 5.2. 2 ROC and PR Curves","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"compare_plot","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false"},"cell_type":"markdown","source":"<a id=\"further-analysis\"></a>\n# 6.  Further Analysis\nWe defined a additional evaluation metric called Percentage Monthly Revenue Retained which is defined as,\n\n\\begin{align}\n\\text {Revenue Retained (Monthly)} \\%=\\frac{\\sum_{i=1}^{n_{\\text {test}}} y_{\\text {test}}^{(i)} \\times y_{\\text {pred}}^{(i)} \\times \\text {monthlycharges}^{(i)}}{\\sum_{i=1}^{n_{\\text {test}}} y_{\\text {test}}^{(i)} \\times \\text {monthlycharges}(i)} \\times 100\n\\end{align}\n\nIt is a “revenue” weighted recall score and can be viewed as a business equivalent of Recall. It represents the revenue retained (or saved) by a model as a result of its correct churn predictions i.e., True Positives.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"df_charges = list()\nnew_cols = ['Revenue Retained (Monthly) %', 'Revenue Retained (Total) %']\n\nfor rep in report_list:\n    true_positives = (y_test * rep['test_predictions']).reshape(y_test.shape[0], 1)\n    tp_revenue = (x_test_charges * true_positives).sum(axis=0)\n    df_charges.append(tp_revenue)\n\nrevenue_saved = pd.DataFrame(df_charges, index=clf_labels, \n                             columns=new_cols)\n\ncompare_table_rev = pd.concat([compare_table, revenue_saved], axis=1)\n\n## True Positive Revenue/ Total Churn Revenue\ntotal_churn_revenue = (x_test_charges * y_test.reshape(y_test.shape[0], 1)).sum(axis=0)\ntemp_cols = (compare_table_rev.iloc[:, 8:10] / total_churn_revenue) * 100\ncompare_table_rev.iloc[:, 8:10] = temp_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"compare_table_rev","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"select_cols = ['Overfitting', 'F1-score'] + new_cols\ncompare_table_rev[select_cols].sort_values(by=['Overfitting', 'Revenue Retained (Monthly) %'],\n                                           ascending=[True, False])","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":false},"cell_type":"code","source":"compare_table_rev[select_cols].sort_values(by=['Overfitting', 'F1-score'],\n                                           ascending=[True, False])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}