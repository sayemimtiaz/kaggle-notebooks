{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Diabetes Prediction using Cost Sensitive Learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Highlights:**\n> 1. Correlation analysis using Pearson coefficient\n>\n> 2. Classification of the imbalanced dataset using `class weighted` or `cost sensitive` learning\n>\n> 3. Rationale for choosing SVC with RBF kernel as an optimal classifier for the dataset\n>\n> 4. Results for each ML algorithm are presented after performing 5-fold cross validation based on F1-score","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* [1. Import and Clean data](#import-and-clean-data)\n    * [1.1. Checking for null values in the dataset](#checking-for-null-values-in-the-dataset)\n* [2. Correlations in the data](#correlations-in-the-data)\n* [3. Data Visualization](#data-visualization)\n    * [3.1.  Frequency distribution](#frequency-distribution)\n    * [3.2. Box Plots](#box-plots)\n* [4. Data Preprocessing](#data-preprocessing)\n    * [4.1. Train-Test split](#train-test-split)\n    * [4.2. Standardization](#standardization)\n* [5. Data Modeling](#data-modeling)\n    * [5.1. Utility Functions](#utility-functions)\n    * [5.2. Naive Bayes](#naive-bayes)\n    * [5.3. Logistic Regression](#logistic-regression)\n    * [5.4. K-Nearest Neighbors](#k-nearest-neighbors)\n    * [5.5. Decision Tree](#decision-tree)\n    * [5.6. Decision Trees with Bagging](#decision-trees-with-bagging)\n    * [5.7. Random Forests](#random-forests)\n    * [5.8. Decision Trees with AdaBoost](#decision-trees-with-adaboost)\n    * [5.9. Linear SVC](#linear-svc)\n    * [5.10. SVM with RBF kernel](#svm-with-rbf-kernel)\n    * [5.11. XGBoost](#xgboost)\n    * [5.12. CatBoost](#catboost)\n* [6. Model Comparison](#model-comparison)\n    * [6.1. Evaluation Metrics](#evaluation-metrics)\n    * [6.2. ROC and PR Curves](#roc-and-pr-curves)","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"import os, sys\n\nimport numpy as np\nfrom scipy.stats import chi2_contingency\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"import-and-clean-data\"></a>\n# 1. Import and Clean data","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"checking-for-null-values-in-the-dataset\"></a>\n## 1.1. Checking for null values in the dataset","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values in the dataset.","execution_count":null},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"correlations-in-the-data\"></a>\n# 2. Correlations in the data","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm',\n            cbar_kws={'aspect': 50}, square=True, ax=ax)\nplt.xticks(rotation=30, ha='right');\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Inference:*** Glucose feature is higly correlated with the outcome (whether the patient has diabetes or not) which does make sense given diabetes is closely related to blood sugar levels.","execution_count":null},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"data-visualization\"></a>\n# 3. Data Visualization","execution_count":null},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"frequency-distribution\"></a>\n## 3.1.  Frequency distribution","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"df_grouped = df.groupby(by='Outcome')\n\n_, axes = plt.subplots(nrows=4, ncols=2, figsize=(10, 15))\n\nnumeric_cols = list(df.columns[:-1])\nax_title_pairs = zip(axes.flat, numeric_cols)\n\nfor ax, title in ax_title_pairs:\n    sns.distplot(df_grouped.get_group(0)[title],\n                 bins=10, ax=ax, label='N')\n    sns.distplot(df_grouped.get_group(1)[title],\n                 bins=10, ax=ax, label='Y')\n    ax.set_title(title)\n    ax.set_xlabel('')\n    ax.legend(title='Outcome')\n    \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"box-plots\"></a>\n## 3.2. Box Plots","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"_, axes = plt.subplots(nrows=4, ncols=2, figsize=(10, 15))\nax_title_pairs = zip(axes.flat, numeric_cols)\n\nfor ax, col in ax_title_pairs:\n    sns.boxplot(x='Outcome', y=col, data=df, ax=ax)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"data-preprocessing\"></a>\n# 4. Data Preprocessing\nData needs to be one-hot-encoded before applying machine learning models.","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"x = df.iloc[:, :-1]\ny = df['Outcome']\n\nfeatures = list(df.columns[:-1])","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"train-test-split\"></a>\n## 4.1. Train-Test split\nCatBoost classifier does not require any kind of preprocessing. Therefore, we will use raw/ unmodified data (`x_train_cat, x_test_cat, y_train_cat, y_test_cat`) for CatBoost and preprocessed data (`x_train, x_test, y_train, y_test`) for all other classifiers.","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata_splits = train_test_split(x, y, test_size=0.20, random_state=0,\n                               shuffle=True, stratify=y)\nx_train, x_test, y_train, y_test = data_splits\n\n\n# For CatBoost and Naive Bayes\ndata_splits = train_test_split(x, y, test_size=0.20, random_state=0,\n                               shuffle=True, stratify=y)\nx_train_cat, x_test_cat, y_train_cat, y_test_cat = data_splits\n\n\nlist(map(lambda x: x.shape, [x, y, x_train, x_test, y_train, y_test]))","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"pd.Series(y_test).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"sns.countplot(x=y_test);","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true},"cell_type":"markdown","source":"<a id=\"standardization\"></a>\n## 4.2. Standardization\nWe need to standardize the continuous or quantitative variables/ features before applying Machine Learning models. This is important because if we don't standardize the features, features with high variance that are orders of magnitude larger that others might dominate the model fitting process and causing the model unable to learn from other features (with lower variance) correctly as expected. \n\n***Also we need to standardize the data only after performing train-test split because if we standardize before splitting then there is a chance for some information leak from the test set into the train set. We always want the test set to be completely new to the ML models. [Read more](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data)***","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n\n## Column Transformer\ntransformers = [('standard_scaler', StandardScaler(), features)]\nx_trans = ColumnTransformer(transformers, remainder='passthrough')\n\n## Applying Column Transformer\nx_train = x_trans.fit_transform(x_train)\nx_test = x_trans.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data-modeling\"></a>\n# 5. Data Modeling\nSince the dataset is imbalanced we will be using class-weighted/ cost-sensitive learning. In cost-sensitive learning, a weighted cost function is used. Therefore, misclassifying a sample from the minority class will cost the classifiers more than misclassifying a sample from the majority class. In most of the Sklearn classifiers, cost-sensitive learning can be enabled by setting `class_weight='balanced'`.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"utility-functions\"></a>\n## 5.1. Utility Functions","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"import timeit\nimport pickle\nimport sys\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, \\\n                            precision_recall_curve, roc_curve, accuracy_score\nfrom sklearn.exceptions import NotFittedError","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"def confusion_plot(matrix, labels=None):\n    \"\"\" Display binary confusion matrix as a Seaborn heatmap \"\"\"\n    \n    labels = labels if labels else ['Negative (0)', 'Positive (1)']\n    \n    fig, ax = plt.subplots(nrows=1, ncols=1)\n    sns.heatmap(data=matrix, cmap='Blues', annot=True, fmt='d',\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_xlabel('PREDICTED')\n    ax.set_ylabel('ACTUAL')\n    ax.set_title('Confusion Matrix')\n    plt.close()\n    \n    return fig","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"def roc_plot(y_true, y_probs, label, compare=False, ax=None):\n    \"\"\" Plot Receiver Operating Characteristic (ROC) curve \n        Set `compare=True` to use this function to compare classifiers. \"\"\"\n    \n    fpr, tpr, thresh = roc_curve(y_true, y_probs, drop_intermediate=False)\n    auc = round(roc_auc_score(y_true, y_probs), 2)\n    \n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1)\n    label = ' '.join([label, f'({auc})']) if compare else None\n    sns.lineplot(x=fpr, y=tpr, ax=axis,\n                 estimator=None, label=label)\n    \n    if compare:\n        axis.legend(title='Classifier (AUC)', loc='lower right')\n    else:\n        axis.text(0.72, 0.05, f'AUC = { auc }', fontsize=12,\n                  bbox=dict(facecolor='green', alpha=0.4, pad=5))\n            \n        # Plot No-Info classifier\n        axis.fill_between(fpr, fpr, tpr, alpha=0.3, edgecolor='g',\n                          linestyle='--', linewidth=2)\n        \n    axis.set_xlim(0, 1)\n    axis.set_ylim(0, 1)\n    axis.set_title('ROC Curve')\n    axis.set_xlabel('False Positive Rate [FPR]\\n(1 - Specificity)')\n    axis.set_ylabel('True Positive Rate [TPR]\\n(Sensitivity or Recall)')\n    \n    plt.close()\n    \n    return axis if ax else fig","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"def precision_recall_plot(y_true, y_probs, label, compare=False, ax=None):\n    \"\"\" Plot Precision-Recall curve.\n        Set `compare=True` to use this function to compare classifiers. \"\"\"\n    \n    p, r, thresh = precision_recall_curve(y_true, y_probs)\n    p, r, thresh = list(p), list(r), list(thresh)\n    p.pop()\n    r.pop()\n    \n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1)\n    \n    if compare:\n        sns.lineplot(r, p, estimator=None,\n                     ax=axis, label=label)\n        axis.set_xlabel('Recall')\n        axis.set_ylabel('Precision')\n        axis.legend(loc='lower left')\n    else:\n        sns.lineplot(thresh, p, estimator=None,\n                     label='Precision', ax=axis)\n        axis.set_xlabel('Threshold')\n        axis.set_ylabel('Precision')\n        axis.legend(loc='lower left')\n\n        axis_twin = axis.twinx()\n        sns.lineplot(thresh, r, estimator=None,\n                     color='limegreen', label='Recall', ax=axis_twin)\n        axis_twin.set_ylabel('Recall')\n        axis_twin.set_ylim(0, 1)\n        axis_twin.legend(bbox_to_anchor=(0.24, 0.18))\n    \n    axis.set_xlim(0, 1)\n    axis.set_ylim(0, 1)\n    axis.set_title('Precision Vs Recall')\n    \n    plt.close()\n    \n    return axis if ax else fig","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"def feature_importance_plot(importances, feature_labels, ax=None):\n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1, figsize=(5, 10))\n    sns.barplot(x=importances, y=feature_labels, ax=axis)\n    axis.set_title('Feature Importance Measures')\n    \n    plt.close()\n    \n    return axis if ax else fig","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"def train_clf(clf, x_train, y_train, sample_weight=None, refit=False):\n    train_time = 0\n    \n    try:\n        if refit:\n            raise NotFittedError\n        y_pred_train = clf.predict(x_train)\n    except NotFittedError:\n        start = timeit.default_timer()\n        \n        if sample_weight is not None:\n            clf.fit(x_train, y_train, sample_weight=sample_weight)\n        else:\n            clf.fit(x_train, y_train)\n        \n        end = timeit.default_timer()\n        train_time = end - start\n        \n        y_pred_train = clf.predict(x_train)\n    \n    train_acc = accuracy_score(y_train, y_pred_train)\n    return clf, y_pred_train, train_acc, train_time","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"def model_memory_size(clf):\n    return sys.getsizeof(pickle.dumps(clf))","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"def report(clf, x_train, y_train, x_test, y_test, display_scores=[],\n           sample_weight=None, refit=False, importance_plot=False,\n           confusion_labels=None, feature_labels=None, verbose=True):\n    \"\"\" Trains the passed classifier if not already trained and reports\n        various metrics of the trained classifier \"\"\"\n    \n    dump = dict()\n    \n    ## Train if not already trained\n    clf, train_predictions, \\\n    train_acc, train_time = train_clf(clf, x_train, y_train,\n                                      sample_weight=sample_weight,\n                                      refit=refit)\n    ## Testing\n    start = timeit.default_timer()\n    test_predictions = clf.predict(x_test)\n    end = timeit.default_timer()\n    test_time = end - start\n    \n    test_acc = accuracy_score(y_test, test_predictions)\n    y_probs = clf.predict_proba(x_test)[:, 1]\n    \n    roc_auc = roc_auc_score(y_test, y_probs)\n        \n    ## Additional scores\n    scores_dict = dict()\n    for func in display_scores:\n        scores_dict[func.__name__] = [func(y_train, train_predictions),\n                                      func(y_test, test_predictions)]\n        \n    ## Model Memory\n    model_mem = round(model_memory_size(clf) / 1024, 2)\n    \n    print(clf)\n    print(\"\\n=============================> TRAIN-TEST DETAILS <======================================\")\n    \n    ## Metrics\n    print(f\"Train Size: {x_train.shape[0]} samples\")\n    print(f\" Test Size: {x_test.shape[0]} samples\")\n    print(\"---------------------------------------------\")\n    print(f\"Training Time: {round(train_time, 3)} seconds\")\n    print(f\" Testing Time: {round(test_time, 3)} seconds\")\n    print(\"---------------------------------------------\")\n    print(\"Train Accuracy: \", train_acc)\n    print(\" Test Accuracy: \", test_acc)\n    print(\"---------------------------------------------\")\n    \n    if display_scores:\n        for k, v in scores_dict.items():\n            score_name = ' '.join(map(lambda x: x.title(), k.split('_')))\n            print(f'Train {score_name}: ', v[0])\n            print(f' Test {score_name}: ', v[1])\n            print()\n        print(\"---------------------------------------------\")\n    \n    print(\" Area Under ROC (test): \", roc_auc)\n    print(\"---------------------------------------------\")\n    print(f\"Model Memory Size: {model_mem} kB\")\n    print(\"\\n=============================> CLASSIFICATION REPORT <===================================\")\n    \n    ## Classification Report\n    clf_rep = classification_report(y_test, test_predictions, output_dict=True)\n    \n    print(classification_report(y_test, test_predictions,\n                                target_names=confusion_labels))\n    \n    \n    if verbose:\n        print(\"\\n================================> CONFUSION MATRIX <=====================================\")\n    \n        ## Confusion Matrix HeatMap\n        display(confusion_plot(confusion_matrix(y_test, test_predictions),\n                               labels=confusion_labels))\n        print(\"\\n=======================================> PLOTS <=========================================\")\n\n\n        ## Variable importance plot\n        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\n        roc_axes = axes[0, 0]\n        pr_axes = axes[0, 1]\n        importances = None\n\n        if importance_plot:\n            if not feature_labels:\n                raise RuntimeError(\"'feature_labels' argument not passed \"\n                                   \"when 'importance_plot' is True\")\n\n            try:\n                importances = pd.Series(clf.feature_importances_,\n                                        index=feature_labels) \\\n                                .sort_values(ascending=False)\n            except AttributeError:\n                try:\n                    importances = pd.Series(clf.coef_.ravel(),\n                                            index=feature_labels) \\\n                                    .sort_values(ascending=False)\n                except AttributeError:\n                    pass\n\n            if importances is not None:\n                # Modifying grid\n                grid_spec = axes[0, 0].get_gridspec()\n                for ax in axes[:, 0]:\n                    ax.remove()   # remove first column axes\n                large_axs = fig.add_subplot(grid_spec[0:, 0])\n\n                # Plot importance curve\n                feature_importance_plot(importances=importances.values,\n                                        feature_labels=importances.index,\n                                        ax=large_axs)\n                large_axs.axvline(x=0)\n\n                # Axis for ROC and PR curve\n                roc_axes = axes[0, 1]\n                pr_axes = axes[1, 1]\n            else:\n                # remove second row axes\n                for ax in axes[1, :]:\n                    ax.remove()\n        else:\n            # remove second row axes\n            for ax in axes[1, :]:\n                ax.remove()\n\n\n        ## ROC and Precision-Recall curves\n        clf_name = clf.__class__.__name__\n        roc_plot(y_test, y_probs, clf_name, ax=roc_axes)\n        precision_recall_plot(y_test, y_probs, clf_name, ax=pr_axes)\n\n        fig.subplots_adjust(wspace=5)\n        fig.tight_layout()\n        display(fig)\n    \n    ## Dump to report_dict\n    dump = dict(clf=clf, accuracy=[train_acc, test_acc], **scores_dict,\n                train_time=train_time, train_predictions=train_predictions,\n                test_time=test_time, test_predictions=test_predictions,\n                test_probs=y_probs, report=clf_rep, roc_auc=roc_auc,\n                model_memory=model_mem)\n    \n    return clf, dump","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"def compare_models(y_test=None, clf_reports=[], labels=[], score='accuracy'):\n    \"\"\" Compare evaluation metrics for the True Positive class [1] of \n        binary classifiers passed in the argument and plot ROC and PR curves.\n        \n        Arguments:\n        ---------\n        y_test: to plot ROC and Precision-Recall curves\n         score: is the name corresponding to the sklearn metrics\n        \n        Returns:\n        -------\n        compare_table: pandas DataFrame containing evaluated metrics\n                  fig: `matplotlib` figure object with ROC and PR curves \"\"\"\n\n    \n    ## Classifier Labels\n    default_names = [rep['clf'].__class__.__name__ for rep in clf_reports]\n    clf_names =  labels if len(labels) == len(clf_reports) else default_names\n    \n    \n    ## Compare Table\n    table = dict()\n    index = ['Train ' + score, 'Test ' + score, 'Overfitting', 'ROC Area',\n             'Precision', 'Recall', 'F1-score', 'Support']\n    for i in range(len(clf_reports)):\n        scores = [round(i, 3) for i in clf_reports[i][score]]\n        \n        roc_auc = clf_reports[i]['roc_auc']\n        \n        # Get metrics of True Positive class from sklearn classification_report\n        true_positive_metrics = list(clf_reports[i]['report'][\"1\"].values())\n        \n        table[clf_names[i]] = scores + [scores[1] < scores[0], roc_auc] + \\\n                              true_positive_metrics\n    \n    table = pd.DataFrame(data=table, index=index)\n    \n    \n    ## Compare Plots\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n    \n    # ROC and Precision-Recall\n    for i in range(len(clf_reports)):\n        clf_probs = clf_reports[i]['test_probs']\n        roc_plot(y_test, clf_probs, label=clf_names[i],\n                 compare=True, ax=axes[0])\n        precision_recall_plot(y_test, clf_probs, label=clf_names[i],\n                              compare=True, ax=axes[1])\n    # Plot No-Info classifier\n    axes[0].plot([0,1], [0,1], linestyle='--', color='green')\n        \n    fig.tight_layout()\n    plt.close()\n    \n    return table.T, fig","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"naive-bayes\"></a>\n## 5.2. Naive Bayes\nThe fundamental assumption made by Naive Bayes regarding the data is ***class conditional independence of features***. Sklearn provides different variants of Naive Bayes depending on whether the features follow a categorical distribution (CategoricalNB), normal distribution (GaussianNB), bernoulli distribution (BernoulliNB), multinomial distribution (MultinomialNB). \n \nSince majority of the features are continuous and follow an approximately normal distribution, we will use GaussianNB.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nscore_func = f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB \n\nconfusion_lbs = ['No Diabetes', 'Have Diabetes']  # (0, 1)\n\nnb_clf = GaussianNB()\n\nnb_clf, nb_report = report(nb_clf, x_train, y_train,\n                           x_test, y_test, refit=True,\n                           display_scores=[score_func],\n                           confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"logistic-regression\"></a>\n## 5.3. Logistic Regression","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegressionCV\n\nlogit_cv = LogisticRegressionCV(class_weight='balanced', cv=5, max_iter=500,\n                                scoring='recall', penalty='l2', solver='liblinear',\n                                n_jobs=-1, random_state=0, refit=True, verbose=0)\n\nlogit_cv, logit_report = report(logit_cv, x_train, y_train,\n                                x_test, y_test,\n                                display_scores=[score_func],\n                                refit=True, importance_plot=True,\n                                feature_labels=features,\n                                confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"k-nearest-neighbors\"></a>\n## 5.4. K-Nearest Neighbors\nKNN estimator in Scikit-learn does not provide a way to pass class-weights to enable cost-sensitive/ class-weighted learning.","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=15, p=2,\n                           weights='distance', n_jobs=-1)\n\nknn, knn_report = report(knn, x_train, y_train,\n                         x_test, y_test,\n                         display_scores=[score_func],\n                         importance_plot=True,\n                         feature_labels=features,\n                         confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"decision-tree\"></a>\n## 5.5. Decision Tree","execution_count":null},{"metadata":{"_kg_hide-output":true,"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier(class_weight='balanced',\n                                       criterion='entropy',\n                                       max_depth=5,\n                                       random_state=0)\n\ndecision_tree, decision_tree_report = report(decision_tree, x_train, y_train,\n                                             x_test, y_test,\n                                             display_scores=[score_func],\n                                             importance_plot=True,\n                                             feature_labels=features,\n                                             confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"decision-trees-with-bagging\"></a>\n## 5.6. Decision Trees with Bagging","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\n\nbagging_dtree = DecisionTreeClassifier(max_depth=2, class_weight='balanced',\n                                       criterion='entropy', random_state=0)\n\nbagging_clf = BaggingClassifier(base_estimator=bagging_dtree,\n                                max_samples=0.1873, n_estimators=140,\n                                n_jobs=-1, random_state=0)\n\nbagging_clf, bagging_clf_report = report(bagging_clf, x_train, y_train,\n                                         x_test, y_test,\n                                         display_scores=[score_func],\n                                         feature_labels=features,\n                                         confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"random-forests\"></a>\n## 5.7. Random Forests","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(class_weight='balanced', criterion='entropy',\n                                       max_depth=3, max_samples=0.54, n_estimators=160,\n                                       n_jobs=-1, random_state=0)\n\nrandom_forest, random_forest_report = report(random_forest, x_train, y_train,\n                                             x_test, y_test,\n                                             display_scores=[score_func],\n                                             importance_plot=True,\n                                             feature_labels=features,\n                                             confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"decision-trees-with-adaboost\"></a>\n## 5.8. Decision Trees with AdaBoost\nThe default base estimator for `AdaBoostClassifier` is `DecisionTreeClassifier(max_depth=1)`","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nboosting_dtree = DecisionTreeClassifier(class_weight='balanced',\n                                        criterion='entropy',\n                                        max_depth=1, random_state=0)\n\nadaboot = AdaBoostClassifier(base_estimator=boosting_dtree,\n                             n_estimators=40, learning_rate=0.073,\n                             random_state=0)\n\nadaboot, adaboot_report = report(adaboot, x_train, y_train,\n                                 x_test, y_test,\n                                 display_scores=[score_func],\n                                 importance_plot=True,\n                                 feature_labels=features,\n                                 confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"linear-svc\"></a>\n## 5.9. Linear SVC","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nlinear_svc = SVC(C=0.026, kernel='linear', probability=True,\n                 class_weight='balanced', random_state=0)\n\nlinear_svc, linear_svc_report = report(linear_svc, x_train, y_train,\n                                       x_test, y_test,\n                                       display_scores=[score_func],\n                                       importance_plot=True,\n                                       feature_labels=features,\n                                       confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"svm-with-rbf-kernel\"></a>\n## 5.10. SVM with RBF kernel","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"rbf_svc = SVC(C=0.585, kernel='rbf', probability=True,\n              class_weight='balanced', random_state=0)\n\nrbf_svc, rbf_svc_report = report(rbf_svc, x_train, y_train,\n                                 x_test, y_test,\n                                 display_scores=[score_func],\n                                 importance_plot=True,\n                                 feature_labels=features,\n                                 confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"xgboost\"></a>\n## 5.11. XGBoost","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBClassifier\n\n## Compute `class_weights` using sklearn\ncls_weight = (y_train.shape[0] - np.sum(y_train)) / np.sum(y_train)\n\nparams = {'learning_rate': 0.09995835505176981, 'num_boost_round': 622,\n          'max_depth': 8, 'min_child_weight': 21.477702263891228,\n          'subsample': 0.7783353885462759, 'colsample_bytree': 0.8808385233283204}\n\nxgb_clf = XGBClassifier(**params, scale_pos_weight=cls_weight,\n                        random_state=0, n_jobs=-1)\nxgb_clf.fit(x_train, y_train, eval_set=[(x_train, y_train)],\n            eval_metric='aucpr', early_stopping_rounds=10,\n            verbose=False);\n\nxgb_clf, xgb_report = report(xgb_clf, x_train, y_train,\n                             x_test, y_test,\n                             display_scores=[score_func],\n                             importance_plot=True,\n                             feature_labels=features,\n                             confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"catboost\"></a>\n## 5.12. CatBoost\nCat boost performs better without One-hot encoding because it performs an internal categorical encoding that is similar to Leave One Out Encoding (LOOE). So, we can give the dataframe as input to the catboost classifier.","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\n\n# Basic working\n\ncatboost_clf = CatBoostClassifier(cat_features=[],\n                                  l2_leaf_reg=120, depth=6,\n                                  auto_class_weights='Balanced',\n                                  iterations=200, learning_rate=0.16,\n                                  use_best_model=True,\n                                  early_stopping_rounds=150,\n                                  eval_metric='Recall', random_state=0)\n\ncatboost_clf.fit(x_train_cat, y_train, \n                 eval_set=(x_train_cat, y_train),\n                 verbose=False)\n\n\ncatboost_clf, catboost_report = report(catboost_clf, x_train_cat, y_train,\n                                       x_test_cat, y_test,\n                                       display_scores=[score_func],\n                                       importance_plot=True,\n                                       feature_labels=features,\n                                       confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"model-comparison\"></a>\n# 6. Model Comparison","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"report_list = [nb_report, logit_report, knn_report, decision_tree_report,               \n               bagging_clf_report, random_forest_report, adaboot_report,\n               xgb_report, linear_svc_report, rbf_svc_report, catboost_report]\nclf_labels = [rep['clf'].__class__.__name__ for rep in report_list]\nclf_labels[-3], clf_labels[-2] = 'Linear SVC', 'RBF SVC'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"evaluation-metrics\"></a>\n## 6.1. Evaluation Metrics\nSince the dataset is imbalanced, 'F1-score' is a more appropriate evaluation metric than accuracy.","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"compare_table, compare_plot = compare_models(y_test, clf_reports=report_list,\n                                             labels=clf_labels, score=score_func.__name__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_table.sort_values(by=['Overfitting', 'F1-score'],\n                          ascending=[True, False])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Inference:*** We can see that among the classifiers that do not overfit (based on F1 score), SVC with RBF kernel has the highest F1 score.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"roc-and-pr-curves\"></a>\n## 6.2. ROC and PR Curves","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"compare_plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Thank You!!**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}