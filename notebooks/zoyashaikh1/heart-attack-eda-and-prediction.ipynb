{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORTING LIBRARIES","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IMPORTING DATASET","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(r\"../input/heart-attack-analysis-prediction-dataset/heart.csv\")\ndf.head","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DESCRIPTION OF DATASET\n\nAge : Age of the patient\n\nSex : Sex of the patient (male = 1, female = 0)\n\nexang: exercise induced angina (1 = yes; 0 = no)\n\nca: number of major vessels (0-3)\n\ncp : Chest Pain type chest pain type\n\nValue 1: typical angina\nValue 2: atypical angina\nValue 3: non-anginal pain\nValue 4: asymptomatic\ntrtbps : resting blood pressure (in mm Hg)\n\nchol : cholestoral in mg/dl fetched via BMI sensor\n\nfbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n\noldpeak- T depression induced by exercise relative to rest \nslp -  of the peak exercise ST segment (Ordinal) [ 1: upsloping, 2: flat , 3: downsloping)\n\n\nrest_ecg : resting electrocardiographic results [values 0,1,2]\n\n\nValue 0: normal\nValue 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\nValue 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\nthalach : maximum heart rate achieved(Ordinal) [3 = normal; 6 = fixed defect; 7 = reversable defect]\n\n\ntarget : 0= less chance of heart attack 1= more chance of heart attack')\n","metadata":{}},{"cell_type":"markdown","source":"# Exploratory data analysis","metadata":{}},{"cell_type":"code","source":"#HEAT MAP CORRELATION\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(20, 15))\nsns.heatmap(corrmat, vmax=.8, square=True, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#count plot genders\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"sex\")\nplt.show()\n\n#here 0 - female \n# 1 - male \n#The number of males are more than female\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cholestrol by age\nplt.figure(figsize = (9,9))\nsns.FacetGrid(df, size = 6)\nv = sns.scatterplot(data = df , x = 'age', y = 'chol', hue = 'chol',palette = \"flare\")\nv.axhline(y= 240, linewidth=4, color='r', linestyle= '--')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"IN THIS ABOVE GRAPH, range above 240 mg/dl is considered as high which can lead to heart disease","metadata":{}},{"cell_type":"code","source":"#resting blood pressure by age\nplt.figure(figsize = (9,9))\nsns.FacetGrid(df, size = 6)\nv = sns.scatterplot(data = df , x = 'age', y = 'trtbps', hue = 'chol')\nv.axhline(y= 120, linewidth=4, color='r', linestyle= '--')\nplt.title(\"resting blood pressure by age\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RESTING BLOOD PRESSURE HIGHER THAN 120 mmhg CAN LEAD TO HEART ATTACK","metadata":{}},{"cell_type":"code","source":"#chest pain by age\nplt.figure(figsize=(12,10))\n\nsns.displot(df[df['cp'] == 0][\"age\"], color='green', kde = True) # typical\nsns.displot(df[df['cp'] == 1][\"age\"], color='red', kde = True) # atypical\nsns.displot(df[df['cp'] == 2][\"age\"], color='yellow', kde = True) #non angina\nsns.displot(df[df['cp'] == 3][\"age\"], color='blue', kde = True) # asymptotic resting blood pressure\n\n\nplt.title('chest pain by age', fontsize=15)\nplt.xlim([18,100])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1] 0 - typical angina value, which is due to physical exertion or emotional released by rest\n\n2] 1 - atypical angina value, which is not associated with chest pain, symptoms are weakness, nausea, sweating\n\n3] 2 - non angina value\n\n4] 3- asymptotic resting blood pressure representing the BP reached after a long rest.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\n\nsns.histplot(df[df['restecg'] == 0][\"age\"], color='green', kde = True) # normal value  - green\nsns.histplot(df[df['restecg'] == 1][\"age\"], color='red', kde = True) # having ST-T abnormality- Red\nsns.histplot(df[df['restecg'] == 2][\"age\"], color='yellow', kde = True) # showing probable or definite left ventricular hypertrophy- yellow\n\n\nplt.title('resting electocardiographic result by age', fontsize=15)\nplt.xlim([18,100])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n1 ] Value 0 : normal (color - Green) SHOWS FROM AGE ABOVE 29\n\n2 ] Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) (color - red) SHOWS FROM AGE ABOVE 35\n\n3 ] Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria (color - Yellow)  SHOWS AGE FROM 55 TO 63 AND 69 TO 75","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\n\nsns.histplot(df[df['fbs']== 0][\"age\"], color='green', kde = True) # FASTING BLOOD < 120\nsns.histplot(df[df['fbs']== 1][\"age\"], color='red', kde = True) # FASTING BLOOD > 120\n\nplt.title('fasting blood sugar by age', fontsize=15)\nplt.xlim([18,100])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above graph, age above 42 have chances of diabetis","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\n\nsns.histplot(df[df['exng'] == 0][\"age\"], color='green',kde = True) # no angina induced- green\nsns.histplot(df[df['exng'] == 1][\"age\"], color='red', kde = True) # angina induced due to excercise- Red\n\nplt.title('excercise induced angina v/s no angina', fontsize=15)\nplt.xlim([18,100])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above graph we can see that, the density above 35 age has caused angina due to excercise.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\n\nsns.histplot(df[df['output'] == 0][\"age\"], color='green', kde = True) # less chance of heart attack- green\nsns.histplot(df[df['output'] == 1][\"age\"], color='red', kde=  True) # more chance of heart attack- Red\n\nplt.title('less chance of heart attack v/s more chance of heart attack by Age', fontsize=15)\nplt.xlim([18,100])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"above graph shows that, age above 28 has more chances of heart attack","metadata":{}},{"cell_type":"markdown","source":"# DATA PREPROCESSING\n","metadata":{}},{"cell_type":"code","source":"#features and label\nx = df.iloc[:, :-1 ]\ny = df.iloc[:,-1]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state =0)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n#scal = ['age', 'restecg', 'chol', 'thalachh', 'oldpeak']\n#scaler.fit_transform(df[scal])\nscaler.fit_transform(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL SELECTION\n\n\n- logistic regression\n- knn\n- svm\n- decision tree\n- random forest \n- naive bayes ","metadata":{}},{"cell_type":"code","source":"#importing metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import cross_val_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#logistic regression\nfrom sklearn.linear_model import LogisticRegression\nm1 = LogisticRegression()\nm1.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m1_pred = m1.predict(x_test)\nacc = accuracy_score(y_test,m1_pred )\nrecall =recall_score(y_test,m1_pred )\nprecision = precision_score(y_test,m1_pred )\nf1  = f1_score(y_test,m1_pred )\nroc_curve = roc_auc_score(y_test,m1_pred )\nconf = confusion_matrix(y_test, m1_pred)\nprint(\"Logistic Regression: \", \"\\n\")\nprint(\"confusion matrix : \", conf)\nprint(\"Accuracy: \", acc)\nprint(\"recall:\", recall)\nprint(\"precision score : \", precision)\nprint(\"f score: \", f1)\nprint(\"roc_auc_score: \", roc_curve)\nprint(\"*\"*30)\nprint(\"\\n\")\n\n#knn\nfrom sklearn.neighbors import KNeighborsClassifier\nm2 = KNeighborsClassifier()\nm2.fit(x_train, y_train)\n\nm2_pred = m2.predict(x_test)\nacc = accuracy_score(y_test,m2_pred )\nrecall =recall_score(y_test,m2_pred )\nprecision = precision_score(y_test,m2_pred )\nf1  = f1_score(y_test,m2_pred )\nroc_curve = roc_auc_score(y_test,m2_pred )\nconf = confusion_matrix(y_test, m2_pred)\n\nprint(\"KNN : \", \"\\n\")\nprint(\"confusion matrix : \", conf)\nprint(\"Accuracy: \", acc)\nprint(\"recall:\", recall)\nprint(\"precision score : \", precision)\nprint(\"f score: \", f1)\nprint(\"roc_auc_score: \", roc_curve)\nprint(\"*\"*30)\nprint(\"\\n\")\n\n\n\n#svm\nfrom sklearn.svm import SVC\nm3= SVC()\nm3.fit(x_train, y_train)\nm3_pred = m3.predict(x_test)\nacc = accuracy_score(y_test,m3_pred )\nrecall =recall_score(y_test,m3_pred )\nprecision = precision_score(y_test,m3_pred )\nf1  = f1_score(y_test,m3_pred )\nroc_curve = roc_auc_score(y_test,m3_pred )\nconf = confusion_matrix(y_test, m3_pred)\n\nprint(\"SVC: \", \"\\n\")\nprint(\"confusion matrix : \", conf)\nprint(\"Accuracy: \", acc)\nprint(\"recall:\", recall)\nprint(\"precision score : \", precision)\nprint(\"f score: \", f1)\nprint(\"roc_auc_score: \", roc_curve)\nprint(\"*\"*30)\nprint(\"\\n\")\n\n\n\n#decision tree\nfrom sklearn.tree import DecisionTreeClassifier\nm4 = DecisionTreeClassifier()\nm4.fit(x_train, y_train)\nm4_pred = m4.predict(x_test)\nacc = accuracy_score(y_test, m4_pred)\nrecall =recall_score(y_test,m4_pred )\nprecision = precision_score(y_test,m4_pred )\nf1  = f1_score(y_test,m4_pred )\nroc_curve = roc_auc_score(y_test,m4_pred )\nconf = confusion_matrix(y_test, m4_pred)\n\nprint(\"decision tree\", \"\\n\")\nprint(\"confusion matrix : \", conf)\nprint(\"Accuracy: \", acc)\nprint(\"recall:\", recall)\nprint(\"precision score : \", precision)\nprint(\"f score: \", f1)\nprint(\"roc_auc_score: \", roc_curve)\nprint(\"*\"*30)\nprint(\"\\n\")\n\n#random forest\nfrom sklearn.ensemble import RandomForestClassifier\nm5 =RandomForestClassifier(n_estimators=100, random_state = 0)\nm5.fit(x_train, y_train)\nm5_pred = m5.predict(x_test)\nacc = accuracy_score(y_test, m5_pred)\nrecall =recall_score(y_test,m5_pred )\nprecision = precision_score(y_test,m5_pred )\nf1  = f1_score(y_test,m5_pred )\nroc_curve = roc_auc_score(y_test,m5_pred )\nconf = confusion_matrix(y_test, m5_pred)\n\nprint(\"Random forest: \", \"\\n\")\nprint(\"confusion matrix : \", conf)\nprint(\"Accuracy: \", acc)\nprint(\"recall:\", recall)\nprint(\"precision score : \", precision)\nprint(\"f score: \", f1)\nprint(\"roc_auc_score: \", roc_curve)\nprint(\"*\"*30)\nprint(\"\\n\")\n\n\n\n\n#naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nm6 = GaussianNB()\nm6.fit(x_train, y_train)\nm6_pred = m6.predict(x_test)\nacc = accuracy_score(y_test, m6_pred)\nrecall =recall_score(y_test,m6_pred )\nprecision = precision_score(y_test,m6_pred )\nf1  = f1_score(y_test,m6_pred )\nroc_curve = roc_auc_score(y_test,m6_pred )\nconf = confusion_matrix(y_test, m6_pred)\n\nprint(\"Gaussian model: \", \"\\n\")\nprint(\"confusion matrix : \", conf)\nprint(\"Accuracy: \", acc)\nprint(\"recall:\", recall)\nprint(\"precision score : \", precision)\nprint(\"f score: \", f1)\nprint(\"roc_auc_score: \", roc_curve)\nprint(\"-\"*30)\nprint(\"\\n\")\n\n\n\nm7 = BernoulliNB()\nm7.fit(x_train, y_train)\nm7_pred = m7.predict(x_test)\nrecall =recall_score(y_test,m7_pred )\nprecision = precision_score(y_test,m7_pred )\nf1  = f1_score(y_test,m7_pred )\nacc = accuracy_score(y_test, m7_pred)\nroc_curve = roc_auc_score(y_test,m7_pred )\nconf = confusion_matrix(y_test, m7_pred)\n\nprint(\"bernoulli model: \", \"\\n\")\nprint(\"confusion matrix : \", conf)  \nprint(\"Accuracy: \", acc)\nprint(\"recall:\", recall)\nprint(\"precision score : \", precision)\nprint(\"f score: \", f1)\nprint(\"roc_auc_score: \", roc_curve)\nprint(\"-\"*30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TUNING THE MODEL\n\n- using grid search\n- finding the best accuracy\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection  import GridSearchCV\n#m2 knn\nparams = {\n    'n_neighbors' : [5, 25],\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n}\ngrid2 = GridSearchCV(estimator = m2,\n                        param_grid = params,\n                        scoring = 'accuracy', \n                        cv = 15, \n                        verbose = 1,\n                        )\ngrid2.fit(x_train, y_train)\nbest_acc = grid2.best_score_\nbest_param = grid2.best_params_\nprint(\"best parameters: \", best_param)\nprint(\"accuracy:\", best_acc*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#m3 svm\nparams = {'C':[10, ],'kernel':['linear', 'rbf'],'random_state':[0]}\ngrid4 = GridSearchCV(estimator = m3, param_grid = params, scoring = 'accuracy', cv = 5, verbose = 1)\ngrid4.fit(x_train, y_train)\nbest_acc = grid4.best_score_\nbest_param = grid4.best_params_\nprint(\"best parameters: \", best_param)\nprint(\"accuracy:\", best_acc*100)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#m4 decision tree\nparams = {'criterion':['gini','entropy'],'random_state':[0]}\ngrid5 = GridSearchCV(estimator= m4, param_grid = params, scoring = 'accuracy', cv = 5, verbose = 1)\ngrid5.fit(x_train, y_train)\n\nbest_acc = grid5.best_score_\nbest_param = grid5.best_params_\nprint(\"best parameters: \", best_param)\nprint(\"accuracy: \", best_acc*100)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#m5 random forest\nparams = {'n_estimators':[100, 200, 300],'criterion':['gini','entropy'],'random_state':[0]}\ngrid3 = GridSearchCV(estimator = m5, param_grid = params, scoring = \"accuracy\", cv = 10, verbose = 1)\ngrid3.fit(x_train, y_train)\nbest_acc = grid3.best_score_\nbest_param = grid3.best_params_\nprint(\"best parameters: \", best_param)\nprint(\"accuracy: \", best_acc*100)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#m6 gaaussain nb\nparams = {'var_smoothing': [1e-09]}\ngrid6 = GridSearchCV(estimator = m6, param_grid = params, scoring = 'accuracy' , cv = 5, verbose = 1)\ngrid6.fit(x_train, y_train)\nbest_param = grid6.best_params_\nbest_acc = grid6.best_score_\nprint(\"Accuracy: \", best_acc*100)\nprint(\"best parameters: \", best_param)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#m7 bernouilli nb\nparams = {'alpha': [0.25, 0.5, 1]}\ngrid7 = GridSearchCV(estimator = m7, param_grid = params, scoring = 'accuracy', cv = 10, verbose = 1)\ngrid7.fit(x_train, y_train)\n\nbest_param = grid7.best_params_\nbest_acc = grid7.best_score_\nprint(\"accuracy:\", best_acc*100)\nprint(\"best parameters: \", best_param)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fitting random forest\nm5 =RandomForestClassifier(criterion = 'gini', n_estimators = 100, random_state = 0)\nm5.fit(x_train, y_train)\nm5_pred = m5.predict(x_test)\nacc = accuracy_score(y_test, m5_pred)\nrecall =recall_score(y_test,m5_pred )\nprecision = precision_score(y_test,m5_pred )\nf1  = f1_score(y_test,m5_pred )\nroc_curve = roc_auc_score(y_test,m5_pred )\nconf = confusion_matrix(y_test, m5_pred)\nprint(\"Random forest\")\nprint(\"Accuracy: \", acc*100)\nprint(\"recall:\", recall)\nprint(\"precision score : \", precision)\nprint(\"f score: \", f1)\nprint(\"roc_auc_score: \", roc_curve)\nprint(\"confusion matrix: \", conf)\nprint(\"*\"*30)\nprint(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" #Making the Confusion Matrix\ncm = confusion_matrix(y_test, m5_pred)\nprint('Confusion Matrix:\\n',cm)\n\n# Calculate the Accuracy\naccuracy = accuracy_score(y_test, m5_pred)\nprint('Accuracy:', accuracy*100)\n\n#Visualizing Confusion Matrix\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Blues',annot = True, annot_kws= {'Fontsize': 15},cbar = False,  yticklabels = [\"less chance\", \"more chance\"], xticklabels = ['predicted less chance', 'predicted more chance'])\nplt.yticks(rotation= 0)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Therefore, after tuning the  hyperparameters with the help of GridSearch to get models. After that, I came to conclusion that **RandomForestClassifier** is best model for this dataset.","metadata":{}}]}