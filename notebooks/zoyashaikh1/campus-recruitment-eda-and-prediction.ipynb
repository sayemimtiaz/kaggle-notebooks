{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# IMPORTING LIBRARIES","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(r\"/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['sl_no'], axis = 1, inplace = True) #dropping insignificant values\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf['salary'].fillna(int(df['salary'].mean()), inplace=True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()\ndf['degree_t'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Stat\"] = df[\"status\"]\ndf.head()\ndf.drop(['status'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXPLORATORY DATA ANALYSIS","metadata":{}},{"cell_type":"code","source":"# heat map correlation\n#HEAT MAP CORRELATION\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(20, 15))\nsns.heatmap(corrmat, vmax=.8, square=True, annot=True,cmap=\"YlGnBu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#count plot genders\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"gender\", palette = \"flare\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"IN THE ABOVE GRAPH, WE CAN SEE NUMBER OF MALEs ARE MORE THAN NUMBER OF FEMALES","metadata":{}},{"cell_type":"code","source":"#COUNT PLOT DEGREE\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"degree_t\", palette = \"husl\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"AS WE CAN SEE, THE SIGNIFICANT NUMBER OF PEOPLE HAS BEEN GRADUATED THROUGH COMMERCE & MANAGEMENT FIELD, FOLLOWED BY SCIENCE AND TECHNOLOGY AND OTHER STREAMS","metadata":{}},{"cell_type":"code","source":"#COUNT PLOT WORK EXPERIENCE\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"workex\", palette = \"ch:s=.25,rot=-.25\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"AS DEMONSTRATED, LARGE NUMBER OF PEOPLE DO NOT HAVE WORK EXPERIENCE","metadata":{}},{"cell_type":"code","source":"#COUNT PLOT SPECIALISATION\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"specialisation\", palette = \"Set2\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"AS DEMONSTRATED, A DECENT AMOUNT OF INDIVIUALS ARE SPECIALIZED IN MARKETING & FINANCE","metadata":{}},{"cell_type":"code","source":"#SPECIALISATION V/S STATUS\n#COUNT PLOT SPECIALISATION\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"specialisation\",hue = 'Stat', palette = \"Set2\")\nplt.title(\"specialisation v/s Status\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MARKETING & FINANCE ARE MORE LIKELY TO BE PLACED.","metadata":{}},{"cell_type":"code","source":"#COUNT PLOT \nf = plt.subplots(figsize = (7, 7))\nsns.countplot(data = df, x = \"degree_t\", hue = 'Stat', palette = \"flare\")\nplt.title(\"DEGREE V/S STATUS\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"HERE, THE INDIVIUALS WHO HOLDS COMMERCE & MANAGEMENT DEGREE ARE LIKELY TO PLACED.","metadata":{}},{"cell_type":"code","source":"#WORK EXP V/S STATUS\nf = plt.subplots(figsize = (7, 7))\nsns.countplot(data = df, x = \"workex\", hue = 'Stat', palette = \"pastel\")\nplt.title(\"WORK EXP V/S STATUS\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"HERE,THE INDIVIUALS ARE NOT EXPERIENCED THOUGH , THEY ARE LIKELY TO BE PLACED.","metadata":{}},{"cell_type":"code","source":"#WORK EXP V/S STATUS\nf = plt.subplots(figsize = (12 , 12))\nsns.histplot(data = df, x = \"salary\", hue = 'specialisation', palette = \"pastel\", bins = 30)\nplt.title(\"SALARY V/S SPECIALISATION\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"HERE, MARKETING & HR ARE LIKELY TO PLACED WITH SALARY 3L PER ANNUM , IT IS FOLLOWED BY MARKETING & FINANCE","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.histplot(data=df, x=\"degree_p\", hue=\"gender\", kde = True, palette = \"flare\")\nplt.title('gender v/s DEGREE percentage' , fontsize=15)\nplt.xlim([18,100])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"IN THIS GRAPH , MAXIMUM NUMBER OF PEOPLE HAS SCORED IN RANGE 62%- 65% IN THEIR DEGREE RESPECTIVELY.","metadata":{}},{"cell_type":"code","source":"#placed with percentage\nplt.figure(figsize=(12,10))\nsns.histplot(data=df, x=\"degree_p\", hue=\"Stat\", kde = True, element= 'poly',palette = \"pastel\")\nplt.title('DEGREE percentage V/S STATUS' , fontsize=15)\nplt.xlim([18,100])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OVER HERE, WE CAN SEE PERCENTAGE IN RANGE 62% - 90% ARE LIKELY TO BE PLACED.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.histplot(data=df, x=\"mba_p\", hue=\"gender\", kde = True)\nplt.title('gender v/s MBA percentage' , fontsize=15)\nplt.xlim([18,100])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"HERE, MAXIMUM NUMBER OF PEOPLE HAS SCORED IN RANGE 58% - 65% IN THEIR MBA RESPECTIVELY.","metadata":{}},{"cell_type":"code","source":"#placed with percentage\nplt.figure(figsize=(12,10))\nsns.histplot(data=df, x=\"mba_p\", hue=\"Stat\", kde = True, element= 'poly',palette = \"flare\")\nplt.title(' MBA percentage V/S STATUS' , fontsize=15)\nplt.xlim([18,100])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"IN THIS, MBA % IN RANGE 52% - 75% ARE HIGLY LIKED TO BE PLACED","metadata":{}},{"cell_type":"markdown","source":"# DATA PREPROCESSING","metadata":{}},{"cell_type":"code","source":"x = df.iloc[:, :13]\ny = df.iloc[:, 13]\nx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ONE HOT ENCODING\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [5, 7])], remainder = 'passthrough')\nx = np.array(ct.fit_transform(x))\nprint(x.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#label encoding\nle = LabelEncoder()\nx[:, 6] = le.fit_transform(x[:, 6]) #gender\nx[:, 8] = le.fit_transform(x[:, 8]) #ssc_b\nx[:, 10] = le.fit_transform(x[:, 10]) #hsc_b\nx[:, 12] = le.fit_transform(x[:, 12]) #workexp\nx[:, 14] = le.fit_transform(x[:, 14]) #specialisation\nx[:, 6] = le.fit_transform(x[:, 6])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = le.fit_transform(y)\ny","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x.shape)\nprint(y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#splitting\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#scaling\nfrom sklearn.preprocessing import StandardScaler\nstd = StandardScaler()\nx_train = std.fit_transform(x_train)\nx_test = std.fit_transform(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL SELECTION","metadata":{}},{"cell_type":"code","source":"#importing metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import cross_val_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#logistic regression\nfrom sklearn.linear_model import LogisticRegression\nm1 = LogisticRegression()\nm1.fit(x_train, y_train)\n\nm1_pred = m1.predict(x_test)\nm1_pred\nprint(\"logistic regression\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m1_pred, y_test))\nprint(\"precision score: \", precision_score(m1_pred, y_test))\nprint(\"f1 score: \", f1_score(m1_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m1_pred, y_test))\nprint(\"recall score : \", recall_score(m1_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m1_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n#svm\nfrom sklearn.svm import SVC\nm2 = SVC()\nm2.fit(x_train, y_train)\n\nm2_pred = m2.predict(x_test)\nm2_pred\nprint(\"SVC\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m2_pred, y_test))\nprint(\"precision score: \", precision_score(m2_pred, y_test))\nprint(\"f1 score: \", f1_score(m2_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m2_pred, y_test))\nprint(\"recall score : \", recall_score(m2_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m2_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n\n#random forest\nfrom sklearn.ensemble import RandomForestClassifier\nm3 = RandomForestClassifier()\nm3.fit(x_train, y_train)\n\nm3_pred = m3.predict(x_test)\nm3_pred\nprint(\"Random Forest\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m3_pred, y_test))\nprint(\"precision score: \", precision_score(m3_pred, y_test))\nprint(\"f1 score: \", f1_score(m3_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m3_pred, y_test))\nprint(\"recall score : \", recall_score(m3_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m3_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n\n#knn\nfrom sklearn.neighbors import KNeighborsClassifier\nm4 = KNeighborsClassifier()\nm4.fit(x_train, y_train)\n\nm4_pred = m4.predict(x_test)\nm4_pred\nprint(\"Knn\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m4_pred, y_test))\nprint(\"precision score: \", precision_score(m4_pred, y_test))\nprint(\"f1 score: \", f1_score(m4_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m4_pred, y_test))\nprint(\"recall score : \", recall_score(m4_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m4_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n\n#decision tree\nfrom sklearn.tree import DecisionTreeClassifier\nm5 = DecisionTreeClassifier()\nm5.fit(x_train, y_train)\n\nm5_pred = m5.predict(x_test)\nm5_pred\nprint(\"decision Tree\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m5_pred, y_test))\nprint(\"precision score: \", precision_score(m5_pred, y_test))\nprint(\"f1 score: \", f1_score(m5_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m5_pred, y_test))\nprint(\"recall score : \", recall_score(m5_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m5_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n#naive bayes\nfrom sklearn.naive_bayes import BernoulliNB\nm6 = BernoulliNB()\nm6.fit(x_train, y_train)\n\nm6_pred = m6.predict(x_test)\nm6_pred\nprint(\"naive bayes\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m6_pred, y_test))\nprint(\"precision score: \", precision_score(m6_pred, y_test))\nprint(\"f1 score: \", f1_score(m6_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m6_pred, y_test))\nprint(\"recall score : \", recall_score(m6_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m6_pred, y_test))\nprint(\"\\n\", \"*\" * 30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TUNING PARAMETER","metadata":{}},{"cell_type":"code","source":"#parameter tuning\n#logistic regression\nfrom sklearn.model_selection import GridSearchCV \n\nparams = {'C':[5, 10, 15, 20],'random_state':[0]}\ngrid1 = GridSearchCV(estimator = m1, param_grid = params, scoring = 'accuracy', cv = 10)\ngrid1.fit(x_train, y_train)\nbest_acc = grid1.best_score_\nbest_param = grid1.best_params_\nprint(\"best parameters: \", best_param)\n\nprint('best accuracy:', best_acc*100)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#svc\nparams ={'C':[10, ],'kernel':['linear', 'rbf'],'random_state':[0]}\ngrid2 = GridSearchCV(estimator = m2, param_grid = params , scoring = \"accuracy\", cv = 10)\ngrid2.fit(x_train, y_train)\nbest_acc = grid2.best_score_\nparam = grid2.best_params_\nprint(\"best accuracy :\", best_acc*100)\nprint(\"best parameters :\", param )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#random forest\nparams = {\"n_estimators\": [100, 200, 300], \"criterion\": [\"gini\", \"entropy\"],\"random_state\":[42] }\ngrid3= GridSearchCV(estimator = m3, param_grid = params , scoring = \"accuracy\", cv = 10)\ngrid3.fit(x_train, y_train)\nbest_Acc = grid3.best_score_\nbest_param = grid3.best_params_\nprint(\"best accuracy :\", best_Acc*100)\nprint(\"best parameters : \", best_param)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#knn \nparams = {\n    'n_neighbors' : [5, 25],\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\ngrid4 = GridSearchCV(estimator = m4 , param_grid = params , scoring = \"accuracy\", cv = 10)\ngrid4.fit(x_train, y_train)\nbest_Acc = grid4.best_score_\nbest_param = grid4.best_params_\nprint(\"best parameters: \", best_param)\nprint(\"best accuracy :\", best_Acc*100)   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#decison\nparams = {\"criterion\": [\"gini\", \"entropy\"], \"random_state\": [0] } \ngrid5 = GridSearchCV(estimator = m5, param_grid = params, scoring = \"accuracy\", cv = 10)\ngrid5.fit(x_train, y_train)\nbest_acc = grid5.best_score_\nbest_param = grid5.best_params_\nprint(\"best acuracy: \", best_acc*100)\nprint(\"best parameters : \", best_param )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bernoulli naive bayes\nparams = {'alpha': [0.25, 0.5, 1]}\ngrid6 = GridSearchCV(estimator = m6, param_grid = params , scoring = \"accuracy\", cv = 10)\ngrid6.fit(x_train, y_train)\nbest_acc = grid6.best_score_\nparam = grid6.best_params_\nprint(\"best accuracy :\", best_acc*100)\nprint(\"best parameters :\", param )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#selecting random forest and decision tree as it shows the highest accuracies\n#fiiting random forest model\nmodel1 =  RandomForestClassifier(criterion = 'entropy', n_estimators = 300, random_state = 42)\nmodel1.fit(x_train, y_train)\n\nmodel_pred1 = model1.predict(x_test)\nmodel_pred1\nprint(\"accuracy score : \", accuracy_score(model_pred1, y_test))\nprint(\"precision score:\", precision_score(model_pred1, y_test))\nprint(\"recall score: \", recall_score(model_pred1, y_test))\nprint(\"f1_score :\", f1_score(model_pred1, y_test))\nprint(\"auc score : \", roc_auc_score(model_pred1, y_test))\nprint(\"confusion matrix\", confusion_matrix(model_pred1, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fitting decision tree\nmodel2 =  DecisionTreeClassifier(criterion = 'gini', random_state = 0)\nmodel2.fit(x_train, y_train)\n\nmodel_pred2 = model2.predict(x_test)\nmodel_pred2\nprint(\"accuracy score : \", accuracy_score(model_pred2, y_test))\nprint(\"precision score:\", precision_score(model_pred2, y_test))\nprint(\"recall score: \", recall_score(model_pred2, y_test))\nprint(\"f1_score :\", f1_score(model_pred2, y_test))\nprint(\"auc score : \",  roc_auc_score(model_pred2, y_test))\nprint(\"confusion matrix\", confusion_matrix(model_pred2, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Visualizing Confusion Matrix\ncm = confusion_matrix(model_pred1, y_test)\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Blues',annot = True, annot_kws= {'Fontsize': 15},cbar = False,  yticklabels = [\"NOT PLACED \", \"PLACED\"], xticklabels = ['predicted NOT PLACED', 'predicted PLACED'])\nplt.yticks(rotation= 0)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SHOWING ROU_AUC_CURVE\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, model_pred1)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"after tuning the hyperparameters we can see that, random forest shows the 86% accuracy. \n**Random Forest** model fits the best for this dataset","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}