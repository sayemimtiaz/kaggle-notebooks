{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nfrom torch import nn\nimport math\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms\n\ndevice = 'cpu'\nhidden_dim=64\nim_chan=3\nbatch_size=128\nmax_epochs=100000\ndisplay_step=5\nn_epochs_stop=5\nimage_size=64\ntransform = transforms.Compose([\n        transforms.Resize(image_size),\n        transforms.CenterCrop(image_size),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\nlr=0.01 #learning rate\ndf = pd.read_csv('/kaggle/input/celeba-dataset/list_attr_celeba.csv')\ndf = df.sample(frac=1).reset_index(drop=True)\nn_rows = df.shape[0]\nn_classes=df.shape[1]-1\n#dividir em treinamento validacao e teste\ntrain = df.iloc[:n_rows//2,:]\nval = df.iloc[n_rows//2:3*n_rows//4,:]\ntest = df.iloc[3*n_rows//4:,:]\ntrain_ids = [train.iloc[i*batch_size:min(i*batch_size+batch_size,train.shape[0]),0] for i in range(int(math.ceil(train.shape[0]/batch_size)))]\nbatches = [torch.Tensor(train.iloc[i*batch_size:min(i*batch_size+batch_size,train.shape[0]),1:].clip(0,1).values).long().to(device) for i in range(int(math.ceil(train.shape[0]/batch_size)))]\n#Estou calculando a validacao por batch para economizar memoria\n#como o conjunto de validacao tem metade do tamanho do de treinamento o tamanho do batch ou o numero de batches\n#tem que ser dividido por 2 escolhi reduzir o tamanho do batch em vez da quantidade porque assim eu economizo tempo\n#(uso o mesmo for)\nval_ids = [val.iloc[i*batch_size//2:min(i*batch_size//2+batch_size//2,val.shape[0]),0] for i in range(int(math.ceil(train.shape[0]/batch_size)))]\nval_batches = [torch.Tensor(val.iloc[i*batch_size//2:min(i*batch_size//2+batch_size//2,val.shape[0]),1:].clip(0,1).values).long().to(device) for i in range(int(math.ceil(train.shape[0]/batch_size)))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Definicao do Classificador\nclass Classif(nn.Module):\n    def __init__(self, im_chan=3, hidden_dim=64,n_classes=40):\n        super(Classif, self).__init__()\n        self.classif = nn.Sequential(\n            self.make_classif_block(im_chan, hidden_dim),\n            self.make_classif_block(hidden_dim, hidden_dim * 2),\n            self.make_classif_block(hidden_dim * 2, hidden_dim * 4, stride=3),\n            self.make_classif_block(hidden_dim * 4, n_classes, final_layer=True),\n        )\n\n    def make_classif_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n        '''\n        Function to return a sequence of operations corresponding to a block of the classifier\n        Parameters:\n            input_channels: how many channels the input feature representation has\n            output_channels: how many channels the output feature representation should have\n            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n            stride: the stride of the convolution\n            final_layer: a boolean, true if it is the final layer and false otherwise \n                      (affects activation and batchnorm)\n        '''\n        if not final_layer:\n            return nn.Sequential(\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                nn.BatchNorm2d(output_channels),\n                nn.LeakyReLU(0.2, inplace=True),\n            )\n        else:\n            return nn.Sequential(\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                nn.Tanh()\n            )\n\n    def forward(self, image):\n        '''\n        Function for completing a forward pass of the classifier\n        Parameters:\n            image: a flattened image tensor with dimension (im_chan)\n        '''\n        classif_pred = self.classif(image)\n        return classif_pred.view(len(classif_pred), -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classif_loss = nn.MSELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#inicializar e treinar\nclassif = Classif(im_chan,hidden_dim,n_classes).to(device)\nclassif_opt = torch.optim.Adam(classif.parameters(), lr=lr)\n\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)\nclassif = classif.apply(weights_init)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#treinamento\nmin_val_loss=float('inf')\ncur_step = 0\nepochs_no_improve=0\ntraining_losses = []\nvalidation_losses = []\nfor epoch in range(max_epochs):\n    mean_train_loss=0\n    mean_val_loss=0\n    for batch_index in tqdm(range(len(batches))):\n        batch = batches[batch_index]\n        val_batch = val_batches[batch_index]\n        train_image_list = [transform(Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'+im_id)) for im_id in train_ids[batch_index]]\n        images = torch.stack(train_image_list).to(device)\n        classif_opt.zero_grad()\n        training_pred = classif(images).float()\n        training_loss = classif_loss(training_pred,batch.float())\n        training_loss.backward()\n\n        # Update the weights\n        classif_opt.step()\n        \n        #Estou calculando a validacao por batch para economizar memoria\n        val_image_list = [transform(Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'+im_id)) for im_id in val_ids[batch_index]]\n        val_images = torch.stack(val_image_list).to(device)\n        val_pred = classif(val_images).float()\n        val_loss = classif_loss(val_pred,val_batch)\n        \n        # Keep track of the average losses\n        mean_train_loss += training_loss.item()/len(batches)\n        mean_val_loss += val_loss.item()/len(batches)\n    training_losses += [mean_train_loss]\n    validation_losses += [mean_val_loss]\n    ### Visualization code ###\n    if cur_step % display_step == 0 and cur_step > 0:\n        training_mean = sum(training_losses[-display_step:]) / display_step\n        #step_bins = 20\n        num_examples = (len(training_losses) )#// step_bins) * step_bins\n        plt.plot(\n            range(num_examples),# // step_bins), \n            torch.Tensor(training_losses[:num_examples]),\n            label=\"Training Loss\"\n        )\n        plt.plot(\n            range(num_examples),#// step_bins), \n            torch.Tensor(validation_losses[:num_examples]),\n            label=\"Validation Loss\"\n        )\n        plt.legend()\n        plt.show()\n    if(mean_val_loss < min_val_loss):\n        epochs_no_improve=0\n        min_val_loss = mean_val_loss\n    else:\n        epochs_no_improve+=1\n        if(epochs_no_improve>n_epochs_stop):\n            print('Early stopping!' )\n            break;\n    cur_step += 1\nclassif.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_test_loss = 0\nfor i in range(int(math.ceil(test.shape[0]/batch_size))):\n    test_images = torch.stack([transform(Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'+im_id)).to(device) for im_id in test.iloc[i*batch_size:min(i*batch_size+batch_size,train.shape[0]),0]])\n    test_pred = classif(test_images).float()\n    test_loss = classif_loss(test_pred,torch.Tensor(test.iloc[i*batch_size:min(i*batch_size+batch_size,train.shape[0]),1:].values).long().to(device))\n    mean_test_loss += test_loss.item()/int(math.ceil(test.shape[0]/batch_size))\nprint(mean_test_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(classif,'classif.pt')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}