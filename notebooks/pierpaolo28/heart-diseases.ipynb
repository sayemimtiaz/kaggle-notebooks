{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Heart Disease UCI**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from IPython.display import HTML\n# HTML('''\n# <script>\n#   function code_toggle() {\n#     if (code_shown){\n#       $('div.input').hide('500');\n#       $('#toggleButton').val('Show Code')\n#     } else {\n#       $('div.input').show('500');\n#       $('#toggleButton').val('Hide Code')\n#     }\n#     code_shown = !code_shown\n#   }\n\n#   $( document ).ready(function(){\n#     code_shown=true;\n#   });\n# </script>\n# <form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Hide Code\"></form>''')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv(\"../input/heart.csv\")\n#df = df.drop('Unnamed: 0', axis=1)\nprint(df.head())\nprint(df.shape)\nprint(df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.countplot(x = 'target',data = df,hue = 'sex')\nplt.title(\"Male (1) vs Female (0) affected by Heart Diseases\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,12))\nsns.countplot(x = 'age',hue = 'target',data = df)\nplt.title(\"People affected by heart deseases vs age\", fontsize=20)\nplt.legend([\"Healthy\",\"Ill\"], fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,7))\nplt.scatter(x = 'age',y = 'thalach', c='target',data = df)\nplt.xlabel('Age')\nplt.ylabel('Max heart rate')\nplt.title('Heart rate vs Age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\ncorr=df.corr()\nsns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Machine Learning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop(['target'], axis = 1).values\nY = df['target']\n\nX = StandardScaler().fit_transform(X)\n\nX_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.30, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing :\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom itertools import product\n\n# Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainedmodel = LogisticRegression().fit(X_Train,Y_Train)\npredictions =trainedmodel.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictions))\nprint(classification_report(Y_Test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train,Y_Train)\npredictionforest = trainedforest.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Support Vector Machines"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainedsvm = svm.LinearSVC().fit(X_Train, Y_Train)\npredictionsvm = trainedsvm.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionsvm))\nprint(classification_report(Y_Test,predictionsvm))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainedtree = tree.DecisionTreeClassifier().fit(X_Train, Y_Train)\npredictionstree = trainedtree.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionstree))\nprint(classification_report(Y_Test,predictionstree))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\ndata = export_graphviz(trainedtree,out_file=None,feature_names=df.drop(['target'], axis = 1).columns,\n                       class_names=['0', '1'],  \n                       filled=True, rounded=True,  \n                       max_depth=2,\n                       special_characters=True)\ngraph = graphviz.Source(data)\ngraph","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear Discriminant Anaylsis"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainedlda = LinearDiscriminantAnalysis().fit(X_Train, Y_Train)\npredictionlda = trainedlda.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionlda))\nprint(classification_report(Y_Test,predictionlda))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainednb = GaussianNB().fit(X_Train, Y_Train)\npredictionnb = trainednb.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictionnb))\nprint(classification_report(Y_Test,predictionnb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom xgboost import plot_tree\nimport matplotlib.pyplot as plt\nmodel = XGBClassifier()\n\n# Train\nmodel.fit(X_Train, Y_Train)\n\nplot_tree(model)\nplt.figure(figsize = (50,55))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import product\nimport itertools\n\npredictions =model.predict(X_Test)\nprint(confusion_matrix(Y_Test,predictions))\nprint(classification_report(Y_Test,predictions))\n\n# Thanks to: https://www.kaggle.com/tejainece/data-visualization-and-machine-learning-algorithms\ndef plot_confusion_matrix(cm, classes=[\"0\", \"1\"], title=\"\",\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title('Confusion matrix ' +title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ncm_plot = confusion_matrix(Y_Test,predictions)\n\nplt.figure()\nplot_confusion_matrix(cm_plot, title = 'XGBClassifier')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Engineering**"},{"metadata":{},"cell_type":"markdown","source":"Principal Component Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2,svd_solver='full')\nX_pca = pca.fit_transform(X)\n# print(pca.explained_variance_)\n\nX_reduced, X_test_reduced, Y_Train, Y_Test = train_test_split(X_pca, Y, test_size = 0.30, random_state = 101)\n\n# pca = PCA(n_components=2,svd_solver='full')\n# X_reduced = pca.fit_transform(X_Train)\n#X_reduced = TSNE(n_components=2).fit_transform(X_Train, Y_Train)\n\ntrainednb = GaussianNB().fit(X_reduced, Y_Train)\ntrainedsvm = svm.LinearSVC().fit(X_reduced, Y_Train)\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(X_reduced,Y_Train)\ntrainedmodel = LogisticRegression().fit(X_reduced,Y_Train)\n\n# pca = PCA(n_components=2,svd_solver='full')\n# X_test_reduced = pca.fit_transform(X_Test)\n#X_test_reduced = TSNE(n_components=2).fit_transform(X_Test, Y_Test)\n\nprint('Naive Bayes')\npredictionnb = trainednb.predict(X_test_reduced)\nprint(confusion_matrix(Y_Test,predictionnb))\nprint(classification_report(Y_Test,predictionnb))\n\nprint('SVM')\npredictionsvm = trainedsvm.predict(X_test_reduced)\nprint(confusion_matrix(Y_Test,predictionsvm))\nprint(classification_report(Y_Test,predictionsvm))\n\nprint('Random Forest')\npredictionforest = trainedforest.predict(X_test_reduced)\nprint(confusion_matrix(Y_Test,predictionforest))\nprint(classification_report(Y_Test,predictionforest))\n\nprint('Logistic Regression')\npredictions =trainedmodel.predict(X_test_reduced)\nprint(confusion_matrix(Y_Test,predictions))\nprint(classification_report(Y_Test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_data = X_reduced\n\ntrainednb = GaussianNB().fit(reduced_data, Y_Train)\ntrainedsvm = svm.LinearSVC().fit(reduced_data, Y_Train)\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(reduced_data,Y_Train)\ntrainedmodel = LogisticRegression().fit(reduced_data,Y_Train)\n\n# Thanks to: https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html\n\nx_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\ny_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nf, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))\n\nfor idx, clf, tt in zip(product([0, 1], [0, 1]),\n                        [trainednb, trainedsvm, trainedforest, trainedmodel],\n                        ['Naive Bayes Classifier', 'SVM',\n                         'Random Forest', 'Logistic Regression']):\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n\n    axarr[idx[0], idx[1]].contourf(xx, yy, Z,cmap=plt.cm.coolwarm, alpha=0.4)\n    axarr[idx[0], idx[1]].scatter(reduced_data[:, 0], reduced_data[:, 1], c=Y_Train,\n                                  s=20, edgecolor='k')\n    axarr[idx[0], idx[1]].set_title(tt)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear Discriminant Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load libraries\nfrom sklearn import datasets\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# Create an LDA that will reduce the data down to 1 feature\nlda = LinearDiscriminantAnalysis(n_components=2)\n\n# run an LDA and use it to transform the features\nX_lda = lda.fit(X, Y).transform(X)\n\n# Print the number of features\nprint('Original number of features:', X.shape[1])\nprint('Reduced number of features:', X_lda.shape[1])\n\n## View the ratio of explained variance\nprint(lda.explained_variance_ratio_)\n\nX_reduced, X_test_reduced, Y_Train, Y_Test = train_test_split(X_lda, Y, test_size = 0.30, random_state = 101)\n\ntrainednb = GaussianNB().fit(X_reduced, Y_Train)\ntrainedsvm = svm.LinearSVC().fit(X_reduced, Y_Train)\n\nprint('Naive Bayes')\npredictionnb = trainednb.predict(X_test_reduced)\nprint(confusion_matrix(Y_Test,predictionnb))\nprint(classification_report(Y_Test,predictionnb))\n\nprint('SVM')\npredictionsvm = trainedsvm.predict(X_test_reduced)\nprint(confusion_matrix(Y_Test,predictionsvm))\nprint(classification_report(Y_Test,predictionsvm))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"t-SNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport time\n\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\ntsne_results = tsne.fit_transform(X)\nprint('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,5))\nsns.scatterplot(\n    x=tsne_results[:,0], y=tsne_results[:,1],\n    hue=Y,\n    palette=sns.color_palette(\"hls\", 2),\n    data=df,\n    legend=\"full\",\n    alpha=0.3\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Clustering**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2,svd_solver='full')\nX_pca = pca.fit_transform(X)\n# print(pca.explained_variance_)\n\n# print('Original number of features:', X.shape[1])\n# print('Reduced number of features:', X_lda.shape[1])\nprint(pca.explained_variance_ratio_)\n\nX_reduced, X_test_reduced, Y_Train, Y_Test = train_test_split(X_pca, Y, test_size = 0.30, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"K-Means Clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X_reduced)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kpredictions = kmeans.predict(X_test_reduced)\nprint(confusion_matrix(Y_Test,kpredictions))\nprint(classification_report(Y_Test,kpredictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X_test_reduced[kpredictions ==0,0], X_test_reduced[kpredictions == 0,1], s=100, c='red')\nplt.scatter(X_test_reduced[kpredictions ==1,0], X_test_reduced[kpredictions == 1,1], s=100, c='black')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hierarchical Clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import AgglomerativeClustering\n\n# create dendrogram\ndendrogram = sch.dendrogram(sch.linkage(X_reduced, method='ward'))\n# create clusters\nhc = AgglomerativeClustering(n_clusters=2, affinity = 'euclidean', linkage = 'ward')\n# save clusters for chart\nhierarchicalpredictions = hc.fit_predict(X_test_reduced)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X_test_reduced[hierarchicalpredictions ==0,0], X_test_reduced[hierarchicalpredictions == 0,1], s=100, c='red')\nplt.scatter(X_test_reduced[hierarchicalpredictions ==1,0], X_test_reduced[hierarchicalpredictions == 1,1], s=100, c='black')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}