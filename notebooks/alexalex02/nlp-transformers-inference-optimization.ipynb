{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP Transformers Inference Optimization\n\n![](https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2019/06/65025135_2531780803519285_6381814664434548736_o.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hello everyone!\n\nIn this notebook, we'll compare performance of our models for inference on CPU and GPU after several optimizations. They all could be applied to a lot of nlp [transformers](https://github.com/huggingface/transformers), including BERT, DistilBERT, RoBERTa, ALBERT, GPT-2, DistilGPT2.\n\nWe’ll take a look on three things that can be done after training to improve inference speed:\n* [TorchScript](https://pytorch.org/docs/stable/jit.html)\n* [Dynamic Quantization](https://pytorch.org/docs/stable/quantization.html)\n* [ONNX](https://pytorch.org/docs/stable/onnx.html) and [ONNX Runntime](https://github.com/microsoft/onnxruntime)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As an example, we’ll use trained DistilBERT model on Amazon review dataset. Training part you can find in this [notebook](https://www.kaggle.com/alexalex02/sentiment-analysis-distilbert-amazon-reviews). Our result was 96.22% accuracy and model size was 255 MB.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Measurements and Environments","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We'll use batch size of 1 which is useful for online inference. Maximum sequence length - [64, 128, 256, 512]\n\nTime: `%timeit -r 30 -n 3` to provide stable result\n\nKaggle Kernel Setup\n\nCPU: Intel(R) Xeon(R) CPU @ 2.30GHz 4 CPU(s)\n\nGPU: Tesla P100 16GB, Intel(R) Xeon(R) CPU @ 2.00GHz 2 CPU(s)\n","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import os\nos.environ['WANDB_SILENT'] = 'True'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom transformers import AutoTokenizer\nfrom scipy.special import softmax","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TorchScript","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"TorchScript is a way to create serializable and optimizable models from PyTorch code. The models can be run independently from Python environment, such as C++.\n\nTo trace our model, we must define model input first. \n\n*Note: For GPU inference we must change device to 'cuda'.*","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"MODEL_NAME = 'distilbert-base-uncased'\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ndef sentence_input(sentence: str, max_len: int = 512, device = 'cpu'):\n    encoded = tokenizer.encode_plus(sentence, add_special_tokens=True, \n                                    pad_to_max_length=True, max_length=max_len, \n                                    return_tensors=\"pt\",).to(device)\n    model_input = (encoded['input_ids'], encoded['attention_mask'])\n    return model_input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sentence = \"Super Cute: First of all, I LOVE this product. When I bought it my husband jokingly said that it looked cute and small in the picture, but was really HUGE in real life. Don't tell him I said so, but he was right. It is huge and the cord is really long. Although I wish it was smaller, I still love it. It works really well when we travel and need to plug a lot of things in and although the length is annoying, it's very useful.\"\nmodel_input = sentence_input(test_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_sentence)\nprint(model_input)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting model - CPU and GPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DistilBert(nn.Module):\n    \"\"\"\n    Simplified version of the same class by HuggingFace.\n    See transformers/modeling_distilbert.py in the transformers repository.\n    \"\"\"\n\n    def __init__(self, pretrained_model_name: str, num_classes: int = None):\n        \"\"\"\n        Args:\n            pretrained_model_name (str): HuggingFace model name.\n                See transformers/modeling_auto.py\n            num_classes (int): the number of class labels\n                in the classification task\n        \"\"\"\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(\n             pretrained_model_name)\n\n        self.distilbert = AutoModel.from_pretrained(pretrained_model_name,\n                                                    config=config)\n        self.pre_classifier = nn.Linear(config.dim, config.dim)\n        self.classifier = nn.Linear(config.dim, num_classes)\n        self.dropout = nn.Dropout(config.seq_classif_dropout)\n\n    def forward(self, features, attention_mask=None, head_mask=None):\n        \"\"\"Compute class probabilities for the input sequence.\n\n        Args:\n            features (torch.Tensor): ids of each token,\n                size ([bs, seq_length]\n            attention_mask (torch.Tensor): binary tensor, used to select\n                tokens which are used to compute attention scores\n                in the self-attention heads, size [bs, seq_length]\n            head_mask (torch.Tensor): 1.0 in head_mask indicates that\n                we keep the head, size: [num_heads]\n                or [num_hidden_layers x num_heads]\n        Returns:\n            PyTorch Tensor with predicted class probabilities\n        \"\"\"\n        assert attention_mask is not None, \"attention mask is none\"\n        distilbert_output = self.distilbert(input_ids=features,\n                                            attention_mask=attention_mask,\n                                            head_mask=head_mask)\n        # we only need the hidden state here and don't need\n        # transformer output, so index 0\n        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n        # we take embeddings from the [CLS] token, so again index 0\n        pooled_output = hidden_state[:, 0]  # (bs, dim)\n        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n        logits = self.classifier(pooled_output)  # (bs, dim)\n\n        return logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoConfig, AutoTokenizer, AutoModel\nmodel = DistilBert(pretrained_model_name=MODEL_NAME,\n                                           num_classes=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catalyst.dl.utils import trace\ndef load_chechpoint(model, path):\n    mod = trace.load_checkpoint(path)\n    model.load_state_dict(mod['model_state_dict'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nmodel = load_chechpoint(model, '../input/sentiment-all-models/last 0.9622.pth')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\n\ntraced_cpu = torch.jit.trace(model, model_input)\ntorch.jit.save(traced_cpu, \"cpu.pth\")\n\n#to load\ncpu_model = torch.jit.load(\"cpu.pth\")\n\n# GPU\n# traced_gpu = torch.jit.trace(model.cuda(), gpu_model_input)\n# torch.jit.save(traced_gpu, \"gpu.pth\")\n\n# gpu_model = torch.jit.load(\"gpu.pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cpu_model.graph)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dynamic Quantization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Post Training Dynamic Quantization: This is the form of quantization where the weights are quantized ahead of time but the activations are dynamically quantized during inference.\n\nDynamic quantization support in PyTorch converts a float model to a quantized model with static int8 or float16 data types for the weights and dynamic quantization for the activations. The activations are quantized dynamically (per batch) to int8 when the weights are quantized to int8.\n\nThe mapping is performed by converting the floating point tensors using:\n\n![](https://pytorch.org/docs/stable/_images/math-quantizer-equation.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"quantized_model = torch.quantization.quantize_dynamic(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(quantized_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model size decreased from 255 to 132 MB. If we calculate the total size of word embedding table ~ 4 (Bytes/FP32) * 30522(Vocabulary Size) * 768(Embedding Size) = 90 MB. Then the model size reduced from 165 to 42MB (INT8 Model)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# ONNX Runtime","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard data types.\n\nONNX Runtime is a performance-focused engine(written in C++) for ONNX models, which inferences efficiently across multiple platforms and hardware","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install onnx onnxruntime onnxruntime-tools\n#For GPU Inference: install onnxruntime-gpu","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To export model, first, we need to put our model in eval() mode. Then, provide model_input. Because input size is fixed, we need to specify `dynamic_axis`.\n\n*Note: for 4 sequence lengths - we need 4 different models.*","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"torch.onnx.export(model, model_input, \"model_512.onnx\",\n                  export_params=True,\n                  input_names=[\"input_ids\", \"attention_mask\"],\n                  output_names=[\"targets\"],\n                  dynamic_axes={\n                      \"input_ids\": {0: \"batch_size\"},\n                      \"attention_mask\": {0: \"batch_size\"},\n                      \"targets\": {0: \"batch_size\"}\n                  },\n                  verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To check that the model is well formed","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import onnx\nonnx_model = onnx.load('model_512.onnx')\nonnx.checker.check_model(onnx_model, full_check=True)\nonnx.helper.printable_graph(onnx_model.graph)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To optimize our model, we import optimizer. `opt_level` is a proper graph optimization level: 0 - disable all (default), 1 - basic, 2 - extended, 99 - all, `use_gpu` for GPU Inference","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from onnxruntime_tools import optimizer\noptimized_model_512 = optimizer.optimize_model(\"model_512.onnx\", model_type='bert', \n                                               num_heads=12, hidden_size=768,\n                                              use_gpu=False, opt_level=99)\n\noptimized_model_512.save_model_to_file(\"optimized_512.onnx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For GPU Inference, we can use following methods:\n* change_input_to_int32() - int32 will be used as input, can get better performance.\n* change_input_output_float32_to_float16() - half-precision will be used in computation.\n* convert_model_float32_to_float16() - decreasing model size (255MB -> 128MB)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In order to run the model with ONNX Runtime, we need to create an inference session for the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import onnxruntime as ort\nprint(ort.get_device())\nOPTIMIZED_512 = ort.InferenceSession('./optimized_512.onnx')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_numpy(tensor):\n    if tensor.requires_grad:\n        return tensor.detach().cpu().numpy()\n    return tensor.cpu().numpy()\n\ndef prediction_onnx(model, sentence: str, max_len: int = 512):\n    encoded = tokenizer.encode_plus(sentence, add_special_tokens=True, \n                                    pad_to_max_length=True, max_length=max_len,\n                                    return_tensors=\"pt\",)\n    # compute ONNX Runtime output prediction\n    input_ids = to_numpy(encoded['input_ids'])\n    attention_mask = to_numpy(encoded['attention_mask'])\n    onnx_input = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n    logits = model.run(None, onnx_input)\n    preds = softmax(logits[0][0])\n    print(f\"Class: {['Negative' if preds.argmax() == 0 else 'Positive'][0]}, Probability: {preds.max():.4f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_onnx(OPTIMIZED_512, test_sentence)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CPU Results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Inference time presented in milliseconds.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = pd.DataFrame([[506, 273, 151, 89.1, 0],\n                  [507, 263, 145, 82.7, 5.2],\n                  [516, 237, 126, 72.4, 19],\n                  [388, 180, 92.2, 49.7, 56.2]], index = ['Pytorch', 'TorchScript', \n                                                    'ONNX Runtime', 'Quantization'],\n                  columns=['512', '256', '128', '64', \"Av.SpeedUp (%)\"])\ndisplay(df)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cpu = pd.DataFrame([[64, 'Pytorch', 89.1],\n                  [64, 'TorchScript', 82.7],\n                  [64, 'ONNX Runtime', 72.4],\n                  [64, 'Quantization', 49.4],\n                   [128, 'Pytorch', 151],\n                   [256, 'Pytorch', 273],\n                   [512, 'Pytorch', 506],\n                   [128, 'TorchScript', 145],\n                   [256, 'TorchScript', 263],\n                   [512, 'TorchScript', 507],\n                   [128, 'ONNX Runtime', 126],\n                   [256, 'ONNX Runtime', 237],\n                   [512, 'ONNX Runtime', 516],\n                   [128, 'Quantization', 92.2],\n                   [256, 'Quantization', 180],\n                    [512, 'Quantization', 388]],\n                  columns=['Sequence', 'Optimization', 'Time (ms)'])\n\nsns.set_style(\"darkgrid\")\nsns.catplot(x='Optimization', y='Time (ms)', data=cpu, kind='bar',\n            ci=None, col='Sequence', col_wrap=2,\n           col_order=[512,256,128,64]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that quantization gave us the most significant improvement in inference speed. After checking validation accuracy, we can see the drop from 96.22 to 96.03%. It’s not serious considering model size drop and speedup. If we extend maximum sequence lengths further to 32 and 16, then we can observe that speedup ~ 85% in [16, 32, 64, 128].","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# GPU Results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"GPU support isn’t provided for quantization in Pytorch yet.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"gpu_df = pd.DataFrame([[16.1, 12.1, 11.9, 11.9, 0],\n                  [15.9, 11.2, 9.2, 8.92, 18],\n                  [14.2, 10, 8.14, 7.57, 35]], index = ['Pytorch', 'TorchScript', \n                                                    'ONNX Runtime'],\n                  columns=['512', '256', '128', '64', \"Av.SpeedUp (%)\"])\ndisplay(gpu_df)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"gpu = pd.DataFrame([[64, 'Pytorch', 11.9],\n                  [64, 'TorchScript', 8.92],\n                  [64, 'ONNX Runtime', 7.57],\n                   [128, 'Pytorch', 11.9],\n                   [256, 'Pytorch', 12.1],\n                   [512, 'Pytorch', 16.1],\n                   [128, 'TorchScript', 9.2],\n                   [256, 'TorchScript', 11.2],\n                   [512, 'TorchScript', 15.9],\n                   [128, 'ONNX Runtime', 8.14],\n                   [256, 'ONNX Runtime', 10],\n                   [512, 'ONNX Runtime', 14.2]],\n                  columns=['Sequence', 'Optimization', 'Time (ms)'])\n\nsns.catplot(x='Optimization', y='Time (ms)', data=gpu, kind='bar',\n            ci=None, col='Sequence', col_wrap=2,\n           col_order=[512,256,128,64]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although TorchScript wasn't created for speedup improvement, it still yield solid 20% boost versus non-traced model.\n\nFP16 ONNX model showed us very good performance gains. And there are more optimization available, such as disable/enable some fusions and GPU support for quantization.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}