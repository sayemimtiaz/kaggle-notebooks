{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img style=\"float:left;\" src=\"https://img.icons8.com/carbon-copy/100/000000/futurama-bender.png\"/>\n<h1 style=\"text-align: center; font-size:50px;\"> AGE, GENDER and ETHNICITY </h1> \n<hr>\n<hr>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport tensorflow as tf\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing import text\nfrom sklearn.metrics import classification_report\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport plotly.express as px\nimport eli5\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Content\n\nThis dataset includes a CSV of facial images that are labeled on the basis of age, gender, and ethnicity.\nThe dataset includes 27305 rows and 5 columns.\n\n***To see the preprocessing please unhide the following cell***"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/age-gender-and-ethnicity-face-data-csv/age_gender.csv')\ndata['pixels'] = data.pixels.apply(lambda x: x.split(' '))\ndata['pixels'] = data.pixels.apply(lambda x: np.array([int(v) for v in x]))\ndata['pixels'] = data.pixels.apply(lambda x: x.reshape(48,48))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Table of contents\n\n- [Exploring the data](#a)\n- [SE-RESNET block](#b)\n- [SENET model](#c)\n- [Learning rate](#d)\n- [Training step](#e)"},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align: center;\">Exploring the data</h2> <a id=a><a/>\n<hr>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[16,16])\nfor i in range(1500,1520):\n    plt.subplot(5,5,(i%25)+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(data['pixels'].iloc[i])\n    plt.xlabel(\n        \"Age:\"+str(data['age'].iloc[i])+\n        \"  Ethnicity:\"+str(data['ethnicity'].iloc[i])+\n        \"  Gender:\"+ str(data['gender'].iloc[i])\n    )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We decided to split 25% of the data for the validation set**\n\nThen we expand the dimension of the predictors (X_train and X_val). This is an essential step.\n\n**Reminder:**\n- the inputs for conv 2D is (height, width, channels).\n- In our case channel is equal to 1 because there is only one color (1 array).\n- height and width can be easily calculated as the racine of the array's length which is 48."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(data.drop(['age','ethnicity','gender','img_name'], axis=1),\n                                                  data[['age','ethnicity','gender']], random_state=0, test_size=0.25)\n\n\ndef preprocess (df, y):\n    \"\"\"Redim df\"\"\"\n    X = np.zeros((len(df.values), 48, 48, 1))\n    for idx,array in enumerate(df[y]):\n        X[idx, :, :, 0] = array\n    return X\n\n# We expand dimension to fit with the CNN inputs\nXtrain = preprocess(X_train, 'pixels')\nXval = preprocess(X_val, 'pixels')\n\n# We decided to make prediction only on age but it can easily be done on the other \nytrain = y_train.age.values\nyval = y_val.age.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align: center;\">SE-RESNET Block<h2><a id=b><a/>\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"OK, the following cell is probably the most important cell of the notebook.\nWe implement here a residual bloc and an SE block which combined together can make really good predictions.\n\nA SE block is not looking for spatial patterns like CNN, it learns the caracteristics which work well in group. Like nose and mouth are relatively close on a face the NN will expect to see eyes. If it constats a high activation for the nose and mouth feature cards and a medium one for the eyes, the block will excite the last one.\n\nA block SE has only 3 layers and pulls out a vector which will multiply the feature cards of a previous resnet block."},{"metadata":{},"cell_type":"markdown","source":"<img style=\"float:left;\" src=\"https://www.bing.com/images/search?view=detailV2&ccid=hRXYOTa3&id=C717317A9D1CA65A7614794180E81FD091010768&thid=OIP.hRXYOTa3gMIA9iyvnuNyEgHaEJ&mediaurl=https%3a%2f%2fpic1.zhimg.com%2fv2-8515d83936b780c200f62caf9ee37212_r.jpg&exph=705&expw=1260&q=senet+resnet+block&simid=608043120371368827&ck=FC3F8BB7ABDA784C8F6C42541078B8EC&selectedIndex=4&FORM=IRPRST&ajaxhist=0\"/> "},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResidualUnit(keras.layers.Layer):\n    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n        super().__init__(**kwargs)\n        self.activation = keras.activations.get(activation)\n        self.main_layers = [keras.layers.Conv2D(filters, 3, strides=strides, padding=\"same\",use_bias=False),\n                            keras.layers.BatchNormalization(), # Normalize the outputs\n                            self.activation,\n                            keras.layers.Conv2D(filters, 3, strides=1, padding='same', use_bias=False),\n                            keras.layers.BatchNormalization()]\n        self.skip_layers = [\n            keras.layers.Conv2D(filters, 1, strides=strides,padding=\"same\",use_bias=False),\n            keras.layers.BatchNormalization()\n        ]\n    \n    # We don't forget the call method which is called during the training and prediction\n    def call(self, inputs):\n        Z = inputs\n        for layer in self.main_layers:\n            Z = layer(Z)\n        skip_Z = inputs\n        for layer in self.skip_layers:\n            skip_Z = layer(skip_Z)\n        return self.activation(Z + skip_Z)\n    \nclass SEBloc(keras.layers.Layer):\n    def __init__(self, pool, **kwargs):\n        super().__init__(**kwargs)\n        self.main_layers = [keras.layers.AveragePooling2D(\n                                pool_size=pool, strides=1, padding=\"same\"), # pool_size is important, we need a scalar per feature card\n                            keras.layers.Dense(5, activation='relu'), # embedding\n                            keras.layers.Dense(64, activation='sigmoid')] # outputs \n    \n    def call(self, inputs):\n        Z = inputs\n        for layer in self.main_layers:\n            Z = layer(Z)\n        return Z","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align: center;\">SENET model<h2><a id=c><a/>\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"this is not a real SENET because it contains only one SE block and RES block but this is the theory."},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 50\nBATCH_SIZE = 32\n\ninputs = tf.keras.Input(shape=(48,48,1), dtype=\"float32\")\nx = keras.layers.Conv2D(64, 5, strides=2, input_shape=[48,48,1])(inputs)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Activation(\"relu\")(x)\nx = keras.layers.MaxPool2D(pool_size=2, strides=2, padding='same')(x)\n\nx_res = ResidualUnit(64, strides=1)(x) # RES\n\nx_se = SEBloc(x.shape[1])(x) #SE\n\nx_res_se = keras.layers.Multiply()([x_res, x_se]) # Multiply outputs of SE and RES\nx = keras.layers.Add()([x_res_se, x])\n\nx = keras.layers.Dropout(0.5)(x)\nx = keras.layers.Flatten()(x)\noutput = keras.layers.Dense(1, activation='relu')(x) # One output with relu for the regression\nmodel = tf.keras.Model(inputs, output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align: center;\">Learning rate<h2><a id=d><a/>\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"the learning rate is an important hyperparameter. It can save you a lot of time. the following cell show you how to have a precise idea of the optimal lr to choose."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Learning Rate is one of the most important hyperparameter so the following piece of code is a way to find a good LR\nimport keras\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    \n    def __init__(self, K, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n        self.K = K\n        \n    def on_batch_end(self, batch, logs):\n        \n        self.rates.append(self.K.get_value(self.model.optimizer.lr))\n        self.losses.append(logs[\"loss\"])\n        self.K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n        \n        \ndef bestLearningRate():\n        \n        print(\"\\n\\n********************** Best learning rate calculation ******************\\n\\n\")\n        K = keras.backend\n        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.MeanAbsoluteError()])\n        expon_lr = ExponentialLearningRate(K,factor=1.0003)\n        model.fit(Xtrain, ytrain, validation_data=(Xval, yval), epochs = 20, callbacks=[expon_lr])\n        print(\"*************************************************************************\\n\\n\")\n        \n        print(\"********************** Loss as function of learning rate plot displayed ********************\\n\\n\")\n        \n        fig = px.line(\n        x=expon_lr.rates, y=expon_lr.losses,\n        labels={'index': 'learning rate', 'value': 'loss'}, \n        title='Training History')\n        fig.show()\n        \n        id_min = np.argmin(expon_lr.losses)\n        return expon_lr.rates[id_min]\n        \nlr = bestLearningRate()\nprint('the best learning rate is: ',lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"text-align: center;\">Training step<h2><a id=e><a/>\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"I used two callbacks. One to stop the training when the loss is under a limit. The second is a learning rate decreasing processus in case of plateau."},{"metadata":{"trusted":true},"cell_type":"code","source":"class myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_loss')<91):\n            print(\"\\nReached 110 val_loss so cancelling training!\")\n            self.model.stop_training = True\n        \ncallback = myCallback()\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n\nSGD = tf.keras.optimizers.Adam(learning_rate=0.0035) \nmodel.compile(loss='mse', optimizer=SGD ,metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\nhistory = model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(Xval, yval), callbacks = [callback, reduce_lr], verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally let's see the graph of losses."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(\n    history.history, y=['loss', 'val_loss'],\n    labels={'index': 'epoch', 'value': 'loss'}, \n    title='Training History')\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}