{"cells":[{"metadata":{},"cell_type":"markdown","source":"* # Stroke Prediction"},{"metadata":{},"cell_type":"markdown","source":"---\n**1. Importing the necessary libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To prevent the annoying warning from scikit learn package\nimport warnings  \nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\ncmap = sns.cm.mako_r\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Importing the Data using Pandas read_csv(). And calling head() and info() on the DataFrame**"},{"metadata":{"trusted":true},"cell_type":"code","source":"stroke = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stroke.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stroke.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stroke.drop(columns=['id']).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n**3. Preprocessing Data before Exploratory Data Analysis**\n<ol>\n    <li>Using <strong>round()</strong> to round off Age.</li><br>\n    <li>Setting values to <strong>NaN</strong> where BMI is less than <strong>12</strong> and greater than <strong>60</strong>. Found out from google search that these can be considered as outliers</li><br>\n    <li>We will sort the DataFrame first based on <strong>Gender</strong> then on <strong>Age</strong> and use <strong>Forward Filling</strong> to fill those missing BMI values</li>\n</ol>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Round off Age\nstroke['age'] = stroke['age'].apply(lambda x : round(x))\n\n# BMI to NaN\nstroke['bmi'] = stroke['bmi'].apply(lambda bmi_value: bmi_value if 12 < bmi_value < 60 else np.nan)\n\n# Sorting DataFrame based on Gender then on Age and using Forward Fill-ffill() to fill NaN value for BMI\nstroke.sort_values(['gender', 'age'], inplace=True) \nstroke.reset_index(drop=True, inplace=True)\nstroke['bmi'].ffill(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stroke.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have <strong>Age</strong> Column as <em style='color:blue'>int64</em> and no missing values in <strong>Bmi</strong> Column\n\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if Data is balanced\nxs = stroke['stroke'].value_counts().index\nys = stroke['stroke'].value_counts().values\n\nax = sns.barplot(xs, ys)\nax.set_xlabel(\"Stroke\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the above plot that the Data is not balanced which will result in a bad model. To resolve this issue we can use SMOTE to balance the Data. This is will done before fitting our data to the model.\n\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age vs BMI with hue = stroke\nplt.figure(figsize=(12,8))\nax = sns.scatterplot(x=\"bmi\", y=\"age\", alpha=0.4, data=stroke[stroke['stroke'] == 0])\nsns.scatterplot(x=\"bmi\", y=\"age\", alpha=1, data=stroke[stroke['stroke'] == 1], ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above <strong>Age vs BMI</strong> plot we can clearly see that when people attain an age of <strong>40</strong> or greater the chances of getting a stroke increases and after <strong>60+</strong> it tends to increase even more. Also, people with a BMI of <strong>25+</strong> have shown a higher chances of encountering a stroke. \n\nSo, people with 40+ years and BMI of 25+ have a greater probability of encountering a stroke.\n\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age vs BMI with hue = stroke\nplt.figure(figsize=(12,8))\nax = sns.scatterplot(x=\"bmi\", y=\"avg_glucose_level\", alpha=0.4, data=stroke[stroke['stroke'] == 0])\nsns.scatterplot(x=\"bmi\", y=\"avg_glucose_level\", alpha=1, data=stroke[stroke['stroke'] == 1], ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Percentage of People\ndef plot_percent_of_stroke_in_each_category(df, column, axis):\n    x_axis = []\n    y_axis = []\n    \n    unique_values = df[column].unique()\n    \n    for value in unique_values:\n        stroke_yes = len(df[(df[column] == value) & (df['stroke'] == 1)])\n        total = len(df[df[column] == value])\n        percentage = (stroke_yes/total) * 100\n        x_axis.append(value)\n        y_axis.append(percentage)\n    \n    sns.barplot(x_axis, y_axis, ax=axis)\n    \ncolumns = ['gender', 'hypertension', 'heart_disease', 'ever_married', \n           'work_type', 'Residence_type', 'smoking_status']\n\nfig, axes = plt.subplots(4, 2, figsize=(16, 18))\naxes[3, 1].remove()\n\nplot_percent_of_stroke_in_each_category(stroke, 'gender', axes[0,0])\naxes[0,0].set_xlabel(\"Gender\")\naxes[0,0].set_ylabel(\"Percentage\")\n\nplot_percent_of_stroke_in_each_category(stroke, 'hypertension', axes[0,1])\naxes[0,1].set_xlabel(\"Hypertension\")\n\nplot_percent_of_stroke_in_each_category(stroke, 'heart_disease', axes[1,0])\naxes[1,0].set_xlabel(\"Heart Disease\")\naxes[1,0].set_ylabel(\"Percentage\")\n\nplot_percent_of_stroke_in_each_category(stroke, 'ever_married', axes[1,1])\naxes[1,1].set_xlabel(\"Ever Married\")\n\n\nplot_percent_of_stroke_in_each_category(stroke, 'work_type', axes[2,0])\naxes[2,0].set_xlabel(\"Work Type\")\naxes[2,0].set_ylabel(\"Percentage\")\n\nplot_percent_of_stroke_in_each_category(stroke, 'Residence_type', axes[2,1])\naxes[2,1].set_xlabel(\"Residence Type\")\n\nplot_percent_of_stroke_in_each_category(stroke, 'smoking_status', axes[3,0])\naxes[3,0].set_xlabel(\"Smoking Status\")\naxes[3,0].set_ylabel(\"Percentage\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights drawn from the above plot with respect to the Stroke Data**\n\n<ol>\n    <li>Both the Genders have arround 5% chance.</li><br>\n    <li>People with history of Hypertension and Heart Disease have shown an increased in percentage of Stroke with around 12.5% and 16.5% respectively.</li><br>\n    <li>Married/Divorced people have a 6.5% chance of stroke. No wonder why people these days choose to stay single.</li><br>\n    <li>Self Employed people have a higher chance compared to Private and Govt Jobs.</li><br>\n    <li>Rural and Urban doesn't show much difference.</li><br>\n    <li>For some reason people who once used to smoke have higher chance compared to people who are still smoking. If you have already started smoking, don't stop. JK, do as you wish. </li>\n</ol>\n\n---"},{"metadata":{},"cell_type":"markdown","source":"---\n\n**5. Preparing the Data for Prediction**\n\n<ol>\n    <li>Converting the Categorical Columns into Numerical by Mapping each category to an integer value using <strong>map()</strong> on pandas series object</li><br>\n    <li>As we saw earlier that data is <strong>Imbalanced</strong>. To make it balanced we use a technique called as <strong>SMOTE (Synthetic Minority Oversampling Technique)</strong>. There are other techniques like NearMiss Algorithm. But I prefer SMOTE. The major difference between SMOTE and NMA is that SMOTE increases number of sample of minority classes by linear interpolation. Whereas the later one randomly eliminates majority class examples. When instances of two different classes are very close to each other.</li><br>  \n    <li>Spliting the Data in Training and Testing Samples</li>\n</ol>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting Categorical Data to Numerical\ngender_dict = {'Male': 0, 'Female': 1, 'Other': 2}\never_married_dict = {'No': 0, 'Yes': 1}\nwork_type_dict = {'children': 0, 'Never_worked': 1, 'Govt_job': 2, 'Private': 3, 'Self-employed': 4}\nresidence_type_dict = {'Rural': 0, 'Urban': 1}\nsmoking_status_dict = {'Unknown': 0, 'never smoked': 1, 'formerly smoked':2, 'smokes': 3}\n\nstroke['gender'] = stroke['gender'].map(gender_dict)\nstroke['ever_married'] = stroke['ever_married'].map(ever_married_dict)\nstroke['work_type'] = stroke['work_type'].map(work_type_dict)\nstroke['Residence_type'] = stroke['Residence_type'].map(residence_type_dict)\nstroke['smoking_status'] = stroke['smoking_status'].map(smoking_status_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting into features and value to be predicted\nX = stroke.drop(columns=['id', 'stroke'])\ny = stroke['stroke']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2)\n\nsns.barplot(x=['0', '1'], y =[sum(y == 0), sum(y == 1)], ax = ax1)\nax1.set_title(\"Before Oversampling\")\nax1.set_xlabel('Stroke')\n\n#Using SMOTE to balance the Data\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state = 2) \nX, y = sm.fit_resample(X, y) \n\nsns.barplot(x=['0', '1'], y =[sum(y == 0), sum(y == 1)], ax = ax2)\nax2.set_title(\"After Oversampling\")\nax2.set_xlabel('Stroke')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spliting the Data into Train and Test\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6. Creating a Model for Stroke Prediction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, plot_confusion_matrix\n\npipeline = make_pipeline(StandardScaler(), RandomForestClassifier())\npipeline.fit(X_train, y_train)\nprediction = pipeline.predict(X_test)\n\nprint(f\"Accuracy Score : {round(accuracy_score(y_test, prediction) * 100, 2)}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(pipeline, X_test, y_test, cmap=cmap)\nplt.grid(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n# Thanks a lot for showing your Interest "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}