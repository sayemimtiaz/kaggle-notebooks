{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What we will do in this notebook\n\n- Data cleaning\n- Basic EDA\n- Visualization\n- Try to understand why those people left\n- Typical employee that left profiling.\n- Untypical employee that left profiling.\n- Regression analysis.\n- Random Forest classificaion.\n\nPlus we got <font color=deepskyblue> highest </font> classification score so far  <font color=deepskyblue> : ) </font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## This notebook is still under construction.\n## Feel free to <font color=deepskyblue> FORK  </font> this notebook, Please  <font color=deepskyblue> UPVOTE !! </font> if it's helpful to you  <font color=deepskyblue> : ) </font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ngeneral_data=pd.read_csv('/kaggle/input/hr-analytics-case-study/general_data.csv')\nemployee_survey_data=pd.read_csv('/kaggle/input/hr-analytics-case-study/employee_survey_data.csv')\nmanager_survey_data=pd.read_csv('/kaggle/input/hr-analytics-case-study/manager_survey_data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. KNOW YOUR DATA\n1. we have 401 entries and 35 columns or features\n2. we have 3 datasets for this project,\n3. we have 23 numerical features，8 string features.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\n','-'*20,'General info','-'*20,'\\n')\ngeneral_data.info()\nprint('\\n','-'*20,'Employee Survey info','-'*20,'\\n')\nemployee_survey_data.info()\nprint('\\n','-'*20,'Manager Survey info','-'*20,'\\n')\nmanager_survey_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check Uniqueness of EmployeeID column, we gonna merge those three dataframe using this column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(general_data.EmployeeID.nunique())\nprint(employee_survey_data.EmployeeID.nunique())\nprint(manager_survey_data.EmployeeID.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"general_data.set_index('EmployeeID')\nemployee_survey_data.set_index('EmployeeID')\nmanager_survey_data.set_index('EmployeeID')\ndata=pd.concat([general_data,employee_survey_data,manager_survey_data],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking nans in Our data set.\n\n Normally,we should carefully check every nan data to find out why it is missing and deal with them. We gonna leave them there and ignore them when analyzing,since this is just a quick analysis,and nans are just a tiny portion of the whole dataset. \n \n **What we know**\n \n 1. we have 110 enteries that have at least one Nan data.About 2% of all dataset.\n \n **What we do**\n \n 1. we ignore them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'There is {data.isna().any(axis=1).mean()*100:.2f}% Nans')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. DESCRIPTIVE STATISTICS\n\n** what we know**\n\n1. we have 16% employees left last year.\n2. 60% employees are Male.\n3. Most employees are during 30~38 years old.\n4. employees that left are generally younger than employees stay.\n5. The top relative features to attrition is MaritalStatus,EnvironmentSatisfaction,JobSatisfaction,YearsAtCompany,YearsWithCurrManager,Age,TotalWorkingYears\n6. people who left have pretty low jobsatisfaction,whereas enviromentssatisfaction varies.\n7. Noticed that there is relatively low ralation between leaving and performanceRating or jobinvolvements.\n8. Single empolyees are more likely to leave.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# last year arrtrition\ndata.Attrition.value_counts(normalize=True).to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap = plt.get_cmap(\"tab20c\")\nouter_colors = cmap(np.arange(2)*13)\n\nplt.subplots(figsize=(20,10))\nax=plt.subplot(1,2,1)\nsns.distplot(data.Age,bins=23)\nax.spines['left'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nplt.subplot(1,2,2)\nplt.pie(data.Gender.value_counts(normalize=True),radius=1,autopct='%1.1f%%',wedgeprops=dict(width=0.5, edgecolor='w'),colors=outer_colors,labels=['Male','Female'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.violinplot(data=data,x='Gender',y='Age',hue='Attrition')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelEncoder_X = LabelEncoder()\ndata_labeled=data.copy()\ndata_labeled['Attrition'] = labelEncoder_X.fit_transform(data_labeled['Attrition'])\ndata_labeled['BusinessTravel'] = labelEncoder_X.fit_transform(data_labeled['BusinessTravel'])\ndata_labeled['Department'] = labelEncoder_X.fit_transform(data_labeled['Department'])\ndata_labeled['EducationField'] = labelEncoder_X.fit_transform(data_labeled['EducationField'])\ndata_labeled['Gender'] = labelEncoder_X.fit_transform(data_labeled['Gender'])\ndata_labeled['JobRole'] = labelEncoder_X.fit_transform(data_labeled['JobRole'])\ndata_labeled['MaritalStatus'] = labelEncoder_X.fit_transform(data_labeled['MaritalStatus'])\ndata_labeled.drop(['Over18','StandardHours','EmployeeCount'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"# check relationship between attrition and other features.\ndata_labeled.corr()['Attrition'].sort_values().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_labeled.dropna(inplace=True)\nax=plt.figure(figsize=(10,5))\nsns.distplot(data_labeled[data_labeled['Attrition']==1].TotalWorkingYears,label='yes')\nsns.distplot(data_labeled.TotalWorkingYears,label='all')\nplt.xticks(range(0,40,2))\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10,5))\nsns.distplot(data[data['Attrition']=='Yes'].Age,label='yes')\nsns.distplot(data.Age,label='all')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10,5))\nsns.distplot(data[data['Attrition']=='Yes'].YearsWithCurrManager,label='yes')\nsns.distplot(data.YearsWithCurrManager,label='all')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10,5))\nsns.distplot(data[data['Attrition']=='Yes'].YearsAtCompany,label='yes')\nsns.distplot(data.YearsAtCompany,label='all')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(10,10))\nplt.subplot(2,1,1)\nsns.boxplot(data=data,x='Attrition',y='JobSatisfaction')\nplt.subplot(2,1,2)\nsns.boxplot(data=data,x='Attrition',y='EnvironmentSatisfaction')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data['Attrition'],data['MaritalStatus'],margins=True,normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.Typical employee left profiling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. You are a pretty young people in this company,under 32 yrs old,possiblely a male,slightly more chance you are single.\n2. You have been woking at this company for 1~6 years.Toltal working years is under 12.\n3. but you hate it, your jobsatisfaction is pretty low, which is 1.0\n4. Despite that,your emotion doesnt affect your jobinvolment and job performance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_left=data[data.Attrition=='Yes']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nax=plt.subplot(131)\nsns.distplot(data_left.Age,ax=ax)\nax=plt.subplot(132)\nsns.distplot(data_left.YearsAtCompany,ax=ax)\nax=plt.subplot(133)\nsns.distplot(data_left.TotalWorkingYears,ax=ax)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.Attrition=='Yes'].Gender.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4 Untypical employee left profiling\n\nBut,why those senior people / higher job satisfaction people leave?\n\n1. Senior People tend to leave because of bad Environment Satisfaction,and TotalWorkingYears is another negative factor.\n2. people with high JobSatisfaction tend to leave because of Age, and TotalWorkingYears","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_labeled[data_labeled.Age>40].corr()['Attrition']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nax=plt.subplot(121)\nsns.boxplot(data=data_labeled[data_labeled.JobSatisfaction>3],x='Attrition',y='TotalWorkingYears',ax=ax)\nax=plt.subplot(122)\nsns.boxplot(data=data_labeled[data_labeled.JobSatisfaction>3],x='Attrition',y='Age',ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_labeled.drop(columns=['EmployeeID'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_cols = ['Age','Attrition','BusinessTravel','DistanceFromHome','Education', 'EducationField','Gender', 'JobLevel', 'JobRole',\n       'MaritalStatus', 'MonthlyIncome', 'NumCompaniesWorked',\n       'PercentSalaryHike', 'StockOptionLevel', 'TotalWorkingYears',\n       'TrainingTimesLastYear', 'YearsAtCompany', 'YearsSinceLastPromotion',\n       'YearsWithCurrManager']\ncorr = data_labeled[corr_cols].corr()\nplt.figure(figsize=(16,14))\nsns.heatmap(corr, annot =True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5 Modeling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.1 LogisticRegression\nLet's first use LogisticRegression to deal with this data.\nIt seems like LogisticRegression is not very ideal for this dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data_labeled['Attrition']\nx = data_labeled.drop('Attrition', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test, y_train, y_test = train_test_split(x,y, test_size = 0.20, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nScaler_X = StandardScaler()\nX_train = Scaler_X.fit_transform(X_train)\nX_test = Scaler_X.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\n#confusion matrix\nprint(accuracy_score(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Model Selection\n\nLet's see if there is any other model can generate better reuslt.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_todrop = ['JobLevel','Department','JobRole','NumCompaniesWorked','PercentSalaryHike','StockOptionLevel',\n               'YearsWithCurrManager']\nx = data_labeled.drop(['Attrition'], axis=1).reset_index(drop=True)\ny = data_labeled['Attrition'].values\nx.drop(cols_todrop, axis=1, inplace=True)\nx.Age = pd.cut(x.Age, 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = pd.get_dummies(x)\nx_copy=x.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nx = scaler.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve, train_test_split, RandomizedSearchCV\nfrom sklearn import preprocessing\nfrom sklearn.metrics import r2_score, accuracy_score, roc_auc_score, mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_scores(score1, score2):\n    models = []\n    models.append(('LR', LogisticRegression()))\n    models.append(('LDA', LinearDiscriminantAnalysis()))\n    models.append(('KNN', KNeighborsClassifier()))\n    models.append(('CART', DecisionTreeClassifier()))\n    models.append(('NB', GaussianNB()))\n    models.append(('SVM', SVC()))\n    models.append(('ADA', AdaBoostClassifier()))\n    models.append(('GradientBooster', GradientBoostingClassifier()))\n    models.append(('ExtraTrees', ExtraTreesClassifier()))\n    models.append(('RandomForest', RandomForestClassifier()))\n    cv_scores = []\n    test_scores = []\n    names = []\n    stds = []\n    differences = []\n    #res = pd.DataFrame(columns = {'Model',score+('(train)'), 'Std', score+('(test_score)'), 'difference'})\n    #res = res[['Model',score+('(train)'), 'Std', score+('(test_score)'), 'difference']]\n    res = pd.DataFrame()\n    for index, model in enumerate(models):\n        kfold = StratifiedKFold(n_splits=7)\n        cv_results = cross_val_score(model[1], x_train, y_train, cv=kfold, scoring=score1)\n        cv_scores.append(cv_results)\n        names.append(model[0])\n        model[1].fit(x_train,y_train)\n        predictions = model[1].predict(x_test)\n        test_score = score2(predictions, y_test)\n        test_scores.append(test_score)\n        stds.append(cv_results.std())\n        differences.append((cv_results.mean() - test_score))\n        res.loc[index,'Model'] = model[0]\n        res.loc[index,score1+('(train)')] = cv_results.mean()\n        res.loc[index,score1+('(test_score)')] = test_score\n        res.loc[index,'Std'] = cv_results.std()\n        res.loc[index,'difference'] = cv_results.mean() - test_score\n    return res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see RandomForest can generate better results in this models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"get_scores('accuracy', accuracy_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use RandomizedSearchCV to try tuning parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'bootstrap': [True, False],\n 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [200, 400, 600, 800, 1000]}\nRandomForest = RandomForestClassifier()\nrandomgrid_forest = RandomizedSearchCV(estimator=RandomForest, param_distributions = params, \n                               cv=5, n_iter=25, scoring = 'accuracy',\n                               n_jobs = 4, verbose = 3, random_state = 42,\n                               return_train_score = True)\nrandomgrid_forest.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_preds = randomgrid_forest.predict(x_test)\nprint(classification_report(y_test,forest_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances_=randomgrid_forest.best_estimator_.feature_importances_.tolist()\nfeature_names = x_copy.columns\npd.DataFrame(pd.Series(feature_importances_,feature_names),columns=['importance']).sort_values('importance',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's draw a ROC curve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\ny_score = randomgrid_forest.predict_proba(x_test)  # 随机森林\nfpr, tpr, thresholds = roc_curve(y_test, y_score[:, 1])\nroc_auc = auc(fpr, tpr)\ndef drawRoc(roc_auc,fpr,tpr):\n    plt.subplots(figsize=(7, 5.5))\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([-0.1, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \ndrawRoc(roc_auc, fpr, tpr)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}