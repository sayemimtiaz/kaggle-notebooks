{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What matters the most when you buy a house? Location, location, location!\n\n#### That a very popular statement nowadays and I am a believer of it. Because it's true, no matter in which country all over the world. \n\n#### Based on our life experience, house price would be strongly related to below factors:\n1. Which city? A well-developed and modern city must be more attractive than others\n2. Which area in the city? House in CBD, or city center must have higher price than surburb area\n3. Within the same area? There might be other factors fluctuate the sales price, like transpotaion, convenience, education, etc. Those factors normally just slighly increase or decrease the price, compared to the average price of this area. But those factors would not be that crucial, to overtune the price in countryside to higher than an apartment in downtown.\n\n#### From checking below column description\n* ADDRESS + LONGITUDE + LATITUDE might help identify location?\n* Other columns might be the factors that bring the price higher or lower than the average?\n\n|Column | Description |\n| --- | --- |\n| POSTED_BY          | Category marking who has listed the property |\n| UNDER_CONSTRUCTION | Under Construction or Not|\n| RERA\t             | Rera approved or Not|\n| BHK_NO\t         | Number of Rooms|\n| BHK_OR_RK\t         | Type of property|\n| SQUARE_FT\t         | Total area of the house in square feet|\n| READY_TO_MOVE\t     | Category marking Ready to move or Not|\n| RESALE \t         | Category marking Resale or not|\n| ADDRESS\t         | Address of the property|\n| LONGITUDE \t     | Longitude of the property|\n| LATITUDE  \t     | Latitude of the property|\n\n\n"},{"metadata":{},"cell_type":"markdown","source":" ### So above are my initial thinking when firstly glance on the data. Let's do a quick EDA and see if they are applicable in this dataset."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc\nimport xgboost as xgb \nimport sklearn.metrics as metrics\nfrom sklearn.metrics import mean_squared_error as MSE \n\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nimport warnings\nimport datetime as dt\nwarnings.filterwarnings('ignore')\npd.options.display.max_rows = None\npd.options.display.max_columns = None\ntrain_file = '/kaggle/input/house-price-prediction-challenge/train.csv'\ntrain_df = pd.read_csv(train_file)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checkout the datatype and cleanliness\n* Datatype\n* Description\n* Null value ratio"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()\n# Data has no null value, pretty clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe(include='all')\n# On BHK_OR_RK field, there are 29427 BHKs out of 29451, might not be useful in our final model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checkout distribution of target column and what might be most related to it.\nSize (SQUARE_FT) should be strongly correlated"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(ncols=2,nrows=3,dpi=100,figsize=(20,20))\nsns.distplot(a=train_df['TARGET(PRICE_IN_LACS)'], kde=False, ax=ax[0][0])\ndf1 = train_df[train_df['TARGET(PRICE_IN_LACS)']<500]\nsns.distplot(a=df1['TARGET(PRICE_IN_LACS)'], kde=False, ax=ax[0][1])\n\n\nsns.distplot(a=train_df['SQUARE_FT'], kde=False, ax=ax[1][0])\ndf1 = train_df[train_df['SQUARE_FT'] < 25000]\nsns.distplot(a=df1['SQUARE_FT'], kde=False, ax=ax[1][1])\n\n\nsns.scatterplot(x=train_df['SQUARE_FT'], y=train_df['TARGET(PRICE_IN_LACS)'], ax=ax[2][0])\ndf1 = train_df[(train_df['TARGET(PRICE_IN_LACS)']<500) & (train_df['SQUARE_FT'] < 25000)]\nsns.scatterplot(x=df1['SQUARE_FT'], y=df1['TARGET(PRICE_IN_LACS)'], ax=ax[2][1])\n\nax[0][0].set_title('TARGET(PRICE_IN_LACS) Histogram')\nax[0][1].set_title('TARGET(PRICE_IN_LACS) Histogram (<500)')\n\nax[1][0].set_title('SQUARE_FT Histogram')\nax[1][1].set_title('SQUARE_FT Histogram (<25000)')\n\nax[2][0].set_title('SQUARE_FT vs TARGET(PRICE_IN_LACS)')\nax[2][1].set_title('SQUARE_FT vs TARGET(PRICE_IN_LACS) without outlier')\n# ax[1].set_title('AC power & DC power during day hours')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see the SQUARE_FT is somehow correlated to target price. There are a lot of outliers with very low price/SQF. We need to keep explore other fields and see if we can find out something useful to address them."},{"metadata":{},"cell_type":"markdown","source":"#### For categorical fields, we can see all the distinct values"},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in train_df.columns:\n    if train_df[x].dtype != 'float64':        \n        print(x, train_df[x].unique())\n        print('-'*10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(ncols=2,nrows=3,dpi=100,figsize=(20,20))\ndf1 = train_df[(train_df['TARGET(PRICE_IN_LACS)']<500) & (train_df['SQUARE_FT'] < 25000)]\nsns.scatterplot(x=df1['SQUARE_FT'], y=df1['TARGET(PRICE_IN_LACS)'], hue=df1['POSTED_BY'], ax=ax[0][0])\nsns.scatterplot(x=df1['SQUARE_FT'], y=df1['TARGET(PRICE_IN_LACS)'], hue=df1['UNDER_CONSTRUCTION'], ax=ax[0][1])\nsns.scatterplot(x=df1['SQUARE_FT'], y=df1['TARGET(PRICE_IN_LACS)'], hue=df1['RERA'], ax=ax[1][0])\nsns.scatterplot(x=df1['SQUARE_FT'], y=df1['TARGET(PRICE_IN_LACS)'], hue=df1['BHK_OR_RK'], ax=ax[1][1])\nsns.scatterplot(x=df1['SQUARE_FT'], y=df1['TARGET(PRICE_IN_LACS)'], hue=df1['READY_TO_MOVE'], ax=ax[2][0])\nsns.scatterplot(x=df1['SQUARE_FT'], y=df1['TARGET(PRICE_IN_LACS)'], hue=df1['RESALE'], ax=ax[2][1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check correlation heatmap between those features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=10)\n\n## Map poster and bhk ind to integer\nposted_by_map = {'Owner':1, 'Dealer':2, 'Builder':3}\ntrain_df['POSTED_BY_CODE'] = train_df['POSTED_BY'].map(posted_by_map)\n\ncandidates_col = ['POSTED_BY_CODE', 'UNDER_CONSTRUCTION', 'RERA', 'BHK_NO.', 'READY_TO_MOVE', 'RESALE']\ncorrelation_heatmap(train_df[candidates_col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From above diagrams, we can draw these conclusion\n1. those fields are having very limit effect to the price, if we were only to put those into the model, don't think that's gonna work.\n2. except POSTED_BY, we can see house price POSTED_BY owner is slighly lower than others\n3. UNDER_CONSTRUCTION & READY_TO_MOVE are exactly the same field, with corr == -1. We can just keep one of them during model train\n\n### This is more convincing for me that I need extra location-related features\n\n### Let's label those categorical fields first then further explore on below features\n* BHK_NO.\n* ADDRESS\n* LONGITUDE/LATITUDE"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Generate a unit price feature for each hosue\ntrain_df['Price/SQF'] = train_df['TARGET(PRICE_IN_LACS)']/train_df['SQUARE_FT'] * 1000\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore BHK_OR_RK\n#### BHK_OR_RK has too imbalance distribution, we won't use this in the model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Checkout how price/SQF related to BHK_NO.\ntrain_df[['BHK_OR_RK','Price/SQF']].groupby('BHK_OR_RK', as_index=False).agg({'Price/SQF':['mean','count']})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore BHK_NO."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Checkout how price/SQF related to BHK_NO.\ntrain_df[['BHK_NO.','Price/SQF']].groupby('BHK_NO.', as_index=False).agg({'Price/SQF':['mean','count']})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see there are very limit price data for house num larger than 7, and unit price is very random when > 7. \nSo Let's mark it as 2 (mode of the dataset) for those BNK_NO. > 7"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['BHK_NO.'] = train_df['BHK_NO.'].apply(lambda x: 2 if x > 7 else x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore ADDRESS / LONGITUDE / LATITUDE\n* In address field, we can see the street, sometimes building or block number, and city at the end\n    * e.g. Ksfc Layout,Bangalore\n    *      Sector-1 Vaishali,Ghaziabad\n* Based on initial assumption, city is one of the most decisive factor for price\n\n### Looks like we can derive city from each address by splitting ',' and take the latest element. \n### So let's do it.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_city(x):\n    l = x.split(',')\n    return ' '.join(l[:len(l)-1])\n\ntrain_df['City'] = train_df['ADDRESS'].apply(lambda x: x.split(',')[-1])\ntrain_df['Street'] = train_df['ADDRESS'].apply(lambda x: extract_city(x))\ncity_count = train_df.groupby('City', as_index=False)['Price/SQF'].count()\ncity_count.rename(columns={'Price/SQF':'count'},inplace=True)\nprint(\"City number of the dataset:\", city_count.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"city_count.sort_values(by='count').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"city_count.sort_values(by='count').tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looks like we now have the 'City', can see famous cities like Bangalore, Lalitpur, Mumbai, Pune, and etc, dominating the dataset, which is expected.\n\n### let's verify if city would be greatly impact the unit price (Price/SQF)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train_df[['City','Price/SQF']] \\\n        .groupby('City', as_index=False)['Price/SQF'] \\\n        .mean() \\\n        .sort_values(by='Price/SQF', ascending=False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Showing top 30 only, there 256 in total.\nplt.figure(figsize=(14,7))\ng = sns.barplot(x=df.head(20)['City'], y=df.head(20)['Price/SQF'])\ng.set_xticklabels(g.get_xticklabels(), rotation=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looks like we are getting somewhere, but Hajipur, Haldia are outlier, they only have 1 row each in train_df. \n\n### Let's remove them and see how it goes."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Showing top 30 only, there 256 in total.\ndf = df[~df['City'].isin(['Hajipur','Haldia'])]\nplt.figure(figsize=(14,7))\ng = sns.barplot(x=df.head(30)['City'], y=df.head(30)['Price/SQF'])\ng.set_xticklabels(g.get_xticklabels(), rotation=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### City could be a very decisive feature, so we will include it to the model. But what might differentiate price within the same city?\n* As mentioned before, CBD or downtown would have higher price than suburb area"},{"metadata":{},"cell_type":"markdown","source":"### So let's check if LONGITUDE/LATITUDE can help with that\n#### Initial thought of how to use these 2 features - \n1. Assuming CBD has the highest house price, we can rank the price for houses in each city, get the highest percentile of price and take the average long/lat value as city center long/lat (name the field C_LONG, C_LAT) \n2. Calculate the distance to (C_LONG, C_LAT), further away from CBD would lower the price\n3. Calculate the average price for each city as a cursor of price, assuming when we do prediction, each city should be fluctuating from its own average price. In short, training set and prediction set should presumebly share the same average price.\n\n### Now, let's use Mumbai for example to explore"},{"metadata":{"trusted":true},"cell_type":"code","source":"Mumbai_df = train_df[train_df['City']=='Mumbai'].copy()\nfig,ax = plt.subplots(ncols=2,nrows=1,dpi=100,figsize=(20,5))\nsns.distplot(a=Mumbai_df['LONGITUDE'], kde=False, ax=ax[0])\nsns.distplot(a=Mumbai_df['LATITUDE'], kde=False, ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### There are a some outlier\n* India's longitude/latitude should be within long(6.75,35.5) and lat(68.12,97.42)\n* also long/lat should not be negative\n\n### So data quality issue here. Does that mean we can't use these feature? \n\n### Not yet to give up, because not all the values are wrong, maybe we can derive the approximate distanc for those."},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_in_India(long, lat):\n    if 6.75 < long < 35.5 and 68.12 < lat < 97.42:\n        return 1\n    else:\n        return 0\n\nMumbai_df['IS_IN_INDIA'] = Mumbai_df.apply(lambda x: is_in_India(x['LONGITUDE'], x['LATITUDE']), axis=1)\nprint(Mumbai_df[['IS_IN_INDIA','Price/SQF']].groupby('IS_IN_INDIA',as_index=False).count())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"city_df = train_df.copy()\ncity_df['IS_IN_INDIA'] = city_df.apply(lambda x: is_in_India(x['LONGITUDE'], x['LATITUDE']), axis=1)\ncity_df = city_df.merge(city_count, on='City')\ndf = city_df[['City','IS_IN_INDIA','count']].groupby(['City','count'],as_index=False).sum()\ndf['outlier_ratio'] = (1 - df['IS_IN_INDIA']/df['count'])*100\ndf.sort_values(by='outlier_ratio',ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## How many cities has count < 10\n(df['count'] < 10).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Only 1 city without any long/lat in INDIA. Think it's good enough for us to derive the C_LONG/C_LAT and calculate distance for each house, if we make below assumption\n1. Big city has lots of house price data, small city would only have very few data here.\n2. Small city's house price would be very close to its neighbor\n3. Deriving distance based on C_LONG/C_LAT of the neighbor might get larger distance than it's reality, but don't think it would cause much impact. Assuming it's neighbor is also a small city, price in city center is still very low compared to big city. If neigher is a big city, then you get larger distance then the price has larger decline from a higher price.\n\n### Below are the basic idea and the code I come up with after considering all different edge cases (can skip to check if it's too trivial), if the rules doesn't apply, will just remove the data from training set\n"},{"metadata":{},"cell_type":"markdown","source":"1. Divide by big and small cities\n2. For Big city\n    - Get mode of long/lat\n    - Get C_LONG/C_LAT = average LONG/LAT of house with top 1% percentile ranked by price (LONG/LAT has to be in INDIA)\n    - If long/lat not correct, marked the long/lat closer to mode to be considered as in the same city\n    - Calculate the distance between LONG/LAT and C_LONG/C_LAT\n    - Record C_LONG/C_LAT, mean price for each big city in to a dict (later used by feature engineering for test result set)\n3. For small city\n    - Get mode of long/lat which is in India\n    - Try to get it's neighbor city comparing with big city's C_LONG/C_LAT\n    - calculate the distance and get the mean long/lat as C_LONG/C_LAT\n    - Record C_LONG/C_LAT, mean price in to a dict (later used by feature engineering for test result set)\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"city_count = train_df.groupby('City', as_index=False)['Price/SQF'].count()\ncity_count.rename(columns={'Price/SQF':'count'},inplace=True)\n# print('City less than 100 price', city_count[city_count['count']<10].shape[0])\n# city_count = city_count[city_count['count'] > 10]\nlarge_cities = city_count[city_count['count'] > 10]['City'].unique().tolist()\nsmall_cities = city_count[city_count['count'] <= 10]['City'].unique().tolist()\n\ndef is_in_India(long, lat):\n    if 6.75 < long < 35.5 and 68.12 < lat < 97.42:\n        return 1\n    else:\n        return 0\n    \ncity_df = train_df.merge(city_count, on='City', how='inner')\ncity_df['IS_COOR_INDIA'] = city_df.apply(lambda x: is_in_India(x['LONGITUDE'], x['LATITUDE']), axis=1)\n\n\nlong_lat_list = []\n\ndef fix_long_lat(l, l_mode):\n    if l < l_mode - 0.3:\n        return l_mode\n    if l > l_mode + 0.3:\n        return l_mode\n    return l\n\ndef get_distance_to_center(long,lat,c_long, c_lat, is_wrong_coor):\n    if is_wrong_coor==0:\n        return 0.25\n    else:\n        return np.sqrt((long-c_long)**2 + (lat-c_lat)**2)\n\ncity_df_with_dist_list = []\n\nfor city in large_cities:\n    df = city_df[(city_df['City'] == city) & (city_df['IS_COOR_INDIA'] == 1)].copy()\n    long_mode = df['LONGITUDE'].mode().values[0]\n    lat_mode = df['LATITUDE'].mode().values[0]\n    count = df['count'].mode().values[0]\n    df['LONG_MODE'] = long_mode\n    df['LAT_MODE'] = lat_mode\n    df['LONGITUDE'] = df.apply(lambda x: fix_long_lat(x['LONGITUDE'],x['LONG_MODE']), axis=1)\n    df['LATITUDE'] = df.apply(lambda x: fix_long_lat(x['LATITUDE'],x['LAT_MODE']), axis=1)\n    \n    # city_suburbs.head()\n    \n    top_percentile = 1 if count < 10 else (10 if count < 100 else int(count * .01))\n    city_center_long_lat = df.sort_values(by='Price/SQF').head(top_percentile)[['LONGITUDE','LATITUDE']].mean().values.tolist()\n    city_center_long = city_center_long_lat[0]\n    city_center_lat = city_center_long_lat[1]\n    df['distance'] = df.apply(lambda x: get_distance_to_center(x['LONGITUDE'], x['LATITUDE'],city_center_long, city_center_lat, x['IS_COOR_INDIA']), \\\n                              axis=1)\n    city_df_with_dist_list.append(df)\n    city_mean_price = df['Price/SQF'].mean()\n    long_lat_list.append([city, city_center_long, city_center_lat, city_mean_price])\n\nlong_lat_center_df = pd.DataFrame(data=long_lat_list, columns=['City','C_LONG','C_LAT', 'Price/SQF Mean'])\n\n# long_lat_center_df\nfor city in small_cities:\n    df = city_df[city_df['City'] == city].copy()\n    if df[df['IS_COOR_INDIA']==1].shape[0] > 1:\n        long_mean = df[df['IS_COOR_INDIA'] == 1]['LONGITUDE'].mean()\n        lat_mean = df[df['IS_COOR_INDIA'] == 1]['LATITUDE'].mean()\n        ll_df = long_lat_center_df.copy()\n        ll_df['LONG_MEAN'] = long_mean\n        ll_df['LAT_MEAN'] = lat_mean\n        ll_df['distance'] = ll_df.apply(lambda x: get_distance_to_center(x['LONG_MEAN'], x['LAT_MEAN'], x['C_LONG'], x['C_LAT'], 1), axis=1)\n        min_distance = ll_df.sort_values(by='distance')['distance'].values[0]\n        df['distance'] = min_distance\n        city_df_with_dist_list.append(df)\n        city_center_long = df[df['IS_COOR_INDIA'] == 1]['LONGITUDE'].mean()\n        city_center_lat = df[df['IS_COOR_INDIA'] == 1]['LATITUDE'].mean()\n        city_mean_price = df[df['IS_COOR_INDIA'] == 1]['Price/SQF'].mean()\n        long_lat_list.append([city, city_center_long, city_center_lat, city_mean_price])\n    else:\n        df['distance'] = 0\n        city_df_with_dist_list.append(df)\n        city_mean_price = df['Price/SQF'].mean()\n        long_lat_list.append([city, None, None, city_mean_price])\n\nlong_lat_center_df = pd.DataFrame(data=long_lat_list, columns=['City','C_LONG','C_LAT', 'Price/SQF Mean'])\ncity_df_with_dist = pd.concat(city_df_with_dist_list)\ncity_df_with_dist.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we've derived below feature to represent the location factor\n* distance: how far away the house from CBD (own city or neighbor if the long/lat is wrong)\n* Price/SQF mean: baseline house price for each city\n\n### Let's verify the distribution for each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(ncols=2,nrows=1,dpi=100,figsize=(20,5))\ndf = city_df_with_dist[city_df_with_dist['Price/SQF'] < 1500] # remove outlier\ndf = df.merge(long_lat_center_df, on='City', how='inner')\ndf = df[df['Price/SQF Mean'] < 1500] \nsns.scatterplot(x=df['distance'], y=df['Price/SQF'], ax=ax[0])\nsns.scatterplot(x=df['Price/SQF Mean'], y=df['Price/SQF'], ax=ax[1])\nax[0].set_title('Distance')\nax[1].set_title('Price/SQF Mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we can start train the model, here I'm using XGBBoosting Regressor with hyper param tuning\n### Here are feature being used\n* POSTED_BY_CODE\n* UNDER_CONSTRUCTION\n* RERA\n* BHK_NO.\n* RESALE\n* distance\n* SQUARE_FT \n* Price/SQF Mean'\n\n### Target\n* TARGET(PRICE_IN_LACS)    -- here I'm not predicting Price/SQT, because seems there's size effect, small size might give higher unit price, and vice versa"},{"metadata":{},"cell_type":"markdown","source":"#### Setup param tuning function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def hyper_param_tuning(X_train, X_test, Y_train, Y_test):\n    params = {\n        # Parameters that we are going to tune.\n        'max_depth':6,\n        'min_child_weight': 1,\n        'eta':.3,\n        'subsample': 1,\n        'colsample_bytree': 1,\n        # Other parameters\n        'objective':'reg:squarederror',\n    }\n    dtrain = xgb.DMatrix(X_train, label=Y_train)\n    dtest = xgb.DMatrix(X_test, label=Y_test)\n\n    params['eval_metric'] = \"mae\"\n    num_boost_round = 999\n\n    gridsearch_params = [\n        (max_depth, min_child_weight)\n        for max_depth in range(5,20)\n        for min_child_weight in range(3,10)\n    ]\n    min_mae = float(\"Inf\")\n    best_params = None\n    for max_depth, min_child_weight in gridsearch_params:\n        print(\"CV with max_depth={}, min_child_weight={}\".format(\n                                 max_depth,\n                                 min_child_weight))\n        # Update our parameters\n        params['max_depth'] = max_depth\n        params['min_child_weight'] = min_child_weight\n        # Run CV\n        cv_results = xgb.cv(\n            params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            seed=42,\n            nfold=5,\n            metrics={'mae'},\n            early_stopping_rounds=10\n        )\n        # Update best MAE\n        mean_mae = cv_results['test-mae-mean'].min()\n        boost_rounds = cv_results['test-mae-mean'].argmin()\n        print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n        if mean_mae < min_mae:\n            min_mae = mean_mae\n            best_params = (max_depth,min_child_weight)\n    print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"city_df_with_dist_df = city_df_with_dist.merge(long_lat_center_df, on='City', how='inner')\nselected_cols = ['POSTED_BY_CODE', 'UNDER_CONSTRUCTION', 'RERA', 'BHK_NO.','RESALE','distance', 'SQUARE_FT', 'Price/SQF Mean'] # removed READY_TO_MOVE\ntarget_col = 'TARGET(PRICE_IN_LACS)'\nX = city_df_with_dist_df[selected_cols]\nY = city_df_with_dist_df[target_col]\n\n# Create train & test sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.25, random_state=1)\nhyper_param_tuning(X_train, X_test, Y_train, Y_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Start training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_r = xgb.XGBRegressor(objective ='reg:squarederror', \n                  max_depth=12, min_child_weight=3)\nxgb_r.fit(X_train, Y_train) \n  \n# Predict the model \nY_test_pred = xgb_r.predict(X_test) \n# test_predictedvalues = np.exp(test_predictedvalues) - 1\n\nplt.figure(figsize=(8,8))\nplt.scatter(Y_test, Y_test_pred)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.tight_layout()\n\n# RMSE Computation \nacc = xgb_r.score(X_test, Y_test)\nprint(\"Accuracy is \", acc)\nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_test_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_test_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_test_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The diagram looks in very good shape, but if looking at it carefully, there are some negative values at the bottom. This is not something we want (Doesn't make sense to have negative house price.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"(Y_test_pred < 0).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### To avoid that, we can try to do a transformation on Y -> np.log(Y+1) instead. \n### So let's try it again with the transformation"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"Y_train = np.log(1+Y_train)\nY_test = np.log(1+Y_test)\nhyper_param_tuning(X_train, X_test, Y_train, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiation \n# objective = 'count:poisson'\nxgb_r = xgb.XGBRegressor(objective ='reg:squarederror', \n                  max_depth=9, min_child_weight=5)\nxgb_r.fit(X_train, Y_train) \n\n# Predict the model\nY_test_pred = xgb_r.predict(X_test) \n# test_predictedvalues = np.exp(test_predictedvalues) - 1\n\nplt.figure(figsize=(8,8))\nplt.scatter(Y_test, Y_test_pred)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.tight_layout()\n\n# RMSE Computation \nacc = xgb_r.score(X_test, Y_test)\nprint(\"Accuracy is \", acc)\nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_test_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_test_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_test_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cool, think we have a model now. Just redo the feature engineering for the test.csv\n1. Label encoding for categorical field: POSTED_BY, BHK_NO.\n2. Enrich with field: City\n3. Derive distance and Price/SQF Mean (we can query from the dict created in previously)\n4. If there's a new city in test.csv, then we get its neighbor city, and get Price/SQF Mean from neighbor city and calcualte distance based on C_LONG/C_LAT of neighbor city"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_file = '/kaggle/input/house-price-prediction-challenge/test.csv'\ntest_df = pd.read_csv(test_file)\n\nposted_by_map = {'Owner':1, 'Dealer':2, 'Builder':3}\ntest_df['POSTED_BY_CODE'] = test_df['POSTED_BY'].map(posted_by_map)\ntest_df['BHK_NO.'] = test_df['BHK_NO.'].apply(lambda x: 2 if x > 7 else x)\ntest_df['City'] = test_df['ADDRESS'].apply(lambda x: x.split(',')[-1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"city_count = test_df.groupby('City', as_index=False)['POSTED_BY'].count()\ncity_count.rename(columns={'POSTED_BY':'count'},inplace=True)\n\ndef is_in_India(long, lat):\n    if 6.75 < long < 35.5 and 68.12 < lat < 97.42:\n        return 1\n    else:\n        return 0\n    \ncity_df = test_df.merge(city_count, on='City', how='inner')\ncity_df['IS_COOR_INDIA'] = city_df.apply(lambda x: is_in_India(x['LONGITUDE'], x['LATITUDE']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def get_distance_to_center(long,lat,c_long, c_lat, is_wrong_coor):\n    if is_wrong_coor == 0:\n        return 0\n    else:\n        return np.sqrt((long-c_long)**2 + (lat-c_lat)**2)\n    \ncity_df = city_df.merge(long_lat_center_df, on='City', how='left')\ncity_df['distance'] = city_df.apply(lambda x: get_distance_to_center(x['LONGITUDE'], x['LATITUDE'], x['C_LONG'], x['C_LAT'], x['IS_COOR_INDIA']), axis=1)\nprint('Null Price/SQF mean',city_df['Price/SQF Mean'].isnull().sum())\nprint('We need to derive those with below code')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef fix_long_lat(l, l_mode):\n    if l < l_mode - 0.3:\n        return l_mode\n    if l > l_mode + 0.3:\n        return l_mode\n    return l\n\ndef get_distance_to_center(long,lat,c_long, c_lat, is_wrong_coor):\n    if is_wrong_coor == 0:\n        return 0.25\n    else:\n        return np.sqrt((long-c_long)**2 + (lat-c_lat)**2)\n\nlarge_cities = city_count[city_count['count'] > 10]['City'].unique().tolist()\nsmall_cities = city_count[city_count['count'] <= 10]['City'].unique().tolist()\n\ncity_df_with_dist_list = []\n\nfor city in large_cities:\n    df = city_df[city_df['City'] == city].copy()\n    long_mode = df['LONGITUDE'].mode().values[0]\n    lat_mode = df['LATITUDE'].mode().values[0]\n    count = df['count'].mode().values[0]\n    df['LONG_MODE'] = long_mode\n    df['LAT_MODE'] = lat_mode\n    df['LONGITUDE'] = df.apply(lambda x: fix_long_lat(x['LONGITUDE'],x['LONG_MODE']), axis=1)\n    df['LATITUDE'] = df.apply(lambda x: fix_long_lat(x['LATITUDE'],x['LAT_MODE']), axis=1)    \n#     print(df.columns)\n    df['distance'] = df.apply(lambda x: get_distance_to_center(x['LONGITUDE'], x['LATITUDE'],x['C_LONG'], x['C_LAT'], False), \\\n                              axis=1)\n    city_df_with_dist_list.append(df)\n    \n    \n# long_lat_center_df\nfor city in small_cities:\n    df = city_df[city_df['City'] == city].copy()\n    if df[df['IS_COOR_INDIA']==1].shape[0] > 1:\n        long_mean = df[df['IS_COOR_INDIA'] == 1]['LONGITUDE'].mean()\n        lat_mean = df[df['IS_COOR_INDIA'] == 1]['LATITUDE'].mean()\n        ll_df = long_lat_center_df.copy()\n        ll_df['LONG_MEAN'] = long_mean\n        ll_df['LAT_MEAN'] = lat_mean\n        ll_df['distance'] = ll_df.apply(lambda x: get_distance_to_center(x['LONG_MEAN'], x['LAT_MEAN'], x['C_LONG'], x['C_LAT'], True), axis=1)\n        min_distance = ll_df.sort_values(by='distance')['distance'].values[0]\n        df['distance'] = min_distance\n        city_df_with_dist_list.append(df)\n\n    else:\n        df['distance'] = 0\n        city_df_with_dist_list.append(df)\n\ncity_df_with_dist = pd.concat(city_df_with_dist_list)\n\n\ncity_with_null_mean_price = city_df_with_dist[city_df_with_dist['Price/SQF Mean'].isnull()]['City'].unique()\nprice_distance_map = {}\n\nfor city in city_with_null_mean_price:\n    df = city_df_with_dist[city_df_with_dist['City'] == city].copy()\n    if df[df['IS_COOR_INDIA']==1].shape[0] > 1:\n        long_mean = df[df['IS_COOR_INDIA'] == 1]['LONGITUDE'].mean()\n        lat_mean = df[df['IS_COOR_INDIA'] == 1]['LATITUDE'].mean()\n        ll_df = long_lat_center_df.copy()\n        ll_df['LONG_MEAN'] = long_mean\n        ll_df['LAT_MEAN'] = lat_mean\n        ll_df['distance'] = ll_df.apply(lambda x: get_distance_to_center(x['LONG_MEAN'], x['LAT_MEAN'], x['C_LONG'], x['C_LAT'], 1), axis=1)\n        values = ll_df.sort_values(by='distance')[['distance','Price/SQF Mean']].head(1).values.tolist()\n        min_distance = values[0][0]\n        price_mean = values[0][1]\n        price_distance_map[city] = (min_distance, price_mean)\n    else:\n        long_mean = df['LONGITUDE'].mean()\n        lat_mean = df['LATITUDE'].mean()\n        ll_df = long_lat_center_df.copy()\n        ll_df['LONG_MEAN'] = long_mean\n        ll_df['LAT_MEAN'] = lat_mean\n        ll_df['distance'] = ll_df.apply(lambda x: get_distance_to_center(x['LONG_MEAN'], x['LAT_MEAN'], x['C_LONG'], x['C_LAT'], 0), axis=1)\n        min_distance = ll_df.sort_values(by='distance')['distance'].values[0]\n        values = ll_df.sort_values(by='distance')[['distance','Price/SQF Mean']].head(1).values.tolist()\n        # wrong coordinate, find a closest city based on wrong coordinate, and assign distance 0\n        # Data quality issue on the raw data, can't do much about it\n        min_distance = 0 \n        price_mean = values[0][1]\n        price_distance_map[city] = (0, price_mean)\n\nimport math\n\ndef fillna_price_mean(city, price_mean):\n    if math.isnan(price_mean):\n        return price_distance_map[city][1]\n    return price_mean\n\ndef fillna_distance(city, distance):\n    if math.isnan(distance):\n        return price_distance_map[city][0]\n    return distance\n\ncity_df_with_dist['Price/SQF Mean'] = city_df_with_dist.apply(lambda x: fillna_price_mean(x['City'],x['Price/SQF Mean']), axis=1)\ncity_df_with_dist['distance'] = city_df_with_dist.apply(lambda x: fillna_distance(x['City'],x['distance']), axis=1)\ncity_df_with_dist.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction with the trained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_cols = ['POSTED_BY_CODE', 'UNDER_CONSTRUCTION', 'RERA', 'BHK_NO.','RESALE','distance', 'SQUARE_FT', 'Price/SQF Mean'] # removed READY_TO_MOVE\n# target_col = 'Price/SQF'\nX = city_df_with_dist[selected_cols]\n\ntest_predictedvalues = xgb_r.predict(X) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Do not forget to transform back from np.log(Y+1) -> np.exp(Y_pred) - 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictedvalues = np.exp(test_predictedvalues) - 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(test_predictedvalues).to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\n### 1. Thanks for the dataset and the contest. Dataset is pretty clean and data structure is easy to understand\n### 2. Useful features for house price prediction is (rank by importance) ADDRESS, LONGITUDE, LATITUDE, POSTED_BY, and others)\n### 3. There are some discussions saying that longitude and latitude might not be useful. But I think they are very important features. Location always matters the most for a property. Would be great if the long/lat value is correct. Without the data quality issue, I believe we could have a better model here."},{"metadata":{},"cell_type":"markdown","source":"## Thanks for reading and please upvote if you like it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}