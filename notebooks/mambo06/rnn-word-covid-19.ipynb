{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The Knowledge - a RNN covid-19 knowledge base\nThe final model will try to build a knowledge base that answer question from user input. The answer is a resume from english papers that already collected from the competition provider.\n\n## Steps\nMainly, there are two big step to reproduce the model.\n1. Data Preparation\nThis step need the most time. in Kaggle it will take at least 4-5 hours to finish.\nThe succesful of the model is mainly due the succesfullness of this task. The most importance one is the regex part.\n2. model build\nthe model use RNN model like the one from tensorflow page. however instead of using char, it use word level.\neach epoch need 7 minute to finisg eith GPU or 3,5 haours without gpu. \n\nTotal time to run with GPU for 20 epoch is 4-5 hours\n\n## Pros and cons\n### Pros\nThe model takes all text from the the dataset. To handle this, the data preparation task need to create temporary files and del objects regurally. \n\nThe model takes only english literature by using pycld2 library.\n\n### Cons\nThe data need to be tuned with more powerful machine and will takes time. tehr esult is not the best and i belive can be improved better.\n\nThe regular expression need to be tuned to get better words.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation\nUsing common data preparation routine to properly extract the data.\n### Import library"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom nltk.stem import WordNetLemmatizer   \nlemmatizer = WordNetLemmatizer() \n\nfrom nltk.tokenize import word_tokenize \nimport re\nimport json\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import pycld2 library\ninstall pycld2 library because it is not available by default in kaggle environment. This is a library that will be used to extract english only papers."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U pycld2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pycld2 as cld2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build class to extract english only papers. I assumed that doc with 70% english is an english document."},{"metadata":{"trusted":true},"cell_type":"code","source":"def checkEnglish(text):\n    isReliable, textBytesFound, details, vectors = cld2.detect(text, returnVectors=True)\n    best = 0\n    for each in vectors:\n        if (each[-1] == 'en'):\n            best += each[1]\n    if best != 0 :\n        if (best/len(text) > 0.7): # only if the doc has many recognized english\n            return True\n    return False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import List of Files\nimport list of file from kaggle dataset. this will takes everything without any filetering."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\njson_file = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename.split(\".\")[1] == 'json' :\n            json_file.append(os.path.join(dirname, filename))\n#         print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(json_file)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text filtering\nGets each text document that contain the text paper from each file,the do several treatment:\n1. regex filtering\n2. transform to lower case\n3. check english\n\nNote the check english is the last one, it is because i hope the previous treatment can extract the words better.\nAfter several trial, the regex filtering can improve the model alot. As better regex result in better and more human readable prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"total= len(json_file)\ni = 0\ndb = []\nfor item in json_file:\n    i += 1\n    with open(item,\"r\") as output:\n        file_1 = json.load(output)\n#     print ( \"Process Doc # \" + str(i) + \" left \" + str(total - i) + \" docs.\")\n#     file_1= pd.read_json(item,orient = 'index')''\n\n    for texts in file_1[\"body_text\"]:\n#         print(texts['text'])\n        text_temp = re.sub(\"(\\s\\.)|(\\.\\s)\",\" . \", texts['text'])\n        text_temp = ' '.join([item for sublist in re.findall(\"(\\/{2,}[\\w\\d\\/]+[-.\\w\\d]+)|([\\w\\d]+[\\s.?,]{0,})|(\\d+)|(\\w)|(\\s[.,?])\",\n                                        text_temp) for item in sublist if len(item) > 0]\n                           ).lower()\n        text_temp = re.sub(\"\\s{2,}|,\",\" \", text_temp)\n        text_temp = re.sub(\"coronavirus\",\"corona virus\", text_temp)\n        if(checkEnglish(text_temp)):\n            db.append(text_temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(db)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> free memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save Memory by Splitting\nTo save the memory the text object (db) is splitted and dumped into several files. After this all task will use the load and unload files method. we use pandas as panads support gzip compression with pickle."},{"metadata":{"trusted":true},"cell_type":"code","source":"# split dataset to save memory\ni = 0\nwhile i < len(db) :\n    pd.DataFrame(db[i:i+100000-1]).to_pickle(\"/kaggle/working/df_\" + str(i), compression='gzip')\n    i += 100000\n    \n# del db object is it is very big\ndel db","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"free memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatize\nWe use lemmatize to get the base word.This can be helpful to decrease words count. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper function to lemmatize\ndef text_process(text):\n    temp = word_tokenize(text)\n    for i in range(len(temp)):\n        temp[i] = lemmatizer.lemmatize(temp[i])\n    return temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply lemitizer\nset_temp = ()\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        if filename[:3] == 'df_' :\n            print(\"Processing file : \" + os.path.join(dirname, filename))\n            df_temp = pd.read_pickle(os.path.join(dirname, filename), compression='gzip')\n            df_temp = df_temp.apply(lambda x : word_tokenize(x.values[0]), axis=1)\n            df_temp.to_pickle(os.path.join(dirname, filename), compression='gzip')\n            \n                                     \n#             json_file.append(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del df_temp\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate BoW( Bag of Words)\nGenerate the set of words for first initialisation of BoW, the sort it to make it understandable. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate set of word, in case needed\nset_temp = set()\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        if filename[:3] == 'df_' :\n            print(\"Processing file : \" + os.path.join(dirname, filename))\n            df_temp = pd.read_pickle(os.path.join(dirname, filename), compression='gzip')\n            df_temp.apply(lambda x : set_temp.update(set(x)))\n            del df_temp\n#             print(set_temp)\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sort it for easy understanding of the document and set."},{"metadata":{"trusted":true},"cell_type":"code","source":"set_temp = sorted(list(set_temp))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set_temp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"get insight of the BoW"},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 100\n\" \".join(set_temp[i:i+100])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dimension reduction\nAs part of the task, we reduce the word list. For the model i pick only words that exist in more than 1% or the documnet.\nby doing this, i hope i can remove the typo and reduce train time."},{"metadata":{"trusted":true},"cell_type":"code","source":"char2count = {u:0 for i, u in enumerate(set_temp)}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"function to help generate BoW"},{"metadata":{"trusted":true},"cell_type":"code","source":"def char2count_helper(text):\n    global total_count\n    global char2count\n    total_count += 1\n    text = set(text)\n    for each in text:\n        char2count[each] += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate first BoW"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_count = 0\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        if filename[:3] == 'df_' :\n            print(\"Processing file : \" + os.path.join(dirname, filename))\n            df_temp = pd.read_pickle(os.path.join(dirname, filename), compression='gzip')\n            df_temp.apply(lambda x : char2count_helper(x))\n            del df_temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate final BoW that only contain 2-100% words in docs"},{"metadata":{"trusted":true},"cell_type":"code","source":"words_count = 0\nchar2count_clean = []\nfor key, item in char2count.items():\n    if ((item > total_count * 0.01) and (item < total_count*1) ): \n        words_count += 1\n        char2count_clean.append(key)\nprint(words_count)\n\n#BoW\nchar2idx = {u:i for i, u in enumerate(char2count_clean)}\n# idx2char = np.array(vocab)\nidx2char = {i:u for i, u in enumerate(char2count_clean)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"check some words of bow"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"viruses\" in char2idx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the model\nwe user RNN word level model to capture the history of a word / words. The train and label file is huge, it is not possible to load and train the data at the same time. The train is 100 seq , and the label also 100 seq. To manage this, i use one single dataset that consist 101 seq each row. A helper class is also created to handle train and label dataset, "},{"metadata":{"trusted":true},"cell_type":"code","source":"import psutil\npsutil.virtual_memory()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import the library"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport re\n# import scipy.sparse as sps\n# import sparse\nimport random as rd\nfrom tqdm.notebook import tqdm\nimport sys\nimport pandas as pd\nimport tensorflow as tf\n \n\nimport numpy as np\nimport os\nimport time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set the parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Batch size\nBATCH_SIZE = 128 * 2\n\n# len char of prediction\nlenData = 100\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n# Length of the vocabulary in chars\nvocab_size = len(char2count_clean)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nrnn_units = 1024\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Helper class to generate the dataset\nThe class generate dataset for x and label for the model. Each row of the original dataset consist of 101 seq. The details are :\n1. X is seq 1 to 100\n2. Label is  seq 2 to 101"},{"metadata":{"trusted":true},"cell_type":"code","source":"class helperDf():\n  def __init__(self, df, batch_size, coll_size):\n    # self.df = df.sample(frac=1).reset_index(drop=True)\n    self.df =df\n    self.batch_size = batch_size\n    self.coll_size = coll_size\n    self.pointer = 0\n    self.batch_idx = batch_size\n    self.epocs = 0\n\n  def _set_index_array(self):\n        self.index_array = np.arange(self.getDf)\n        if self.shuffle:\n            self.index_array = np.random.permutation(self.df)\n  \n  def __iter__(self):\n    self.pointer = 0\n    self.reset()\n    return self\n\n  def __next__(self):\n    if (self.batch_size + self.batch_idx > self.df.shape[0]) : self.reset()\n    return self.batch()\n\n  def __len__(self):\n    return self.df.shape[0]//self.batch_idx\n\n  def take(self, count):\n    counter = 0\n    self.pointer = 0\n    self.reset()\n    while counter < count :\n      yield self.batch()\n      counter += 1\n\n  def getDf(self):\n    return self.df\n\n    #  use list\n  # def data(self):\n  #   return np.array([sublist[:self.coll_size] for sublist in self.df[self.pointer:self.batch_size]])\n\n  # def target(self):\n  #   return np.array([sublist[1:self.coll_size+1] for sublist in self.df[self.pointer:self.batch_size]])\n\n\n# use pandas df\n  def data(self):\n    return self.df.loc[self.pointer:(self.batch_size-1),:(self.coll_size-1)].values\n\n  def target(self):\n    return self.df.loc[self.pointer:(self.batch_size-1 ),1:self.coll_size].values\n\n  # use numpy array\n  # def data(self):\n  #   return self.df[self.pointer:(self.batch_size),:(self.coll_size)]\n\n  # def target(self):\n  #   return self.df[self.pointer:(self.batch_size ),1:self.coll_size+1]\n\n  def addPointer(self):\n    self.pointer = self.batch_size\n    self.batch_size += self.batch_idx\n    self.epocs +=1\n\n  def reset(self):\n    self.pointer = 0\n    self.batch_size = self.batch_idx\n\n  def batch(self):\n    x = self.data()\n    y = self.target()\n    self.addPointer()\n    return (x,y)\n\n  def count(start=0, step=1):\n    # count(10) --> 10 11 12 13 14 ...\n    # count(2.5, 0.5) -> 2.5 3.0 3.5 ...\n    n = start\n    while True:\n        yield n\n        n += step\n        \n  def repeat(self, times=None):\n    if times is None:\n        while True:\n            yield self.df\n    else:\n      for i in xrange(times):\n        yield self.df\n\n  def on_epoch_end():\n    self._set_index_array()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Helper class for sampling\nprevious tries need sampling to reduce the dataset number. it is because the machine can not handle more than 1M rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"class buildData():\n  def __init__(self,lenRow):\n    self.count = lenRow\n    if (lenRow < 1000000) :\n        self.maxCount = lenRow\n    else:\n        self.maxCount = 1000000\n    self.df = pd.DataFrame(columns=[\"input\",\"target\"])\n    self.df = []\n    self.pbar = tqdm(total=self.maxCount)\n    self.pbar_total = tqdm(total=self.count)\n\n    print(\"generating sample (to save memory)\")\n    self.sampled = sorted(rd.sample(range(self.count),k=self.maxCount))\n    self.counter = 0\n    self.idxSampled = 0\n\n  def getCounter(self):\n    return self.counter\n\n  def getList(self):\n    return self.df\n\n  def build(self,text, lentext):\n    text = \" \".join(text)\n    text = re.sub('coronavirus', 'corona virus', text)\n    text = text.split()\n    step = lentext + 1\n    text = [item for item in text if item in char2idx]\n    for i in range(0,len(text),step):\n      if (len(text[i:]) > step) :\n        if (len(self.sampled) <= self.idxSampled) : break\n        if (self.counter == self.sampled[self.idxSampled]):\n          # for c in self.text[i:i+step]:\n            # self.df.append(char2idx[c])\n          input = np.array([char2idx[c] for c in text[i:i+step]])\n          # target = np.array([char2idx[c] for c in self.text[i+1:i+101]])\n          # self.df = self.df.append(pd.DataFrame([[input,target]]))\n          self.df.append(input)\n          self.pbar.update(1)\n          self.idxSampled += 1\n        self.counter += 1\n        self.pbar_total.update(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### generate sample dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# del generate\ngenerate = buildData(774905)\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        if filename[:3] == 'df_' :\n            print(\"Processing file : \" + os.path.join(dirname, filename))\n            # df_temp = 0\n            df_temp = pd.read_pickle(os.path.join(dirname, filename), compression='gzip')\n            # print()\n            df_temp.apply(lambda x : generate.build(x, lenData))\n            del df_temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate dataset with Generation Helper Class\nThe dataset object is the final dataset to be used for the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(generate.getList())\ndel generate\ndataset = helperDf(df, BATCH_SIZE,lenData)\ndel df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build model\nBuild RNN simple model. it is very basic model as i am not sure that complicated model can be handle with kaglle machine. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.GRU(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n  return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with tpu_strategy.scope():\n    model = build_model(\n      vocab_size = len(char2idx),\n      embedding_dim=embedding_dim,\n      rnn_units=rnn_units,\n      batch_size=BATCH_SIZE)\n    model.compile(optimizer='adam', loss=loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = '/kaggle/working' \n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train the model\ntrain the wodel with 10 epoch. "},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS=10\nhistory = model.fit(dataset,steps_per_epoch = len(dataset), epochs=EPOCHS, callbacks=[checkpoint_callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.train.latest_checkpoint(checkpoint_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction\nThe model accept word/words and generate 100 words as a paragraph. This model is not the best due to resource limitation. This model can answer almost anything as long as the word/word is included in BoW."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))\n\n# model.summary()\n\nimport tensorflow as tff\ndef generate_text(model, start_string):\n  # Evaluation step (generating text using the learned model)\n  # Number of characters to generate\n  num_generate = 100\n  start_string = start_string.split()\n\n  # Converting our start string to numbers (vectorizing)\n  input_eval = [char2idx[s] for s in start_string if s in char2idx]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # Empty string to store our results\n  text_generated = start_string\n\n  # Low temperatures results in more predictable text.\n  # Higher temperatures results in more surprising text.\n  # Experiment to find the best setting.\n  temperature = 1.0\n\n  # Here batch size == 1\n  model.reset_states()\n  for i in range(num_generate):\n      predictions = model(input_eval)\n      # remove the batch dimension\n      predictions = tff.squeeze(predictions, 0)\n\n      # using a categorical distribution to predict the character returned by the model\n      predictions = predictions / temperature\n      predicted_id = tff.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n      # We pass the predicted character as the next input to the model\n      # along with the previous hidden state\n      input_eval = tff.expand_dims([predicted_id], 0)\n\n      text_generated.append(idx2char[predicted_id])\n#   print(text_generated)\n  return (' '.join(text_generated))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"to try the model, words \" corona virus\" are given. The prediction is quite useful consider it only took 10 epoch."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(generate_text(model, start_string=u\"corona virus \"))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finish\nThanks You"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}