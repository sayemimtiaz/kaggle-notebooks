{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Quick and easy overview testing a couple of ML supervised learning models\n# as classifiers for the Mushroom dataset.\n\n# The dataset itself consists of multiple categorical features so guess what,\n# probably a tree will work wonders (don't need fancy deeplearning here)\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom bokeh.plotting import output_notebook, figure, show\nfrom bokeh.models import HoverTool, ColumnDataSource\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors as mcolors\n\n%matplotlib inline\noutput_notebook()\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/mushroom-classification/mushrooms.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find out the sizing of edible versus poisoneous mushrooms?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('class').size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode the categories - don't use LabelEncoder as all features should be treated equal\n# the below is a short cut for OneHotEncoding\n\nfor col in df.columns:\n    df = pd.get_dummies(df,prefix=col, columns = [col], drop_first=True)   \n \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We know that data has 'class' as the y (output or predicted value), so all other columns are features and col 0 is label","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,1:96]  # all rows, not col0 but all the remainng cols which are features\ny = df.iloc[:, 0]  # all rows, label col only","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if there are highly correlated features to reduce dimensionality","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# find correlations to target\ncorr_matrix = df.corr().abs()\nprint(corr_matrix['class_p'].sort_values(ascending=False).head(11))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See https://www.slideshare.net/rayborg/mushroom-tutorial for mushroom explanation.\n\nThe following 10 features explains a lot if the class is edible or not:\n\n* odor_n                        0.785557\n* odor_f                        0.623842\n* stalk-surface-above-ring_k    0.587658\n* stalk-surface-below-ring_k    0.573524\n* ring-type_p                   0.540469\n* gill-size_n                   0.540024\n* bruises_t                     0.501530\n* stalk-surface-above-ring_s    0.491314\n1. * spore-print-color_h           0.490229\n* ring-type_l                   0.451619\n\nParticularly we see that if a mushroom has no smell or a foul smell, then it's likely to explain edible/poisenous.\n\nWith these 10 features we will test how good a model we can create.\n\nLet's check the opposite - what features explains the least:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(corr_matrix['class_p'].sort_values(ascending=True).head(11))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting - if a mushroom has a green or pink cap color it says very little about edibility.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# first let's run all the data and setup models\n# to find a model that works the best\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n# evaluate each model in turn\nresults = []\nresultsmean = []\nresultsstddev = []\nnames = []\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n    resultsmean.append(cv_results.mean())\n    resultsstddev.append(cv_results.std())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resultsDf = pd.DataFrame(\n    {'name': names,\n     'mean': resultsmean,\n     'std dev': resultsstddev\n    }\n)\nresultsDf = resultsDf.sort_values(by=['mean'], ascending=False)\nprint(resultsDf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions using validation dataset using CART model\nmodel1 = DecisionTreeClassifier()\nmodel1.fit(X_train, Y_train)\npredictions = model1.predict(X_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot graph of feature importances\nfeat_importances = pd.Series(model1.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the features that are the most important in this model. Edible mushrooms tends to be those that have no odeour if in combo with a club shaped root.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* One could store this model using joblib for later use... https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/ and perhaps transfer to CoreML https://stackoverflow.com/questions/45291093/scikit-learn-convert-multi-output-decision-tree-to-coreml-model#45519253 in order to create a nifty little iOS app?\n\nSee also https://developer.apple.com/documentation/coreml/converting_trained_models_to_core_ml\n\nBut... 23 questions to an end user with 96 total answers (on avg. 4 options per question) is way too much. Let's see if we can reduce the problem...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# a desire to understand if we can compress feature dimensionality\n# using PCA\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA\nmodel3 = PCA(n_components=2)\npc = model3.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(pca.components_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"29 of the 96 answers are important that they explain most of the edibility.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"principalDf = pd.DataFrame(data = pc, columns = ['principal component 1', 'principal component 2'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"finalDf = pd.concat([principalDf, df[['class_p']]], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.matshow(pca.components_,cmap='viridis')\n#plt.yticks([0,1],['1st Comp','2nd Comp'],fontsize=10)\nplt.colorbar()\nplt.xticks(range(len(df.columns)),df.columns,rotation=65,ha='left')\nplt.tight_layout()\nplt.show()# ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [1, 0]\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['class_p'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Explained variation per principal component: {}'.format(model3.explained_variance_ratio_))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Univariate feature selection works by selecting the best features based on univariate statistical tests.\nWe can use sklearnâ€™s SelectKBest to select a number of features to keep. This method uses statistical tests to select features having the highest correlation to the target. Here we will keep the top 10 features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\n# feature extraction\nk_best = SelectKBest(score_func=f_classif, k=10)\n# fit on train set\nfit = k_best.fit(X_train, Y_train)\n# transform train set\nunivariate_features = fit.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.head()\nfeatureScores.columns = ['Column','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10, 'Score'))  #print 10 best features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}