{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter('ignore')\nsns.set(rc={'figure.figsize' : (12, 6)})\nsns.set_style(\"darkgrid\", {'axes.grid' : True})\nimport skimage\n\n# Импортируем TensorFlow и tf.keras\nimport tensorflow as tf\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/IMDB Dataset.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore the data\nLet's take a moment to understand the format of the data. The dataset comes unprocessed: each example is an array of words representing the movie review. Each label is a string value of either negative or positive sentiment of author of review."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of poitive and negative reviews\ndata.sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets encode labels: each label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.\nfrom sklearn import preprocessing\nlabel_encoder = preprocessing.LabelEncoder()\ndata['sentiment'] = label_encoder.fit_transform(data['sentiment'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, let's see the average number of words per sample\nplt.figure(figsize=(10, 6))\nplt.hist([len(sample) for sample in list(data['review'])], 50)\nplt.xlabel('Length of samples')\nplt.ylabel('Number of samples')\nplt.title('Sample length distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text feature extraction\nText Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n\nIn order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n\ntokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\ncounting the occurrences of tokens in each document.\nnormalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents.\nIn this scheme, features and samples are defined as follows:\n\neach individual token occurrence frequency (normalized or not) is treated as a feature.\nthe vector of all the token frequencies for a given document is considered a multivariate sample.\nA corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n\nWe call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."},{"metadata":{},"cell_type":"markdown","source":"Now use the **CountVectorizer** provided by the scikit-learn library to vectorize sentences. It takes the words of each sentence and creates a vocabulary of all the unique words in the sentences. This vocabulary can then be used to create a feature vector of the count of the words:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n# So, we get such structure:\n#        | word1  | word2  |  word3 | word4\n# text1  |   1    |    1   |   1    |   0\n# text2  |   0    |    1   |   1    |   0\n# text3  |   2    |    1   |   0    |   0\n# text4  |   0    |    0   |   0    |   1\nvect_texts = vectorizer.fit_transform(list(data['review']))\n# ['word1', 'word2', 'word3', 'word4']\nall_ngrams = vectorizer.get_feature_names()\nnum_ngrams = min(50, len(all_ngrams))\nall_counts = vect_texts.sum(axis=0).tolist()[0]\n\nall_ngrams, all_counts = zip(*[(n, c) for c, n in sorted(zip(all_counts, all_ngrams), reverse=True)])\nngrams = all_ngrams[:num_ngrams]\ncounts = all_counts[:num_ngrams]\n\nidx = np.arange(num_ngrams)\n\n# Let's now plot a frequency distribution plot of the most seen words in the corpus.\nplt.figure(figsize=(30, 30))\nplt.bar(idx, counts, width=0.8)\nplt.xlabel('N-grams')\nplt.ylabel('Frequencies')\nplt.title('Frequency distribution of ngrams')\nplt.xticks(idx, ngrams, rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, the highest frequency words are the stop words. We not consider them while performing our analysis, as they don't provide insights as to what the sentiment of the document might be or to which class a document might belong."},{"metadata":{},"cell_type":"markdown","source":"**tf–idf or TFIDF**, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\nThe tf–idf is the product of two statistics, term frequency and inverse document frequency.\n\n**Term frequency**\nIn the case of the term frequency tf(t,d), the simplest choice is to use the raw count of a term in a document, i.e., the number of times that term t occurs in document d.\n\n**Inverse document frequency**\nThe inverse document frequency is a measure of how much information the word provides, i.e., if it's common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient).\n\n**Term frequency–Inverse document frequency**\nThen tf–idf is calculated as:\n![](https://wikimedia.org/api/rest_v1/media/math/render/svg/fa3cf0b54c09151473641f8364c2da3480cc98f1)\n\nThe goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus."},{"metadata":{},"cell_type":"markdown","source":"Let's now prepare the data to feed into the model. For the data preparation step we will get bigrams and unigrams from the data and encode it using tf-idf. And will select the top 20000 features from the vector of tokens. Discard features that occurs less than two times, and will f_classif to get feature importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nNGRAM_RANGE = (1, 2)\nTOP_K = 20000\nTOKEN_MODE = 'word'\nMIN_DOC_FREQ = 2\n\ndef ngram_vectorize(texts, labels):\n    kwargs = {\n        'ngram_range' : NGRAM_RANGE,\n        'dtype' : 'int32',\n        'strip_accents' : 'unicode',\n        'decode_error' : 'replace',\n        'analyzer' : TOKEN_MODE,\n        'min_df' : MIN_DOC_FREQ,\n    }\n    # Learn Vocab from train texts and vectorize train and val sets\n    tfidf_vectorizer = TfidfVectorizer(**kwargs)\n    transformed_texts = tfidf_vectorizer.fit_transform(texts)\n    \n    # Select best k features, with feature importance measured by f_classif\n    # Set k as 20000 or (if number of ngrams is less) number of ngrams   \n    selector = SelectKBest(f_classif, k=min(TOP_K, transformed_texts.shape[1]))\n    selector.fit(transformed_texts, labels)\n    transformed_texts = selector.transform(transformed_texts).astype('float32')\n    return transformed_texts\n# Vectorize the data\nvect_data = ngram_vectorize(data['review'], data['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer()\ntr_texts = tfidf.fit_transform(data['review'])\ntr_texts.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split data to target (y) and features (X)\nX = vect_data.toarray()\ny = (np.array(data['sentiment']))\n\n# Here we split data to training and testing parts\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\nprint(\"Train dataset shape: {0}, \\nTest dataset shape: {1}\".format(X_train.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the model\nThe neural network is created by stacking layers—this requires two main architectural decisions:\n\n1. *How many layers to use in the model?*\n2. *How many hidden units to use for each layer?*\n\nIn this example, the input data consists of an array of word-probabilities. The labels to predict are either 0 or 1. Let's build a model for this problem:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras import models\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, let's create a function that returns the appropriate number of units and the activation for the last layer.\ndef get_last_layer_units_and_activation(num_classes):\n    if num_classes == 2:\n        activation = 'sigmoid'\n        units = 1\n    else:\n        activation = 'softmax'\n        units = num_classes\n    return units, activation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# input shape is the vocabulary count used for the movie reviews (10,000 words)\nDROPOUT_RATE = 0.2\nUNITS = 64\nNUM_CLASSES = 2\nLAYERS = 2\ninput_shape = X_train.shape[1:]\n\nop_units, op_activation = get_last_layer_units_and_activation(NUM_CLASSES)\n\nmodel = keras.Sequential()\n# Applies Dropout to the input\nmodel.add(Dropout(rate=DROPOUT_RATE, input_shape=input_shape))\nfor _ in range(LAYERS-1):\n    model.add(Dense(units=UNITS, activation='relu'))\n    model.add(Dropout(rate=DROPOUT_RATE))\n    \nmodel.add(Dense(units=op_units, activation=op_activation))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compile the model\nBefore the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n\n* **Loss function** —This measures how accurate the model is during training. We want to minimize this function to \"steer\" the model in the right direction.\n* **Optimizer** —This is how the model is updated based on the data it sees and its loss function.\n* **Metrics** —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified."},{"metadata":{"trusted":true},"cell_type":"code","source":"LEARNING_RATE = 1e-3\n\n# Compile model with parameters\nif NUM_CLASSES == 2:\n    loss = 'binary_crossentropy'\nelse:\n    loss = 'sparse_categorical_crossentropy'\noptimizer = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 100\nBATCH_SIZE = 128\n\n# Create callback for early stopping on validation loss. If the loss does\n# not decrease on two consecutive tries, stop training\ncallbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n\n# Train and validate model\n# To start training, call the model.fit method—the model is \"fit\" to the training data.\n# Note that fit() will return a History object which we can use to plot training vs. validation accuracy and loss.\nhistory = model.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test), verbose=1, batch_size=BATCH_SIZE, callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next, compare how the model performs on the test dataset:\ntest_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\nprint('Test loss:', test_loss)\nprint('Test accuracy:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot training and validation accuracy as well as loss.\ndef plot_history(history):\n    accuracy = history.history['acc']\n    val_accuracy = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs = range(1,len(accuracy) + 1)\n    \n    # Plot accuracy  \n    plt.figure(1)\n    plt.plot(epochs, accuracy, 'b', label='Training accuracy')\n    plt.plot(epochs, val_accuracy, 'g', label='Validation accuracy')\n    plt.title('Training and validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    \n    # Plot loss\n    plt.figure(2)\n    plt.plot(epochs, loss, 'b', label='Training loss')\n    plt.plot(epochs, val_loss, 'g', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\nplot_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Save the model\nThen we can use model.save(filepath) to save a Keras model into a single HDF5 file which will contain:\n* the architecture of the model, allowing to re-create the model\n* the weights of the model\n* the training configuration (loss, optimizer)\n* the state of the optimizer, allowing to resume training exactly where you left off."},{"metadata":{"trusted":true},"cell_type":"code","source":" # Save model\nmodel.save('IMDB_model_dropout_nn.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}