{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Loading"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\ntest = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Preprocessing\n\n## 2.1 Missing Value\n\n### Count Missing value by features"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing = train.isnull().sum().sort_values(ascending=False).reset_index()\nmissing.columns = ['features','missing_num']\nmissing['percentage'] = missing['missing_num']/train.shape[0]\nmissing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Among a total of 13 features,' + str((missing['missing_num']>0).sum())+ ' features contains missing values.')\nprint('And ' + str((missing['percentage']>0.3).sum()) + ' features contains over 30% missing values.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove features that have too many missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# here, set the threshold to 30%\nthr = 0.7*train.shape[0]\ntrain2 = train.dropna(thresh = thr, axis = 1) #drop columns with too many missing val\ntrain2.isnull().sum().sort_values(ascending=False).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fill in values for remaining features "},{"metadata":{"trusted":true},"cell_type":"code","source":"train2.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since features with missing values are all categorical, we can use mode or modeling (such as random forest) to fill in. Here, we simply use mode."},{"metadata":{"trusted":true},"cell_type":"code","source":"#gender\ntrain2.loc[train2['gender'].isnull(),'gender'] = train2['gender'].value_counts().index[0]\n\n#major_discipline\ntrain2.loc[train2['major_discipline'].isnull(),'major_discipline'] = train2['major_discipline'].value_counts().index[0]\n\n#education_level\ntrain2.loc[train2['education_level'].isnull(),'education_level'] = train2['education_level'].value_counts().index[0]\n\n#last_new_job\ntrain2.loc[train2['last_new_job'].isnull(),'last_new_job'] = train2['last_new_job'].value_counts().index[0]\n\n#enrolled_university\ntrain2.loc[train2['enrolled_university'].isnull(),'enrolled_university'] = train2['enrolled_university'].value_counts().index[0]\n\n#experience\ntrain2.loc[train2['experience'].isnull(),'experience'] = train2['experience'].value_counts().index[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2.isnull().sum().sort_values(ascending=False).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#city_development_index\nplt.boxplot(train2['city_development_index'],vert=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q3 =  train2['city_development_index'].describe()['75%']\nq1 =  train2['city_development_index'].describe()['25%']\niqr = q3-q1\ntrain2.loc[train2['city_development_index'] < q1 - 1.5*iqr,'city_development_index']= q1-1.5*iqr #reset outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training_hours \nplt.boxplot(train2['training_hours'],vert=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q3 =  train2['training_hours'].describe()['75%']\nq1 =  train2['training_hours'].describe()['25%']\niqr = q3-q1\ntrain2.loc[train2['training_hours'] < q1 - 1.5*iqr,'training_hours']= 200 #reset outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature Engineering - Encoding\n\n- One-hot encoding for gender, enrolled_university, major_discipline (nominal)\n- Hash encoding for city (deal with high cardinality)\n- Label encoding for relevent experience,education_level, experience, last_new_job (ordinal)\n\n\nMore on Encoding: https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/"},{"metadata":{"trusted":true},"cell_type":"code","source":"#one-hot encoding\ntrain2 = pd.get_dummies(train2, columns=['gender','enrolled_university','major_discipline'])\n#ohe_test = pd.get_dummies(test, columns=['gender','enrolled_university','major_discipline'])\n#train2,test = train2.align(ohe_test,join='left',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#hash encoding\nimport category_encoders as ce\nencoder_city = ce.HashingEncoder(cols=['city'])\ncity_he = encoder_city.fit_transform(train2['city'], train2['target'])\ntrain2=train2.drop(columns=['city'])\ntrain2= pd.concat([train2, city_he],axis=1)\n\n#city_he_test = encoder_city.transform(test['city'], test['target'])\n#test = test.drop(columns=['city'])\n#test = pd.concat([test, city_he_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#label encoding\nfrom sklearn.preprocessing import LabelEncoder as le\nfrom collections import defaultdict\nd = defaultdict(le)\n\nle_train = train2[['relevent_experience','education_level','experience','last_new_job']].apply(lambda x: d[x.name].fit_transform(x),axis=0)\n#le_test = test[['relevent_experience','education_level','experience','last_new_job']].apply(lambda x: d[x.name].transform(x) if type(x) == str else x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = train2.drop(columns=['relevent_experience','education_level','experience','last_new_job'])\ntrain2 = pd.concat([train2,le_train],axis=1)\n\n#test = test.drop(columns=['relevent_experience','education_level','experience','last_new_job'])\n#test = pd.concat([test, le_test], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 4. Oversampling Using SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter #summerize class distribution\nfrom imblearn.over_sampling import SMOTE\n\nX = train2.drop(columns=['target', 'enrollee_id'])\ny = train2['target']\n\n#summerize class distribution: before\ncounter = Counter(y)\nprint(counter)\n\n#Oversampling using SMOTE\nsmt = SMOTE(random_state=42)\nX,y = smt.fit_sample(X,y)\n\n#summerize class distribution: after\ncounter = Counter(y)\nprint(counter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create training set and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create models\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n\ndef Model(model, X_train, X_test, y_train, y_test, title):\n    \n    #train\n    model.fit(X_train, y_train)\n    \n    #predict\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n    \n    #evaluate\n    print(title + ' - training set - accuracy score: ', accuracy_score(y_train, y_train_pred))\n    print(title + ' - test set - accuracy score: ' , accuracy_score(y_test, y_test_pred))\n    print(title + ' - training set - confusion matrix: \\n' , confusion_matrix(y_train, y_train_pred))\n    print(title + ' - test set - confusion matrix: \\n' ,confusion_matrix(y_test, y_test_pred))\n  \n    \n#find important features\n\ndef ImportantFeatures(model):\n    model.fit(X_train, y_train)\n    importances = model.feature_importances_\n    features = X_train.columns.values\n    imp = pd.DataFrame({'Features': features, 'Importance': importances})\n    imp.sort_values(by='Importance')\n    \n    return imp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nModel(LogisticRegression(solver='lbfgs', max_iter=10000,random_state=42),X_train, X_test, y_train, y_test, 'Logistic Regression w/ SMOTE')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nModel(SVC(random_state=42), X_train, X_test, y_train, y_test, 'SVM w/ SMOTE')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3 GaussianNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nModel(GaussianNB(), X_train, X_test, y_train, y_test, 'GaussianNB w/ SMOTE')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.4 KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nModel(KNeighborsClassifier(), X_train, X_test, y_train, y_test, 'KNN w/ SMOTE')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.5 Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nModel(DecisionTreeClassifier(max_depth=8), X_train, X_test, y_train, y_test, 'Decision Tree w/ SMOTE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ImportantFeatures(DecisionTreeClassifier(max_depth=14))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.6 Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nModel(RandomForestClassifier(max_features=8,n_estimators=4000,max_depth=10,random_state=42), X_train, X_test, y_train, y_test, 'RandomForest w/ SMOTE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ImportantFeatures(RandomForestClassifier(max_features=8,n_estimators=4000,max_depth=10,random_state=42))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.7 XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nModel(XGBClassifier(random_state=42), X_train, X_test, y_train, y_test, 'XGBoost w/ SMOTE')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"## Top 3 Models\n\n1. XGBoost (training accuracy: ~87%, test accuracy: ~83%)\n2. Logistic Regression (training/test accuracy: ~82%)\n3. Random Forest(training accuracy: ~84%, test accuracy: ~81%)\n\n\n## The Most important Factor - city_development_index (Based on Random Forest)\n"},{"metadata":{},"cell_type":"markdown","source":"=================================\n\n### Key Takeaway\n\n- how to handle missing values \n- categorical encoding (https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/)\n- deal with unbalanced data (SMOTE: https://towardsdatascience.com/5-smote-techniques-for-oversampling-your-imbalance-data-b8155bdbe2b5#:~:text=Borderline%2DSMOTE%20is%20a%20variation,boundary%20between%20the%20two%20classes.)\n\n\nLastly, huge thanks to Huynh Dong Nguyen's notebook! Learned a lot from it!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}