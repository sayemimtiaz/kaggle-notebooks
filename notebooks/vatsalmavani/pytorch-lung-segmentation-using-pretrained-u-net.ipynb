{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Lung Segmentation using pretrained U-net**\n\n* For this notebook, UNet architecture with pre-trained ResNet34 as an encoder is used, I've used [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch) library which has many inbuilt segmentation architectures with different backbones.\n\n* The purpose of this notebook is to identify \"Pneumothorax\" or a collapsed lung from chest x-rays. Pneumothorax is a condition that is responsible for making people suddenly gasp for air, and feel helplessly breathless for no apparent reason. So ultimately, we want to develop a model to identify and segment pneumothorax from a set of chest radiographic images."},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"!pip install segmentation-models-pytorch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import defaultdict, Counter\n\nfrom PIL import Image,ImageFile\nimport albumentations as A\nimport matplotlib.pyplot as plt\n\nfrom sklearn import model_selection\nimport segmentation_models_pytorch as smp\n\nimport torch\nfrom torch import nn,optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Utility Functions**\n\n* Most Segmentation problems like this should have two images: input and mask. In this case of multiple objects, there will be multiple masks. In this dataset, we are provided with **RLE** instead. **RLE** stands for **Run Length Encoding** and is a way to represent binary mask to save space.\n\n* Here is the utility function which is used to create mask from this RLE data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_length_decode(rle, height=1024, width=1024, fill_value=1):\n    component = np.zeros((height, width), np.float32)\n    component = component.reshape(-1)\n    rle = np.array([int(s) for s in rle.strip().split(' ')])\n    rle = rle.reshape(-1, 2)\n    start = 0\n    for index, length in rle:\n        start = start+index\n        end = start+length\n        component[start: end] = fill_value\n        start = end\n    component = component.reshape(width, height).T\n    return component","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Create Dataset Class**\n\n* In this cell, the traditional Dataset class has been created. Please note that, this class is created in such way that it can applied to almost any segmentation problem.\n* Here training dataset is a CSV file consisting only ImageIds which are also filenames and other column contains RLE data. So, in the **init()** function we fatch the image ids and initialize some other parameters as well which are then used during **getitem()** method.\n* In the **getitem()** method Augmentation and preprocessing has been done. This method returns dictionary contains image and mask."},{"metadata":{"trusted":true},"cell_type":"code","source":"class SIIMDataset(Dataset):\n    \n    def __init__(self, df, data_dir, transform=None, preprocessing_fun=None, channel_first=True):\n        self.data_dir = data_dir\n        self.transform = transform                       # for augmentations\n        self.preprocessing_fun = preprocessing_fun       # preprocessing_fun to normalize images\n        self.channel_first = channel_first               # set channels as first dimension\n        self.image_ids = df.ImageId.values\n        self.group_by = df.groupby('ImageId')\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.image_ids[idx]\n        df = self.group_by.get_group(img_id)\n        annotations = df[' EncodedPixels'].tolist()\n        \n        img_path = os.path.join(self.data_dir, img_id + \".png\")\n        img = Image.open(img_path).convert('RGB')\n        img = np.array(img)\n\n        mask = np.zeros(shape=(1024,1024))\n        if annotations[0] != ' -1':\n            for rle in annotations:\n                mask += run_length_decode(rle)\n        mask = (mask >= 1).astype('float32')\n        mask = np.expand_dims(mask, axis=-1)\n        \n        # apply augmentation\n        if self.transform:\n            augmented = self.transform(image=img, mask=mask)\n            img = augmented['image']\n            mask = augmented['mask']\n\n        if self.preprocessing_fun:\n            img = self.preprocessing_fun(img)\n        \n        # convert shape from (width, height, channel) ----> (channel, width, height) \n        if self.channel_first:\n            img = np.transpose(img, (2, 0, 1)).astype(np.float32)\n            mask = np.transpose(mask, (2, 0, 1)).astype(np.float32)\n\n        return {\n            'image': torch.Tensor(img),\n            'mask': torch.Tensor(mask)\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Create Training and Evaluation Function**\n\n* In this section, Training and Evaluation function is created for one epoch. The functions takes model, dataloader, criterion(loss function), optimizer and returns average loss for one epoch."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train model for one epoch\n\ndef train(data_loader, model, criterion, optimizer):\n    model.train()\n    train_loss = 0\n    for data in tqdm(data_loader):\n        inputs = data['image']\n        labels = data['mask']\n\n        inputs = inputs.to(device, dtype=torch.float)\n        labels = labels.to(device, dtype=torch.float)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n    return train_loss/len(data_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate the model\n\ndef evaluate(data_loader, model, criterion):\n    model.eval()\n    eval_loss = 0\n    with torch.no_grad():\n        for data in tqdm(data_loader):\n            inputs = data['image']\n            labels = data['mask']\n\n            inputs = inputs.to(device, dtype=torch.float)\n            labels = labels.to(device, dtype=torch.float)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            eval_loss += loss.item()\n\n    return eval_loss/len(data_loader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Define train-val Dataset and Create DataLoader**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Intialize some useful variables\n\nDATA_DIR = '../input/siim-png-images/train_png'\ndata_csv = '../input/siim-acr-pneumothorax-segmentation-data/train-rle.csv'\nbatch_size = 32\n\nEncoder = 'resnet34'\nWeights = 'imagenet'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define augmentation and preprocessing-function(according to Encoder)\n\ntransform = A.Compose([\n    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=10, p=0.5),\n    A.OneOf([A.RandomGamma(gamma_limit=(90,110)),\n             A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1)], p=0.5),\n    A.Resize(width=224, height=224)\n])\n\nprep_fun = smp.encoders.get_preprocessing_fn(\n    Encoder,\n    Weights\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data into training and validation\n\ndf = pd.read_csv(data_csv)\ndf_train, df_val = model_selection.train_test_split(df, test_size=0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize Dataset\ntrain_dataset = SIIMDataset(df_train,\n                            DATA_DIR,\n                            transform = transform,\n                            preprocessing_fun = prep_fun)\n\nval_dataset = SIIMDataset(df_val,\n                          DATA_DIR,\n                          transform = transform,\n                          preprocessing_fun = prep_fun)\n\n# Create DataLoader\ntrain_loader = DataLoader(train_dataset,\n                          batch_size = batch_size,\n                          shuffle = True,\n                          num_workers = 8)\n\nval_loader = DataLoader(val_dataset,\n                        batch_size = batch_size,\n                        num_workers = 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore DataLoader\n\nprint('Training data Info:')\ndataiter = iter(train_loader)\ndata = dataiter.next()\nimages,labels = data['image'],data['mask']\nprint(\"shape of images : {}\".format(images.shape))\nprint(\"shape of labels : {}\".format(labels.shape))\n\nprint('\\nValidation data Info:')\ndataiter = iter(val_loader)\ndata = dataiter.next()\nimages,labels = data['image'],data['mask']\nprint(\"shape of images : {}\".format(images.shape))\nprint(\"shape of labels : {}\".format(labels.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Visualize Images**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def denoramlize(img):\n    img = img.permute(1,2,0)            # change shape ---> (width, height, channel)\n    mean = torch.FloatTensor([0.485, 0.456, 0.406])\n    std = torch.FloatTensor([0.229, 0.224, 0.225])\n    img = img*std + mean\n    img = np.clip(img,0,1)              # convert the pixel values range(min=0, max=1)\n    return img\n\ndef imshow(img, mask):\n    fig = plt.figure(figsize=(15, 10))\n    a = fig.add_subplot(1, 3, 1)\n    plt.imshow(denoramlize(img), cmap='bone')\n    a.set_title(\"Original x-ray image\")\n    plt.grid(False)\n    plt.axis(\"off\")\n\n    a = fig.add_subplot(1, 3, 2)\n    imgplot = plt.imshow(torch.squeeze(mask, dim=1).permute(1,2,0), cmap='binary')\n    a.set_title(\"The mask\")\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n\n    a = fig.add_subplot(1, 3, 3)\n    plt.imshow(denoramlize(img), cmap='bone')\n    plt.imshow(torch.squeeze(mask, dim=1).permute(1,2,0), cmap='binary', alpha=0.3)\n    a.set_title(\"Mask on the X-ray image\")\n\n    plt.axis(\"off\")\n    plt.grid(False)\n\n\ndef show_batch_image(dataloader, num_images):\n    data = next(iter(dataloader))\n    image,mask = data['image'],data['mask']\n    img_idx = torch.randint(0, dataloader.batch_size, (num_images,))\n    for i in img_idx:\n        imshow(image[i], mask[i])\n\nshow_batch_image(train_loader, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Create Loss Class**\n\n* Here, Dice Loss is define as well as Focal Loss is also created to get better results.\n* For this notebook, I have used the combination of two loss: diceloss and focalloss"},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_loss(input, target):\n    input = torch.sigmoid(input)\n    smooth = 1.0\n    iflat = input.view(-1)\n    tflat = target.view(-1)\n    intersection = (iflat * tflat).sum()\n    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma):\n        super().__init__()\n        self.gamma = gamma\n\n    def forward(self, input, target):\n        if not (target.size() == input.size()):\n            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n                             .format(target.size(), input.size()))\n        max_val = (-input).clamp(min=0)\n        loss = input - input * target + max_val + \\\n            ((-max_val).exp() + (-input - max_val).exp()).log()\n        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n        loss = (invprobs * self.gamma).exp() * loss\n        return loss.mean()\n\n\nclass MixedLoss(nn.Module):\n    def __init__(self, alpha, gamma):\n        super().__init__()\n        self.alpha = alpha\n        self.focal = FocalLoss(gamma)\n\n    def forward(self, input, target):\n        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n        return loss.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Train The Model**"},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Create Model\n\nmodel = smp.Unet(\n    encoder_name = Encoder,\n    encoder_weights = Weights,\n    in_channels = 3,\n    classes = 1,\n    activation = None\n)\n\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define loss function and Set Optimizer\n\ncriterion = MixedLoss(alpha = 10.0,\n                      gamma = 2.0)\n\noptimizer = optim.Adam(model.parameters(),\n                       lr = 0.0001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loop over all Epochs\n\nepochs = 15\n\nfor epoch in range(epochs):\n\n    train_loss = train(train_loader,\n                       model,\n                       criterion,\n                       optimizer)\n\n    val_loss = evaluate(val_loader,\n                        model,\n                        criterion)\n\n    print(f'Epoch: {epoch+1}')\n    print(f'Training Loss: {train_loss}, \\t Validation Loss: {val_loss}\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# **Finally, UPVOTE the notebook if you found it useful, feel free in comments**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}