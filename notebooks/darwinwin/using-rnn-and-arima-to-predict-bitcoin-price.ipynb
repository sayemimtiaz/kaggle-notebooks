{"cells":[{"metadata":{"_uuid":"8ff4a71bb0ceecbc818a759107c5c11c91ef8c85"},"cell_type":"markdown","source":"![](https://bitcoinist.com/wp-content/uploads/2018/06/shutterstock_1018654609.jpg)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"**Goal of this kernel is to compare NN and ARIMA modelling. We will be predicting Bitcoin prices with help of Bitcoin historical data.**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**There are 4 csv files. CSV files for select bitcoin exchanges for the time period of Jan 2012 to July 2018, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price. Timestamps are in Unix time. Timestamps without any trades or activity have their data fields forward filled from the last valid time period. If a timestamp is missing, or if there are jumps, this may be because the exchange (or its API) was down, the exchange (or its API) did not exist, or some other unforseen technical error in data reporting or gathering. **\n\ncoincheckJPY_1-min_data_2014-10-31_to_2018-06-27.csv\n\nbitflyerJPY_1-min_data_2017-07-04_to_2018-06-27.csv\n\ncoinbaseUSD_1-min_data_2014-12-01_to_2018-06-27.csv\n\nbitstampUSD_1-min_data_2012-01-01_to_2018-06-27.csv\n\n**All from different Bitcoin exchanges**"},{"metadata":{"_uuid":"794936030db685ac980a2a6f84c6196d26f6f5b4"},"cell_type":"markdown","source":"**RNN** To predict bitcoin prices"},{"metadata":{"trusted":true,"_uuid":"fe2f4a4d672d8fffc8569633ecde136b9b78fba3"},"cell_type":"code","source":"# First step, import libraries and then dataset\nimport numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8b0d7ec59102288537c49b56a4cad48d8d19f74"},"cell_type":"code","source":"# Import the dataset and encode the date\ndf = pd.read_csv(\"../input/coinbaseUSD_1-min_data_2014-12-01_to_2018-11-11.csv\")\ndf['date'] = pd.to_datetime(df['Timestamp'],unit='s').dt.date\ngroup = df.groupby('date')\nReal_Price = group['Weighted_Price'].mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75816fc71b5114991678b59ab03e468b44c78e1b"},"cell_type":"markdown","source":"Bitcoin predictions are going to be for a month, that is why we need to split the dataset accordingly"},{"metadata":{"trusted":true,"_uuid":"e1acf607516505a536a4432d99e1e0c114edc1b5"},"cell_type":"code","source":"# split data\nprediction_days = 30\ndf_train= Real_Price[len(Real_Price)-prediction_days:]\ndf_test= Real_Price[:len(Real_Price)-prediction_days]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cbcda5a498a5b2d5a73f858cf5e00647d14d6b0"},"cell_type":"markdown","source":"Some pre-processing is also necessary:\n"},{"metadata":{"trusted":true,"_uuid":"f67e6eb9b0bdfab4f23040ff37e054eb3e40ee38"},"cell_type":"code","source":"# Data preprocess\ntraining_set = df_train.values\ntraining_set = np.reshape(training_set, (len(training_set), 1))\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\ntraining_set = sc.fit_transform(training_set)\nX_train = training_set[0:len(training_set)-1]\ny_train = training_set[1:len(training_set)]\nX_train = np.reshape(X_train, (len(X_train), 1, 1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5838cc546f91700965eb078dd03d2d33003120cb"},"cell_type":"markdown","source":"Now keras  to build the rNN, Long short-term memory!!! LSTM"},{"metadata":{"trusted":true,"_uuid":"68354ae10f922d378e1d887b54cdd78dec78ae84"},"cell_type":"code","source":"# Importing the Keras libraries and packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\n# Initialising the RNN\nregressor = Sequential()\n\n# Adding the input layer and the LSTM layer\nregressor.add(LSTM(units = 4, activation = 'sigmoid', input_shape = (None, 1)))\n\n# Adding the output layer\nregressor.add(Dense(units = 1))\n\n# Compiling the RNN\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the RNN to the Training set\nregressor.fit(X_train, y_train, batch_size = 5, epochs = 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f161a3181047b949b80bcf045704a13aaf4fa0c"},"cell_type":"markdown","source":"**NOTE!!!** Key thing is following and that is why NN COULD \"fail\". WE used values of today to predict the future values. That is not really failure of NN but jsut goes to show that we need to think about what are we \"feeding\" our NN with. Because it can happen that NN will only learn that price will be slightly higher than yesterdays price. Which is true, except when it is not. Than we fail big."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set = df_test.values[1:]\nsc = MinMaxScaler()\ninputs = np.reshape(df_test.values[0:len(df_test)-1], (len(test_set), 1))\ninputs = sc.transform(inputs)\ninputs = np.reshape(inputs, (len(inputs), 1, 1))\npredicted_BTC_price = regressor.predict(inputs)\npredicted_BTC_price = sc.inverse_transform(predicted_BTC_price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fc5d83199f99442778ca89d05720962e3001ad7"},"cell_type":"code","source":"# Visualising the results\nplt.figure(figsize=(25,15), dpi=80, facecolor='w', edgecolor='k')\nax = plt.gca()  \nplt.plot(test_set, color = 'red', label = 'Real BTC Price')\nplt.plot(predicted_BTC_price, color = 'blue', label = 'Predicted BTC Price')\nplt.title('BTC Price Prediction', fontsize=40)\ndf_test = df_test.reset_index()\nx=df_test.index\nlabels = df_test['date']\nplt.xticks(x, labels, rotation = 'vertical')\nfor tick in ax.xaxis.get_major_ticks():\n    tick.label1.set_fontsize(18)\nfor tick in ax.yaxis.get_major_ticks():\n    tick.label1.set_fontsize(18)\nplt.xlabel('Time', fontsize=40)\nplt.ylabel('BTC Price(USD)', fontsize=40)\nplt.legend(loc=2, prop={'size': 25})\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a1a39899c2dfece8d3e8fca4c74d92bb4eeb3f4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82c77aef7e63988f53571fb17af0ad35f0921a41"},"cell_type":"markdown","source":"**ARIMA** Let us first go through theoretical part of ARIMA. (NN should be already familiar, if not visit my other kernels"},{"metadata":{"_uuid":"80110939f554828388e6baddfd0c0dd300d2bfdc"},"cell_type":"markdown","source":"An **ARIMA** model is a class of statistical models for analyzing and forecasting time series data. ARIMA model is one model for non-stationarity. It assumes that the data becomes stationary after differencing.\n\n**ARIMA** is an acronym that stands for AutoRegressive Integrated Moving Average. It is a generalization of the simpler AutoRegressive Moving Average and adds the notion of integration.\n\n\n\nThese acronyms describe it pretty well:\n1. **AR**: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\n2.**I**: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n3. **MA**: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n\n\n\nEach of these components are explicitly specified in the model as a parameter. A standard notation is used of ARIMA(p,d,q) where the parameters are substituted with integer values to quickly indicate the specific ARIMA model being used.\n\nParameters are defined as follows:\n\n1. **p**: The number of lag observations included in the model, also called the lag order.\n2. **d**: The number of times that the raw observations are differenced, also called the degree of differencing.\n3. **q**: The size of the moving average window, also called the order of moving average.\n\n\n"},{"metadata":{"_uuid":"81c4aa3b2ddd5c6a093af0f88f23dfae3f9274f0"},"cell_type":"markdown","source":"**IMPORTANT**\n\nAdopting an ARIMA model for a time series assumes that the underlying process that generated the observations is an ARIMA process. This may seem obvious, but helps to motivate the need to confirm the assumptions of the model in the raw observations and in the residual errors of forecasts from the model."},{"metadata":{"_uuid":"d1c62bd535618848bfbdb7fcca0c2b0f8d24e23f"},"cell_type":"markdown","source":"But how do we check that? And how to de determine the parameters p,d,q in the model?\nFirst of all we need to make sure that the time-series is stationary, thats where differencing comes into place (degree corrects the level of non-stationarity if possible) And model parameters can be determined with the Box-Jenkins Method.\n\nBasicaly we have the following situation:\n1. Define the model by calling ARIMA() and passing in the p, d, and q parameters.\n2. The model is prepared on the training data by calling the fit() function.\n3. Predictions can be made by calling the predict() function and specifying the index of the time or times to be predicted."},{"metadata":{"_uuid":"a8388d50b20f2796bc198faa7371f07e19dba792"},"cell_type":"markdown","source":"How does **Box-Jenkins Method** work?\n[https://machinelearningmastery.com/gentle-introduction-box-jenkins-method-time-series-forecasting/](http://)\n\n"},{"metadata":{"_uuid":"8c4c1f68774520cc54384f6e834ca093824b44bb"},"cell_type":"markdown","source":"Let us start coding"},{"metadata":{"_uuid":"b3a99c0863aefa585d4e96090269c123b7d19646","trusted":true},"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom scipy import stats\nimport statsmodels.api as sm\nimport warnings\nfrom itertools import product\nfrom datetime import datetime\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-poster')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d23167cc675fce980d145509359e7030295b86b1","trusted":true},"cell_type":"code","source":"# Load data\ndf = pd.read_csv(\"../input/coinbaseUSD_1-min_data_2014-12-01_to_2018-11-11.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"826c21a5b2a68b6514a2b603f7cac149c0d25567"},"cell_type":"markdown","source":"We need to transform our index into time data and then split the time intervals"},{"metadata":{"trusted":true,"_uuid":"eb9db2a75b46598968ec3874db742dfbf0b1dd71"},"cell_type":"code","source":"# Unix-time to \ndf.Timestamp = pd.to_datetime(df.Timestamp, unit='s')\n\n# Resampling to daily frequency\ndf.index = df.Timestamp\ndf = df.resample('D').mean()\n\n# Resampling to monthly frequency\ndf_month = df.resample('M').mean()\n\n# Resampling to annual frequency\ndf_year = df.resample('A-DEC').mean()\n\n# Resampling to quarterly frequency\ndf_Q = df.resample('Q-DEC').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5ee2826751fb6e1432fbd51d4072e489cb4e49d"},"cell_type":"markdown","source":"Visualize the trend"},{"metadata":{"trusted":true,"_uuid":"374a3256d727b011f6b256e97a7f71958c5f8433"},"cell_type":"code","source":"# PLOTS\nfig = plt.figure(figsize=[15, 7])\nplt.suptitle('Bitcoin exchanges, mean USD', fontsize=22)\n\nplt.subplot(221)\nplt.plot(df.Weighted_Price, '-', label='By Days')\nplt.legend()\n\nplt.subplot(222)\nplt.plot(df_month.Weighted_Price, '-', label='By Months')\nplt.legend()\n\nplt.subplot(223)\nplt.plot(df_Q.Weighted_Price, '-', label='By Quarters')\nplt.legend()\n\nplt.subplot(224)\nplt.plot(df_year.Weighted_Price, '-', label='By Years')\nplt.legend()\n\n# plt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ccf91e4ba1372d1596ebb4c3b20e9ac7cf9ea89"},"cell_type":"markdown","source":"**Stationarity check and STL-decomposition of the series*** Lower the p value the better. Stationarity is our models main assumption and dickey fuller is just hypothesis test of the unit root test"},{"metadata":{"trusted":true,"_uuid":"8468d27b5ffcbc4005e2b48aee7dd4dd2fe40d11"},"cell_type":"code","source":"plt.figure(figsize=[15,7])\nsm.tsa.seasonal_decompose(df_month.Weighted_Price).plot()\nprint(\"Dickey–Fuller test: p=%f\" % sm.tsa.stattools.adfuller(df_month.Weighted_Price)[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cee7a1d09c18f0fcb7d42cd9f9db3e52fa2fb62"},"cell_type":"markdown","source":"Obviously not stationary, hence we ought transform our data. First Box-cox transformation then check the test"},{"metadata":{"trusted":true,"_uuid":"926c1613f39ab3155108e59be0ca427c50754024"},"cell_type":"code","source":"# Box-Cox Transformations\ndf_month['Weighted_Price_box'], lmbda = stats.boxcox(df_month.Weighted_Price)\nprint(\"Dickey–Fuller test: p=%f\" % sm.tsa.stattools.adfuller(df_month.Weighted_Price)[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10f15f389aa16851bc4541c37841fd392ab47d97"},"cell_type":"markdown","source":"We need another transformation. Seasonal differentiation"},{"metadata":{"trusted":true,"_uuid":"de97769ac38c8cecaa6e1440659e9c1af6eead4c"},"cell_type":"code","source":"# Seasonal differentiation\ndf_month['prices_box_diff'] = df_month.Weighted_Price_box - df_month.Weighted_Price_box.shift(12)\nprint(\"Dickey–Fuller test: p=%f\" % sm.tsa.stattools.adfuller(df_month.prices_box_diff[12:])[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f4cf9ff3abdeced446e71f90988c87f7c087804"},"cell_type":"markdown","source":"Again series is not stationary, finally let us try regular differentiation"},{"metadata":{"trusted":true,"_uuid":"8f3b844f3fd7b63812f45d2d722ee753c1fd2630"},"cell_type":"code","source":"# Regular differentiation\ndf_month['prices_box_diff2'] = df_month.prices_box_diff - df_month.prices_box_diff.shift(1)\nplt.figure(figsize=(15,7))\n\n# STL-decomposition\nsm.tsa.seasonal_decompose(df_month.prices_box_diff2[13:]).plot()   \nprint(\"Dickey–Fuller test: p=%f\" % sm.tsa.stattools.adfuller(df_month.prices_box_diff2[13:])[1])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1a86ef6e88d1201e7e9f9c45cdf017e114c0018"},"cell_type":"markdown","source":"Now we need to make model selection, with help of :\n* Autocorrelation Function (ACF). The plot summarizes the correlation of an observation with lag values. The x-axis shows the lag and the y-axis shows the correlation coefficient between -1 and 1 for negative and positive correlation.\n* Partial Autocorrelation Function (PACF). The plot summarizes the correlations for an observation with lag values that is not accounted for by prior lagged observations.\nWe can get a basic picture of the parameter interval, and using this heuristic we can  with help of AIC- Akaike information criterion decide which are the best p,q,d for ARIMA"},{"metadata":{"trusted":true,"_uuid":"76c114e52fd8a50809f944d18ce6f440c2bb181a"},"cell_type":"code","source":"# Initial approximation of parameters\nQs = range(0, 2)\nqs = range(0, 3)\nPs = range(0, 3)\nps = range(0, 3)\nD=1\nd=1\nparameters = product(ps, qs, Ps, Qs)\nparameters_list = list(parameters)\nlen(parameters_list)\n\n# Model Selection\nresults = []\nbest_aic = float(\"inf\")\nwarnings.filterwarnings('ignore')\nfor param in parameters_list:\n    try:\n        model=sm.tsa.statespace.SARIMAX(df_month.Weighted_Price_box, order=(param[0], d, param[1]), \n                                        seasonal_order=(param[2], D, param[3], 12)).fit(disp=-1)\n    except ValueError:\n        print('wrong parameters:', param)\n        continue\n    aic = model.aic\n    if aic < best_aic:\n        best_model = model\n        best_aic = aic\n        best_param = param\n    results.append([param, model.aic])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33052257cef491e8fd11784255f7b6d10b56e566"},"cell_type":"code","source":"# Best Models\nresult_table = pd.DataFrame(results)\nresult_table.columns = ['parameters', 'aic']\nprint(result_table.sort_values(by = 'aic', ascending=True).head())\nprint(best_model.summary())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17a71f38f4781ce5e6228ff97f8f94c4dedef7df"},"cell_type":"markdown","source":"Good, now we can make predictions with our (ARIMA) model:"},{"metadata":{"trusted":true,"_uuid":"1a3580483132a7500f8321b49dfca64d2b498eb3"},"cell_type":"code","source":"# Inverse Box-Cox Transformation Function\ndef invboxcox(y,lmbda):\n   if lmbda == 0:\n      return(np.exp(y))\n   else:\n      return(np.exp(np.log(lmbda*y+1)/lmbda))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfa67cfbd748f30b062feafee83053724769db89"},"cell_type":"code","source":"# Prediction\ndf_month2 = df_month[['Weighted_Price']]\ndate_list = [datetime(2017, 6, 30), datetime(2017, 7, 31), datetime(2017, 8, 31), datetime(2017, 9, 30), \n             datetime(2017, 10, 31), datetime(2017, 11, 30), datetime(2017, 12, 31), datetime(2018, 1, 31),\n             datetime(2018, 1, 28)]\nfuture = pd.DataFrame(index=date_list, columns= df_month.columns)\ndf_month2 = pd.concat([df_month2, future])\ndf_month2['forecast'] = invboxcox(best_model.predict(start=0, end=75), lmbda)\nplt.figure(figsize=(15,7))\ndf_month2.Weighted_Price.plot()\ndf_month2.forecast.plot(color='r', ls='--', label='Predicted Weighted_Price')\nplt.legend()\nplt.title('Bitcoin exchanges, by months')\nplt.ylabel('mean USD')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69c2dd785ae465bf7a412f7d68787055d950a141"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e18dfdd6049c089909075a6f33773a48d6e836bb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01b72aeb5d1973f5711277e0fcb9f487b2f10988"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b7af6ceaac00dff6c13fc89d7c3b02360fc5328"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"5d7959b134cd1f8f882ae55d25bf4a6d42b2eb5f"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"0e1a7edeb828ab4d133e7b4b625d8d356d1208f0"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"2b6b94e2c82fde34ed865464b818d6c9c48e8da2"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}