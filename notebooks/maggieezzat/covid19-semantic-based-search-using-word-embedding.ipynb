{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Covid19 Semantic-based Search using Word Embedding\n"},{"metadata":{},"cell_type":"markdown","source":"# Goal\n\n\nWith over 57,000 scholarly articles on the coronavirus family, it is extremely difficult for the medical researchers to go through this tremendous amount of research papers, hence very difficult to get useful insights about the new Covid-19 pandemic. **The main goal** is to implement a **semantic-based search** rather a *keyword-based* search.\n"},{"metadata":{},"cell_type":"markdown","source":"# Approach\n\n\nInstead of comparing occurences and counts, we will use *gensim's word2vec* in order to generate word embedding using the abstract texts as our corpus. For each document, we calculate the centroid of its abstract and for each query word, we map it to a vector then calculate the word centroid similarity for the query and each document's abstract. The top ranked papers are then selected and output."},{"metadata":{},"cell_type":"markdown","source":"# Dataset Loading and Preprocessing\n\n\nSome loading and pre-processing steps are introduced by the notebooks by ****Ivan Ega Pratama****, ****Maksim Ekin**** from Kaggle. \n\n\n\n**Citation: ** [COVID EDA: Initial Exploration Tool](https://www.kaggle.com/ivanegapratama/covid-eda-initial-exploration-tool) and [COVID-19 Literature Clustering](https://www.kaggle.com/maksimeren/covid-19-literature-clustering#Loading-the-Data)"},{"metadata":{},"cell_type":"markdown","source":"Since the data is too large to work with, we will use the abstracts only. We will start by loading the metadata file and extract from it the paper_id, title, abstract and doi."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imports \n!pip install langdetect\n\nimport spacy\nimport string\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nfrom pprint import pprint\nfrom IPython.utils import io\nfrom tqdm.notebook import tqdm\nfrom gensim.models import Word2Vec\nfrom langdetect import DetectorFactory, detect\nfrom IPython.core.display import HTML, display\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"root_path = '/kaggle/input/CORD-19-research-challenge'\nmetadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str,\n    'abstract': str\n})\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid = pd.DataFrame(columns=['paper_id', 'title','abstract', 'doi'])\ndf_covid['paper_id'] = meta_df.sha\ndf_covid['title'] = meta_df.title\ndf_covid['abstract'] = meta_df.abstract\ndf_covid['doi'] = meta_df.doi\n\ndf_covid.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Duplicates and Null values.\n\nWe will look into the data and check if we have any null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid.drop_duplicates(['abstract'], inplace=True)\ndf_covid.dropna(inplace=True)\ndf_covid.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dropping non-English articles.\n\nNow we dropped the null values, and removed the duplicates as well. Now we will check the number of non-english articles and see if we can drop them for the sake of simplicity."},{"metadata":{"trusted":true},"cell_type":"code","source":"# set seed\nDetectorFactory.seed = 0\n\n# hold label - language\nlanguages = []\n\n# go through each text\nfor ii in tqdm(range(0,len(df_covid))):\n    # split by space into list, take the first x intex, join with space\n    text = df_covid.iloc[ii]['abstract'].split(\" \")\n    \n    lang = \"en\"\n    try:\n        if len(text) > 50:\n            lang = detect(\" \".join(text[:50]))\n        elif len(text) > 0:\n            lang = detect(\" \".join(text[:len(text)]))\n    # ught... beginning of the document was not in a good format\n    except Exception as e:\n        all_words = set(text)\n        try:\n            lang = detect(\" \".join(all_words))\n        \n        except Exception as e:        \n            lang = \"unknown\"\n            pass\n    \n    # get the language    \n    languages.append(lang)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the numbers of articles for each language."},{"metadata":{"trusted":true},"cell_type":"code","source":"languages_dict = {}\nfor lang in set(languages):\n    languages_dict[lang] = languages.count(lang)\n    \nprint(\"Total: {}\\n\".format(len(languages)))\npprint(languages_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since most of the articles are in English, we can safely drop non-English articles."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid['language'] = languages\ndf_covid = df_covid[df_covid['language'] == 'en'] \ndf_covid.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid = df_covid.drop(['language'], axis = 1) \ndf_covid.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spacy Parser and Tokenizer\n\nWe will be using spacy for the pre-processing. We will use use en_core_sci_lg which is spacy's model for scientific and medical documents and create \"processed abstracts\" feature. It will later be used to calculate the centroid for each abstract."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Download the spacy bio parser\nwith io.capture_output() as captured:\n    !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NLP \nimport en_core_sci_lg  # model downloaded in previous step","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punctuations = string.punctuation\n\nstopwords = list(STOP_WORDS)\ncustom_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n]\n\nfor w in custom_stop_words:\n    if w not in stopwords:\n        stopwords.append(w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parser\nparser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nparser.max_length = 7000000\n\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens\n\n\ntqdm.pandas()\ndf_covid[\"processed_abstract\"] = df_covid[\"abstract\"].progress_apply(spacy_tokenizer)\ndf_covid.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentence Tokenization\n\ngensim's word2vec corpus should be in the form of separate sentences, therefore we will use spacy's tokenizer in order to split the corpus (all the abstracts) into sentences."},{"metadata":{"trusted":true},"cell_type":"code","source":"#sentence tokenization to prepare the corpus\nabstracts = df_covid['abstract'].values\n\nnlp = en_core_sci_lg.load(disable = ['ner', 'tagger'])\nnlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\nword2vec_corpus = []\n\nfor i in tqdm(range(0, len(abstracts))):\n    raw_text = abstracts[i]\n    doc = nlp(raw_text)\n    sentences = [sent.string.strip() for sent in doc.sents]\n    \n    for sent in sentences:\n        processed_sent = spacy_tokenizer(sent)\n        processed_sent_list = processed_sent.split(\" \")\n        word2vec_corpus.append(processed_sent_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"check out the corpus."},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec_corpus[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word2vec Training\n\nWe will use gensim's word2vec and train it on the corpus we prepared. `min_count` is the minimum count for a word to occur in the corpus in order to be mapped to a vector. `size` is the size of the vectors produced. `workers` is the number of cores. `window` is the context size to consider. `sg` is skipgram model. The `min_count`, `size` and `window` were calculated empirically. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the genisim word2vec model with our own custom corpus\nmodel = Word2Vec(word2vec_corpus, min_count=3,size= 50,workers=4, window =5, sg = 1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word Centroid Similarity (WCS)\n\nNow that we have word embedding for our corpus, the approach we will take is to measure the cosine similarity between the centroid of each document and the query. This idea is inspired from https://github.com/lgalke/vec4ir#word-centroid-similarity-wcs where it is mentioned:\n\n>An intuitive approach to use word embeddings in information retrieval is the word centroid similarity (WCS). The representation for each document is the centroid of its respective word vectors. Since word vectors carry semantic information of the words, one could assume that the centroid of the word vectors within a document encodes its meaning to some extent. At query time, the centroid of the queryâ€™s word vectors is computed. The cosine similarity to the centroids of the (matching) documents is used as a measure of relevance. When the initial word frequencies of the queries and documents are first re-weighted according to inverse-document frequency (i.e. frequent words in the corpus are discounted), the technique is labeled IDF re-weighted word centroid similarity (IWCS).\n\n\nFor this purpose, we will calculate the centroid for each abstract using the vectors of all the words incorporating the anstract."},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate the centroid for each abstract\n\na = [0.0]*50\ndf_covid[\"centroid\"] = [a]*df_covid.shape[0]\n\nfor index, row in df_covid.iterrows():\n    abstract = row['processed_abstract']\n    total_sim = 0\n    words = abstract.split(\" \")\n    centroid = np.array([0.0]*50)\n    for word in words:\n        try:\n            b = model[word]\n        except:\n            continue\n        centroid = np.add(centroid, b)\n\n    df_covid.at[index,'centroid'] = centroid.tolist()\n\ndf_covid.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ranking documents\n\nNow we will create a function that given a query, would rank the documents from most similar to least similar."},{"metadata":{"trusted":true},"cell_type":"code","source":"def rank_docs(model, query, df_covid, num) :\n    #[(paper_id, processed_abstract, url, cosine_sim)]\n    cosine_list = []\n    \n    a = []\n    query = query.split(\" \")\n    for q in query:\n        try:\n            a.append(model[q])\n        except:\n            continue\n    \n    for index, row in df_covid.iterrows():\n        centroid = row['centroid']\n        total_sim = 0\n        for a_i in a:\n            cos_sim = np.dot(a_i, centroid)/(np.linalg.norm(a_i)*np.linalg.norm(centroid))\n            total_sim += cos_sim\n        cosine_list.append((row['title'], row['doi'], total_sim)) \n    \n    \n    cosine_list.sort(key=lambda x:x[2], reverse=True) ## in Descedning order \n    \n    papers_list = []\n    for item in cosine_list[:num]:\n        papers_list.append((item[0], item[1], item[2]))\n    return papers_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Saving the model and the dataframe"},{"metadata":{},"cell_type":"markdown","source":"## saving the model and the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"./model.model\")\ndf_covid.to_pickle(\"./df_covid.pkl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the model and the data\nThe save/load steps are done in order to avoid re-training the model each time."},{"metadata":{"trusted":true},"cell_type":"code","source":"saved_model = Word2Vec.load(\"./model.model\")\nsaved_df_covid = pd.read_pickle(\"./df_covid.pkl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\nNow let's see some results using our model. query() function takes the query string as an input, together with a number representing the top matches you want, and print the titles of the top matches most relevant articles retrieved for each query, clicking the title a new tab will open with the paper."},{"metadata":{"trusted":true},"cell_type":"code","source":"def query(the_query, top_matches=10):\n    q = spacy_tokenizer(the_query)\n    try:\n        model_to_use = model\n    except:\n        model_to_use = saved_model\n    try:\n        df_covid_to_use = df_covid\n    except:\n        df_covid_to_use = saved_df_covid\n    results = rank_docs(model_to_use, q, df_covid_to_use, top_matches)\n    html = \"\"\"\n    <html>\n    <style>\n        body {\n            margin: 0;\n            padding: 0;\n            background: #ffffff;\n            font-family: sans-serif;\n        }\n\n        ul {\n            position: relative;\n            width: 100%;\n            margin: 100px auto 0;\n            padding: 20px;\n            box-sizing: border-box;\n            background: rgba(0, 0, 0, 0);\n            box-shadow: inset 0 0 15px rgba(0, 0, 0, .2);\n            border-radius: 5px;\n            overflow: hidden;\n        }\n\n        ul li {\n            display: flex;\n            background: rgba(255, 0, 0, 0.25);\n            padding: 10px 20px;\n            color: #fff;\n            margin: 5px 0;\n            transition: .6s;\n            border-radius: 5px;\n        }\n\n        ul li:hover {\n            transform: scale(1.02);\n            background: rgba(255, 0, 0, 0.5);\n        }\n    </style>\n        \n        <body>\n        <ul>\n    \"\"\"\n    for i in range(len(results)):\n        paper_name = results[i][0]\n        paper_doi = results[i][1]\n        paper_link = \"https://doi.org/\" + str(paper_doi)\n        html += \"\"\"\n                <li>\n                <a href=\" \"\"\" + str(paper_link) + \"\"\" \" target=\"_blank\">\n                    <div>\n                        <h3>\n                            \"\"\" + str(i+1) + \"&emsp;\" +str(paper_name) + \"\"\" \n                        </h3>\n                        <cite>\n                            doi\n                            <span >\n                                > \"\"\" + str(paper_doi) + \"\"\"\n                            </span>\n                        </cite>\n                    </div>\n                </a>\n        \"\"\"\n        \n    html += \"</body></html>\"\n    display(HTML(html))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # What do we know about virus genetics, origin, and evolution?"},{"metadata":{"trusted":true},"cell_type":"code","source":"query('origin of coronavirus')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query('covid19 genetics', top_matches=7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # What is known about transmission, incubation, and environmental stability?"},{"metadata":{"trusted":true},"cell_type":"code","source":"query('transmission')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query('incubation period of covid19')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query('environmental stability of coronavirus', top_matches=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # What do we know about diagnostics and surveillance?"},{"metadata":{"trusted":true},"cell_type":"code","source":"query('diagnostics')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thank you for your time, your reviews and suggenstions are highly appreciated !"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}