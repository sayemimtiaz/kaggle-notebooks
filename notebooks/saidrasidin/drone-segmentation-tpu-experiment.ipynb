{"cells":[{"metadata":{"id":"mEZRUJgWJhyx"},"cell_type":"markdown","source":"# Setup TPU XLA","execution_count":null},{"metadata":{"id":"0yKHJIWCygYF","outputId":"f5a107bd-69a6-4767-a151-4364b059bf7f","trusted":true},"cell_type":"code","source":"# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n\n%reload_ext autoreload\n%autoreload\nimport os\n\nif 'TPU_NAME' in os.environ.keys():\n    \n    try:\n        import torch_xla\n    except:\n        # XLA powers the TPU support for PyTorch\n        !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n        !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Library","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytorch-lightning\n!pip install git+https://github.com/qubvel/segmentation_models.pytorch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"kFSYoYRqJSfB","outputId":"e11c8cc1-dcd3-43e6-8854-16923b975096"},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T\nimport torchvision\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom PIL import Image\nimport cv2\nimport albumentations as A\n\nimport time\nimport os\nfrom tqdm.notebook import tqdm\n\nimport segmentation_models_pytorch as smp\nimport pytorch_lightning as pl\nfrom pytorch_lightning.metrics.functional import iou","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_PATH = '../input/semantic-drone-dataset/semantic_drone_dataset/original_images/'\nMASK_PATH = '../input/semantic-drone-dataset/semantic_drone_dataset/label_images_semantic/'\n\n\nn_classes = 23 \n# tree, gras, other vegetation, dirt, gravel, rocks, water, \n#paved area, pool, person, dog, car, bicycle, roof, wall, fence, \n#fence-pole, window, door, obstacle\n\n#read file id in directory\nname = []\nfor dirname, _, filenames in os.walk(IMAGE_PATH):\n    for filename in filenames:\n        name.append(filename.split('.')[0])\n\ndf = pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\nprint('Number of Data: ', len(df))\n\n#split data\nX_trainval, X_test = train_test_split(df['id'].values, test_size=0.1, random_state=19)\nX_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=19)\n\nprint('Train Size   : ', len(X_train))\nprint('Val Size     : ', len(X_val))\nprint('Test Size    : ', len(X_test))\n\n#Costum datasets\n\nclass Drone_data(Dataset):\n    \n    def __init__(self, img_path, mask_path, X, mean, std, transform=None, patch=False):\n        self.img_path = img_path\n        self.mask_path = mask_path\n        self.X = X\n        self.transform = transform\n        self.patches = patch\n        self.mean = mean\n        self.std = std\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        get item per index\n        \"\"\"\n        img = cv2.imread(self.img_path + self.X[idx] + '.jpg')\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.mask_path + self.X[idx] + '.png', cv2.IMREAD_GRAYSCALE)\n        \n        if self.transform is not None:\n            aug = self.transform(image=img, mask=mask)\n            img = Image.fromarray(aug['image'])\n            mask = aug['mask']\n        \n        if self.transform is None:\n            img = Image.fromarray(img)\n        \n        #build in transfrom image\n        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n        img = t(img)\n        mask = torch.from_numpy(mask).long()\n        \n        if self.patches:\n            img, mask = self.tiles(img, mask)\n            \n        return img, mask\n    \n    def tiles(self, img, mask):\n        \"\"\"\n        split image into smaler patches \n        \"\"\"\n        #for image\n        img_patches = img.unfold(1, 512, 512).unfold(2, 768, 768) #tile overlap 50pixel\n        img_patches  = img_patches.contiguous().view(3,-1, 512, 768) #change to total tile\n        img_patches = img_patches.permute(1,0,2,3)#place the tiles number in the 0 index\n        \n        #for mask\n        mask_patches = mask.unfold(0, 512, 512).unfold(1, 768, 768)\n        mask_patches = mask_patches.contiguous().view(-1, 512, 768)\n        \n        return img_patches, mask_patches\n    \n    \ndef calc_resize_ration(re_ration_to_depth=10, unet_depth=5, width=6000, height=4000):\n    \"\"\"\n    help function to resize image in order to be used in U-Net base model\n    \"\"\"\n    width_to_height_rat = width/height\n    unet_ratio = 2**unet_depth\n\n    wid = unet_ratio * re_ration_to_depth * width_to_height_rat\n    heig = unet_ratio * re_ration_to_depth \n    \n    print('After resize with the same ration:')\n    print(f' height:width = {heig, wid}')\n    print(f' size after encoding: {heig/(2**unet_depth), wid/(2**unet_depth)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lovasz Loss","execution_count":null},{"metadata":{"trusted":true,"id":"gmPIy2ZFJSfQ","outputId":"7c5971b4-d215-4fd1-e9bf-f30a2a1a7483"},"cell_type":"code","source":"#===================================================================================================================\n#source :https://github.com/bermanmaxim/LovaszSoftmax/blob/master/pytorch/lovasz_losses.py\n\ndef lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None):\n    \"\"\"\n    Multi-class Lovasz-Softmax loss\n      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).\n              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n    \"\"\"\n    if per_image:\n        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)\n                          for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)\n    return loss\n\ndef lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection / union\n    if p > 1: # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\ndef lovasz_softmax_flat(probas, labels, classes='present'):\n    \"\"\"\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n    \"\"\"\n    if probas.numel() == 0:\n        # only void pixels, the gradients should be 0\n        return probas * 0.\n    C = probas.size(1)\n    losses = []\n    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n    for c in class_to_sum:\n        fg = (labels == c).float() # foreground for class c\n        if (classes is 'present' and fg.sum() == 0):\n            continue\n        if C == 1:\n            if len(classes) > 1:\n                raise ValueError('Sigmoid output possible only with 1 class')\n            class_pred = probas[:, 0]\n        else:\n            class_pred = probas[:, c]\n        errors = (Variable(fg) - class_pred).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n    return mean(losses)\n\n\ndef flatten_probas(probas, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch\n    \"\"\"\n    if probas.dim() == 3:\n        # assumes output of a sigmoid layer\n        B, H, W = probas.size()\n        probas = probas.view(B, 1, H, W)\n    B, C, H, W = probas.size()\n    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n    labels = labels.view(-1)\n    if ignore is None:\n        return probas, labels\n    valid = (labels != ignore)\n    vprobas = probas[valid.nonzero().squeeze()]\n    vlabels = labels[valid]\n    return vprobas, vlabels\n\ndef xloss(logits, labels, ignore=None):\n    \"\"\"\n    Cross entropy loss\n    \"\"\"\n    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n\n\n# --------------------------- HELPER FUNCTIONS ---------------------------\ndef isnan(x):\n    return x != x\n    \n    \ndef mean(l, ignore_nan=False, empty=0):\n    \"\"\"\n    nanmean compatible with generators.\n    \"\"\"\n    l = iter(l)\n    if ignore_nan:\n        l = ifilterfalse(isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == 'raise':\n            raise ValueError('Empty mean')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataloader","execution_count":null},{"metadata":{"trusted":true,"id":"CCL93_giJSft"},"cell_type":"code","source":"mean=[0.485, 0.456, 0.406]\nstd=[0.229, 0.224, 0.225]\n\n\nt_train = A.Compose([A.Resize(672, 448, interpolation=cv2.INTER_NEAREST), \n                     A.HorizontalFlip(), A.VerticalFlip(), \n                     A.GridDistortion(p=0.2),\n                     A.RandomBrightnessContrast((0,0.5),(0,0.5)),\n                     A.GaussNoise()])\n\nt_val = A.Compose([A.Resize(672, 448, interpolation=cv2.INTER_NEAREST),\n                   A.HorizontalFlip(),\n                   A.GridDistortion(p=0.2)])\n\nt_test = A.Resize(768, 1152, interpolation=cv2.INTER_NEAREST)\n\n#datasets\ntrain_set = Drone_data(IMAGE_PATH, MASK_PATH, X_train, mean, std, t_train, patch=False)\nval_set = Drone_data(IMAGE_PATH, MASK_PATH, X_val, mean, std, t_val, patch=False)\n#test_set = Drone_data(IMAGE_PATH, MASK_PATH, X_test, mean, std, transform=t_test, patch=False)\n\n#dataloader\nbatch_size= 3 #4\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0)               ","execution_count":null,"outputs":[]},{"metadata":{"id":"jJ-ZypiFJSgJ"},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"id":"U7-IAYH40S6Y","trusted":true},"cell_type":"code","source":"class Drone_Net(pl.LightningModule):\n    def __init__(self, max_lr, epoch):\n        super(Drone_Net, self).__init__()\n\n        self.model = smp.FPN('efficientnet-b3', encoder_weights='imagenet', \n                             classes=23, activation=None)\n        \n        self.max_lr = max_lr\n        self.epoch = epoch\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    def configure_optimizers(self):\n        # REQUIRED\n        optimizer = torch.optim.AdamW(model.parameters(), lr=self.max_lr, weight_decay=1e-4)\n        sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.max_lr, epochs=self.epoch,\n                                                    steps_per_epoch=len(train_loader))\n        return optimizer\n\n    def training_step(self, batch, batch_nb):\n        # REQUIRED\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        \n        return {'loss':loss}\n\n    def validation_step(self, batch, batch_nb):\n        # OPTIONAL\n        x, y = batch\n        y_hat = self(x)\n        \n        loss = F.cross_entropy(y_hat, y)\n     \n        return {'val_loss': loss}\n\n    def validation_epoch_end(self, outputs):\n        # OPTIONAL\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n   \n        tensorboard_logs = {'val_loss': avg_loss}\n        return {'avg_val_loss': avg_loss,\n                'log': tensorboard_logs} ","execution_count":null,"outputs":[]},{"metadata":{"id":"V46M7OhUeQ3n","outputId":"b79d5d78-c6d2-411d-f8f5-d8ac2a4202d9","trusted":true},"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir lightning_logs","execution_count":null,"outputs":[]},{"metadata":{"id":"9r6yTwAP2t7w","outputId":"0dd91ed5-894a-4113-f188-7c20c9799915","trusted":true},"cell_type":"code","source":"from pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nlr = 1e-3\nepoch = 20\nmodel = Drone_Net(lr, epoch)\n\n# early_stop_callback = EarlyStopping(\n#    monitor='val_loss',\n#    min_delta=0.00,\n#    patience=5,\n#    verbose=False,\n#    mode='min')\n\n\n\n# # default used by the Trainer\n# checkpoint_callback = ModelCheckpoint(\n#     filepath='/kaggle/working/',\n#     save_top_k=True,\n#     verbose=True,\n#     monitor='val_loss',\n#     mode='min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# most basic trainer, uses good defaults (1 TPU)\ntrainer = pl.Trainer(tpu_cores=8, max_epochs=epoch, precision=16, fast_dev_run=True,\n                     early_stop_callback=True, \n                     #checkpoint_callback=checkpoint_callback,\n                     check_val_every_n_epoch=1,\n                     num_sanity_val_steps=1)\n\ntrainer.fit(model, train_loader, val_loader)","execution_count":null,"outputs":[]},{"metadata":{"id":"6BdSV7zxJSgu"},"cell_type":"markdown","source":"# Evaluation","execution_count":null},{"metadata":{"trusted":true,"id":"H1Ho48bAJSgu"},"cell_type":"code","source":"def predict_image_mask(model, image, mask):\n    model.eval()\n    model.to(device); image=image.to(device)\n    mask = mask.to(device)\n    with torch.no_grad():\n        image = image.unsqueeze(0)\n        mask = mask.unsqueeze(0)\n        \n        output = model(image)\n        score = mIoU(output, mask)\n        masked = torch.argmax(output, dim=1)\n        masked = masked.cpu().squeeze(0)\n    return masked, score\n\nmean=[0.485, 0.456, 0.406]\nstd=[0.229, 0.224, 0.225]\n\nclass UnNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        \"\"\"\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized image.\n        \"\"\"\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n            # The normalize code -> t.sub_(m).div_(s)\n        return \n        \nunormal = UnNormalize(mean, std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"FW_sWn-tJSg3","outputId":"58060017-3205-410b-a578-2b78dd2c8752"},"cell_type":"code","source":"%%time\nimage, mask = test_set[0]\npred_mask, score = predict_image_mask(model, image,mask)\n\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"id":"NOzhasM6M1RZ","outputId":"89b29bf7-2141-43b9-b485-c3b85b2923af","trusted":true},"cell_type":"code","source":"unormal(image)\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,10))\nax1.imshow(pred_mask)\nax1.set_title('Prediction Mask | mIoU Score {:.3f}'.format(score))\nax2.imshow(mask)\nax2.set_title('Ground truth')\nax3.imshow(image.permute(1,2,0))\nax3.set_title('Picture');\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"id":"-UY_OKGW-Zo5","trusted":true},"cell_type":"code","source":"pred = pred_mask.view(-1).long().detach().numpy()\ntrue = mask.view(-1).long().detach().numpy()","execution_count":null,"outputs":[]},{"metadata":{"id":"CXex36iYRHMp","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}