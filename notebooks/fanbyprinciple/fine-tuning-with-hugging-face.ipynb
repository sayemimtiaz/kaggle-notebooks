{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\"In this example, weâ€™ll show how to download, tokenize, and train a model on the IMDb reviews dataset. This task takes the text of a review and requires the model to predict whether the sentiment of the review is positive or negative. Letâ€™s start by downloading the dataset from the Large Movie Review Dataset webpage.\"\n\n    - Hugging face tutorial (https://huggingface.co/transformers/master/custom_datasets.html)\n\n![](data:image/png;base64,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)","metadata":{}},{"cell_type":"markdown","source":"### 1. Getting the data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:25.482357Z","iopub.execute_input":"2021-07-02T01:58:25.482683Z","iopub.status.idle":"2021-07-02T01:58:25.992101Z","shell.execute_reply.started":"2021-07-02T01:58:25.482659Z","shell.execute_reply":"2021-07-02T01:58:25.991089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This data is organized into pos and neg folders with one text file per example. Letâ€™s write a function that can read this in.","metadata":{}},{"cell_type":"code","source":"def give_me_text_and_labels(input_csv=\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"):\n    df = pd.read_csv(input_csv)\n    \n    df['label'] = [1 if x==\"positive\" else 0 for x in df['sentiment'] ]\n    return df['review'].values, df['label'].values\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-02T01:58:25.993465Z","iopub.execute_input":"2021-07-02T01:58:25.993664Z","iopub.status.idle":"2021-07-02T01:58:25.999022Z","shell.execute_reply.started":"2021-07-02T01:58:25.993643Z","shell.execute_reply":"2021-07-02T01:58:25.997734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts, labels = give_me_text_and_labels()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.001481Z","iopub.execute_input":"2021-07-02T01:58:26.001814Z","iopub.status.idle":"2021-07-02T01:58:26.529812Z","shell.execute_reply.started":"2021-07-02T01:58:26.001783Z","shell.execute_reply":"2021-07-02T01:58:26.528875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts[0][:100], labels[0], len(labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.5313Z","iopub.execute_input":"2021-07-02T01:58:26.531617Z","iopub.status.idle":"2021-07-02T01:58:26.536288Z","shell.execute_reply.started":"2021-07-02T01:58:26.531587Z","shell.execute_reply":"2021-07-02T01:58:26.535612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_length = 40000\ntrain_texts, train_labels = texts[:train_length], labels[:train_length]\ntest_texts, test_labels = texts[train_length:], labels[train_length:]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.537099Z","iopub.execute_input":"2021-07-02T01:58:26.537288Z","iopub.status.idle":"2021-07-02T01:58:26.557723Z","shell.execute_reply.started":"2021-07-02T01:58:26.537268Z","shell.execute_reply":"2021-07-02T01:58:26.557113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have a train and test dataset, but letâ€™s also also create a validation set which we can use for for evaluation and tuning without tainting our test set results. Sklearn has a convenient utility for creating such splits:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.56086Z","iopub.execute_input":"2021-07-02T01:58:26.561186Z","iopub.status.idle":"2021-07-02T01:58:26.580763Z","shell.execute_reply.started":"2021-07-02T01:58:26.561162Z","shell.execute_reply":"2021-07-02T01:58:26.579932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Tokenizer\n\nAlright, weâ€™ve read in our dataset. Now letâ€™s tackle tokenization. Weâ€™ll eventually train a classifier using pre-trained DistilBert, so letâ€™s use the DistilBert tokenizer.","metadata":{}},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\n\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:26.582217Z","iopub.execute_input":"2021-07-02T01:58:26.582523Z","iopub.status.idle":"2021-07-02T01:58:28.936638Z","shell.execute_reply.started":"2021-07-02T01:58:26.582501Z","shell.execute_reply":"2021-07-02T01:58:28.935187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can simply pass our texts to the tokenizer. Weâ€™ll pass `truncation=True` and `padding=True`, which will ensure that all of our sequences are padded to the same length and are truncated to be no longer modelâ€™s maximum input length. This will allow us to feed batches of sequences into the model at the same time.","metadata":{}},{"cell_type":"code","source":"type(train_texts), type(list(train_texts))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:28.940271Z","iopub.execute_input":"2021-07-02T01:58:28.940503Z","iopub.status.idle":"2021-07-02T01:58:28.948607Z","shell.execute_reply.started":"2021-07-02T01:58:28.940482Z","shell.execute_reply":"2021-07-02T01:58:28.94769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\ntest_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\nval_encodings = tokenizer(list(val_texts), truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:28.949946Z","iopub.execute_input":"2021-07-02T01:58:28.950155Z","iopub.status.idle":"2021-07-02T01:58:50.837609Z","shell.execute_reply.started":"2021-07-02T01:58:28.950129Z","shell.execute_reply":"2021-07-02T01:58:50.83666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Creating a dataset object\n\nNow, letâ€™s turn our labels and encodings into a Dataset object. In PyTorch, this is done by subclassing a torch.utils.data.Dataset object and implementing __len__ and __getitem__. In TensorFlow, we pass our input encodings and labels to the from_tensor_slices constructor method. We put the data in this format so that the data can be easily batched such that each key in the batch encoding corresponds to a named parameter of the forward() method of the model we will train.","metadata":{}},{"cell_type":"code","source":"import torch\n\nclass IMDBdataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:50.838716Z","iopub.execute_input":"2021-07-02T01:58:50.838996Z","iopub.status.idle":"2021-07-02T01:58:50.845047Z","shell.execute_reply.started":"2021-07-02T01:58:50.838968Z","shell.execute_reply":"2021-07-02T01:58:50.844546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = IMDBdataset(train_encodings, train_labels)\ntest_dataset = IMDBdataset(test_encodings,test_labels)\nval_dataset = IMDBdataset(val_encodings, val_labels)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:50.845818Z","iopub.execute_input":"2021-07-02T01:58:50.846094Z","iopub.status.idle":"2021-07-02T01:58:51.333571Z","shell.execute_reply.started":"2021-07-02T01:58:50.846072Z","shell.execute_reply":"2021-07-02T01:58:51.332878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# idx = 0\n# # print(train_encodings.items())\n# item = {key: torch.tensor(val[idx]) for key, val in train_encodings.items()}\n# print(item)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:51.334571Z","iopub.execute_input":"2021-07-02T01:58:51.334794Z","iopub.status.idle":"2021-07-02T01:58:51.33878Z","shell.execute_reply.started":"2021-07-02T01:58:51.334767Z","shell.execute_reply":"2021-07-02T01:58:51.338089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that our datasets our ready, we can fine-tune a model either with the ðŸ¤— Trainer/TFTrainer or with native PyTorch/TensorFlow.","metadata":{}},{"cell_type":"markdown","source":"### 4. Fine Tuning with native pytorch\n\nThe steps above prepared the datasets in the way that the trainer is expected. Now all we need to do is create a model to fine-tune, define the TrainingArguments/TFTrainingArguments and instantiate a Trainer/TFTrainer.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import DistilBertForSequenceClassification, AdamW\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:51.339655Z","iopub.execute_input":"2021-07-02T01:58:51.339985Z","iopub.status.idle":"2021-07-02T01:58:51.353384Z","shell.execute_reply.started":"2021-07-02T01:58:51.33996Z","shell.execute_reply":"2021-07-02T01:58:51.352623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\nmodel = model.to(device=device)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:51.354219Z","iopub.execute_input":"2021-07-02T01:58:51.354453Z","iopub.status.idle":"2021-07-02T01:58:53.857217Z","shell.execute_reply.started":"2021-07-02T01:58:51.354426Z","shell.execute_reply":"2021-07-02T01:58:53.856592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:53.858547Z","iopub.execute_input":"2021-07-02T01:58:53.859077Z","iopub.status.idle":"2021-07-02T01:58:53.868156Z","shell.execute_reply.started":"2021-07-02T01:58:53.859038Z","shell.execute_reply":"2021-07-02T01:58:53.866908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:53.869567Z","iopub.execute_input":"2021-07-02T01:58:53.869831Z","iopub.status.idle":"2021-07-02T01:58:55.00495Z","shell.execute_reply.started":"2021-07-02T01:58:53.869803Z","shell.execute_reply":"2021-07-02T01:58:55.00377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optim = AdamW(model.parameters(),lr=5e-5)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.006658Z","iopub.execute_input":"2021-07-02T01:58:55.006893Z","iopub.status.idle":"2021-07-02T01:58:55.022033Z","shell.execute_reply.started":"2021-07-02T01:58:55.00687Z","shell.execute_reply":"2021-07-02T01:58:55.021121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the all important training loop. It is giving me meory limit exceeded error! if it happens to you, comment the training loop.","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nfor epoch in range(3):\n    for batch in tqdm(train_dataloader):\n        optim.zero_grad()\n        input_ids= batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        loss.backward()\n        optim.step()\n    print(f\"Loss for epoch {epoch} is {loss}\")\n\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.023155Z","iopub.execute_input":"2021-07-02T01:58:55.023397Z","iopub.status.idle":"2021-07-02T01:58:55.034953Z","shell.execute_reply.started":"2021-07-02T01:58:55.02335Z","shell.execute_reply":"2021-07-02T01:58:55.034154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"once trained save it","metadata":{}},{"cell_type":"code","source":"save_directory = \"./\"","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.035884Z","iopub.execute_input":"2021-07-02T01:58:55.036107Z","iopub.status.idle":"2021-07-02T01:58:55.048427Z","shell.execute_reply.started":"2021-07-02T01:58:55.036085Z","shell.execute_reply":"2021-07-02T01:58:55.047542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.save_pretrained(save_directory)\nmodel.save_pretrained(save_directory)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.049359Z","iopub.execute_input":"2021-07-02T01:58:55.049618Z","iopub.status.idle":"2021-07-02T01:58:55.553797Z","shell.execute_reply.started":"2021-07-02T01:58:55.049573Z","shell.execute_reply":"2021-07-02T01:58:55.552558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"and loading it back","metadata":{}},{"cell_type":"code","source":"# from transformers import TFAutoModel, AutoTokenizer\n# tokenizer = AutoTokenizer.from_pretrained(save_directory)\n# model = TFAutoModel.from_pretrained(save_directory, from_pt=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.555088Z","iopub.execute_input":"2021-07-02T01:58:55.555479Z","iopub.status.idle":"2021-07-02T01:58:55.559742Z","shell.execute_reply.started":"2021-07-02T01:58:55.555445Z","shell.execute_reply":"2021-07-02T01:58:55.558397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Building the classifier and using sequence selection\n\nNow lets do something that is not covered in the official tutorial","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nclassifier =pipeline('sentiment-analysis',model=model, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.561027Z","iopub.execute_input":"2021-07-02T01:58:55.561253Z","iopub.status.idle":"2021-07-02T01:58:55.582148Z","shell.execute_reply.started":"2021-07-02T01:58:55.56123Z","shell.execute_reply":"2021-07-02T01:58:55.58098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_test= test_dataset[0]\nsample_test","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.585744Z","iopub.execute_input":"2021-07-02T01:58:55.585975Z","iopub.status.idle":"2021-07-02T01:58:55.605886Z","shell.execute_reply.started":"2021-07-02T01:58:55.585953Z","shell.execute_reply":"2021-07-02T01:58:55.605432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier(\"I love it\")[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.607352Z","iopub.execute_input":"2021-07-02T01:58:55.607689Z","iopub.status.idle":"2021-07-02T01:58:55.638496Z","shell.execute_reply.started":"2021-07-02T01:58:55.607659Z","shell.execute_reply":"2021-07-02T01:58:55.637594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_texts[1001], test_labels[1001]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.639625Z","iopub.execute_input":"2021-07-02T01:58:55.639923Z","iopub.status.idle":"2021-07-02T01:58:55.645182Z","shell.execute_reply.started":"2021-07-02T01:58:55.639891Z","shell.execute_reply":"2021-07-02T01:58:55.644209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier(test_texts[1001])[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-02T01:58:55.646327Z","iopub.execute_input":"2021-07-02T01:58:55.646655Z","iopub.status.idle":"2021-07-02T01:58:56.841485Z","shell.execute_reply.started":"2021-07-02T01:58:55.646625Z","shell.execute_reply":"2021-07-02T01:58:56.840568Z"},"trusted":true},"execution_count":null,"outputs":[]}]}