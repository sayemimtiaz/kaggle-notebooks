{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\n\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom scipy.stats import shapiro\nfrom sklearn import preprocessing\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom sklearn import metrics\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/health-insurance-cross-sell-prediction/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/health-insurance-cross-sell-prediction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(\"id\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop(\"id\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Missing_Values(data):\n    variable_name=[]\n    total_value=[]\n    total_missing_value=[]\n    missing_value_rate=[]\n    unique_value_list=[]\n    total_unique_value=[]\n    data_type=[]\n    for col in data.columns:\n        variable_name.append(col)\n        data_type.append(data[col].dtype)\n        total_value.append(data[col].shape[0])\n        total_missing_value.append(data[col].isnull().sum())\n        missing_value_rate.append(round(data[col].isnull().sum()/data[col].shape[0],3))\n        unique_value_list.append(data[col].unique())\n        total_unique_value.append(len(data[col].unique()))\n    missing_data=pd.DataFrame({\"Variable\":variable_name,\"Total_Value\":total_value,\\\n                             \"Total_Missing_Value\":total_missing_value,\"Missing_Value_Rate\":missing_value_rate,\n                             \"Data_Type\":data_type,\"Unique_Value\":unique_value_list,\\\n                               \"Total_Unique_Value\":total_unique_value})\n    return missing_data.sort_values(\"Missing_Value_Rate\",ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Missing_Values(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Gender\"] = train[\"Gender\"].replace(\"Male\", 0)\ntrain[\"Gender\"] = train[\"Gender\"].replace(\"Female\", 1)\n\n\n\ntrain[\"Vehicle_Damage\"] = train[\"Vehicle_Damage\"].replace(\"No\", 0)\ntrain[\"Vehicle_Damage\"] = train[\"Vehicle_Damage\"].replace(\"Yes\", 1)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Male = 0\n# Female = 1\n\n# Vehicle_Damage Yes = 1\n# Vehicle_Damage No = 0\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dummies = pd.get_dummies(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dummies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dummies.groupby(\"Response\").count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratio = 46710 / 334399\nratio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dummies.rename(columns={\"Vehicle_Age_1-2 Year\": \"Vehicle_Age_1_2 Year\", \n                              \"Vehicle_Age_< 1 Year\": \"Vehicle_Age_0_1_Year\", \"Vehicle_Age_> 2 Years\":\"Vehicle_Age_2_inf\"},inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_dummies.drop(\"Response\", axis = 1)\ny = train_dummies[\"Response\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_real, y_pred):\n    cm = confusion_matrix(y_real, y_pred)\n\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, fmt='g')\n\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB"},{"metadata":{},"cell_type":"markdown","source":"The important thing is to catch customers who are not interested so estimating 0s is more important for us.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1 : Customer is interested, 0 : Customer is not interested\nXGB = XGBClassifier(scale_pos_weight=1)\nXGB_model = XGB.fit(X_train, y_train)\ny_pred = XGB_model.predict(X_test)\n\nxgb_roc_auc_score = roc_auc_score(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nclass_reportt = classification_report(y_test, y_pred)\n\n\nprint(\"AC: {}\\nROC AUC: {}\\nCLASS REPORT:\\n {}\".format(ac,xgb_roc_auc_score,class_reportt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_probs = XGB_model.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_probs > 0.5, 1, 0)\n\nxgb_roc_auc_score = roc_auc_score(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nclass_reportt = classification_report(y_test, y_pred)\n\nprint(\"AC: {}\\nROC AUC: {}\\nCLASS REPORT:\\n {}\".format(ac,xgb_roc_auc_score,class_reportt))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nLet's check the Importance of values "},{"metadata":{"trusted":true},"cell_type":"code","source":"Importance=pd.DataFrame({\"Importance\":XGB_model.feature_importances_*100},\n                       index=X_train.columns)\nImportance.sort_values(by=\"Importance\",\n                      axis=0,ascending=True).plot(kind=\"barh\",color=\"r\")\nplt.xlabel(\"Feature Importance\")\n\nımportance_sort_values = Importance.sort_values(\"Importance\", ascending = False)\n#ımportance_sort_values[ımportance_sort_values['Importance']>1]\nımportance_sort_values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I fit the model with the best parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB = XGBClassifier(scale_pos_weight=3, learning_rate = 0.1, n_estimators = 100,  max_depth = 5,  min_child_weight= 7, colsample_bytree = 0.5)\nXGB_model = XGB.fit(X_train, y_train)\ny_pred = XGB_model.predict(X_test)\n\nxgb_roc_auc_score = roc_auc_score(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nclass_reportt = classification_report(y_test, y_pred)\n\n\nprint(\"AC: {}\\nROC AUC: {}\\nCLASS REPORT:\\n {}\".format(ac,xgb_roc_auc_score,class_reportt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_probs = XGB_model.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_probs > 0.5, 1, 0)\n\nxgb_roc_auc_score = roc_auc_score(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nclass_reportt = classification_report(y_test, y_pred)\n\nprint(\"AC: {}\\nROC AUC: {}\\nCLASS REPORT:\\n {}\".format(ac,xgb_roc_auc_score,class_reportt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Importance=pd.DataFrame({\"Importance\":XGB_model.feature_importances_*100},\n                       index=X_train.columns)\nImportance.sort_values(by=\"Importance\",\n                      axis=0,ascending=True).plot(kind=\"barh\",color=\"r\")\nplt.xlabel(\"Feature Importance\")\n\nımportance_sort_values = Importance.sort_values(\"Importance\", ascending = False)\n#ımportance_sort_values[ımportance_sort_values['Importance']>1]\nımportance_sort_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(fpr, tpr,model_name):\n    plt.figure(figsize=(5,5))\n    plt.title(model_name)\n    plt.plot(fpr,tpr, label = roc_auc_score) \n    plt.plot([0,1],ls='--')\n    plt.plot([0,0],[1,0],c='.5')\n    plt.plot([1,1],c='.5')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nplot_roc_curve(fpr, tpr, \"XGB  ROC CURVE\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a result  roc curve is 0.76 \n\nf1 score is 0.47 for 1. I think it could be better.\n\n\nThis is my first kernel. What do you think about importance and Roc Curve(did not satisfy me) waiting for your comment."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}