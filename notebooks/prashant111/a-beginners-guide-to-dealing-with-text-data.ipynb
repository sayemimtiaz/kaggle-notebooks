{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# **A Beginners Guide to dealing with Text Data**\n\n\nHello friends,\n\n\nIn this notebook, we will discuss several common ways to deal with text data. We will discuss different feature extraction methods. We will start with some basic techniques which will lead into advanced Natural Language Processing techniques. We will also learn about pre-processing of the text data in order to extract better features from clean data.\n\nSo, let's dive in."},{"metadata":{},"cell_type":"markdown","source":"**I hope you find this kernel useful and your <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated**\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n# **Table of Contents** \n\n1.\t[Introduction to Natural Language Processing](#1)\n2.\t[Basic feature extraction using text data](#2)\n   - 2.1 [Count number of words](#2.1)\n   - 2.2 [Count number of characters](#2.2)\n   - 2.3 [Average word length](#2.3)\n   - 2.4 [Number of stopwords](#2.4)\n   - 2.5 [Number of special characters](#2.5)\n   - 2.6 [Number of numerics](#2.6)\n   - 2.7 [Number of uppercase words](#2.7)\n3.\t[Basic Text Pre-processing of text data](#3)\n   - 3.1 [CountVectorization](#3.1)\n   - 3.2 [HashingVectorizer](#3.2)\n   - 3.3 [Lower casing](#3.3)\n   - 3.4 [Punctuation removal](#3.4)\n   - 3.5 [Stopwords removal](#3.5)\n   - 3.6 [Frequent words removal](#3.6)\n   - 3.7 [Rare words removal](#3.7)\n   - 3.8 [Spelling correction](#3.8)\n   - 3.9 [Tokenization](#3.9)\n   - 3.10 [Stemming](#3.10)\n   - 3.11 [Lemmatization](#3.11)\n4.\t[Advance Text Processing](#4)\n   - 4.1 [N-grams](#4.1)\n   - 4.2 [Term Frequency](#4.2)\n   - 4.3 [Inverse Document Frequency](#4.3)\n   - 4.4 [Term Frequency-Inverse Document Frequency (TF-IDF)](#4.4)\n   - 4.5 [Bag of Words](#4.5)\n   - 4.6 [Sentiment Analysis](#4.6)\n5.  [References](#5)\n\n"},{"metadata":{},"cell_type":"markdown","source":"# **1. Introduction to Natural Language Processing** <a class=\"anchor\" id=\"1\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- In a data scientist's journey, we will often come across a term **Natural Language Processing (NLP)**. But, have you ever wondered what do we mean by Natural Language Processing. In short, NLP is the art of understanding the meaning and influence of words.\n\n\n- In terms of wikipedia - \n\n\n[Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n\nChallenges in natural language processing frequently involve [speech recognition](https://en.wikipedia.org/wiki/Speech_recognition), [natural language understanding](https://en.wikipedia.org/wiki/Natural-language_understanding) and [natural language generation](https://en.wikipedia.org/wiki/Natural-language_generation)."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Some of the most useful NLP techniques currently used in practice are:-\n\n- **CountVectorization**, \n- **Hashing Vectorization**, \n- **Term Frequency-Inverse Document Frequency (TF-IDF)**, \n- **Lemmatization**, \n- **Stemming**, \n- **Parsing**, and \n- **Sentiment Analysis**."},{"metadata":{},"cell_type":"markdown","source":"# **2. Basic feature extraction using text data** <a class=\"anchor\" id=\"2\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- Suppose, we don't have sufficient knowledge of Natural Language Processing.\n\n- We can still use some basic feature extraction techniques to extract features from text data.\n\n- In this section, we will discuss some basic feature extraction techniques.\n\n- But, first let's import the data.\n\n- We have used the [Twitter Sentiment Analysis](https://www.kaggle.com/arkhoshghalb/twitter-sentiment-analysis-hatred-speech) data for this notebook.\n"},{"metadata":{},"cell_type":"markdown","source":"### **Load necessary libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Read dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv')\ntest = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Preview dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **2.1 Count number of words** <a class=\"anchor\" id=\"2.1\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- One of the most basic requirement in NLP analysis is to count the number of words in each tweet. The idea behind this is that **the negative sentiments contain a lesser amount of words than the positive ones**.\n\n- We can accomplish the above task (count the number of words) by using the **split** function in python as follows-"},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_of_words(df):\n    df['word_count'] = df['tweet'].apply(lambda x : len(str(x).split(\" \")))\n    print(df[['tweet','word_count']].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_of_words(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_of_words(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that word counts in every tweet has been calculated above."},{"metadata":{},"cell_type":"markdown","source":"##  **2.2 Count number of characters**  <a class=\"anchor\" id=\"2.2\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- We can also calculate the number of characters in every tweet. The intuition is same as above.\n\n- This can be accomplised by calculating the length of the tweet as follows -"},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_of_chars(df):\n    df['char_count'] = df['tweet'].str.len() ## this also includes spaces\n    print(df[['tweet','char_count']].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_of_chars(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_of_chars(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that character counts in every tweet has been calculated above.\n\n- The above calculation will also include the number of spaces, which we can remove, if required."},{"metadata":{},"cell_type":"markdown","source":"## **2.3 Average word length** <a class=\"anchor\" id=\"2.3\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- Now, number of words and number of characters are important. But, there is another feature which is also important is **average word length** of each tweet. This feature can help us to improve our model.\n\n- We can accomplish the above task by simply taking the sum of the length of all the words and divide it by the total length of the tweet."},{"metadata":{"trusted":true},"cell_type":"code","source":"def avg_word(sentence):\n    words = sentence.split()    \n    return (sum(len(word) for word in words)/len(words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def avg_word_length(df):\n    df['avg_word'] = df['tweet'].apply(lambda x: avg_word(x))\n    print(df[['tweet','avg_word']].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_word_length(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_word_length(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **2.4 Number of stopwords** <a class=\"anchor\" id=\"2.4\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- Generally, while solving any NLP problem, the first thing we do is to remove the stopwords. But, what are stop words?\n\n- **Stop Words** : A stop word is a commonly used word such as `the`, `a`, `an`, `in` which are filtered out before or after processing of natural language data (text). Sometimes calculating the number of stopwords can also give us some extra information which we might have been losing before.\n\n- For more information on stop words, please visit the following links-\n\n- https://en.wikipedia.org/wiki/Stop_words\n\n- https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n\n- https://kavita-ganesan.com/what-are-stop-words/#.XowrncgzbIU\n\n\n- To check the list of stopwords we can type the following commands."},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nset(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can count the number of stopwords as follows-"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stop_words(df):\n    df['stopwords'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n    print(df[['tweet','stopwords']].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **2.5 Number of special characters** <a class=\"anchor\" id=\"2.5\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- One more interesting feature which we can extract from a tweet is to calculate the number of hashtags in it. It also helps in extracting extra information from our text data.\n\n- Here, we make use of the `starts with` function because hashtags always appear at the beginning of a word."},{"metadata":{"trusted":true},"cell_type":"code","source":"def hash_tags(df):\n    df['hashtags'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n    print(df[['tweet','hashtags']].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hash_tags(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hash_tags(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **2.6 Number of numerics** <a class=\"anchor\" id=\"2.6\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- Just like we calculated the number of words, we can also calculate the number of numerics which are present in the tweets. It is a useful feature that should be run while doing similar exercises. For example - "},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_numerics(df):\n    df['numerics'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n    print(df[['tweet','numerics']].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_numerics(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_numerics(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **2.7 Number of Uppercase words**  <a class=\"anchor\" id=\"2.7\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- Anger or rage is quite often expressed by writing in UPPERCASE words which makes this a necessary operation to identify those words."},{"metadata":{"trusted":true},"cell_type":"code","source":"def num_uppercase(df):\n    df['upper_case'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n    print(df[['tweet','upper_case']].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_uppercase(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_uppercase(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# **3. Basic Text Processing** <a class=\"anchor\" id=\"3\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- So far, we have learned how to extract basic features from text data. \n\n- Now, we move onto text and feature extraction. \n\n- Our first step should be to clean the data in order to obtain better features. \n\n- We will achieve this by doing some of the basic pre-processing steps on our training data.\n\n- So, let’s get into it.\n"},{"metadata":{},"cell_type":"markdown","source":"## **3.1 CountVectorization** <a class=\"anchor\" id=\"3.1\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- **[CounterVectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)** is a SciKitLearn library takes any text document and returns each unique word as a feature with the count of number of times that word occurs.\n\n- While this can generate lot of features with some extremely useful parameters that help avoid that including stop_words, n_grams, and max_features. \n\n- **Stop words** generates a list of words that will not be included as a feature. The primary use of this is the “English” dictionary where it will get rid of insignificant words like “is, the, a, it, as “which can appear quite frequently, but have little to no influence on our end goal. \n\n- **ngrams_range** selects how you can group words together. Instead of having NLP return each word separately, we can get results like “Hello again” if it equals 2 or “See you later” if it equals 3. \n\n- **max_features** is how many features you choose to create. If we choose it to equal none it means that we will get every and all words as features, but if we set it equal to 50 you will only get the 50 most frequently used words.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n          'This is the first document.',\n          'This document is the second document.',\n          'And this is the third one.',\n          'Is this the first document?',\n         ]\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\nX2 = vectorizer2.fit_transform(corpus)\nprint(vectorizer2.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X2.toarray())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **3.2 HashingVectorizer** <a class=\"anchor\" id=\"3.2\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- [Hashing Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) converts text to a matrix of occurrences using the “hashing trick”.\n\n\n- It converts a collection of text documents to a matrix of token occurrences.\n\n- It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=’l1’ or projected on the euclidean unit sphere if norm=’l2’.\n\n- This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.\n\n- Each word is mapped to a feature and using the hash function converts it to a hash. \n\n- If the word occurs again in the body of the text it is converted to that same feature which allows us to count it in the same feature without retaining a dictionary in memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import HashingVectorizer\ncorpus = [\n          'This is the first document.',\n          'This document is the second document.',\n          'And this is the third one.',\n          'Is this the first document?',\n         ]\nvectorizer = HashingVectorizer(n_features=2**4)\nX = vectorizer.fit_transform(corpus)\nprint(X.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **3.3 Lower Casing** <a class=\"anchor\" id=\"3.3\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- Another pre-processing step which we will do is to transform our tweets into lower case. \n- This avoids having multiple copies of the same words. \n- For example, while calculating the word count, ‘Analytics’ and ‘analytics’ will be taken as different words."},{"metadata":{"trusted":true},"cell_type":"code","source":"def lower_case(df):\n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    print(df['tweet'].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_case(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_case(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## **3.4 Punctuation Removal** <a class=\"anchor\" id=\"3.4\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- The next step is to remove punctuation as it doesn’t add any extra information while treating text data. \n\n- Therefore removing all instances of it will help us reduce the size of the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def punctuation_removal(df):\n    df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n    print(df['tweet'].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punctuation_removal(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punctuation_removal(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that all the punctuation, including ‘#’ and ‘@’, has been removed from the training data."},{"metadata":{},"cell_type":"markdown","source":"## **3.5 Stop Words Removal**  <a class=\"anchor\" id=\"3.5\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- As we discussed earlier, stop words (or commonly occurring words) should be removed from the text data. \n- For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stop_words_removal(df):\n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n    print(df['tweet'].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words_removal(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words_removal(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## **3.6 Frequent Words Removal**  <a class=\"anchor\" id=\"3.6\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- We can also remove commonly occurring words from our text data.\n\n- First, let’s check the 10 most frequently occurring words in our text data then take call to remove or retain.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[:10]\nfreq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Now, we will remove these words as their presence will not of any use in classification of our text data."},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = list(freq.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def frequent_words_removal(df):    \n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n    print(df['tweet'].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_words_removal(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_words_removal(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## **3.7 Rare Words Removal**  <a class=\"anchor\" id=\"3.7\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- Now, we will remove rarely occurring words from the text. \n- Because they’re so rare, the association between them and other words is dominated by noise. \n- We can replace rare words with a more general form and then this will have higher counts."},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[-10:]\nfreq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = list(freq.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rare_words_removal(df):\n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n    print(df['tweet'].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rare_words_removal(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rare_words_removal(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- All these pre-processing steps are essential and help us in reducing our vocabulary clutter so that the features produced in the end are more effective."},{"metadata":{},"cell_type":"markdown","source":"## **3.8 Spelling Correction**  <a class=\"anchor\" id=\"3.8\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n\n- Now tweets can be filled with plethora of spelling mistakes. Our task is to rectify these spelling mistakes.\n\n- In that context, spelling correction is a useful pre-processing step because this also will help us in reducing multiple copies of words. For example, “Analytics” and “analytcs” will be treated as different words even if they are used in the same sense.\n\n- To accomplish the above task, we will use the textblob library as follows-\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def spell_correction(df):\n    return df['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spell_correction(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spell_correction(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## **3.9 Tokenization** <a class=\"anchor\" id=\"3.9\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- [Tokenization](https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/) refers to dividing the text into a sequence of words or sentences. \n\n- In our example, we have used the textblob library to first transform our tweets into a blob and then converted them into a series of words."},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokens(df):\n    return TextBlob(df['tweet'][1]).words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **3.10 Stemming** <a class=\"anchor\" id=\"3.10\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- [Stemming](https://www.geeksforgeeks.org/introduction-to-stemming/) refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. \n\n- So, stemming takes a word and refers it back to its base or root form. **Stems**, **Stemming**, **Stemmed** and **Stemtization** are all based on the single word **stem**.\n\n- For this purpose, we will use *PorterStemmer* from the NLTK library."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nst = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stemming(df):\n    return df['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemming(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemming(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that *dysfunctional* has been transformed into *dysfunct*, among other changes."},{"metadata":{},"cell_type":"markdown","source":"## **3.11 Lemmatization** <a class=\"anchor\" id=\"3.11\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- [Lemmatization](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/) is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n\n- Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. \n\n- Lemmatization makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore, we usually prefer using lemmatization over stemming."},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import Word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatization(df):\n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n    print(df['tweet'].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatization(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatization(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# **4. Advanced Text Processing** <a class=\"anchor\" id=\"4\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- Until now, we have covered all the basic pre-processing steps which are helpful in order to clean our data.\n\n- Now,we will move on and focus on advanced text-processing or NLP techniques."},{"metadata":{},"cell_type":"markdown","source":"## **4.1 N-grams** <a class=\"anchor\" id=\"4.1\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- [N-grams](https://kavita-ganesan.com/what-are-n-grams/#.Xo3jccgzbIU) are the combination of multiple words used together. Ngrams with N=1 are called **unigrams**. Similarly, **bigrams (N=2)**, **trigrams (N=3)** and so on.\n\n- **Unigrams** do not usually contain as much information as compared to **bigrams** and **trigrams**. The basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. \n\n- The longer the n-gram (the higher the n), the more context you have to work with. Optimum length really depends on the application – if your n-grams are too short, you may fail to capture important differences. On the other hand, if they are too long, you may fail to capture the “general knowledge” and only stick to particular cases.\n\n- Now, we will extract bigrams from our tweets using the ngrams function of the textblob library."},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def combination_of_words(df):\n    return (TextBlob(df['tweet'][0]).ngrams(2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combination_of_words(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combination_of_words(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **4.2 Term Frequency**  <a class=\"anchor\" id=\"4.2\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.\n\n- Therefore, we can generalize term frequency as:\n\n**TF = (Number of times term T appears in the particular row) / (number of terms in that row)**\n\n\n- We will create a Term-Frequency table of a tweet as follows-"},{"metadata":{"trusted":true},"cell_type":"code","source":"def term_frequency(df):\n    tf1 = (df['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n    tf1.columns = ['words','tf']\n    return tf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"term_frequency(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"term_frequency(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **4.3 Inverse Document Frequency (IDF)** <a class=\"anchor\" id=\"4.3\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- The intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it’s appearing in all the documents.\n\n- Therefore, the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.\n\n- IDF can be calculated as follows -\n\n   **IDF = log(N/n)**, \n   \n where, N is the total number of rows and n is the number of rows in which the word was present.\n\n- Now, we will calculate IDF for the same tweets for which we calculated the term frequency."},{"metadata":{"trusted":true},"cell_type":"code","source":"tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf1.columns = ['words','tf']\ntf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf2 = (test['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf2.columns = ['words','tf']\ntf2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The more the value of IDF, the more unique is the word."},{"metadata":{},"cell_type":"markdown","source":"## **4.4 Term Frequency – Inverse Document Frequency (TF-IDF)** <a class=\"anchor\" id=\"4.4\"></a>\n\n[Table of Contents](#0.1)\n\n\n- **TF-IDF** is the multiplication of the TF and IDF which we calculated again below for convinience."},{"metadata":{"trusted":true},"cell_type":"code","source":"tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf1.columns = ['words','tf']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,word in enumerate(tf1['words']):\n    tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['tweet'].str.contains(word)])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf1['tfidf'] = tf1['tf'] * tf1['idf']\ntf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"- We can see that the TF-IDF has penalized words like ‘don’t’, ‘can’t’, and ‘use’ because they are commonly occurring words. However, it has given a high weight to “disappointed” since that will be very useful in determining the sentiment of the tweet.\n\n- We don’t have to calculate TF and IDF every time beforehand and then multiply it to obtain TF-IDF. Instead, sklearn has a separate function to directly obtain it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n stop_words= 'english',ngram_range=(1,1))\ntrain_vect = tfidf.fit_transform(train['tweet'])\ntrain_vect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## **4.5 Bag of Words** <a class=\"anchor\" id=\"4.5\"></a>\n\n[Table of Contents](#0.1)\n\n\n- [Bag of Words (BoW)](https://machinelearningmastery.com/gentle-introduction-bag-words-model/) refers to the representation of text which describes the presence of words within the text data. The intuition behind this is that two similar text fields will contain similar kind of words, and will therefore have a similar bag of words. Further, that from the text alone we can learn something about the meaning of the document.\n\n- For implementation, sklearn provides a separate function for it as shown below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nbow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\ntrain_bow = bow.fit_transform(train['tweet'])\ntrain_bow","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **4.6 Sentiment Analysis**  <a class=\"anchor\" id=\"4.6\"></a>\n\n[Table of Contents](#0.1)\n\n\n- Now we come to our problem which was to detect the sentiment of the tweet. So, before applying any ML/DL models (which can have a separate feature detecting the sentiment using the textblob library).\n\n- We will check the sentiment of the first few tweets as follows -"},{"metadata":{"trusted":true},"cell_type":"code","source":"def polarity_subjectivity(df):\n    return df['tweet'][:5].apply(lambda x: TextBlob(x).sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"polarity_subjectivity(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"polarity_subjectivity(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can can see that it returns a tuple representing polarity and subjectivity of each tweet. Here, we only extract polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment. This can also work as a feature for building a machine learning model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def sentiment_analysis(df):\n    df['sentiment'] = df['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\n    return df[['tweet','sentiment']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_analysis(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_analysis(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **5. References** <a class=\"anchor\" id=\"5\"></a>\n\n\n[Table of Contents](#0.1)\n\n\nThis notebook is based on excellent article by Shubham Jain - \n\n- [Ultimate Guide to deal with Text Data](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/)"},{"metadata":{},"cell_type":"markdown","source":"We have discussed several common ways of dealing with text data in this notebook. These methods will give us a basic understanding of how to deal with text data in predictive modeling. "},{"metadata":{},"cell_type":"markdown","source":"I hope you find this notebook useful and enjoyable. Your comments and feedback are most welcome.\n\nThank you"},{"metadata":{},"cell_type":"markdown","source":"[Go to Top](#0)\t"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}