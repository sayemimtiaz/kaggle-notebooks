{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing libraries\n\nimport numpy as np\nimport pylab as plt\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = \"../input/coursesdata/\"\nstudentInfo = pd.read_csv(PATH + 'studentInfo.csv')\ncourses = pd.read_csv(PATH + 'courses.csv')\nassessments = pd.read_csv(PATH + 'assessments.csv')\nstudentAssessment = pd.read_csv(PATH + 'studentAssessment.csv')\nstudentReview = pd.read_csv(PATH + 'studentReview.csv')\n\nstudentInfo.head()\n# courses.head()\n# assessments.head()\n# studentAssessment.head()\n# studentReview.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging Tables\nresult = pd.merge(studentInfo, courses, left_on=('course','run'), right_on=('course','run'),how='left', sort=False);\nresult = pd.merge(result, assessments, left_on=('course','run'), right_on=('course','run'),how='left', sort=False);\nresult = pd.merge(result, studentAssessment, left_on=('student_id','assessment_id'), right_on=('student_id','assessment_id'),how='left', sort=False);\nresult = pd.merge(result, studentReview, left_on=('student_id','course'), right_on=('student_id','course'),how='left', sort=False);\n\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reorder Columns\nresult = result[['student_id','course', 'run',  'gender', 'region', 'highest_education_level', 'age_range', 'completed', \n                 'date_enrolled', 'date_unenrolled', 'course_length', 'assessment_id','assessment_type', 'date', 'weight',\n                 'date_submitted', 'score', 'student_review','upgraded']]\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping by (student, Course and run), so we can predict for each (user, couurse, run) the upgraded value\n\nresult.groupby(['student_id', 'course','run']).agg({\n    'gender': lambda x: x[0],\n    'region': lambda x: x[0],\n    'highest_education_level': lambda x: x[0],\n    'age_range': lambda x: x[0],\n    'completed': lambda x: x[0],\n    'date_enrolled': lambda x: x[0],\n    'date_unenrolled': lambda x: x[0],\n    'course_length': lambda x: x[0],\n    'assessment_id': 'count',\n    'assessment_type': lambda x: x[0],\n    'date': lambda x: x[0],\n    'weight': lambda x: x[0],\n    'date_submitted': lambda x: x[0],\n    'score': lambda x: x[0],\n    'student_review': lambda x: x[0],\n    'upgraded' :lambda x: x[0]\n})\n\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new User_Course_Run identifier\nresult['ID'] = result['student_id'].map(str) + '_' + result['course'] + '_' + result['run']\n\n# Making User_Course_Run the first in the dataframe, and removing [student_id, course, run]\nresult['student_id'] = result['ID']\nresult.rename(columns={'student_id': 'Student_course_Run_id'}, inplace=True)\nresult.drop(['course', 'run', 'ID'], axis=1, inplace=True)\n\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turning non numeric values into numbers using labelEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# Lebel encoding Target column\nleup = LabelEncoder()\nleup.fit(result.upgraded)\nresult.upgraded=leup.transform(result.upgraded)\n\ncat_cols = ['gender','region','highest_education_level','age_range','completed','date_enrolled','assessment_type']\nfor col in cat_cols:\n    if col in result.columns:\n        le = LabelEncoder()\n        le.fit(list(result[col].astype(str).values))\n        result[col] = le.transform(list(result[col].astype(str).values))\n        \nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Target (Most studnets don't upgrade)\nimport seaborn as sns\n\nsns.countplot(x='upgraded', data=result);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running this command, we can see that some columns have missing values\nresult.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the distribution of each column\nresult.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Destribution of columns ['date_enrolled', 'course_length', 'date', 'weight', 'score']\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(16,6))\nboxplot = result.boxplot(column=['date_enrolled', 'course_length', 'date', 'weight', 'score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling missig values (We will use the mean to impute the missing values)\nresult.score = result.score.fillna(result.score.mean())\n\nresult['date_submitted'] = result['date_submitted'].fillna(result['date_submitted'].mean())\nresult['date_unenrolled'] = result['date_unenrolled'].fillna(result['date_unenrolled'].mean())\nresult['date'] = result['date'].fillna(result['date'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Baseline Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# For the baseline model, we will use just the numeric columns.\n# In order to not lose the review effect, We will create a \"student_review_len\"\n# column before removing the \"student_review\" column.\n\ndef add_review_features(df):\n    df['student_review'] = df['student_review'].apply(lambda x:str(x))\n    df['student_review_len'] = df['student_review'].apply(len)\n    df['student_review_n_capitals'] = df['student_review'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['student_review_n_words'] = df['student_review'].str.count('\\S+')\n    return df\n\nresult = add_review_features(result)\n# Removing unique identifiers + studnt review\ndata = result.drop(['assessment_id','student_review'],axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting data into 80% training and 20% test\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nX = data.drop(['Student_course_Run_id', 'upgraded'],axis=1)\ny = data.upgraded\n\n# Standardize features by removing the mean and deviding by variance\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX = pd.DataFrame(X_scaled)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\n\nskf = StratifiedKFold(n_splits=5)\nskf.get_n_splits(X, y)\n\n# Accuracies and F-Scores across k folds\naccs, fsc = [], []\n\nprint(skf)\nStratifiedKFold(n_splits=5, random_state=10, shuffle=False)\nfor train_index, test_index in skf.split(X, y):\n    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # Create Model\n    clf =  RandomForestClassifier(n_estimators=10, random_state=10)\n    # Train Decision Tree Classifer\n    clf = clf.fit(X_train,y_train)\n    # Predict the response for test dataset\n    y_pred = clf.predict(X_test)\n    \n    # Evaluate performance\n    print(\"Fold Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n    print(\"Fold F1-Score:\",metrics.f1_score(y_test, y_pred), end='\\n\\n')\n    accs.append(metrics.accuracy_score(y_test, y_pred))\n    fsc.append(metrics.f1_score(y_test, y_pred))\n    \nprint(\"Overall Accuracy: {:0.2f} +/- {:0.2f}\".format(np.mean(accs), np.std(accs)))\nprint(\"Overall F1-Score: {:0.2f} +/- {:0.2f}\".format(np.mean(fsc), np.std(fsc)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WordCrouds","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# First we will plot WordCrouds for the two classes (upgrade) and (Not upgrade)\n# We can see that words like \"Great\" are indicators for the decision of the student\n\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\n\nn_posts = 1000\ndata = result\nrev_Up = ' '.join(data[data['upgraded'] == 0]['student_review'].str.lower().values[:n_posts])\nrev_Nup = ' '.join(data[data['upgraded'] == 1]['student_review'].str.lower().values[:n_posts])\n\nwordcloud_S = WordCloud(max_words=20, scale = 2, stopwords=stop, contour_width=3, contour_color='steelblue').generate(rev_Up)\nwordcloud_I = WordCloud(max_words=20, scale = 2, stopwords=stop, contour_width=3, contour_color='steelblue').generate(rev_Nup)\n\nfig, ax = plt.subplots(1,2, figsize=(22, 6))\nax[0].imshow(wordcloud_S)\nax[0].set_title('Top words studnet review (Not upgrade)',fontsize = 20)\nax[0].axis(\"off\")\n\nax[1].imshow(wordcloud_I)\nax[1].set_title('Top words studnet review (upgrade)',fontsize = 20)\nax[1].axis(\"off\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Creating model using student Review (CNN-1D)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing libraries\nimport os, re, pickle\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\n# pytorch bert imports\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel\n\n# keras imports\nfrom keras.utils import np_utils\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import CuDNNLSTM, LSTM, Activation, Dense, Dropout, Input, Embedding, concatenate, Flatten, Bidirectional\nfrom keras.layers import Conv1D,  SpatialDropout1D, Dropout, add, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import BatchNormalization, Reshape\nfrom keras.optimizers import Adam, SGD\nfrom keras.models import Sequential, Model, load_model\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\nfrom keras.losses import binary_crossentropy\nfrom keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_PRETRAINED_DIR = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/'\nBERT_VOCAB_DIR = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt'\nMAX_LENGTH = 50 # Because review_len_mean is near 40","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nlp_preprocessing(text):\n    filter_char = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n    text = text.lower()\n    text = text.replace(filter_char,'')\n    text = text.replace('[^a-zA-Z0-9 ]', '')\n    return text\n\nresult[\"student_review\"] = result[\"student_review\"].apply(nlp_preprocessing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer(vocab_file=BERT_VOCAB_DIR)\ndef tokenization(row):\n    row = tokenizer.tokenize(row)\n    row = tokenizer.convert_tokens_to_ids(row)\n    return row\n\nresult[\"student_review\"] = result[\"student_review\"].apply(tokenization)\n\n# Cheking some review after tokenization\nresult[\"student_review\"].sample(20).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def string_ids(doc):\n    doc = [str(i) for i in doc]\n    return ' '.join(doc)\n\nresult[\"student_review\"] = result[\"student_review\"].apply(string_ids)\nresult[\"student_review\"].sample(20).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_text = np.zeros((result.shape[0],MAX_LENGTH),dtype=np.int)\nX_num  = X_scaled #Numerical features\n# Make same length\nfor i,ids in enumerate(list(result['student_review'])):\n    input_ids = [int(i) for i in ids.split()[:MAX_LENGTH]]\n    inp_len = len(input_ids)\n    X_text[i,:inp_len] = np.array(input_ids)\n    \nprint(X_text[:2, :])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_bert_embed_matrix():\n    bert = BertModel.from_pretrained(BERT_PRETRAINED_DIR)\n    bert_embeddings = list(bert.children())[0]\n    bert_word_embeddings = list(bert_embeddings.children())[0]\n    mat = bert_word_embeddings.weight.data.numpy()\n    return mat\n\nembedding_matrix = get_bert_embed_matrix()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## LSTM Model\n####################\nLSTM_UNITS = 128\nHIDDEN_UNITS = 4 * LSTM_UNITS\nN_NUMERICAL  = X_scaled.shape[-1]\ndrop_out_rate = 0.4\n\ndef build_lstm_model(embedding_matrix):\n    \n    words = Input(shape=(MAX_LENGTH,))\n    numerics = Input(shape=(N_NUMERICAL,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.5)(x)\n    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x),])\n    #hidden = add([hidden, Dense(HIDDEN_UNITS, activation='relu')(hidden)])\n    #hidden = add([hidden, Dense(HIDDEN_UNITS, activation='relu')(hidden)])\n    \n    ##\n    hidden = Dropout(drop_out_rate)(hidden)\n    hidden = Dense(4, activation='relu')(hidden)\n    ##\n    \n    hidden = concatenate([hidden, numerics])\n    out = Dense(1, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=[words, numerics], outputs=out)\n    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam(lr = 0.01))\n\n    return model\n\n# Checking Model Architecture\nbuild_lstm_model(embedding_matrix).summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LSTM_UNITS = 128\nHIDDEN_UNITS = 4 * LSTM_UNITS\nN_NUMERICAL  = X_scaled.shape[-1]\ndrop_out_rate = 0.4\n\ndef build_cnn_model(embedding_matrix):\n    \n    words = Input(shape=(MAX_LENGTH,))\n    numerics = Input(shape=(N_NUMERICAL,))\n    \n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = Conv1D(8, 11, activation='relu', strides=1)(x)\n    x = MaxPooling1D(2)(x)\n    x = BatchNormalization()(x)\n    x = Conv1D(16, 7, activation='relu', strides=1)(x)\n    x = MaxPooling1D(2)(x)\n    x = BatchNormalization()(x)\n    #x = Flatten()(x)\n    x = Dropout(drop_out_rate)(x)\n    \n    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x),])\n    hidden = Dropout(drop_out_rate)(hidden)\n    hidden = Dense(4, activation='relu')(hidden)\n    \n    hidden = concatenate([hidden, numerics])\n    hidden = Dense(20, activation='relu')(hidden)\n    \n    hidden = Dropout(drop_out_rate)(hidden)\n    out = Dense(1, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=[words, numerics], outputs=out)\n    model.compile(loss=f1_loss, metrics=['accuracy', f1], optimizer=Adam(lr = 0.01))\n\n    return model\n\n# Checking Model Architecture\n#build_cnn_model(embedding_matrix).summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\ndef f1(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\n\ndef f1_loss(y_true, y_pred):\n    \n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1 - K.mean(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn import metrics\n\nskf = StratifiedKFold(n_splits=5, random_state=49, shuffle=True)\nskf.get_n_splits(X_num, y)\n\n# Accuracies and F-Scores across k folds\naccs, precs, fsc = [], [], []\n\n# numerc Input Shape\nn_num = X_num.shape[-1]\nBsize = 2048#*2 #1024 #512 #128 #256\nEPOCHS = 50 #15\n\nmy_weights = '../working/model.h5'\nincrease = True\n\nfor train_index, test_index in skf.split(X_num, y):\n    \n    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_num_train, X_num_test = X_num[train_index, :], X_num[test_index, :]\n    X_text_train, X_text_test = X_text[train_index, :], X_text[test_index, :]\n    \n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    early_stop  = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=8, verbose=True)\n    check_point = ModelCheckpoint(my_weights, monitor=\"val_loss\", mode=\"min\", verbose=True, save_best_only=True)\n    reduceLROnPlat = ReduceLROnPlateau( monitor='val_loss', factor=0.1, patience=4, verbose=1, mode='min')\n    #lr_sched    = LearningRateScheduler(lambda epoch: 1e-2 * (0.8 ** epoch), verbose=True)\n    \n    # Create Model\n    model = build_lstm_model(embedding_matrix)\n                                  \n    # Upsample during cross validation to avoid having the same samples\n    # in both train and validation sets\n    # Validation set is not up-sampled to monitor overfitting\n    if increase:\n        # Get positive examples\n        pos = pd.Series(y_train)\n        pos = pos[pos == 1]\n        \n        for i in range(25): #\n            # Add positive examples\n            X_num_train = np.vstack((X_num_train, X_num[pos.index, :]))\n            X_text_train = np.vstack((X_text_train, X_text[pos.index, :]))\n            y_train = np.concatenate((y_train, y[pos.index]))\n\n    # Model Training and prediction phase\n    model.fit(\n        [X_text_train, X_num_train], y_train,\n        validation_data = ([X_text_test, X_num_test], y_test),\n        batch_size = Bsize,\n        epochs = EPOCHS,\n        verbose = 1,\n        callbacks=[early_stop, check_point, reduceLROnPlat],\n        shuffle = True\n    )\n    # Load Best model\n    model = load_model(my_weights, custom_objects={'f1_loss': f1_loss, 'f1': f1})\n                                  \n    # Predict the response for test dataset\n    y_pred = model.predict([X_text_test, X_num_test], batch_size=Bsize)\n    \n    # Convert predictions to int, so we can compute metrics\n    y_pred = (np.array(y_pred) > 0.5).astype(np.int)\n    \n    # Evaluate performance\n    print(\"Fold Accuracy:\",metrics.accuracy_score(y_test, y_pred.round()))\n    print(\"Fold Precision:\",metrics.precision_score(y_test, y_pred.round()))\n    print(\"Fold F1-Score:\",metrics.f1_score(y_test, y_pred.round()), end='\\n\\n')\n    \n    accs.append(metrics.accuracy_score(y_test, y_pred.round()))\n    precs.append(metrics.precision_score(y_test, y_pred.round()))\n    fsc.append(metrics.f1_score(y_test, y_pred.round()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Overall Accuracy: {:0.2f} +/- {:0.2f}\".format(np.mean(accs), np.std(accs)))\nprint(\"Overall Precision: {:0.2f} +/- {:0.2f}\".format(np.mean(precs), np.std(precs)))\nprint(\"Overall F1-Score: {:0.2f} +/- {:0.2f}\".format(np.mean(fsc), np.std(fsc)))\n\n# Overall Accuracy: 0.99 +/- 0.00\n# Overall Precision: 0.40 +/- 0.21\n# Overall F1-Score: 0.17 +/- 0.09\n\n# With bigger batch size\n# Overall Accuracy: 0.99 +/- 0.00\n# Overall Precision: 0.52 +/- 0.29\n# Overall F1-Score: 0.19 +/- 0.19\n\n# After Upsampling\n# Overall Accuracy: 0.99 +/- 0.00\n# Overall Precision: 0.24 +/- 0.17\n# Overall F1-Score: 0.08 +/- 0.09\n\n# After Upsampling and Shuffle\n# Overall Accuracy: 0.99 +/- 0.00\n# Overall Precision: 0.40 +/- 0.15\n# Overall F1-Score: 0.21 +/- 0.12\n\n# After reducing Text contribution\n# Overall Accuracy: 0.99 +/- 0.00\n# Overall Precision: 0.35 +/- 0.14\n# Overall F1-Score: 0.34 +/- 0.14\n\n# After LrSchedular\n# Overall Accuracy: 0.98 +/- 0.01\n# Overall Precision: 0.38 +/- 0.16\n# Overall F1-Score: 0.49 +/- 0.18\n\n# After increasing BatchSize\n# Overall Accuracy: 0.99 +/- 0.01\n# Overall Precision: 0.44 +/- 0.18\n# Overall F1-Score: 0.51 +/- 0.19\n\n#=======\n# Random Forest with just numerical data\n# Overall Accuracy: 0.99 +/- 0.00\n# Overall Precision: 0.71 +/- 0.26\n# Overall F1-Score: 0.51 +/- 0.13\n\n# Random Forest with all data\n# Overall Accuracy: 0.99 +/- 0.00\n# Overall Precision: 0.70 +/- 0.23\n# Overall F1-Score: 0.27 +/- 0.11","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ressources:\n- https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}