{"cells":[{"metadata":{},"cell_type":"markdown","source":"We will use the BioBERT Vocab uncased model as that is what is recommended on the official GitHub Page."},{"metadata":{},"cell_type":"markdown","source":"# BioBERT Embeddings Analysis\nThis is a basic tutorial of how to download and use the BioBERT model to create naive embeddings, which can be used for exploring concepts in the literature corpus. Of course long term we would probably want to fine-tune this model in a unsupervised fashion on the document corpus. Additionally, many of the demonstrated techniques are naive (for instance simply averaging the word embeddings to form a sentence embedding), however this demonstrates how embeddings could be used for this challenge"},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"!pip install transformers\n!wget -O scibert_uncased.tar https://github.com/naver/biobert-pretrained/releases/download/v1.1-pubmed/biobert_v1.1_pubmed.tar.gz\n!tar -xvf scibert_uncased.tar\n\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport argparse\nimport logging\n\nimport torch\n\nfrom transformers import BertConfig, BertForPreTraining, load_tf_weights_in_bert\n\n\nlogging.basicConfig(level=logging.INFO)\n\n\ndef convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\n    # Initialise PyTorch model\n    config = BertConfig.from_json_file(bert_config_file)\n    print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n    model = BertForPreTraining(config)\n\n    # Load weights from tf checkpoint\n    load_tf_weights_in_bert(model, config, tf_checkpoint_path)\n\n    # Save pytorch-model\n    print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n    torch.save(model.state_dict(), pytorch_dump_path)\nconvert_tf_checkpoint_to_pytorch(\"biobert_v1.1_pubmed/model.ckpt-1000000\", \"biobert_v1.1_pubmed/bert_config.json\", \"biobert_v1.1_pubmed/pytorch_model.bin\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls biobert_v1.1_pubmed\n!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json\n!ls biobert_v1.1_pubmed\nmodel_version = 'biobert_v1.1_pubmed'\ndo_lower_case = True\nmodel = BertModel.from_pretrained(model_version)\ntokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\ndef embed_text(text, model):\n    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n    outputs = model(input_ids)\n    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n    return last_hidden_states\n\ndef get_similarity(em, em2):\n    return cosine_similarity(em.detach().numpy(), em2.detach().numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coronavirus_em = embed_text(\"Coronavirus\", model).mean(1)\n# We will use a mean of all word embeddings.\nmers_em = embed_text(\"Middle East Respiratory Virus\", model).mean(1)\nflu_em = embed_text(\"Flu\", model).mean(1)\ndog_em = embed_text(\"Bog\", model).mean(1)\nprint(\"Similarity for Coronavirus and Flu:\" + str(get_similarity(coronavirus_em, flu_em)))\nprint(\"Similarity for Coronavirus and MERs:\" + str(get_similarity(coronavirus_em, mers_em)))\nprint(\"Similarity for Coronavirus and Bog:\" + str(get_similarity(coronavirus_em, dog_em)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see anecdotally even in the raw embeddings there seems to be at least some correlation between concepts. Note that our embedding method \n\nLet's now look at visualizing some of these vectors with U-Map. I'm choosing U-Map here due to the high-dimensionality of the data (768-D) and its ability to scale. However, I will also add some T-SNE visualizations below if I have time"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install umap-learn\nimport umap\nreducer = umap.UMAP()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport json \ndef make_the_embeds(number_files, start_range=0, \n                    the_path=\"/kaggle/input/CORD-19-research-challenge/2020-03-13/comm_use_subset/comm_use_subset\", data_key=[\"metadata\", \"title\"]):\n    the_list = os.listdir(the_path)\n    title_embedding_list = [] \n    title_list = []\n    for i in range(start_range, number_files):\n        file_name = the_list[i]\n        final_path = os.path.join(the_path, file_name)\n        with open(final_path) as f:\n            data = json.load(f)\n        tensor, title = make_data_embedding(data, data_key)\n        title_embedding_list.append(tensor)\n        title_list.append(title)\n    return torch.cat(title_embedding_list, dim=0), title_list\n        \ndef make_data_embedding(article_data, data_keys, method=\"mean\", dim=1):\n    data = article_data\n    for key in data_keys:\n        data = data[key]\n    text = embed_text(data, model)\n    if method == \"mean\":\n        return text.mean(dim), data\n    \n#embed_list, title_list = make_the_embeds(200)\n#red = reducer.fit_transform(embed_list.detach().numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I found 200 to be a good chunk size for running quick analysis as doing a full plot can get kind of crowded and is slow to compute."},{"metadata":{"trusted":true},"cell_type":"code","source":"from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\nfrom bokeh.palettes import Spectral10, Category20c\nfrom bokeh.palettes import magma\nimport pandas as pd\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_plot(red, title_list, number, color = True):\n    digits_df = pd.DataFrame(red, columns=('x', 'y'))\n    digits_df['digit'] = title_list\n    datasource = ColumnDataSource(digits_df)\n    plot_figure = figure(\n    title='UMAP projection of the article title embeddings',\n    plot_width=890,\n    plot_height=600,\n    tools=('pan, wheel_zoom, reset')\n    )\n\n    plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n    <div>\n    <div>\n        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'/>\n    </div>\n    <div>\n        <span style='font-size: 10px; color: #224499'></span>\n        <span style='font-size: 10px'>@digit</span>\n    </div>\n    </div>\n    \"\"\"))\n    if color:\n        color_mapping = CategoricalColorMapper(factors=title_list, palette=magma(number))\n        plot_figure.circle(\n            'x',\n            'y',\n            source=datasource,\n            color=dict(field='digit', transform=color_mapping),\n            line_alpha=0.6,\n            fill_alpha=0.6,\n            size=7\n        )\n        show(plot_figure)\n    else:\n        \n        plot_figure.circle(\n            'x',\n            'y',\n            source=datasource,\n            color=dict(field='digit'),\n            line_alpha=0.6,\n            fill_alpha=0.6,\n            size=7\n        )\n        show(plot_figure)\n    \n#make_plot(red, title_list, 200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There do seem to be a few interesing patterns when analyizng with U-Map. However, I believe fine-tuning methods could definitely improve the clustering of groups. Let's examine another chunk:"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_list2, title_list2 = make_the_embeds(401, 201)\n#red2 = reducer.fit_transform(embed_list.detach().numpy())\n#print(len(title_list2))\n#make_plot(red2, title_list2, 200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll attempt to make a plot of all ~~9000~~ ~~1000~~ (that did make it run out of RAM)  articles in that directory (warning this might crash your notebook). For fun we'll make these a different 1000 then what we already viewed."},{"metadata":{"trusted":true},"cell_type":"code","source":"#max_len = len(os.listdir(\"/kaggle/input/CORD-19-research-challenge/2020-03-13/comm_use_subset/comm_use_subset\"))\n#embed_list, title_list_full = make_the_embeds(2000,1200)\n#red_full = reducer.fit_transform(embed_list.detach().numpy())\n#make_plot(red_full, title_list_full, 256, color=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing with T-SNE "},{"metadata":{},"cell_type":"markdown","source":"## Part 2 Search Attempts on Titles"},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections\nsearch_terms = embed_text(\"coronavirus infection origin\", model).mean(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def top_n_closest(search_term_embedding, title_embeddings, original_titles, n=10):\n    proximity_dict = {}\n    i = 0 \n    for title_embedding in title_embeddings:\n        proximity_dict[original_titles[i]] = {\"score\": get_similarity(title_embedding.unsqueeze(0),search_term_embedding), \n                                              \"title_embedding\":title_embedding}\n        i+=1\n    order_dict = collections.OrderedDict({k: v for k, v in sorted(proximity_dict.items(), key=lambda item: item[1][\"score\"])})\n    proper_list = list(order_dict.keys())[-n:]\n    return proper_list, order_dict\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_titles, order_dict = top_n_closest(search_terms, embed_list2, title_list2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_titles","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results actually don't seem that bad given the model doesn't have any specific training."},{"metadata":{},"cell_type":"markdown","source":"DEMO"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz\nimport en_core_sci_sm as en\n\n\nnlp = en.load()\n#while True:\n#text = input(\"question \")\ntext = \"What is known about covid-19 incubation period?\"\ndoc = nlp(text)\n\nprint(list(doc.ents))\ntxt = \"\"\nfor ent in list(doc.ents):\n    txt += str(ent)\n    txt += \" \"\n\nsearch_terms2 = embed_text(txt, model).mean(1)\ntop_titles2, order_dict1 = top_n_closest(search_terms, embed_list2, title_list2)\nprint(top_titles2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Embedding Abstracts\nJust for fun and to enrich our knowledge later let's try embedding abstracts. "},{"metadata":{"trusted":true},"cell_type":"code","source":"absd_embeds, abs_orig = make_the_embeds(4, 2, data_key=['abstract', 0, \"text\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the following abstracts will be hard to display in U-Map I won't plot them. Instead let's just look at these two"},{"metadata":{"trusted":true},"cell_type":"code","source":"abs_orig[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abs_orig[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_similarity(absd_embeds[0].unsqueeze(0), absd_embeds[1].unsqueeze(0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I honestly don't know enough about the subject area to tell if that is a good similarity score for those two. I'll add some more examples in a bit, but for now that should serve as good intro."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}