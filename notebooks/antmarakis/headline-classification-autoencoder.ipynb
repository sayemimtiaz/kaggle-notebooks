{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nIn this kernel we will build an autoencoder to discriminate between real and fake news headlines. The dataset we will use is [A Million News Headlines](https://www.kaggle.com/therohk/million-headlines), which contains real news headlines. For evaluation, we will use [Fake News Net](https://github.com/KaiDMML/FakeNewsNet) and [Fake News Dataset](http://web.eecs.umich.edu/~mihalcea/downloads.html#FakeNews). These datasets contain both real and fake headlines. After performing some basic pre-processing, we will train an autoencoder to represent real headlines. To perform classification, we will reconstruct inputs and calculate the overall reconstruction error. If the error is above a certain threshold, the item will be classified as fake, otherwise as real. To evaluate the network's performance, we will measure percentile and F1-score accuracy metrics."},{"metadata":{},"cell_type":"markdown","source":"## Reading Data\n\nFirst we are going to read the evaluation data for real and fake headlines, and then we are going to concatenate the two dataframes."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nd_fake = pd.read_csv('../input/fake-news-data/fnn_politics_fake.csv')\nheadlines_fake = d_fake.drop(['id', 'news_url', 'tweet_ids'], axis=1).rename(columns={'title': 'headline'})\nheadlines_fake['fake'] = 1\n\nd_real = pd.read_csv('../input/fake-news-data/fnn_politics_real.csv')\nheadlines_real = d_real.drop(['id', 'news_url', 'tweet_ids'], axis=1).rename(columns={'title': 'headline'})\nheadlines_real['fake'] = 0\n\neval_data = pd.concat([headlines_fake, headlines_real])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we will read fake and real headlines from the Fake News Dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\ndef read_data(d):\n    \"\"\"Each file has a headline as the first line, followed by some white space and then the article content.\n    We need to exract the headline and the content of each file and store them in lists.\"\"\"\n    files = os.listdir(d)\n    headlines, contents = [], []\n    for fname in files:\n        if fname[:5] != 'polit':\n            continue\n        \n        f = open(d + '/' + fname)\n        text = f.readlines()\n        f.close()\n\n        if len(text) == 2:\n            # One of the lines is missing\n            if len(text[1]) <= 1:\n                # There is no article content or headline\n                continue\n        elif len(text) >= 3:\n            # More than one empty line encountered\n            text[1] = text[-1]\n        else:\n            # Only one or zero lines is file\n            continue\n        \n        headline, content = text[0][:-1].strip().rstrip(), text[1][:-1]\n        headlines.append(headline)\n        contents.append(content)\n    \n    return headlines, contents\n\n\nfake_dir = '../input/fake-news-data/fnd_news_fake'\nfake_headlines, fake_content = read_data(fake_dir)\nfake_headlines = pd.DataFrame(fake_headlines, columns=['headline'])\nfake_headlines['fake'] = 1\n\nreal_dir = '../input/fake-news-data/fnd_news_real'\nreal_headlines, real_content = read_data(real_dir)\nreal_headlines = pd.DataFrame(real_headlines, columns=['headline'])\nreal_headlines['fake'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now concatenate these two new datasets into an evaluation dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_data = pd.concat([eval_data, fake_headlines, real_headlines])\neval_data['fake'].value_counts()\neval_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's read our training data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_news = pd.read_csv('../input/all-the-news/articles3.csv', nrows=300000)\nall_news = all_news.rename(columns={'title': 'headline'})\nall_news['fake'] = 0\ndata = all_news[['headline', 'fake']]\n\n# data = pd.concat([data, all_news])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Processing\n\nWe also need to format the data. We will split the dataset into features `X` and target `Y`. For `Y`, we simply store the label at the target column. For `X`, we are first going to tokenise and pad our text input before storing it."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef format_data(data, max_features, maxlen, tokenizer=None, shuffle=False):\n    if shuffle:\n        data = data.sample(frac=1).reset_index(drop=True)\n    \n    data['headline'] = data['headline'].apply(lambda x: str(x).lower())\n\n    X = data['headline']\n    Y = data['fake'].values # 0: Real; 1: Fake\n\n    if not tokenizer:\n        filters = \"\\\"#$%&()*+./<=>@[\\\\]^_`{|}~\\t\\n\"\n        tokenizer = Tokenizer(num_words=max_features, filters=filters)\n        tokenizer.fit_on_texts(list(X))\n\n    X = tokenizer.texts_to_sequences(X)\n    X = pad_sequences(X, maxlen=maxlen)\n\n    return X, Y, tokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `max_features` and `max_len` variables denote the length of each vector and the vocabulary length."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features, max_len = 5000, 25\nX, Y, tokenizer = format_data(data, max_features, max_len, shuffle=True)\nX_eval, Y_eval, tokenizer = format_data(eval_data, max_features, max_len, tokenizer=tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\npickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model\n\nThe model we will use is based around a bi-directional RNN (either GRU or LSTM), with max pooling. The encoder is comprised of two RNN layers, while the decoder uses an RNN with a dense layer on top of it. The reconstruction of the original input occurs on a final Dense layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input, Dense, Bidirectional, GRU, Embedding, Dropout, LSTM\nfrom keras.layers import concatenate, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.models import Model\nfrom keras import regularizers\n\nepochs=20\n\n# Input shape\ninp = Input(shape=(max_len,))\n\nencoder = Embedding(max_features, 50)(inp)\nencoder = Bidirectional(LSTM(75, return_sequences=True))(encoder)\nencoder = Bidirectional(LSTM(25, return_sequences=True,\n                        activity_regularizer=regularizers.l1(10e-5)))(encoder)\n\ndecoder = Bidirectional(LSTM(75, return_sequences=True))(encoder)\ndecoder = GlobalMaxPooling1D()(decoder)\ndecoder = Dense(50, activation='relu')(decoder)\ndecoder = Dense(max_len)(decoder)\n\nmodel = Model(inputs=inp, outputs=decoder)\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X, X, epochs=epochs, batch_size=64, verbose=1)\n\nmodel.save_weights('model{}.h5'.format(epochs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X, X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time to compute our results!"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.predict(X_eval, batch_size=1, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification\n\n*(code here is modified from [this blog](https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd))*\n\nNow we need to calculate the reconstruction error of the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"mse = np.mean(np.power(X_eval - results, 2), axis=1)\nerror_df = pd.DataFrame({'reconstruction_error': mse,\n                         'true_class': Y_eval})\nerror_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now need to compute the optimal threshold to make our predictions. We will split the process into two:\n\n1. Find the general range where the threshold lies.\n2. In that range, find a more specific threshold value."},{"metadata":{"trusted":true},"cell_type":"code","source":"LABELS = ['REAL', 'FAKE']\nbest, threshold = -1, -1\n\n# General Search\nfor t in range(0, 3500000, 10000):\n    y_pred = [1 if e > t else 0 for e in error_df.reconstruction_error.values]\n    score = f1_score(y_pred, error_df.true_class, average='micro', labels=[0, 1])\n    if score > best:\n        best, threshold = score, t\n\n# Specialized Search around general best\nfor t in range(threshold-10000, threshold+10000):\n    y_pred = [1 if e > t else 0 for e in error_df.reconstruction_error.values]\n    score = f1_score(y_pred, error_df.true_class, average='micro', labels=[0, 1])\n    if score > best:\n        best, threshold = score, t\n\nprint(threshold, best)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to visualize the data points against the threshold line."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ngroups = error_df.groupby('true_class')\nfig, ax = plt.subplots()\n\nfor name, group in groups:\n    ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n            label=\"Fake\" if name == 1 else \"Real\")\n\nax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\nax.legend()\nplt.title(\"Reconstruction error for different classes\")\nplt.ylabel(\"Reconstruction error\")\nplt.xlabel(\"Data point index\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LABELS = ['FAKE', 'REAL']\nerrors = error_df.reconstruction_error.values\ny_pred = [1 if e > threshold else 0 for e in errors] # final predictions\nconf_matrix = confusion_matrix(error_df.true_class, y_pred)\nplt.figure(figsize=(12, 12))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we are going to compute the F1 score as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef accuracy_f1(preds, correct):\n    \"\"\"Returns F1-Score for predictions\"\"\"\n    return f1_score(preds, correct, average='micro', labels=[0, 1])\n\naccuracy_f1(y_pred, error_df.true_class)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling Error\n\nRight now the errors lie in $[0, \\infty)$. It is useful in some cases (for example, using these predictions in ensembling) to scale the error in $[0, 1]$. We cannot though simply min-max all of the values together, since then the final output wouldn't be a representative probability. Instead, we are going to do the following: Values below the threshold will be scaled to $[0, 0.5]$ and values above the threshold will be scaled to $[0.5, 1]$. The threshold, after scaling, is set to `0.5`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nminmax_0_05 = MinMaxScaler(feature_range=(0, 0.5))\nminmax_05_1 = MinMaxScaler(feature_range=(0.5, 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the scalers initialized, we need to fit them. We are going to fit `minmax_0_05` to items below the threshold, and `minmax_05_1` to items above the threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"errors_below = np.array([i for i, e in enumerate(errors) if e <= threshold])\nerrors_above = np.array([i for i, e in enumerate(errors) if e > threshold])\n\nminmax_0_05.fit(errors[errors_below].reshape(-1, 1))\nminmax_05_1.fit(errors[errors_above].reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we are going to convert our errors array to the scaled outputs."},{"metadata":{"trusted":true},"cell_type":"code","source":"errors_mm = np.array([minmax_0_05.transform(e.reshape(1, -1)) if i in errors_below\n                      else minmax_05_1.transform(e.reshape(1, -1))\n                      for i, e in enumerate(errors)]).flatten()\n\ny_pred2 = [1 if e > 0.5 else 0 for e in errors_mm]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are now going to calculate the percentile accuracy of the scaled predictions, alongside the F1-score (this score may differ slightly for values right around the threshold, but the difference is negligible)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy_percentile(preds, Y_validate):\n    \"\"\"Return the percentage of correct predictions for each class and in total\"\"\"\n    real_correct, fake_correct, total_correct = 0, 0, 0\n    _, (fake_count, real_count) = np.unique(Y_validate, return_counts=True)\n\n    for i, r in enumerate(preds):\n        if r == Y_validate[i]:\n            total_correct += 1\n            if r == 0:\n                fake_correct += 1\n            else:\n                real_correct += 1\n\n    print('Real Accuracy:', real_correct/real_count * 100, '%')\n    print('Fake Accuracy:', fake_correct/fake_count * 100, '%')\n    print('Total Accuracy:', total_correct/(real_count + fake_count) * 100, '%')\n\n\naccuracy_percentile(y_pred2, error_df.true_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef accuracy_f1(preds, correct):\n    \"\"\"Returns F1-Score for predictions\"\"\"\n    return f1_score(preds, correct, average='micro', labels=[0, 1])\n\naccuracy_f1(y_pred2, error_df.true_class)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we'll store the predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(errors_mm).to_csv('autoencoder.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}