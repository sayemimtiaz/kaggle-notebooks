{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Goal\nCOVID-19 Open Research Dataset Challenge has 10 Tasks where each task has several sub tasks, each an insightful question focused on some aspect of Covid-19 pandemic and the virus causing it. In recent times we have seen remarkable progress in Deep-Learning models for machine reading comprehension (MRC) style question answering (Q&A). The MRC style Q&A model takes as input a question and a context and extracts the answer from the context. \n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pip install pickledb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport pandas as pd\n\ndef extractData(csvfile, jsonfile):\n    data = pd.read_csv(csvfile) # add exception handling incase filepath it wrong or does not exists\n    data_with_abstract = data[data.abstract.notnull()]\n    with open(jsonfile, 'a+') as json_out_file:\n        for index, row in data_with_abstract.iterrows():\n            json_data = {}\n            json_data[\"id\"] = index\n            json_data['articleID'] = row['cord_uid']\n            json_data[\"text\"] = row['abstract']\n            if json_data[\"text\"] == \"Unknown\":\n                continue\n            json.dump(json_data, json_out_file)\n            json_out_file.write('\\n')\n    print(\"Done !!!\")\n            \ninputfile = \"/kaggle/input/CORD-19-research-challenge/metadata.csv\"\noutputfile = \"covid-19.json\"\n\nextractData(inputfile, outputfile)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are going to compute the USE embeddings for all the abstract and store it in a pickle file. We will also store the corresponding abstract in a pickleDB file."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_hub as hub\nimport numpy as np\nimport pickle\nimport pickledb\nimport random\n\nclass EmbeddingStore:\n    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n\n    def __init__(self, filename):\n        self.embedding_file = filename\n        self.all_docs_embeddings = self.read_embeddings()\n        if self.all_docs_embeddings is None:\n            print(\"all_docs initialized to None\")\n\n    def save_embeddings(self):\n        f = open(self.embedding_file, 'wb')\n        pickle.dump(self.all_docs_embeddings, f)\n\n    def read_embeddings(self):\n        try:\n            f = open(self.embedding_file, 'rb')\n        except FileNotFoundError:\n            return None\n        try:\n            embeddings_ = pickle.load(f)\n        except EOFError:\n            embeddings_ = None\n        return embeddings_\n\n    def add_embeddings(self, new_embeddings_):\n        if self.all_docs_embeddings is None:\n            combined_embeddings_ = new_embeddings_\n        else:\n            combined_embeddings_ = np.concatenate((self.all_docs_embeddings, new_embeddings_))\n        self.all_docs_embeddings = combined_embeddings_\n\n    def dump(self):\n        self.save_embeddings()\n\n    def add_para(self, new_para_):\n        new_embeddings_ = self.embed([new_para_])\n        self.add_embeddings(new_embeddings_)\n\n    def find_top_k_match(self, query, k):\n        #print(\"Query received = {}\".format(query))\n        query_embedding_ = self.embed(query)\n        #print(query_embedding_.shape)\n        #print(query_embedding_[0].shape)\n        #print(self.all_docs_embeddings.shape)\n        corr = np.inner(query_embedding_[0], self.all_docs_embeddings)\n        #print(corr)\n        values = np.argpartition(corr, -k)[-k:]\n        #print(\"Top K matches for = {} at {}\".format(query, values))\n        return values\n\nclass ParaStore:\n\n    def __init__(self, docsfilename, embeddingsfile):\n        self.docfile = docsfilename\n        self.embeddingstore = EmbeddingStore(embeddingsfile)\n        self.all_docs = self.read_para()\n        self.nos_docs = self.all_docs.totalkeys()\n        print(\"There are {} docs in the store\".format(self.nos_docs))\n\n    def save_para(self):\n        self.all_docs.dump()\n\n    def read_para(self):\n        docs = pickledb.load(self.docfile, False)\n        return docs\n\n    # Changed this function to use the aid instead of text\n    def already_exists(self, new_para):\n        allkeys = self.all_docs.getall()\n        for key in allkeys:\n            if new_para[\"aid\"] == self.all_docs.get(key)[\"aid\"]:\n                print(\"{} already in the doc store\".format(new_para[\"aid\"]))\n                return True\n        return False\n\n    def add_para(self, new_para):\n        text = new_para[\"text\"]\n        if not self.already_exists(new_para):\n            self.all_docs.set(str(self.nos_docs), new_para)\n            self.embeddingstore.add_para(text)\n            self.nos_docs = self.nos_docs+1\n            if self.nos_docs % 1000 == 0:\n                self.dump()\n    def dump(self):\n        self.all_docs.dump()\n        self.embeddingstore.dump()\n\n    def get_para(self, nos):\n        return self.all_docs.get(str(nos))\n\n    def get_matching_para(self, query):\n        pos = self.embeddingstore.find_top_match(query)\n        match = self.get_para(pos)\n        return match\n\n    def get_matching_k_para(self, query, k):\n        positions = self.embeddingstore.find_top_k_match(query, k)\n        matches = {}\n        for i in positions:\n            match = self.get_para(i)\n            matches[str(i)] = match\n        return matches\n\n    def filter_k_randomly(self, matches, k):\n        kmatches = {}\n        #print(\"Filtering {} from {} matches\".format(k, len(matches)))\n        if len(matches) <= k:\n            return matches\n        else:\n           indexes = random.sample(matches.keys(), k)\n        for i in indexes:\n            kmatches[i] = matches[i]\n        return kmatches\n\n    def applyFilter(self, filt, text):\n        for name in filt:\n            if text and name.lower() in text.lower():\n                return True\n        return False\n        \n    def get_including_k_para(self, query, filt, k):\n        matches = {}\n        allkeys = self.all_docs.getall()\n        for key in allkeys:\n            candidate = self.all_docs.get(key)\n            if (query[0] in candidate[\"text\"]) and (self.applyFilter(filt, candidate[\"text\"])):\n                matches[key] = candidate\n        kmatches = self.filter_k_randomly(matches, k)\n        return kmatches","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optional\nImport the data i.e. compute the USE embeddings for all the data and store them. Its take considerable time to compute these embeddings. Since it needs to be computed only once, we have computed the embeddings of 37K abstracts and uploaded it as a dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"def openStore(docfile, embedding_file):\n    docstore = ParaStore(docfile, embedding_file)\n    return docstore\n\ndef importFromDrqa(filename, dbfile, embedfile):\n    docstore = openStore(dbfile, embedfile)\n    with open(filename) as f:\n        line = f.readline()\n        while line:\n            para = {}\n            #print(line)\n            data = json.loads(line)\n            #print(data[\"id\"])\n            para[\"aid\"] = data[\"articleID\"]\n            para[\"text\"] = data[\"text\"]\n            docstore.add_para(para)\n            line = f.readline()\n        docstore.dump()\n        \nfilename = \"covid-19.json\"\npickledbfile = \"cord19.db\"\nembedfile = \"cord19.pkl\"\n\nimportFromDrqa(filename, pickledbfile, embedfile)\nprint(\"All Document Imported\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to use the sub task description to find the abstract that are related to them using cosine similarity."},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\nimport json\n\ndef openStore(docfile, embedding_file):\n    docstore = ParaStore(docfile, embedding_file)\n    return docstore\n\ndef getTopKMatch(dbfile, embedfile, query, k=1):\n    # open the store\n    query_vector = [query]\n    print(\"The query vector is {}\".format(query_vector))\n    docstore = openStore(dbfile, embedfile)\n    matches = docstore.get_matching_k_para(query_vector, k)\n    return matches\n\ndef getTopKStringMatch(dbfile, embedfile, filt, query, k=1):\n    # open the store\n    query_vector = [query]\n    print(\"The query vector is {}\".format(query_vector))\n    docstore = openStore(dbfile, embedfile)\n    matches = docstore.get_including_k_para(query_vector, filt, k)\n    return matches\n\ndef createBigRow(med, aids, abstracts):\n    print(\"Creating a big row of {}, {}\".format(med,aids))\n    big_row = list()\n    big_row.append(med)\n    for i in aids:\n        big_row.append(i)\n    for abs in abstracts:\n        big_row.append(abs)\n    return big_row\n\ndef pad2K(data, k):\n    if len(data) >= k:\n        return data\n    else:\n        for i in range(len(data), k):\n            data.append(\"None\")\n    return data\n\ncovid19_names = {\n    'COVID19',\n    'COVID-19',\n    '2019-nCoV',\n    '2019-nCoV.',\n    'coronavirus disease 2019',\n    'Corona Virus Disease 2019',\n    '2019-novel Coronavirus',\n    'SARS-CoV-2',\n}\n\nheader = ['Question', 'covid-uid 1', 'covid-uid 2', 'covid-uid 3', 'covid-uid 4', 'covid-uid 5', 'Abstract 1', 'Abstract 2', 'Abstract 3', 'Abstract 4', 'Abstract 5']\ndef evalQueryDetailCSV(filename, outfile, match):\n    with open(filename) as f, open(outfile, 'w') as output:\n        filewriter = csv.writer(output, delimiter=',')\n        line = f.readline()\n        filewriter.writerow(header)\n        while line:\n            line = line.replace(\"\\'\", \"\\\"\")\n            data = json.loads(line)\n            q_id = data['id']\n            q = data['text']\n            med = q\n            if match == 'USE':\n                topk = getTopKMatch(paraFile, embeddingFile, q, 5)\n            else:\n                topk = getTopKStringMatch(paraFile, embeddingFile, covid19_names, q, 5)\n            aids = []\n            abstracts = []\n            for key in topk.keys():\n                para = topk.get(key)\n                aids.append(para[\"aid\"])\n                #print(para[\"aid\"])\n                abstracts.append(para[\"text\"].encode('utf-8'))\n            aids = pad2K(aids, 5)\n            abstracts = pad2K(abstracts, 5)\n            big_row = createBigRow(med, aids, abstracts)\n            filewriter.writerow(big_row)\n            line = f.readline()\n            \nqueryFile = \"/kaggle/input/taskdescription/task-1-original-q.json\"\nparaFile = \"cord19.db\"  # Use precomputed embeddings, this is to save time\nembeddingFile = \"cord19.pkl\" # Use precomputed embeddings, this is to save time\n\n\n#paraFile = \"/kaggle/input/embeddings/cord19.db\"  # Use precomputed embeddings, this is to save time\n#embeddingFile = \"/kaggle/input/embeddings/cord19.pkl\" # Use precomputed embeddings, this is to save time\n\n\nevalQueryDetailCSV(queryFile, \"top-5.csv\", 'USE')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}