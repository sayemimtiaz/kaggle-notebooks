{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom numpy.random import RandomState\nimport matplotlib.pyplot as plt\nimport pickle\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n#CLASSIFIERS FOR TRAINING\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/fetal-health-classification/fetal_health.csv\") \nprint(\"♦ LOOK AND CHECK DATA:\")\nprint(data.head())\nprint()\nprint(\"♦ DATASET LENGHT = \", len(data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"There are 22 columns here (21 columns are our input data and the last one column will be used as prediction column). Also, it has 2126 rows, it is 2126 measurements extracted from cardiotocograms and classified by expert obstetricians into 3 categories:\n\n* Normal\n* Suspect\n* Pathological"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = data.columns\nprint(cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ANALYZE DATA\n\nOne of the most important things is to understand data which you work with. Here we will use some well-known methods for easier understanding of our data.\n\nFor all dataset columns we will find some statistical information, like: Mean, Median, Mode, Standard Deviation and Correlation using Pandas functions."},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = data.mean(axis=0)\nprint(mean)\n\nmedian = data.median(axis=0)\nprint(median)\n\nmode = data.mode(axis=0)\nprint(mode)\n\nstd = data.std(axis=0)\nprint(std)\n\nprint(data.corrwith(data[\"fetal_health\"]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NORMALIZE DATA\n\nIt is necessary to normalize data before making some classification tasks. Here i will normalize it in a range from 0 to 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"#STEP-1\nscaler = MinMaxScaler(feature_range=(0, 1))\n#STEP-2\ntemp = data[\"fetal_health\"]\n#STEP-3\nnorm_data = scaler.fit_transform(data)\n#STEP-4\ndata = pd.DataFrame(data=norm_data, columns=cols)\n#STEP-5\ndata[\"fetal_health\"] = temp\n#STEP-6\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SPLIT DATASET INTO TRAIN AND TEST DATA\n\nSo, here i will divide the dataset into two parts (for model training and validation); 70%:30% respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrng = RandomState()\n\n#STEP-2\ntrain = data.sample(frac=0.7, random_state=rng)\nval = data.loc[~data.index.isin(train.index)]\n\n#STEP-3\ntrain.reset_index(drop=True, inplace=True)\nval.reset_index(drop=True, inplace=True)\n\n#STEP-4\nprint(\"♦ TRAIN SET:\")\nprint(train.head())\nprint()\nprint(\"♦ VALIDATION SET:\")\nprint(val.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"RENAMING VARIABLES\n\nI will create the X and Y variables for input and target values respectively for better comprehension"},{"metadata":{"trusted":true},"cell_type":"code","source":"#STEP-1\nx_columns = cols[:-1]\ny_column = cols[-1]\n\n#STEP-2\nx_raw_train = train[x_columns]\ny_raw_train = train[y_column]\n\n#STEP-3\nX_train = x_raw_train.copy()\nY_train = y_raw_train.copy()\n\n#STEP-4\nprint(\"♦ X_TRAIN: \")\nprint(X_train.head())\nprint()\nprint(\"♦ Y_TRAIN: \")\nprint(Y_train.head())\nprint()\n\n#STEP-5\nx_raw_val = val[x_columns]\ny_raw_val = val[y_column]\n\n#STEP-6\nX_val = x_raw_val.copy()\nY_val = y_raw_val.copy()\n\n#STEP-7\nprint(\"♦ X_VAL: \")\nprint(X_val.head())\nprint()\nprint(\"♦ Y_VAL: \")\nprint(Y_val.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we are going to use classifiers from the sklearn library on our dataset\n\nThen we will train, validate, save and load models"},{"metadata":{"trusted":true},"cell_type":"code","source":"#STEP-1\nall_classifers = [\n    KNeighborsClassifier(3),\n    SVC(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression()\n]\n\n#STEP-2\nall_acc = {}\n\n#STEP-3\nfor classifier in all_classifers:\n    #STEP-4\n    model = classifier\n    model.fit(X_train, Y_train)\n    #STEP-5\n    model_pred = model.predict(X_val)\n    model_acc = accuracy_score(Y_val, model_pred)\n    #STEP-6\n    classfier_name = classifier.__class__.__name__\n    #STEP-7\n    all_acc[classfier_name] = model_acc\n    #STEP-8\n    filename = classfier_name+'_model.pickle'\n    pickle.dump(model, open(filename, 'wb'))  \n    #STEP-9\n    loaded_model = pickle.load(open(filename, 'rb'))\n    result = loaded_model.score(X_val, Y_val)     \n    #STEP-10\n    print(\"♦ {:<30} = {:<12} {:>25} = {:>12}\".format(classfier_name, model_acc, 'loaded pickle model', result))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to plot and visualize the output from the selected classifiers to determine which is the most appropriate"},{"metadata":{"trusted":true},"cell_type":"code","source":"#STEP-1\nall_acc = dict(sorted(all_acc.items(), key=lambda item: item[1], reverse=True))\n\n#STEP-2\nkeys = all_acc.keys()\nvalues = all_acc.values()\n\n#STEP-3\nplt.figure(figsize=(10,5))\nplt.title('ACCURCY OF CLASSIFIERS')\nplt.xlabel('classifiers')\nplt.ylabel('accuracy')\nplt.bar(keys, values, color=\"g\")\n\n#STEP-4\nplt.xticks(rotation=90)\n\n#STEP-5\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CONCLUSION\n\nFrom our plot, we can say that the GradientBoostingClassifier is the most appropriate classifier for this dataset.\n\nThank you for following through"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}