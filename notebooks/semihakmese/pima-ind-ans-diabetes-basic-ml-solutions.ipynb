{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Tutorial w/ Pima Indians Diabates Database "},{"metadata":{},"cell_type":"markdown","source":"<font color =\"green\">\n### Content:\n1. [Load and Check Data](#1)\n2. [Basic Data Analysis](#2)\n3. [Visualisation of the Data](#3)\n4. [Machine Learning Algorithms](#4)\n    * [Supervised Learning](#5) \n        * [Regression](#6) \n            1. [Linear Regression](#7)\n            1. [Polynomial Linear Regression](#8) \n            1. [Decision Tree Regression](#9)\n            1. [Random Forest Regression](#10)\n            \n        * [Classification](#11) \n            1. [Logistic Regression Classification](#12)\n            1. [K-Nearest Neighbour (KNN) Classification](#13)\n            1. [Support Vector Machine (SVM) Classification](#14)\n            1. [Naive Bayes Classification](#15)\n            1. [Decision Tree Classification](#16)\n            1. [Random Forest Classification](#17)\n                \n                - [Perfomance Comparison of Classification Methods](#18)\n       \n       * [Evaluation Classification Method](#19) \n            1. [Confusion Matrix](#19)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id ='1'></a>\n## Loading and Check Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Loading Data\ndata = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id ='2'></a>\n## Basic Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see the columns\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# then we need to see info\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outcome is 1, means that patience is sick 0 is healthy\n"},{"metadata":{},"cell_type":"markdown","source":"<a id ='3'></a>\n## Visualisation of the Data\n- pd.plotting.scatter_matrix:\n- \n    - green: healthy\n    - red: sick\n    - c: color\n    - figsize: figure size\n    - diagonal: histohram of each features\n    - alpha: opacity\n    - s: size of marker\n    - marker: marker type"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's Visualize the Data \ncolor_list = [\"red\" if i == 1 else \"green\" for i in data.loc[:,\"Outcome\"]]\npd.plotting.scatter_matrix(data.loc[:,data.columns !=\"Outcome\"],\n                          c=color_list,\n                          figsize = [20,20],\n                          diagonal =\"hist\",\n                          alpha = 0.6,\n                          s=200,\n                          marker =\"*\",\n                          edgecolor = \"black\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To see the distrubution of the outcome we'll use seaborn sns.countplot\nimport seaborn as sns\nsns.countplot(x=\"Outcome\", data = data)\ndata.loc[:,\"Outcome\"].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see there are 500 healthy and 268 sick people"},{"metadata":{},"cell_type":"markdown","source":"<a id ='4'></a>\n## Machine Learning "},{"metadata":{},"cell_type":"markdown","source":"<a id ='5'></a>\n### ***Supervised Learning Algorithms***"},{"metadata":{},"cell_type":"markdown","source":"### Train and Test Data"},{"metadata":{},"cell_type":"markdown","source":"First of all, we need to split our data for training and testing "},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.Outcome.values\nx_data = data.iloc[:,:-1]\nx = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data))\n\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.30, random_state = 42) #Yüzde 25 i x_test vey_test e atanacak \n\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T\n\nprint(\"x_train shape\",x_train.shape)\nprint(\"y_train shape\",y_train.shape)\nprint(\"x_test shape\",x_test.shape)\nprint(\"y_test shape\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"To apply manual method we need to get Transpose version of the data"},{"metadata":{},"cell_type":"markdown","source":"### Inıtializing Parameters and Sigmoid Function\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Initializing Parameters and Sigmoid Function \n\ndef initialize_weights_and_bias(dimension): #30 feature var o zaman 30 dimension olmalı\n    \n    w = np.full((dimension,1),0.01) #burada dimension 30 girdiğimiz zaman [0,0.01] lik weightler atayacagız\n    b = 0.0 #float olsun diye 0.0 yazdım\n    return w,b\n# w,b = initialize_weight_and_bias(30)\n\ndef sigmoid(z):\n    y_head = 1/(1+ np.exp(-z)) #formülü budur z nin\n    return y_head\n\n#sigmoid(0) değeri 0.5 vermelidir \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Forward and Backward Propagation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Forward - Backward Propagation\n#Bu kısımda w ile train data mızı çarpacağız Bias ekleyip sigmoid fonksiyona sokacağız \n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    #Forward Prop \n    z = np.dot(w.T,x_train) + b #Transpoz alma sebebimiz matris carpımını yapabilmek için \n    y_head = sigmoid(z) #Sigmoid fonksiyonuna soktuk\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head) #Loss fonksiyonunu yazdık \n    cost = (np.sum(loss)) / x_train.shape[1] #Losslar toplamını normalize etmek için sample sayısına böldük \n    #x_train_shape[1] = 455\n    \n\n    #Backward Prop\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] #Formül bu, shape bölmek normalize etmek için\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Updating Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Updating Parameters \n\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    #Iteration \n    for i in range(number_of_iterarion):\n        #Doing forward and Backward Propagation \n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost) #Güncelleme öncesi cost list e atıyorum (Tüm cost listleri depolamak)\n        \n        #Updating \n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n    \n        if i %10 == 0:\n            cost_list2.append(cost) #Her 10 adımda bir costları depola    \n            index.append(i)\n            print(\"Cost after iteration %i: %f\"%(i,cost))\n            \n    #Number of iteration kaç olacagı kararını deneyerek bulacagız Türevi 0 a yaklaşınca yeterli olacaktır\n    #We updaate (learn) parameters weights and Bias \n    parameters = {\"weight\":w , \"bias\":b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation ='vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters,gradients,cost_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction "},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Prediction \ndef predict(w,b,x_test): #w,b zaten lazım ama x_test de class ı belli olmayan ve test edeceğim (tahmin edeceğim) data \n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    #Eger z 0.5 den büyük ise y_head = 1 yani kötü huylu\n    #Eger < 0.5 ise y_head = 0 yani iyi huylu \n    \n    for i in range(z.shape[1]):\n        if z[0,i]<=0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n            \n    return y_prediction\n        \n#şimdi y prediction u y test ile karşılastırıp eğitimin dogruluguna bakıcaz \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 400) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **By Using SKLEARN Library**\n- In this section we'll use just sklearn library to find exact values for all the process (regression, classification models) "},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading Data\ndata = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id ='6'></a>\n## Regression \n- In this section we'll learn also apply the Regression Methods such as:\n    * Linear Regression (y=b0 + b1x1)\n    * Polynomial Linear Regression (y=b0 + b1x1 + b2*x^2 + ... + bn*x^n)\n    * Decision Tree Regression \n    * Random Forest Regression \n    * EXTRA = Performance Analysis by using R-Square Method"},{"metadata":{},"cell_type":"markdown","source":"We use Regression Models to predict the future values by using data\nFor Example, we try to predict a house value,price in the California by using real data California house prices. "},{"metadata":{},"cell_type":"markdown","source":"<a id ='7'></a>\n### 1-) Linear Regression \n- y=b0 + b1x1"},{"metadata":{"trusted":true},"cell_type":"code","source":"data0 = data[data.Outcome == 0] # Healthy group\ndata1 = data[data.Outcome == 1] # Sick group\ndata1.sort_values(by=\"Age\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will use BloodPressure and Age parameters in the Sick group\ndata1 = data[data.Outcome == 1]\nxlin=data1.BloodPressure.values.reshape(-1,1)\nylin=data1.Age.values.reshape(-1,1)\n\n#Linear Regression Model\nfrom sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\n#Creating Prediction Space to get more efficient results\npredict_space = np.linspace(min(xlin),max(xlin)).reshape(-1,1)  \n#Fit\nlinear_reg.fit(xlin,ylin)\n#Prediction \npredicted = linear_reg.predict(predict_space)\n#Perfomance Analysis w/R^2 Score method\nprint(\"R^2 Score is :\",linear_reg.score(xlin,ylin))\n\nplt.plot(predict_space, predicted, color=\"black\",linewidth=2)\nplt.scatter(x=xlin, y=ylin)\nplt.xlabel(\"Blood Pressure\")\nplt.ylabel(\"Age\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **As you see our R^2 Score is too bad because this features are not able to use Linear Regression efficiently**\n- **But ı wanted to add this table here as an example** \n"},{"metadata":{},"cell_type":"markdown","source":"<a id ='8'></a>\n### 2-) Polynomial Linear Regression\n- y=b0 + b1x1 + b2*x^2 + ... + bn*x^n \n- This method kind a complex of Polynomial and Linear Regression thats why in this method's solution there are 2 steps and libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will use Glucose and Age parameters in the Sick group\ndata1 = data[data.Outcome == 1]\nxpol=data1.Glucose.values.reshape(-1,1) #In Sklearn we need to reshape our data like that\nypol=data1.Age.values.reshape(-1,1)\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\npoly_reg = PolynomialFeatures(degree = 3) #degree = 3 means we have limited the equation with x^3\nx_polynomial = poly_reg.fit_transform(xpol) #We transformed our xpol values to x^3\n#Fit (For fitting we use Linear Regression again..)\nlinear_reg2 = LinearRegression()  \nlinear_reg2.fit(x_polynomial,ypol)\n#Prediction \ny_head = linear_reg2.predict(x_polynomial)\n#Visualisation \nplt.plot(x_polynomial,y_head,color=\"red\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id ='9'></a>\n### 3-) Decision Tree Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will use BloodPressure and Age parameters in the Sick group\ndata1 = data[data.Outcome == 1]\nxdt=data1.BloodPressure.values.reshape(-1,1)\nydt=data1.Age.values.reshape(-1,1)\n\nfrom sklearn.tree import DecisionTreeRegressor\ndtreg = DecisionTreeRegressor()\n#Fit\ndtreg.fit(xdt,ydt) \n#Prediction space\nxdt_ = np.arange(min(xdt),max(xdt),0.01).reshape(-1,1)\ny_headdt = dtreg.predict(xdt_)\n#Visualisation \nplt.scatter(xdt,ydt,color =\"red\",label=\"Values\")\nplt.plot(xdt_,y_headdt,color=\"blue\",label=\"Predicted\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id ='10'></a>\n### 4-) Random Forest Regression \n- Random Forest Regression is a complex form of Decision Tree Regression that we work with many of Decision Tree Regression model \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's work same features in the Decision Tree Regression model that BloodPressure and Age \ndata1 = data[data.Outcome == 1]\nxrf=data1.BloodPressure.values.reshape(-1,1)\nyrf=data1.Age.values.reshape(-1,1)\n\nfrom sklearn.ensemble import RandomForestRegressor  \nrfreg = RandomForestRegressor(n_estimators = 100, #We work with 100 times decision tree reg\n                             random_state= 42)\n#Fit\nrfreg.fit(xrf,yrf) \n#Prediction space\nxrf_ = np.arange(min(xrf),max(xrf),0.01).reshape(-1,1)\ny_headrf = rfreg.predict(xdt_)\n#Visualisation \nplt.scatter(xrf,yrf,color =\"red\",label=\"Values\")\nplt.plot(xrf_,y_headrf,color=\"blue\",label=\"Predicted\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id ='11'></a>\n## Classification\n- In this section we'll learn and apply all the Classification Methods such as:\n    + Logistic Regression Classification\n    + K-Nearest Neighbour (KNN) Classification\n    + Support Vector Machine (SVM) Classification\n    + Naive Bayes Classification \n    + Decision Tree Classification \n    + Random Forest Classification\n    EXTRA : Evaluation Classification Methods (Alternative to score(x_test,y_test) method)\n        + Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.Outcome.values\nx_data = data.iloc[:,:-1]\nx = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data))\n\nfrom sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.30, random_state = 42) #Yüzde 25 i x_test vey_test e atanacak \n\n\nprint(\"x_train shape\",x_train.shape)\nprint(\"y_train shape\",y_train.shape)\nprint(\"x_test shape\",x_test.shape)\nprint(\"y_test shape\",y_test.shape)\n\n#When we need to do any process without any mistake, error arrays should be like that \n# 537,8 \n# 537,\n# 231,8\n# 231","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id ='12'></a>\n### Logistic Regression w/Sklearn Library\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"Test Accuracy : %{}\".format(lr.score(x_test,y_test)*100))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We trained the data with Logistic Regression Model and the model will predict the values truely by %74.4 ratio"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a>\n### K - Nearest Neighbour (KNN) Classification\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 40) #n_neighbors is a hyperparameter that's why we need to try to examine the Optimum value \nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n# print(\"Prediction:\",prediction) if you want to compare the test data and predictions you can remove # and try \nprint(\"for n={} KNN Score : {}\".format(40,knn.score(x_test,y_test))) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alternative 1 - Find best n number to get optimum value"},{"metadata":{"trusted":true},"cell_type":"code","source":"#if we want to see which n number will be optimum we can define a for loop for that \nscore_list = []\nfor each in range(1,50):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train, y_train)\n    score_list.append(knn2.score(x_test, y_test))\n\nplt.figure(figsize=(8,5))\nplt.scatter(range(1,50),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show() \n\n#40 might be the optimum number for N \n#The answer is 0.753","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alternative 2 - Find best n number to get optimum value ( More Beneficial )\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"neig = np.arange(1,50)\ntrain_accuracy = []\ntest_accuracy = []\n#Loop all over in k values\nfor i,k in enumerate(neig):\n    knn3 = KNeighborsClassifier(n_neighbors = k)\n    #Fit process\n    knn3.fit(x_train,y_train)\n    #Train Accuracy\n    train_accuracy.append(knn3.score(x_train,y_train))\n    #Test Accuracy\n    test_accuracy.append(knn3.score(x_test,y_test))\n    \n#Plotting the Values \nplt.figure(figsize =(13,10))\nplt.plot(neig, test_accuracy, label = \"Testing Accuracy\")\nplt.plot(neig, train_accuracy, label = \"Training Accuracy\")\nplt.legend()\nplt.title(\"Values vs Accuracy\")\nplt.xlabel(\"Number of Neighbors\")\nplt.ylabel(\"Accuracy\")\nplt.xticks(neig) #We limit the Max min values in the plot axis according to Number of max neighbor\nplt.savefig(\"graph.png\")\nplt.show()\nprint(\"Best Accuracy : {} with K : {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"14\"></a>\n### Support Vector Machine (SVM) Classification \n- This method provides that the optimum line which seperates the \"2\" classes objects "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC \nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\n\n#Test \nprint(\"Accuracy of the SVM Algorithm : \",svm.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"15\"></a>\n### Naive Bayes Classification "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB \nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\nprint(\"Accuracy of the Naive Bayes Algorithm :\",nb.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"16\"></a>\n### Decision Tree Classification\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\nprint(\"Accuracy of the Decision Tree :\",dt.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"17\"></a>\n### Random Forest Classification \n- This method is a advanced version of Decision Tree Classification Method \n- In this method we use many of the Decision Tree algorithm but this number is a hyperparameter and we need to find a exact number for optimum solution\n- Obvious that **RF Classification Accuracy must better than Decision Tree C.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators= 24, random_state=42)\nrf.fit(x_train,y_train)\nprint(\"Accuracy of the Random Forest Classification : \",rf.score(x_test,y_test))\n\n#We need to find optimum value that's why need to decide best number for n_estimators parameter\n#if we want to see which n number will be optimum we can define a for loop for that \nscore_list2 = []\nfor each in range(1,200):\n    rf2 = RandomForestClassifier(n_estimators = each)\n    rf2.fit(x_train, y_train)\n    score_list2.append(rf2.score(x_test, y_test))\n\nplt.figure(figsize=(8,5))\nplt.scatter(range(1,200),score_list2)\nplt.xlabel(\"n values\")\nplt.ylabel(\"accuracy\")\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aa = np.max(score_list2) #We can see the max value would be 0.7878 and n = 24 might be the great option\naa","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"18\"></a>\n## Performans Comparison of Classification Methods\n ****Finally Let's check all of the classification methods' accuracy results****"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test Accuracy for Logistic Regression: %{}\".format(lr.score(x_test,y_test)*100))\nprint(\"for n={} KNN Score : %{}\".format(40,knn.score(x_test,y_test)*100))\nprint(\"Accuracy of the SVM Algorithm : %{}\".format(svm.score(x_test,y_test)*100))\nprint(\"Accuracy of the Naive Bayes Algorithm : %{}\".format(nb.score(x_test,y_test)*100))\nprint(\"Accuracy of the Decision Tree : %{}\".format(dt.score(x_test,y_test)*100))\nprint(\"Accuracy of the Random Forest Classification : %{}\".format(rf.score(x_test,y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As you see for classification best results given from Random Forest Classification by difference from the nearest oppenent %2"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"19\"></a>\n## Evaluation Classification Methods \n  ### Confusion Matrix \n  - In this section we'll learn the accuracy test method alternative to .score(x_test,y_test)) (**Actually better way to examine details**)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'm gonna show an example on Random Forest C. model \nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators= 24, random_state=42)\nrf.fit(x_train,y_train)\nprint(\"Accuracy of the Random Forest Classification : \",rf.score(x_test,y_test))\n\n#In this method we need to predict x test values \ny_pred = rf.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n#Lets Visualize it \nimport seaborn as sns \nf,ax =plt.subplots(figsize=(6,6))\nsns.heatmap(cm, annot = True, linecolor = \"blue\", ax = ax)\n\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- *First square says that the model predicts value 0 (healthy) as value 0 for 13*e^02 values (correct)\n- *Second sqare says that the model predict  value 0 (healthy) as value 1 for 24 values (incorrect)\n- *Third square says that the model predicts value 1 (sick) as value 0 for 29 values (incorrect)\n- *Forth square says that the model predicts value 1 (sick) as value 1 for 51 values (correct)\n\n- So the model predicts wrong values for 29 + 24 = 53 times "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}