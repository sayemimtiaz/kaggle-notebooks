{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overview of Project: Summary, Methods Used, and Findings\n\n## Task: Create summary tables that address relevant factors related to COVID-19\n\nIn this notebook, we used insight from our [initial findings](https://www.kaggle.com/ralphlozanoc1/team-alcapone-submission) (utilizing Specter embeddings of titles and abstracts from the COVID-19 dataset for clustering, and then evaluating the data for similarities and separation) to develop a plan for further evaluation of the dataset. In particular, we have built a system that creates summary tables using texts from relevant literature, given a query. \n\nBelow, we will show the following: \n\n* Step I: Retrieval of relevant documents for a query\n    * Create indexes - [leveraging Anserini to create indexes for documents](https://github.com/castorini/anserini)\n    * Retrieve the top 100 relevant documents based on similarity using query-document feature vectors (bag of words) \n* Step II: Re-Rank to improve initial set of scores\n    * Get embeddings using XLNet\n    * Measure semantic similarity between a query and document pair using cosine distance\n* Step III: QA System \n    * Reformat and generate training data extracted from manually labeled [QA dataset](https://github.com/deepset-ai/COVID-QA/blob/master/data/question-answering/200423_covidQA.json)\n    * Finetune [SQuAD](https://github.com/huggingface/transformers/tree/master/examples/question-answering) with a [RoBERTa-based-SQuAD](https://huggingface.co/deepset/roberta-base-squad2#list-files) model using our modified training data\n    * Run QA system to get relevant factors, the respective excerpt, and associated evidence.\n\nWe also provide the code for reproducing these results. Note that the code is not suited for running in kaggle kernel, because of memory limitations. We suggest changing the input and output paths appropriately for running the code.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.display import display, Image\ndisplay(Image(filename='/kaggle/input/images/image1.png', embed=True, width=1000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Document Retrieval \n\nAs the first step for document retrieval, we leveraged Anserini to take advantage of it's low-latency Lucene indexes. We followed the steps outlined [here](https://github.com/castorini/anserini/blob/master/docs/experiments-cord19.md) to create document indexes for full text. Under the hood, Anserini uses a BM25 scoring system, which builds on the default bag of words approach, to calculate similarity between query and document pairs.\n\nThe top 100 documents were retrieved for each of the queries given under the task. Here is an example of documents retrieved for the query \"Effectiveness of case isolation/isolation of exposed individuals (i.e. quarantine)\":","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nquery_1_retrieved = pd.read_csv(\"/kaggle/input/retrievaloutput/doc_scores_1_retrieved.csv\")\nquery_1_retrieved[['query', 'docid', 'score']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Re-Rank to Improve Initial Scores\n\nSince BM25 relies on frequency of word occurrences, semantic similarity is not captured appropriately by this measure. To address this, we used similarity scores based on cosine distance between embedding vectors for document and query pairs to re-rank the documents once retrieved. \n\nWe used a pre-trained XLNet model from the [Transformers library](https://huggingface.co/transformers/) to generate embeddings of text in queries and documents. To generate a single fixed-length embedding, we obtained vectors for each token in the input text from the last hidden state of the model and calculated the mean. For documents, we took averages of each section in the document to get a single vector representation. \n\nOnce  embedding vectors were created for a query-document pair, we calculated the similarity score by using the cosine distance. Using this similarity score, the retrieved documents were re-ranked so that the documents most relevant to the query would appear at the top of the summary table. Here is an example table that shows documents reranked for the same query:","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"query_1_reranked = pd.read_csv(\"/kaggle/input/rerankeddocs/sim_scores_1_retrieved.csv\")\ncombined = pd.merge(left=query_1_retrieved[['query', 'docid', 'score']], right=query_1_reranked, left_on=['docid', 'query', 'score'], right_on=['docid', 'query', 'score'])\ncombined['old_rank'] = combined['score'].rank(ascending=False)\ncombined['new_rank'] = combined['sim_score'].rank(ascending=False)\ncombined.rename(columns={'score': 'bm25_score', 'sim_score': 'cosine_score'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extract answers:\nAs the next step in building summary tables, we used two different approaches for extracting answers for the columns in the summary table. For `Study Type` which is a categorical variable, we used a string search method. For the rest of the columns (`Factors`, `Excerpt` and `Measure of Evidence`) we used a QA system to populate column values.  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## String Search\nWe used string search to infer the value for the `Study Type` column. We used the [Aho-Corasick algorithm](https://pypi.org/project/pyahocorasick/), which leverages a trie-based data structure for fast and efficient string searching. First, we identified associated words or phrases for each of the category labels. Then, we identified the category label with the maximum number of occurences of associated words/phrases under that category label. If no matches were found for the associated words, the column is adjusted to _null_. Here is an example for the same query from above:","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"query_1_retrieved[['query', 'docid', 'study_type']].fillna(\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## QA System \n\nHere, we built an automated QA (Question Answering) System capable of identifying and retrieving relevant content when posed with a series of questions. To begin, we finetuned the original RoBERTa SQuAD model using a manually labeled Covid-QA dataset as a starting point.\n\nRoBERTa has a limit of 512 words/tokens per processing batch, so we reformatted the manually labeled data to a sequence of 500 word document slices. These slices included the answer to a particular question, and used a 65/35 train/eval split to subset our data for finetuning. Additional in-depth details and discussion about how this was performed and can be replicated is covered in the associated code portion of the notebook [below](https://www.kaggle.com/katieflannigan/c1nlp-cord19-submission-6-16#Using-the-Training-Data-and-running-the-SQuAD-FineTuner). Our SQuAD finetuner was subsequently run on this dataset and produced the following metrics:\n\n    exact: 39.75155279503105,\n    f1: 72.57937317850947,\n    total: 483,\n    HasAns_exact: 39.75155279503105,\n    HasAns_f1: 72.57937317850947,\n    HasAns_total: 483,\n    best_exact: 39.75155279503105,\n    best_exact_thresh: 0.0,\n    best_f1: 72.57937317850947, \n    best_f1_thresh': 0.0\n\nUsing this finetuned SQuAD model, we asked a series of questions against the ranked documents created earlier, to identify relevant factors, the respective excerpt from the document, and associated evidence. The questions used to extract relevant factors can be found below in the code section. A simple example (for Query 1) is as follows:\n\n    What factors are related to quarantine and isolation?\n\nIn order to identify the respective excerpt from the document we dynamically re-phrased questions to the following format:\n\n    What is relevant to <identified factors>?\n\nFinally, to identify the associated evidence, we first looked to the acknowledgements (if present) in the research paper itself, and if none were provided, posed the following question to the QA system:\n\n    How was this study performed?\n\nAn example of the output of this process for Query 1 can be seen below:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pd.read_csv(\"/kaggle/input/qaoutput/qa_1.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\nWe built summary tables for documents retrieved for the following set of queries, and placed the resulting tables in the `output` folder.\n* Effectiveness of case isolation/isolation of exposed individuals (i.e. quarantine)\n* Effectiveness of community contact reduction\n* Effectiveness of inter/inner travel restriction\n* Effectiveness of school distancing\n* Effectiveness of workplace distancing\n* Effectiveness of a multifactorial strategy prevent secondary transmission\n* Seasonality of transmission\n* How does temperature and humidity affect the transmission of 2019-nCoV?\n* Significant changes in transmissibility in changing seasons?\n* Effectiveness of personal protective equipment (PPE)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pd.read_csv(\"/kaggle/input/summarytables/Effectiveness_of_case_isolation_isolation_of_exposed_individuals_quarantine.csv\", index_col=0).fillna(\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of Findings and Areas of Improvement\n   \n  Here, we take a few of the summary tables produced from the queries above, and take a closer look at the results, misplaced evidence (if a factor), and  factors identified and excerpts. \n  \n  #### Query 1\n  To provide an example of our results and findings, we will walk through the first query \"Effectiveness of case isolation/isolation of exposed individuals (i.e. quarantine)\". This query, as well as the others not shown here, were re-phrased to optimize QA system's performance. In our case, query 1 was modified to \"What factors are related to quarantine and isolation?\". At a cursory glance, many of the factors can be broadly categorized into:\n* The query-indicated factors themselves (isolation,quarantine): isolation, distancing, and contact-- or lack thereof \n* Individuals: direction from a third party (including authorities/healthcare providers) \n* Economy: toll on non-disease factors like travel and construction \n* Science and Global Effects: references to biology, vaccinations, other diseases\n* Statistical measures: median, mean, rates/proportions","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pd.read_csv(\"/kaggle/input/qaoutput/qa_1.csv\")[:21]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This was a very successful query; its results (associated excerpt and evidence) here are definitely relevant to quarantine and isolation. Furthermore, they are appropriate to hone in on for more information on the intended query. However, there are some minor exceptions. For example, some results apply more broadly to the disease affecting the world economy as a whole, instead of just isolation-specific material for COVID-19 (Result 14, Query 1, Factor):\n\n    \"individual.Traffic control and social distancing in each city!\"\n\nThe associated excerpt/evidence here are more focused on economic factors such as country population and vehicular traffic during a pandemic. While interesting and tangentially related, and certainly a goal of such research projects as this, it may not be the most applicable result for the query in question. \n\nIn addition, some key factors do not necessarily present as a category under which evidence can be summarized. (Result 20, Query 1, Factor): **'growth rate behaves slightly above the rate'**. This factor seems to be opposite of what one would expect when paired with the parameters 'isolation' and 'quarantine', so we take a look at the associated excerpt:\n\n*\"The peak position is also sensitive to this parameter, as well as the curve variance. In relation to the peak, we observed that lower values of guarantee a postponement in the occurrence of the peak of infection, however, the duration of the effects of the epidemic is longer.The following table (Table 3) Table 3 . Characteristic values of peaks of infection in different scenarios (graph in Figure 3). The following comparison was made for the same value, varying the proportions of individuals in quarantine or isolation, also for a single scenario. In the graph of Figure 4 , we can see the behaviour of the peaks of infection for three specific values of and .As these values are higher, this means that the containment measures for tackling the pandemic have been more effective within the specific scenario\"*\n\n\nUltimately, we conclude that this does provide relevant content for the intended query. However, the key factor could not be categorized without further considering the excerpt or evidence. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Query 6\n\n\n","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pd.read_csv(\"/kaggle/input/qaoutput/qa_6.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Query 6 translated to \"what factors are related to preventing secondary transmission?\". We noted that certain key factors are overwhelmingly relevant, honing in on particular biological factors, species/animals, and risk factors that play into secondary transmission. \n\nWe hypothesize that the phrase \"preventing secondary transmission\" is more specific than phrases included in other queries (i.e., in Query 1, \"isolation/quarantine\"), which partially accounts for the more natural results/key factor categories here. Query 9 (\"transmission effects by season\") is similar to query 6-- more targeted verbiage by nature of the query. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Query 10 \n\n","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pd.read_csv(\"/kaggle/input/qaoutput/qa_10.csv\")[55:60]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lastly, we provide another example that deviates slightly from what one might naturally think of when prompted with the question \"What factors are related to personal protective equipment PPE?\". However, it still feeds into our overall conclusion that the returned information is useful. The more open-ended nature of the phrase \"PPE- Personal Protective Equipment\", we believe, leads to the increased ambiguity of this result set. For example, consider the factor retrieved for Result 57, Query 10, Factor: “personal digital assistants (PDAs) were provided to”. The associated excerpt relates to patient experience, and the evidence ultimately links an appropriate window of information detailing PPE wear and consumption. However, the excerpt prominently details a small study relying more on patient satisfaction due to wait time, as opposed to a direct link to PPE. While patient satisfaction is important, and relates to PPE, it is not the expected answer.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion: \n\nIn conclusion, each of our tables returned relevant information pertaining to the queries. Despite some results that may not be as targeted as one would hope, relevant information is provided when querying this particular retrieval and QA system. We feel confident that the summary tables produced are able to provide necessary research material and background for someone looking for information on an aspect of COVID-19.\n\nTwo important limitations of our system are that it does not provide satisfactory answers for the columns `Study Type` and `Influential`. For `Study Type`, which is a categorical variable, we suggest training a multi-label classification model. Similarly, since our QA system extracts answers from text in the input document, it is not suited to answer questions such as *\"Was this factor found to be influential in the experiments/models?\"*, which requires ability to synthesize answers, which goes beyond extraction-based question answering. For this particular value, we suggest training a binary classification model with appropriate texts from scientific literature that studies the impact of certain factors on the outcome of experiments.\n\nHowever, despite these limitations, the system successfully brings together several components: information retrieval, document ranking, and question answering. These components work together to extract relevant papers and present information from the corpus in a concise, summarized manner.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Code","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# imports\nfrom pyserini.search import pysearch\nfrom transformers import XLNetTokenizer, XLNetModel, XLNetConfig, modeling_utils\nimport torch\nimport json\nimport pandas as pd\nfrom scipy import spatial\nimport csv\nimport ahocorasick\nimport operator\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Retrieval, Reranking and String Search","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# define constants\nNUM_DOCS = 100\n\n# the paths should point to location where the lucene indexes generated by Anserini are stored\nPARAGRAPH_INDEX = \"./indexes/lucene-index-cord19-paragraph-2020-05-19\" \nDOCUMENT_INDEX = \"./indexes/lucene-index-cord19-full-text-2020-05-19\"\n\n\nQUERY = [\n    \"Effectiveness of case isolation isolation of exposed individuals quarantine\",\n    \"Effectiveness of community contact reduction\",\n    \"Effectiveness of inter inner travel restriction\",\n    \"Effectiveness of school distancing\",\n    \"Effectiveness of workplace distancing\",\n    \"Effectiveness of a multifactorial strategy prevent secondary transmission\",\n    \"Seasonality of transmission\",\n    \"How does temperature and humidity affect the transmission of 2019-nCoV\",\n    \"Significant changes in transmissibility in changing seasons\",\n    \"Effectiveness of personal protective equipment PPE\",\n]\n\n\ndef get_relevant_documents(query, number_docs, index, print_results=False):\n    \"\"\"Retrieve top k documents from given index, for a query\"\"\"\n    \n    searcher = pysearch.SimpleSearcher(index)\n    hits = searcher.search(query, number_docs)\n    if print_results:\n        for i in range(len(hits)):\n            print(f'{i+1:2} {hits[i].docid:15} {hits[i].score:.5f}')\n    \n    return hits\n\n\ndef get_xlnet_embedding(input_text):\n    \"\"\"create embedding for input_text using pretrained xlnet model\"\"\"\n    \n    tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n    model = XLNetModel.from_pretrained('xlnet-base-cased')\n    model.eval()\n    \n    with torch.no_grad():\n        input_ids = torch.tensor(tokenizer.encode(input_text, add_special_tokens=False)).unsqueeze(0)  # Batch size 1\n        model_output = model(input_ids)\n        summarized_output = model_output[0].mean(1)\n    \n    return summarized_output\n\n\ndef embed_document(doc):\n    \"\"\"create single embedding for a document\"\"\"\n    \n    contents = doc.contents.split(\"\\n\")\n    text_vecs = []\n    for i,c in enumerate(contents):\n        # ignore short paragraphs\n        if len(c) > 50:\n            text_vecs.append(get_xlnet_embedding(c))\n    # concatnate paragraphs into a single vector and caculate mean\n    X = torch.cat(text_vecs, dim=0)\n    doc_mean = torch.mean(X,dim=0).reshape(1,768)\n    return doc_mean\n\n\ndef get_cosine_distance(vec_a, vec_b):\n    \"\"\"calculate cosine distance between given vectors\"\"\"\n    \n    a = vec_a.detach().numpy()\n    b = vec_b.detach().numpy()\n    result = 1 - spatial.distance.cosine(a,b)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# define dictionary to link category labels to associated words and phrases\nPATTERN = {\n    \"systematic review\": [\"systematic review\", \"meta-analysis\", \"meta analysis\"],\n    \"prospective observational study\": [\"prospective observational\"], \n    \"retrospective observational study\": [\"retrospective observational\"],\n    \"observational study\": [\"observational study\"],\n    \"cross-sectional study\": [\"cross-sectional study\", \"cross sectional study\"],\n    \"case series\": [\"case series\"],\n    \"expert review\": [\"expert review\"],\n    \"editorial review\": [\"editorial review\"],\n    \"simulation\": [\"simulation study\", \"simulation based study\"],\n    \"model based\": [\"model based study\", \"modeling experiment\", \"mathematical model\", \"statistical model\"]\n}\n\n\ndef make_automations(input_pattern):\n    \"\"\"Make AhoCorasick automatons from given pattern\"\"\"\n    \n    automatons = []\n    for term, keywords in input_pattern.items():\n        auto = ahocorasick.Automaton()\n        for keyword in keywords:\n            value = (len(keyword), term)\n            auto.add_word(keyword, value)\n            auto.make_automaton()\n            automatons.append(auto)\n    return automatons\n\n\ndef search_pattern_in_text(input_text_list, pattern):\n    \"\"\"given an input text, find occurances of words in pattern\"\"\"\n\n    pattern_set = {}\n    for key,_ in pattern.items():\n        pattern_set[key] = set()\n    \n    auto_list = make_automations(pattern)\n\n    for input_text in input_text_list:\n        for auto in auto_list:\n            for end_index, (length, keyword) in auto.iter(input_text.lower()):\n                if keyword in pattern_set.keys():\n                    new_set = pattern_set[keyword]\n                    new_set.add(end_index)\n                    pattern_set.update(\n                        {\n                            keyword: new_set\n                        }\n                    )\n\n    return pattern_set\n\n\ndef get_study_type(pattern_dict):\n    \"\"\"infer study type based on the number of occurances of phrases in PATTERN\"\"\"\n    \n    result = {}\n    for key, value in pattern_dict.items():\n        result[key] = len(value)\n\n    # get the maximum value associated with a key\n    max_value = max(result.items(), key=operator.itemgetter(1))[1]\n    # if no matches were found, return empty list\n    \n    keys_with_max_value = []\n    # get all the keys that have the maximum value\n    for key,value in result.items():\n        if result[key] == max_value:\n            keys_with_max_value.append(key)\n    \n    # if no matches were found, return empty list\n    if max_value == 0:\n        return []\n    # else, return list with keys for max value\n    return keys_with_max_value\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def write_csv_files_for_retrived(query_list):\n    \"\"\"function to create csv file for documents retrived by queries in query_list \"\"\"\n    \n    for i,q_text in enumerate(query_list):\n        out_filename = \"doc_scores_\" + str(i+1) + \"_retrieved.csv\"\n        query_vec = get_xlnet_embedding(q_text)\n        docs_retrieved = get_relevant_documents(q_text, NUM_DOCS, DOCUMENT_INDEX, True)\n\n        with open(out_filename,'w') as fileobj:\n            newFile = csv.writer(fileobj)\n            fields = ['query', 'docid', 'score', 'body_text', 'study_type']\n            newFile.writerow(fields)\n            for i, doc in enumerate(docs_retrieved):\n                doc_json = json.loads(doc.raw)\n                if doc_json['has_full_text']:\n                    body_text = doc_json['body_text']\n                else:\n                    body_text = 'Not Available'\n                contents = doc.contents.split(\"\\n\")\n                study_type = get_study_type(search_pattern_in_text(contents, PATTERN))\n                row = [q_text, doc.docid, doc.score, body_text, study_type if study_type else None]\n                \n                newFile.writerow(row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def write_csv_files_for_sim_scores(query_list):\n     \"\"\"function to create csv file with cosine similarity scores for queries in query_list \"\"\"\n\n    \n    for i,q_text in enumerate(query_list):\n        print (\"query: %s\"% q_text)\n        out_filename = \"sim_scores_\" + str(i+1) + \"_retrieved.csv\"\n        query_vec = get_xlnet_embedding(q_text)\n        docs_retrieved = get_relevant_documents(q_text, NUM_DOCS, DOCUMENT_INDEX, True)\n        \n        doc_ids = []\n        with open(out_filename,'w') as fileobj:\n            newFile = csv.writer(fileobj)\n            fields = ['query', 'docid', 'score', 'sim_score']\n            newFile.writerow(fields)\n            for i, doc in enumerate(docs_retrieved):\n                # to handle documents appearing multiple times in the retrieved list\n                # one of the instances where this happens is when same paper is published in \n                # multiple journals\n                if doc.docid in doc_ids:\n                    print (\"docid %s already present, skipping it.\"%doc.docid)\n                    continue\n                else:\n                    doc_ids.append(doc.docid)\n                    doc_vector = embed_document(doc)\n                    doc_sim_score = get_cosine_distance( query_vec, doc_vector)\n                row = [q_text, doc.docid, doc.score, doc_sim_score]\n                newFile.writerow(row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"write_csv_files_for_retrived(QUERY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"write_csv_files_for_sim_scores(QUERY)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## QA System","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Training Data Generation and SQuAD finetuning\n\n### Purpose/Objective\n\nThis code segment leverages a [manually labeled Question Answering dataset](https://rajpurkar.github.io/SQuAD-explorer/) on Covid Research Papers to generate training and evaluation data for a finetuned SQuAD (https://rajpurkar.github.io/SQuAD-explorer/) model for automated QA.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Reading in the Manually Labeled Data\n\nThe manually labeled dataset we are utilizing can be found [here](https://github.com/deepset-ai/COVID-QA/blob/master/data/question-answering/200423_covidQA.json).\n- Link: https://github.com/deepset-ai/COVID-QA/blob/master/data/question-answering/200423_covidQA.json\n\nWe are reading the data in from this json file into a single list which will be used to create train and eval data subsets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"qa = \"/kaggle/input/200423_covidQA.json\" # Adjust to point to data file\n\nwith open(qa, 'r') as f:\n    data = json.load(f)\ndata = [item for topic in data['data'] for item in topic['paragraphs']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Manipulation and Reformatting\n\nThis section reformats and appropriately structures the QA data, to work around the 512 token limit for BERT-based system.\nWe split each single document into multiple 500 word slices which include the respective answer.\nUltimately, instead of parsing the entire document, we only process chunks of the document which include a valid answer according to the manually labeled dataset.\n\nFrom experimentation, we have found that the system performs best when the answer to the question\nis towards the end of the window of the text and have structured it accordingly.\n\nExample:\n\n(text)\n(text)\n(text)\n(text)\n(ANSWER)\n\nWe start from the word after the listed answer and construct each window as encompassing the 499 words before it, creating our 500 word window.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data = []\n\ndef get_indices(string, substring, actual):\n    indexes=[]\n    for i in range(len(string)):\n        ss = \" \".join(string[i:i+len(substring)])\n        if \" \".join(substring) in ss:\n            indexes.append((i, i+len(substring)))\n\n    return indexes[0]\n\n\nfor datum in data:\n    # Reformatting Context\n    orig_context = datum['context'].replace(\"\\n\\n\", \" \").replace(\"\\n\", \" \").replace(\"  \", \"\").replace(u\"\\u202f\", \"\")\n    split_context = orig_context.split(\" \")\n    orig_qas = datum['qas']\n\n    for qas in orig_qas:\n        for answer in qas['answers']:\n\n            # Reformatting Answer\n            a = answer['text'].replace(\"\\n\\n\", \" \").replace(\"\\n\", \" \").replace(\"  \", \"\").replace(u\"\\u202f\", \"\")\n            if a[0] == \" \":\n                a = a[1::]\n            if a[-1] == \" \":\n                a = a[0:-1]\n\n            indices = get_indices(split_context, a.split(), a)\n            start = indices[1] - 499\n            end = indices[1] + 1\n            if start < 0:\n                end = end + (0-start)\n                start = 0\n\n            context_subset = \" \".join(split_context[start:end])\n\n            new_data.append({\n                'title': str(datum['document_id']),\n                'paragraphs': [{\n                    'context': context_subset,\n                    'document_id': datum['document_id'],\n                    'qas': [{\n                        'question': qas['question'],\n                        'id': qas['id'],\n                        'answers': [{\n                            'text': a,\n                            'answer_start': context_subset.find(a),\n                        }],\n                        'is_impossible': False\n                    }]\n                }]\n            })","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Subsetting our Data to Create a Train/Eval Split\n\nSince the data is originally ordered with questions and answers from the same article and same parts of the article together,\nwe shuffle the data.\n\nWe also create a 65/35 train/eval split of our data.\n    - i.e. 65% will be used to train/finetune our model, and the other 35% will be used to evaluate its performance.\n\nFinally, we write these two subsets to corresponding json files which will be used by the SQuAD finetuning script:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shuffled_data = new_data\nrandom.shuffle(shuffled_data)\n\ntrain = shuffled_data[0:int(len(shuffled_data)*0.65)]\ntest = shuffled_data[int(len(shuffled_data)*0.65)::]\n\nwith open('qa_train.json', 'w') as trainfile:\n    json.dump({\"data\": train}, trainfile)\n\nwith open('qa_test.json', 'w') as testfile:\n    json.dump({\"data\": test}, testfile)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using the Training Data and running the SQuAD FineTuner\n\n1) The SQuAD finetuning script needs to be downloaded locally and can be found here:\n    - https://github.com/huggingface/transformers/blob/master/examples/question-answering/run_squad.py\n\n2) As an alternatively to `--model_name_or_path deepset/roberta-base-squad2`, one can also run this by pointing to a local `roberta_model/` folder which includes the files found here:\n    - https://huggingface.co/deepset/roberta-base-squad2#list-files\n\n3) The sample command below is based on the training filenames specified above and should be modified to match any changes.\n    The command also lists the output directory in which the new finetuned model and associated files will be generated/saved.\n    This directory is essential to running the QA system in the next step.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!python /kaggle/input/run_squad.py --model_type roberta --model_name_or_path deepset/roberta-base-squad2 --do_train --do_eval --train_file /kaggle/output/qa_train.json --predict_file /kaggle/output/qa_test.json --learning_rate 3e-5 --num_train_epochs 2.0 --max_seq_length 512 --output_dir /kaggle/output/finetuned_roberta_output/ --max_answer_length 512 --max_query_length 512","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Running an Automated Question-Answering System\n\n### Purpose/Objective\n\nIn this section, we utilize the finetuned SQuAD model to build an automated Question Answering system.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from ast import literal_eval\nimport csv\nimport operator\nimport os\nfrom simpletransformers.question_answering import QuestionAnsweringModel as qam\nimport sys\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\nimport torch\n\ncsv.field_size_limit(sys.maxsize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initializing Input Data\n\nThe variable 'roberta_finetuned_model' points to the output directory specified in the run_squad.py command.\nThis should be adjusted accordingly to match the filepath in which the model output is located.\n\nThe list of query filepaths specified in the 'queries' list point to Covid Research Papers ranked 1-100 based on their relevance to their associated question.\n\n'query_1' documents are relevant to the first question below, and so on.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"roberta_finetuned_model = \"/kaggle/output/finetuned_roberta_output\"\n\ninput_path = \"/kaggle/input/retrievaloutput/\"\nquery_1 = input_path + \"doc_scores_1_retrieved.csv\"\nquery_2 = input_path + \"doc_scores_2_retrieved.csv\"\nquery_3 = input_path + \"doc_scores_3_retrieved.csv\"\nquery_4 = input_path + \"doc_scores_4_retrieved.csv\"\nquery_5 = input_path + \"doc_scores_5_retrieved.csv\"\nquery_6 = input_path + \"doc_scores_6_retrieved.csv\"\nquery_7 = input_path + \"doc_scores_7_retrieved.csv\"\nquery_8 = input_path + \"doc_scores_8_retrieved.csv\"\nquery_9 = input_path + \"doc_scores_9_retrieved.csv\"\nquery_10 = input_path + \"doc_scores_10_retrieved.csv\"\n\nqueries = [\n    query_1, query_2, query_3, query_4, query_5, query_6, query_7, query_8, query_9, query_10\n]\nquestions = [\n    \"What factors are related to quarantine and isolation?\",\n    \"What factors are related to community contact reduction?\",\n    \"What factors are related to travel restrictions?\",\n    \"What factors are related to school distancing?\",\n    \"What factors are related to workplace distancing?\",\n    \"What factors are related to preventing secondary transmission?\",\n    \"What factors are related to seasonality of transmission?\",\n    \"What factors are related to temperature, humidity, and transmission?\",\n    \"What factors are related to changing transmission by season?\",\n    \"What factors are related to personal protective equipment PPE?\"\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initialize Finetuned QA Pipeline and Output Directory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = pipeline('question-answering', model=roberta_finetuned_model, tokenizer='deepset/roberta-base-squad2')\n\noutput_dir = \"/kaggle/output/qa_csv_output/\"\nif os.path.exists(output_dir):\n    pass\nelse:\n    os.mkdir(output_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Output Formatting Helper Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def write_csv_table(data: list, query: str):\n    \"\"\"\n    Writes output csv given data and query\n    \"\"\"\n    with open(f\"{output_dir}{query}.csv\", \"w\") as fileobj:\n        newFile = csv.writer(fileobj)\n        fields = ['Query/Question', 'Document ID', \"Key Factor\", \"Excerpt\", \"Evidence\"]\n        newFile.writerow(fields)\n\n        for datum in data:\n            newFile.writerow([\n                datum[0], # Question\n                datum[1], # Document ID\n                datum[2]['answer'], # Key Factor\n                datum[3]['answer'], # Excerpt\n                datum[4]['answer'], # Evidence\n            ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### QA System Execution\n\nThis code segment is responsible for the execution of the QA system. It poses a series of questions to get answers regarding:\n- Relevant factors associated with the query/question being analyzed\n- Excerpt(s) related to the identified factors\n- Evidence pertaining to the Factors and Excerpts found","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(queries)):\n    data_file = queries[i]\n    question = questions[i]\n\n    answers = []\n    doc_count = 0\n\n    data = []\n\n    with open(data_file, \"r\") as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            if row[0] =='query':\n                data.append(row)\n            else:\n                if row[3] == \"Not Available\":\n                    continue\n                data.append([row[0], row[1], float(row[2]), literal_eval(row[3])])\n    headers = data[0]\n    data = data[1::]\n\n    \"\"\"\n    Processes Documents and Predicts on each documents to find best answer\n    \"\"\"\n    for doc in data:\n\n        acknowledgements =[]\n\n        \"\"\"\n        Aggregating Separated Text\n        \"\"\"\n        full_text = \"\"\n        for text in doc[3]:\n            if text['section'] == \"Acknowledgments\":\n                acknowledgements.append(text['text'])\n            full_text += text['text']\n\n        doc_count += 1\n\n\n        # Get Answer for Relevant Factors\n        best_factor = nlp({\n            'question': question,\n            'context': full_text\n        })\n\n        best_excerpt = nlp({\n            'question': f\"What is relevant to {best_factor['answer']}?\",\n            'context': full_text\n        })\n\n        sentence_index = len(full_text[0:best_excerpt['end']].split(\". \"))\n        sentences = full_text.split(\". \")\n        sent_start = sentence_index - 2 if sentence_index >= 2 else 0\n        new_start = full_text.find(sentences[sent_start])\n        sent_end = sentence_index + 3 if sentence_index < len(sentences)-3 else len(sentences)-1\n        new_end = new_start + len(\". \".join(sentences[sent_start:sent_end+1]))\n\n        # Get Excerpt associated with identified Factors\n        excerpt = {\n            'score': best_excerpt['score'],\n            'start': new_start,\n            'end': new_end,\n            'answer': full_text[new_start:new_end]\n        }\n        \n        # Identify Evidence related to Excerpt and Factors\n        if acknowledgements:\n            proof = \"\".join(acknowledgements)\n            proof_start = full_text.find(proof)\n\n            evidence = {\n                'answer': proof,\n                'start': proof_start,\n                'end': proof_start+len(proof)\n            }\n        else:\n            evidence = nlp({\n                'question': \"How was this study performed?\",\n                'context': full_text\n            })\n\n            proof_idx = len(full_text[0:evidence['end']].split(\". \"))\n            proof_start_idx = proof_idx - 2 if proof_idx >=2 else 0\n            proof_start = full_text.find(sentences[proof_start_idx])\n            proof_end_idx = proof_idx + 3 if proof_idx < len(sentences)-3 else len(sentences)-1\n            proof_end = proof_start + len(\". \".join(sentences[proof_start_idx:proof_end_idx+1]))\n\n            evidence = {\n                'score': evidence['score'],\n                'start': proof_start,\n                'end': proof_end,\n                'answer': full_text[proof_start:proof_end]\n            }\n\n        # Store Factors, Excerpt, and Evidence into a list\n        answers.append((question, doc[1], best_factor, excerpt, evidence))\n        \n    # Write CSV Output\n    write_csv_table(answers, question.replace(\" \", \"_\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Summary Tables\n\nLastly, the intermediate tables from the steps outlined above were combined with the metadata table to create final summary table, in the expected output format.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_summary_table(path_for_retrived, path_for_reranked, path_for_qa, metadata):\n    \"\"\"create summary table by merging the intermediate tables with metadata\"\"\"\n    \n    retrived = pd.read_csv(path_for_retrived, usecols=['docid', 'study_type'])\n    reranked = pd.read_csv(path_for_reranked, usecols=['docid', 'sim_score'], index_col=0)\n    qa = pd.read_csv(path_for_qa, usecols = ['Document ID', 'Key Factor', 'Excerpt', 'Evidence'])\n    metadata = pd.read_csv(metadata)\n\n    docs = pd.merge(left=retrived, right=reranked, left_on=['docid'], right_on=['docid'])\n    docs_metdata = pd.merge(left=docs, right=metadata, left_on=['docid'], right_on=['cord_uid'])\n    qa = qa.assign(Influential=\"\", added_on=\"\")\n    summary_table = pd.merge(left=docs_metdata, right=qa, left_on=['docid'], right_on=['Document ID'])\n    \n    \n    # Sort summary table by similarity score to get most relevant documents at top\n    # Select only relevant columns after sorting\n    final_summary_table = summary_table.sort_values('sim_score', ascending=False).reset_index(drop=True)[['publish_time', 'title', 'url', 'source_x', 'study_type', 'Key Factor', 'Influential', 'Excerpt', 'Evidence', 'added_on', 'doi', 'Document ID']]\n    final_summary_table.rename(\n        columns={\n            \"publish_time\": \"Date\",\n            \"title\": \"Study\",\n            \"url\": \"Study Link\",\n            \"source_x\": \"Journal\",\n            \"study_type\": \"Study Type\",\n            \"Key Factors\": \"Factors\",\n            \"Influential\": \"Influential\",\n            \"Excerpt\": \"Excerpt\",\n            \"Evidence\": \"Measure of Evidence\",\n            \"added_on\": \"Added on\",\n            \"doi\": \"DOI\",\n            \"Document ID\": \"CORD_UID\"\n            },\n    inplace=True\n    )\n    return final_summary_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iterate through the list of queries and create a csv file for each query\nfor i,q_text in enumerate(QUERY):\n    \n    retrived_path = \"/kaggle/input/retrievaloutput/doc_scores_\" + str(i+1) + \"_retrieved.csv\"\n    reranked_path = \"/kaggle/input/rerankeddocs/sim_scores_\" + str(i+1) + \"_retrieved.csv\"\n    qa_path = \"/kaggle/input/qaoutput/qa_\" + str(i+1) + \".csv\"\n    metadata_path = \"/kaggle/input/CORD-19-research-challenge/metadata.csv\"\n    \n    output_file_name = q_text.replace(\" \",\"_\") + \".csv\"\n    output_df = create_summary_table(retrived_path, reranked_path, qa_path, metadata_path)\n    output_df.to_csv(output_file_name)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}