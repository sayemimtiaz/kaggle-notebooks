{"cells":[{"metadata":{"_uuid":"4755c4915c1449fc9d5be0e7d3cba71a3bbcce09","_cell_guid":"ad72abc6-6c95-45b2-aacc-42fbb85b8507"},"cell_type":"markdown","source":"## *Last edit by DLao - 2020/09 updated with full data*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<br>\n<br>\n\n\n![](https://cdn.statically.io/img/thakoni.com/f=auto%2Cq=30/wp-content/uploads/2020/06/1591106722_Lucifer-Season-5-Release-Date-Cast-Netflix-And-Everything-You.jpg)\n# Netflix Analytics - Movie Recommendation through Correlations / CF\n<br>\n\nI love Netflix! Everyone does?\n\nThis project aims to build a movie recommendation mechanism within Netflix. The dataset I used here come directly from Netflix. It consists of 4 text data files, each file contains over 20M rows, i.e. over 4K movies and 400K customers. All together **over 17K movies** and **500K+ customers**! \n\n<br>\nOne of the major challenges is to get all these data loaded into the Kernel for analysis, I have encountered many times of Kernel running out of memory and tried many different ways of how to do it more efficiently. Welcome any suggestions!!!\n\nThis kernel will be consistently be updated! Welcome any suggestions! Let's get started!\n\n<br>\nFeel free to fork and upvote if this notebook is helpful to you in some ways!\n","execution_count":null},{"metadata":{"_uuid":"8482ce5398e66af9faa64f603c4ebd2b5324ad33","_cell_guid":"e7fd6bbb-9fb6-455f-8f14-6fb25d55c866"},"cell_type":"markdown","source":"## Table of Content:\n\n* Objective\n\n* Data manipulation\n    -  Data loading\n    -  Data viewing\n    -  Data cleaning\n    -  Data slicing\n    -  Data mapping\n    \n* Recommendation models\n    -  Recommend with Collaborative Filtering (*Edit on 2017/11/07*)\n    -  Recommend with Pearsons' R correlation","execution_count":null},{"metadata":{"_uuid":"689e0500abe3bd15f72c1cb3010c538c90631b50","_cell_guid":"68066366-4219-4779-a159-d503bdedbfdd"},"cell_type":"markdown","source":"# Objective\n<br>\nLearn from data and recommend best TV shows to users, based on self & others behaviour\n<br>","execution_count":null},{"metadata":{"_uuid":"8656955e16b88d57f19a3789c90069059ba884b9","_cell_guid":"fbfaefbf-fda2-46ce-9e22-59b2c8d17fa2"},"cell_type":"markdown","source":"# Data manipulation","execution_count":null},{"metadata":{"_uuid":"2101d86c2cf3f7a61475ea82aaa3c5dd068cf187","_cell_guid":"5a824059-8c9b-4418-9a24-f833943d49cb"},"cell_type":"markdown","source":"## Data loading","execution_count":null},{"metadata":{"_uuid":"d0978db1b40af98cf11b5b185ef264a9891d183d","_cell_guid":"4092e050-1938-4283-9b18-396c60e94ee1"},"cell_type":"markdown","source":"Each data file (there are 4 of them) contains below columns:\n\n* Movie ID (as first line of each new movie record / file)\n* Customer ID\n* Rating (1 to 5)\n* Date they gave the ratings\n\nThere is another file contains the mapping of Movie ID to the movie background like name, year of release, etc","execution_count":null},{"metadata":{"_uuid":"1f60257741c703435318df7e05e2a46c6e11af63","_cell_guid":"637b34e2-b123-4b2d-8e70-97631b0321f9"},"cell_type":"markdown","source":"Let's import the library we needed before we get started:","execution_count":null},{"metadata":{"_uuid":"3bc39967a41f9ec3989f971c49916b822b0806b7","collapsed":true,"_cell_guid":"046298b9-7ef7-4e68-aef2-a1fe316be5a0","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport re\nfrom scipy.sparse import csr_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom surprise import Reader, Dataset, SVD\nfrom surprise.model_selection import cross_validate\nsns.set_style(\"darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"665b9a4bceca7bb318e39f1a5825170b18c6cc63","_cell_guid":"be4477f1-7a11-48f4-8147-262a6198609f"},"cell_type":"markdown","source":"Next let's load first data file and get a feeling of how huge the dataset is:","execution_count":null},{"metadata":{"_uuid":"2a5476e11ee4539c129f2da35fccdacf2c296765","collapsed":true,"_cell_guid":"0343ba37-0654-469c-98e5-812ecbaca528","trusted":false},"cell_type":"code","source":"# Skip date\ndf1 = pd.read_csv('../input/combined_data_1.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n\ndf1['Rating'] = df1['Rating'].astype(float)\n\nprint('Dataset 1 shape: {}'.format(df1.shape))\nprint('-Dataset examples-')\nprint(df1.iloc[::5000000, :])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3509640b273342e38c2635d1bb003e0d33de9e8c","_cell_guid":"b5d0ced5-5376-4ff5-86f9-e642a7adbd92"},"cell_type":"markdown","source":"Let's try to load the 3 remaining dataset as well:","execution_count":null},{"metadata":{"_uuid":"a6ca9915b92abd2681ae9a355d446e73b6fbe795","collapsed":true,"_cell_guid":"4a093a49-8a80-4afd-bc13-17b84b284142","trusted":false},"cell_type":"code","source":"#df2 = pd.read_csv('../input/combined_data_2.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n#df3 = pd.read_csv('../input/combined_data_3.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n#df4 = pd.read_csv('../input/combined_data_4.txt', header = None, names = ['Cust_Id', 'Rating'], usecols = [0,1])\n\n\n#df2['Rating'] = df2['Rating'].astype(float)\n#df3['Rating'] = df3['Rating'].astype(float)\n#df4['Rating'] = df4['Rating'].astype(float)\n\n#print('Dataset 2 shape: {}'.format(df2.shape))\n#print('Dataset 3 shape: {}'.format(df3.shape))\n#print('Dataset 4 shape: {}'.format(df4.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebf5b154314c1268b4fffdf0449172b71e393c4f","_cell_guid":"11ca529c-e11c-4ec1-b9e9-d6c6c45163de"},"cell_type":"markdown","source":"Now we combine datasets:","execution_count":null},{"metadata":{"_uuid":"4ea5a28d0108d2b272f1d30cf749080c4e94e66d","collapsed":true,"_cell_guid":"ded88177-b586-48f2-bf3d-e1a892aca10e","trusted":false},"cell_type":"code","source":"# load less data for speed\n\ndf = df1\n#df = df1.append(df2)\n#df = df.append(df3)\n#df = df.append(df4)\n\ndf.index = np.arange(0,len(df))\nprint('Full dataset shape: {}'.format(df.shape))\nprint('-Dataset examples-')\nprint(df.iloc[::5000000, :])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bfa706c8f28f965b669dcfb285c9c32c1478bad","_cell_guid":"78a857d7-1ab1-4d93-b750-9c14b4ba2c9a"},"cell_type":"markdown","source":"## Data viewing","execution_count":null},{"metadata":{"_uuid":"b96e6aebfe14e3be18722b759654b732b8fa4d51","_cell_guid":"48f3f057-706a-4667-b58e-79d70893cbb1"},"cell_type":"markdown","source":"Let's give a first look on how the data spread:","execution_count":null},{"metadata":{"_uuid":"7e8780821d463af5bdcee9ec2662cf27d89745e4","collapsed":true,"_cell_guid":"0d82d7df-6c77-44f2-a0bc-70ae0324329f","trusted":false},"cell_type":"code","source":"p = df.groupby('Rating')['Rating'].agg(['count'])\n\n# get movie count\nmovie_count = df.isnull().sum()[1]\n\n# get customer count\ncust_count = df['Cust_Id'].nunique() - movie_count\n\n# get rating count\nrating_count = df['Cust_Id'].count() - movie_count\n\nax = p.plot(kind = 'barh', legend = False, figsize = (15,10))\nplt.title('Total pool: {:,} Movies, {:,} customers, {:,} ratings given'.format(movie_count, cust_count, rating_count), fontsize=20)\nplt.axis('off')\n\nfor i in range(1,6):\n    ax.text(p.iloc[i-1][0]/4, i-1, 'Rating {}: {:.0f}%'.format(i, p.iloc[i-1][0]*100 / p.sum()[0]), color = 'white', weight = 'bold')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dddad55f2699f3f4c02ae64a3e470c314e248643","_cell_guid":"a7394a2b-8c79-40b8-b967-765d3ae0ad10"},"cell_type":"markdown","source":"We can see that the rating tends to be relatively positive (>3). This may be due to the fact that unhappy customers tend to just leave instead of making efforts to rate. We can keep this in mind - low rating movies mean they are generally really bad","execution_count":null},{"metadata":{"_uuid":"bf7bd867b322b3e40c4eb1204d345029b4eb31b6","_cell_guid":"581427e0-87df-46b1-a0af-7eb06932b1a3"},"cell_type":"markdown","source":"## Data cleaning","execution_count":null},{"metadata":{"_uuid":"f232d44b5a8282bdcfbab54861bbd7990132e2c7","_cell_guid":"3165defc-df86-49a8-ba51-6abb9fa253b1"},"cell_type":"markdown","source":"Movie ID is really a mess import! Looping through dataframe to add Movie ID column WILL make the Kernel run out of memory as it is too inefficient. I achieve my task by first creating a numpy array with correct length then add the whole array as column into the main dataframe! Let's see how it is done below:","execution_count":null},{"metadata":{"_uuid":"498476341fad8d25d24090c07ea4b48299f9424a","collapsed":true,"_cell_guid":"d06e0993-d5ff-4f75-87a7-7659f5427ebf","trusted":false},"cell_type":"code","source":"df_nan = pd.DataFrame(pd.isnull(df.Rating))\ndf_nan = df_nan[df_nan['Rating'] == True]\ndf_nan = df_nan.reset_index()\n\nmovie_np = []\nmovie_id = 1\n\nfor i,j in zip(df_nan['index'][1:],df_nan['index'][:-1]):\n    # numpy approach\n    temp = np.full((1,i-j-1), movie_id)\n    movie_np = np.append(movie_np, temp)\n    movie_id += 1\n\n# Account for last record and corresponding length\n# numpy approach\nlast_record = np.full((1,len(df) - df_nan.iloc[-1, 0] - 1),movie_id)\nmovie_np = np.append(movie_np, last_record)\n\nprint('Movie numpy: {}'.format(movie_np))\nprint('Length: {}'.format(len(movie_np)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73c7888f9cf7e1d0f705c6a14019d9371eaa9bf3","collapsed":true,"_cell_guid":"e7da935d-a055-4ce6-9509-9c0439fda1de","trusted":false},"cell_type":"code","source":"# remove those Movie ID rows\ndf = df[pd.notnull(df['Rating'])]\n\ndf['Movie_Id'] = movie_np.astype(int)\ndf['Cust_Id'] = df['Cust_Id'].astype(int)\nprint('-Dataset examples-')\nprint(df.iloc[::5000000, :])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7abf85f047576e1c8fe7742e28bd2a55d33c366c","_cell_guid":"fd1a2d66-78b0-4191-8ca2-0caef60e91fa"},"cell_type":"markdown","source":"## Data slicing","execution_count":null},{"metadata":{"_uuid":"b0107145609698c552ad9e74fd192cbbe93c4bb3","_cell_guid":"6532819a-7b08-45c4-8b25-952568d7d465"},"cell_type":"markdown","source":"The data set now is super huge. I have tried many different ways but can't get the Kernel running as intended without memory error. Therefore I tried to reduce the data volumn by improving the data quality below:\n\n* Remove movie with too less reviews (they are relatively not popular)\n* Remove customer who give too less reviews (they are relatively less active)\n\nHaving above benchmark will have significant improvement on efficiency, since those unpopular movies and non-active customers still occupy same volumn as those popular movies and active customers in the view of matrix (NaN still occupy space). This should help improve the statistical signifiance too.\n\nLet's see how it is implemented:","execution_count":null},{"metadata":{"_uuid":"b8987bf7e2cfcdc2a69fb767c4033d05240cc5a3","collapsed":true,"_cell_guid":"1db45c46-ee82-4db5-be2c-919258c09d47","trusted":false},"cell_type":"code","source":"f = ['count','mean']\n\ndf_movie_summary = df.groupby('Movie_Id')['Rating'].agg(f)\ndf_movie_summary.index = df_movie_summary.index.map(int)\nmovie_benchmark = round(df_movie_summary['count'].quantile(0.7),0)\ndrop_movie_list = df_movie_summary[df_movie_summary['count'] < movie_benchmark].index\n\nprint('Movie minimum times of review: {}'.format(movie_benchmark))\n\ndf_cust_summary = df.groupby('Cust_Id')['Rating'].agg(f)\ndf_cust_summary.index = df_cust_summary.index.map(int)\ncust_benchmark = round(df_cust_summary['count'].quantile(0.7),0)\ndrop_cust_list = df_cust_summary[df_cust_summary['count'] < cust_benchmark].index\n\nprint('Customer minimum times of review: {}'.format(cust_benchmark))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc6022b8d87bfb7679984bcbd4b928a54ef19be8","_cell_guid":"bebeaf19-b3a0-45d9-8a91-deaff2881d71"},"cell_type":"markdown","source":"Now let's trim down our data, whats the difference in data size?","execution_count":null},{"metadata":{"_uuid":"f09c53f0e7b7fea039437c43e5163a5a59250b70","collapsed":true,"_cell_guid":"61f85e6a-3438-456b-b169-f42c0270a752","trusted":false},"cell_type":"code","source":"print('Original Shape: {}'.format(df.shape))\ndf = df[~df['Movie_Id'].isin(drop_movie_list)]\ndf = df[~df['Cust_Id'].isin(drop_cust_list)]\nprint('After Trim Shape: {}'.format(df.shape))\nprint('-Data Examples-')\nprint(df.iloc[::5000000, :])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea0da55846a3795aead5d0365d5fcf91b03636ab","_cell_guid":"7f10cc54-4021-4748-9f2f-933d541acee4"},"cell_type":"markdown","source":"Let's pivot the data set and put it into a giant matrix - we need it for our recommendation system:","execution_count":null},{"metadata":{"_uuid":"528c8ecb8bbd94130e38e68362184087dcc39f83","collapsed":true,"_cell_guid":"9e5a21fd-ccff-4fd3-aebe-cd82e5734ba9","trusted":false},"cell_type":"code","source":"df_p = pd.pivot_table(df,values='Rating',index='Cust_Id',columns='Movie_Id')\n\nprint(df_p.shape)\n\n# Below is another way I used to sparse the dataframe...doesn't seem to work better\n\n#Cust_Id_u = list(sorted(df['Cust_Id'].unique()))\n#Movie_Id_u = list(sorted(df['Movie_Id'].unique()))\n#data = df['Rating'].tolist()\n#row = df['Cust_Id'].astype('category', categories=Cust_Id_u).cat.codes\n#col = df['Movie_Id'].astype('category', categories=Movie_Id_u).cat.codes\n#sparse_matrix = csr_matrix((data, (row, col)), shape=(len(Cust_Id_u), len(Movie_Id_u)))\n#df_p = pd.DataFrame(sparse_matrix.todense(), index=Cust_Id_u, columns=Movie_Id_u)\n#df_p = df_p.replace(0, np.NaN)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"598d5c85304513168b871257f5c3bd810a7a8da4","_cell_guid":"62ba3943-5369-4df9-b33e-3a5a8b47e9f9"},"cell_type":"markdown","source":"## Data mapping","execution_count":null},{"metadata":{"_uuid":"ab718ba4d0e6b3b95d03c25b577884e88af77b93","_cell_guid":"7768ffa2-e387-4b15-8ef1-c808229f4dc0"},"cell_type":"markdown","source":"Now we load the movie mapping file:","execution_count":null},{"metadata":{"_uuid":"d971e5a1ccd038f9a08e126daeb8995d30f9e014","collapsed":true,"_cell_guid":"cec6d42b-adff-49c0-939c-2f92adae15a4","trusted":false},"cell_type":"code","source":"df_title = pd.read_csv('../input/movie_titles.csv', encoding = \"ISO-8859-1\", header = None, names = ['Movie_Id', 'Year', 'Name'])\ndf_title.set_index('Movie_Id', inplace = True)\nprint (df_title.head(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6becd707a3c14a5c76887789e21e9fbf150e9f8","_cell_guid":"c1b464a7-3c69-4acb-adcb-00d8b3ec9c93"},"cell_type":"markdown","source":"# Recommendation models","execution_count":null},{"metadata":{"_uuid":"bd73e13a984d412908272360c701c15b58f412df","_cell_guid":"a73d197f-1700-40ae-b20c-cab9b8e0c008"},"cell_type":"markdown","source":"Well all data required is loaded and cleaned! Next let's get into the recommendation system.","execution_count":null},{"metadata":{"_uuid":"523277beb220f90b2f7fb58dab680e22db2aa325","_cell_guid":"fa6ad634-5c47-41e1-adeb-47fe2bd8f1b9"},"cell_type":"markdown","source":"## Recommend with Collaborative Filtering","execution_count":null},{"metadata":{"_uuid":"5fb7d8cd461f7ab0b279acdfc135bfb3c302c3e8","_cell_guid":"1ca487dc-253b-4a5e-ab00-516ee846306a"},"cell_type":"markdown","source":"Evalute performance of [collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering), with just first 100K rows for faster process:","execution_count":null},{"metadata":{"_uuid":"a59aaac88ca121b93fcd8807e462dc7f0b609254","collapsed":true,"scrolled":true,"_cell_guid":"fa414c7c-f908-40fa-bc99-9b221748c923","trusted":false},"cell_type":"code","source":"reader = Reader()\n\n# get just top 100K rows for faster run time\ndata = Dataset.load_from_df(df[['Cust_Id', 'Movie_Id', 'Rating']][:], reader)\n#data.split(n_folds=3)\n\nsvd = SVD()\ncross_validate(svd, data, measures=['RMSE', 'MAE'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c519e55ef4ae222556048f22bb55e4c8885d864","_cell_guid":"c02b147f-b163-4a67-897e-881fbe2d69cd"},"cell_type":"markdown","source":"Below is what user 783514 liked in the past:","execution_count":null},{"metadata":{"_uuid":"7b832da6358dc5ad2b3c4c2a60e73cacdfe0fe75","collapsed":true,"_cell_guid":"dca8773e-5a63-4ac8-9691-77d6a50e1485","trusted":false},"cell_type":"code","source":"df_785314 = df[(df['Cust_Id'] == 785314) & (df['Rating'] == 5)]\ndf_785314 = df_785314.set_index('Movie_Id')\ndf_785314 = df_785314.join(df_title)['Name']\nprint(df_785314)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c18f9856722f273498b9192e2b722077b9d86962","_cell_guid":"fac49661-d5d9-4d1d-a3f5-110220f18b89"},"cell_type":"markdown","source":"Let's predict which movies user 785314 would love to watch:","execution_count":null},{"metadata":{"_uuid":"6f47935958ad57c568fa58253a83452abe83fbed","collapsed":true,"_cell_guid":"7da1d4f5-ef96-4f33-96ae-a66028f8926d","trusted":false},"cell_type":"code","source":"user_785314 = df_title.copy()\nuser_785314 = user_785314.reset_index()\nuser_785314 = user_785314[~user_785314['Movie_Id'].isin(drop_movie_list)]\n\n# getting full dataset\ndata = Dataset.load_from_df(df[['Cust_Id', 'Movie_Id', 'Rating']], reader)\n\ntrainset = data.build_full_trainset()\nsvd.fit(trainset)\n\nuser_785314['Estimate_Score'] = user_785314['Movie_Id'].apply(lambda x: svd.predict(785314, x).est)\n\nuser_785314 = user_785314.drop('Movie_Id', axis = 1)\n\nuser_785314 = user_785314.sort_values('Estimate_Score', ascending=False)\nprint(user_785314.head(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f24caa335865e13f5e0feb47bfb0b47996c69570","_cell_guid":"25d88f52-36a2-4c62-bdc5-fee174b1aab7"},"cell_type":"markdown","source":"## Recommend with Pearsons' R correlations","execution_count":null},{"metadata":{"_uuid":"4d4082f49c67f7406af44d9125ddab4c16ca967e","_cell_guid":"d1220239-a049-4543-9ab6-89e1d00d6cdd"},"cell_type":"markdown","source":"The way it works is we use Pearsons' R correlation to measure the linear correlation between review scores of all pairs of movies, then we provide the top 10 movies with highest correlations:","execution_count":null},{"metadata":{"_uuid":"531e21998a34956e35f3e0a839e18d528faa6709","collapsed":true,"_cell_guid":"200cf4e8-59d6-459d-a0e5-5c9452bc8ad0","trusted":false},"cell_type":"code","source":"def recommend(movie_title, min_count):\n    print(\"For movie ({})\".format(movie_title))\n    print(\"- Top 10 movies recommended based on Pearsons'R correlation - \")\n    i = int(df_title.index[df_title['Name'] == movie_title][0])\n    target = df_p[i]\n    similar_to_target = df_p.corrwith(target)\n    corr_target = pd.DataFrame(similar_to_target, columns = ['PearsonR'])\n    corr_target.dropna(inplace = True)\n    corr_target = corr_target.sort_values('PearsonR', ascending = False)\n    corr_target.index = corr_target.index.map(int)\n    corr_target = corr_target.join(df_title).join(df_movie_summary)[['PearsonR', 'Name', 'count', 'mean']]\n    print(corr_target[corr_target['count']>min_count][:10].to_string(index=False))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7b9374a3c6bcd9d890f1a1462d6d4f2cb55dad5","_cell_guid":"c06b5afa-cf47-4853-a712-ee0afe60b994"},"cell_type":"markdown","source":"A recommendation for you if you like 'What the #$*! Do We Know!?'","execution_count":null},{"metadata":{"_uuid":"903cb1f6529d9d93deb557b5ac7eeba4b42d8a53","collapsed":true,"_cell_guid":"9691bc55-4bde-4580-ae43-f9698e46ab81","trusted":false},"cell_type":"code","source":"recommend(\"What the #$*! Do We Know!?\", 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3646da79d856c2895001832bedb3bf5236a84166","_cell_guid":"c4f1cfbe-0c57-46b5-97d4-1f9466862802"},"cell_type":"markdown","source":"X2: X-Men United:","execution_count":null},{"metadata":{"_uuid":"8586765d4b658bea95997f4418b7ae14c2c6be3d","collapsed":true,"_cell_guid":"9fc24be5-1037-4208-b1ca-07e2e9f8c4a7","trusted":false},"cell_type":"code","source":"recommend(\"X2: X-Men United\", 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08abd9eabd6c0999751f5cab879e9976517e61d8","_cell_guid":"e9ba141d-cec5-4104-935e-f0492ce099df"},"cell_type":"markdown","source":"Hope it is a good read. I will keep updating this Kernel (more models etc). Welcome any suggestions!\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}