{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TEXT CLASSIFICATION METHODS COMPARISON\nHi,\nThis is my 2nd notebook on NLP, and I want to address the COVID-19 text classification problem. What I mainly wanted to do, is to create a comparative study of the different methods used for NLP text classification/semantic analysis. You can copy my notebook, make changes here and there, and let me know how it goes. I'll also include comments for each step of what I'm doing. Also, I'm available for suggestions/corrections, so do comment if you have any. \n\n*This is going to be a long one, let's go!*","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries\nPretty straigtforward, we'll start with importing the libraries.","metadata":{}},{"cell_type":"markdown","source":"# Reading the dataset\nWe'll use pandas to read the train and test dataset. ","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D, Input\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nimport random\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-25T12:31:53.210075Z","iopub.execute_input":"2021-05-25T12:31:53.210464Z","iopub.status.idle":"2021-05-25T12:32:02.862631Z","shell.execute_reply.started":"2021-05-25T12:31:53.210384Z","shell.execute_reply":"2021-05-25T12:32:02.861541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_train.csv', encoding = 'latin1') \ntest = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_test.csv', encoding = 'latin1')\n\n#Now you can try without the encoding (which I had done before), it throws an error, something like this:  'utf-8' codec can't decode byte <byte> in position <position>: unexpected end of data","metadata":{"execution":{"iopub.status.busy":"2021-05-25T12:32:40.148548Z","iopub.execute_input":"2021-05-25T12:32:40.149063Z","iopub.status.idle":"2021-05-25T12:32:40.531964Z","shell.execute_reply.started":"2021-05-25T12:32:40.149017Z","shell.execute_reply":"2021-05-25T12:32:40.531081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, having read our train and test data, let's get the max number of words in a sentence. We'd need this for padding (explained in later section)","metadata":{}},{"cell_type":"code","source":"train['OriginalTweet'].apply(lambda x:len(str(x).split())).max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check out the unique output classes of the data. We'll store them in a variable to use it for predictions.","metadata":{}},{"cell_type":"code","source":"label = train['Sentiment'].unique()\nlabel","metadata":{"execution":{"iopub.status.busy":"2021-05-25T12:32:49.968302Z","iopub.execute_input":"2021-05-25T12:32:49.968606Z","iopub.status.idle":"2021-05-25T12:32:49.988855Z","shell.execute_reply.started":"2021-05-25T12:32:49.96858Z","shell.execute_reply":"2021-05-25T12:32:49.98798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert categorical variable into dummy/indicator variables.\n\n**Syntax:**\n\n**pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None) **\n\nParameters:\ndataarray-like, Series, or DataFrame\nData of which to get dummy indicators.\n\nprefixstr, list of str, or dict of str, default None\nString to append DataFrame column names. Pass a list with length equal to the number of columns when calling get_dummies on a DataFrame. Alternatively, prefix can be a dictionary mapping column names to prefixes.\n\nprefix_sepstr, default ‘_’\nIf appending prefix, separator/delimiter to use. Or pass a list or dictionary as with prefix.\n\ndummy_nabool, default False\nAdd a column to indicate NaNs, if False NaNs are ignored.\n\ncolumnslist-like, default None\nColumn names in the DataFrame to be encoded. If columns is None then all the columns with object or category dtype will be converted.\n\nsparsebool, default False\nWhether the dummy-encoded columns should be backed by a SparseArray (True) or a regular NumPy array (False).\n\ndrop_firstbool, default False\nWhether to get k-1 dummies out of k categorical levels by removing the first level.\n\ndtypedtype, default np.uint8\nData type for new columns. Only a single dtype is allowed.\n\nReturns\nDataFrame\nDummy-coded data.","metadata":{}},{"cell_type":"code","source":"y=train['Sentiment'].values\ny = pd.get_dummies(y)\nprint('Shape of label tensor:', y)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T12:33:09.123377Z","iopub.execute_input":"2021-05-25T12:33:09.123723Z","iopub.status.idle":"2021-05-25T12:33:09.14649Z","shell.execute_reply.started":"2021-05-25T12:33:09.123691Z","shell.execute_reply":"2021-05-25T12:33:09.145601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization\nTokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types – word, character, and subword (n-gram characters) tokenization.\n\nHere, we use keras.processing class Tokenizer.\n\n**Syntax:**\n\ntf.keras.preprocessing.text.Tokenizer(\n    num_words=None,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True, split=' ', char_level=False, oov_token=None,\n    document_count=0, **kwargs\n)\n\n**num_words**: max number of words to be kept based on word frequency. \n\n**filters**: removing special characters from the data\n\n**lower**: convert to lowecase\n\n**split**: split the data on ' '\n\n**char_level**: (Boolean value true/false) whether every character has to be treated as a token.\n\n**document_count**: An integer count of the total number of documents that were used to fit the Tokenizer\n\n\nSo, next we would use this tokenizer to convert the text into sequences and to ensure a unifrom length, we pad these sequences to the max_len with 0s.","metadata":{}},{"cell_type":"markdown","source":"# Encoding Data\n","metadata":{}},{"cell_type":"markdown","source":"We would tokenize our entire data, so I'd create a new dataframe combining the tweet values of both train and test data, and fit our tokenizer on this new dataframe.","metadata":{}},{"cell_type":"markdown","source":"Now, having initalized a tokenizer in the previous step, we would now use the tokenizer to convert the text from train dataset to tokens, and pad the values with 0s to ensure a uniform length.","metadata":{}},{"cell_type":"code","source":"tmp = train['OriginalTweet'] + test['OriginalTweet']\ntmp = tmp.astype(str)\ntokenizer = text.Tokenizer(num_words=400000,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True,\n    split=\" \")\nmax_len = 70\ntokenizer.fit_on_texts(tmp)\nword_index = tokenizer.word_index\nlen(word_index)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T12:35:16.448777Z","iopub.execute_input":"2021-05-25T12:35:16.449136Z","iopub.status.idle":"2021-05-25T12:35:16.983214Z","shell.execute_reply.started":"2021-05-25T12:35:16.449105Z","shell.execute_reply":"2021-05-25T12:35:16.982308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train['OriginalTweet'].values\nX = tokenizer.texts_to_sequences(X)\nX = sequence.pad_sequences(X, maxlen=max_len)\nprint('Shape of data tensor:', X.shape)\n ","metadata":{"execution":{"iopub.status.busy":"2021-05-25T12:35:20.228267Z","iopub.execute_input":"2021-05-25T12:35:20.228854Z","iopub.status.idle":"2021-05-25T12:35:21.448184Z","shell.execute_reply.started":"2021-05-25T12:35:20.228793Z","shell.execute_reply":"2021-05-25T12:35:21.447095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the data\nWe would split up our train dataset into xtrain, xvalid, ytrain and yvalid. Let's go parameter by parameter.\n\nSyntax:\n**sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\ntest_size\ntrain_size\nshuffle**\n\n***arrays**:\n\ntrain.OriginalTweet.values: X value (Input data on which the model has to be trained), train.Sentiment.values: y value (Output data on which the model has to be trained),\n\n\n**stratify**: If not None, data is split in a stratified fashion, using this as the class labels.\n\n**test_size**: Dividing the train and test data set (In our case 20%)\n\n**train_size**: Dividing the train and test data set (In our case 100% - 20% = 80%)\n\n**random_state**: value for initializing the internal random number generator, which will decide the splitting of data into train and test indices in your case\n\n\n**shuffle**: how the train and test data is divided. (In this case: 20%)\n\n**stratify**:If not None, data is split in a stratified fashion, using this as the class labels.\n","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X, y, \n                                                  random_state=46, \n                                                  test_size=0.3, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T13:14:52.608771Z","iopub.execute_input":"2021-05-25T13:14:52.609158Z","iopub.status.idle":"2021-05-25T13:14:52.631952Z","shell.execute_reply.started":"2021-05-25T13:14:52.609126Z","shell.execute_reply":"2021-05-25T13:14:52.630974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('x_train.shape: ' + str(x_train.shape),' y_train.shape: '+str(y_train.shape))\nprint('x_test.shape: ' + str(x_test.shape),' y_train.shape: '+str(y_test.shape))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Embedding Layer\n\nTo move onto the next step, we need to be familiar with the concept of word embeddings.\n\n> Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning.\n\nIn simple words, we can put it in this way that each word is represented as a vector in vector space, and that's how we maintain the similarity between the words. Consider two similar words, like 'good' and 'great'. The distance of the vectors between these words need to be less to denote their similarity. There are a number of techniques to convert words to vectors, such as:\n\n1. Frequency based Embedding\n\n 1.1. Count Vectors\n \n 1.2. TF-IDF\n \n 1.3. Co-Occurrence Matrix\n \n2. Prediction based Embedding\n\n 2.1. CBOW\n \n 2.2. Skip-Gram\n\n3. Using pre-trained Word Vectors\n\n  3.1. Word2Vec\n  \n  3.2. GloVe\n  \nI won't go into much detail regarding the embeddings, but if you want to know more, you should definitely check out this [link](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)\n\nAlso, while choosing an embedding, there's no right or wrong. It completely depends on the problem statement. I'll be trying out the default [pretrained Keras Embedding Layer](https://keras.io/api/layers/core_layers/embedding/) and the pretrained [GloVe vectors](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010) here. GloVe works excellent when the data size is huge, as they compare the words to a giant global corpus. \n","metadata":{}},{"cell_type":"markdown","source":"I've included the GloVe vector file in input data. Let me just initialize and build the embedding matrix, which would serve as weights in Embedding layer of my neural network models.","metadata":{}},{"cell_type":"code","source":"embeddings_index = {}\nf = open('../input/glove840b300dtxt/glove.840B.300d.txt','r',encoding='utf-8')\nfor line in tqdm(f):\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray([float(val) for val in values[1:]])\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2021-05-25T12:46:50.127215Z","iopub.execute_input":"2021-05-25T12:46:50.127691Z","iopub.status.idle":"2021-05-25T12:50:18.959352Z","shell.execute_reply.started":"2021-05-25T12:46:50.12766Z","shell.execute_reply":"2021-05-25T12:50:18.958519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-05-25T12:51:04.624385Z","iopub.execute_input":"2021-05-25T12:51:04.62471Z","iopub.status.idle":"2021-05-25T12:51:04.711975Z","shell.execute_reply.started":"2021-05-25T12:51:04.624683Z","shell.execute_reply":"2021-05-25T12:51:04.71096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building\n\nThis is the part where it gets interesting. After all the process of cleaning, tokenizing, embedding words, we're now ready to create our model and feed the data into it and check which model gives a better accuracy.","metadata":{}},{"cell_type":"markdown","source":"**1. Simple RNN (Recurrent Neural Networks) Model**\n\nWe'll start off with a very simple RNN model. If you're new to the concept of tensorflow, have a [quick look](https://www.tensorflow.org/tutorials/quickstart/beginner)\n\nLet's first address the question. \n\nWhat is RNN?\n> > > Recurrent neural networks (RNN) are a class of neural networks that are helpful in modeling sequence data. Derived from feedforward networks, RNNs exhibit similar behavior to how human brains function. Simply put: recurrent neural networks produce predictive results in sequential data that other algorithms can’t.\n\nThis is a very good [article](https://builtin.com/data-science/recurrent-neural-networks-and-lstm) to jumpstart with the concepts of RNN and LSTM and understand why they're in much popular demand.","metadata":{}},{"cell_type":"markdown","source":"**Activation Function**\n> In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks.[Continue reading..](https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/#:~:text=Activation%20functions%20are%20mathematical%20equations,relevant%20for%20the%20model's%20prediction.)\n\n**Optimizer**\n> They tie together the loss function and model parameters by updating the model in response to the output of the loss function. In simpler terms, optimizers shape and mold your model into its most accurate possible form by futzing with the weights. The loss function is the guide to the terrain, telling the optimizer when it’s moving in the right or wrong direction.\n[Continue Reading..](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6#:~:text=Many%20people%20may%20be%20using,help%20to%20get%20results%20faster)\nThe [learning rate scheduler](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1) controls the learning rate of the model per epoch according to a predefined scheduler.\n\n**The Sequential Model**\n\nLet me just give a walkthrough on what a sequential model is.\nIn simple words, sequential model is a linear stack of layers, where each layer represent some kind of input, output or computation.\n\nWe'd be using the [sequential model from Keras](https://keras.io/guides/sequential_model/).\n\nIf you checked out the above link, you might have encountered the 'Dense' layer everywhere. So, what exactly is it?\n\n> The dense layer is a neural network layer that is connected deeply, which means each neuron in the dense layer receives input from all neurons of its previous layer. The dense layer is found to be the most commonly used layer in the models. In the background, the dense layer performs a matrix-vector multiplication.\n\nSyntax:\n\ntf.keras.layers.Dense(\n    units,\n    activation=None,\n    use_bias=True,\n    kernel_initializer=\"glorot_uniform\",\n    bias_initializer=\"zeros\",\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    **kwargs\n)\n\n**units**: Positive integer, dimensionality of the output space.\n\n**activation**: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n\n**use_bias**: Boolean, whether the layer uses a bias vector.\n\n**kernel_initializer**: Initializer for the kernel weights matrix.\n\n**bias_initializer**: Initializer for the bias vector.\n\n**kernel_regularizer**: Regularizer function applied to the kernel weights matrix.\n\n**bias_regularizer**: Regularizer function applied to the bias vector.\n\n**activity_regularizer**: Regularizer function applied to the output of the layer (its \"activation\").\n\n**kernel_constraint**: Constraint function applied to the kernel weights matrix.\n\n**bias_constraint**: Constraint function applied to the bias vector.","metadata":{}},{"cell_type":"code","source":"SimpleRNNModel = Sequential()\nSimpleRNNModel.add(Input(shape=x_train.shape[1]))\nSimpleRNNModel.add(Embedding(len(tokenizer.word_index)+1,32))\nSimpleRNNModel.add(SimpleRNN(100))\nSimpleRNNModel.add(Dense(5, activation='softmax'))\n#SimpleRNNModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nSimpleRNNModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy','AUC','Precision','Recall'])    \nSimpleRNNModel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fitting the data\nWe would now [fit the model](https://keras.io/api/models/model_training_apis/) on our data.","metadata":{}},{"cell_type":"code","source":"\nSimpleRNNModelResults = SimpleRNNModel.fit(x_train, y_train, epochs=5, batch_size=64,validation_split=0.1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Overfitting Model Alert!\nSo, I see the accuracy is quite high ~95%, and also high precision, recall and AUC but the val_accuracy is not that great. This means that there might be a possibility of overfitting, where in the model performs well with the train data, but while performing with new data which it isn't trained with, it might not be performing quite well. \n\n","metadata":{}},{"cell_type":"markdown","source":"**Preprocessing the test dataset**","metadata":{}},{"cell_type":"markdown","source":"**2. LSTM (Long Short Term Memory) Networks**","metadata":{}},{"cell_type":"markdown","source":"> Long short-term memory is an artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points, but also entire sequences of data.\n\nTechnically, LSTM was built to overcome the [vanishing gradient](https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577) issue encountered in RNNs. In simple words, RNN training is something like this. From each layer, the error is backpropagated to update the weights of previous layers, but in a case where the gradient is exponentially so less, that it becomes insignificant and the weights are not updated at all. We call this as the *vanishing gradient* problem.","metadata":{}},{"cell_type":"markdown","source":"I'd first try with a simple LSTM model, with a very similar architecture as of the simple RNN model and check how much accuracy that gives us.","metadata":{}},{"cell_type":"code","source":"SimpleLSTMModel = Sequential()\nSimpleLSTMModel.add(Input(shape=x_train.shape[1]))\nSimpleLSTMModel.add(Embedding(len(tokenizer.word_index)+1,32))\nSimpleLSTMModel.add(LSTM(100))\nSimpleLSTMModel.add(Dense(5, activation='softmax'))\nSimpleLSTMModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nSimpleLSTMModel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nSimpleLSTMModelResults = SimpleLSTMModel.fit(x_train, y_train, epochs=5, batch_size=64,validation_split=0.1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So you see, the improvement is noticable changing the model from Simple RNN to LSTM. This is majorly because of the learning structure of LSTM. Let's try with a GRU model and check how much accuracy that provides.","metadata":{}},{"cell_type":"markdown","source":"**3. GRU Gated Recurrent Units**","metadata":{}},{"cell_type":"markdown","source":"Gated recurrent units are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. GRU consists of a update and forget gate.","metadata":{}},{"cell_type":"code","source":"SimpleGRUModel = Sequential()\nSimpleGRUModel.add(Input(shape=x_train.shape[1]))\nSimpleGRUModel.add(Embedding(len(tokenizer.word_index)+1,32))\nSimpleGRUModel.add(GRU(100))\nSimpleGRUModel.add(Dense(5, activation='softmax'))\nSimpleGRUModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nSimpleGRUModel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nSimpleGRUModelResults = SimpleGRUModel.fit(x_train, y_train, epochs=5, batch_size=64,validation_split=0.1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. Bidirectional LSTM Model**\n\nThe Bidirectional LSTM or BiLSTM is a modified version of a simple LSTM, where we use 2 LSTM models, one processing the input and learning occuring in a forward direction and one for backward. It is proven better in terms of accuracy than traditional RNN/GRU and LSTM.\n\nIf you're confused about model selection for your dataset, you can refer [this discussion thread](https://datascience.stackexchange.com/questions/25650/what-is-lstm-bilstm-and-when-to-use-them#:~:text=BiLSTM%20means%20bidirectional%20LSTM%2C%20which,this%20architecture%20to%20other%20RNNs.)","metadata":{}},{"cell_type":"code","source":"BILSTMModel = Sequential()\nBILSTMModel.add(Input(shape=x_train.shape[1]))\nBILSTMModel.add(Embedding(len(tokenizer.word_index)+1,32))\nBILSTMModel.add(Bidirectional(LSTM(100, return_sequences=True)))\nBILSTMModel.add(GlobalMaxPooling1D())\nBILSTMModel.add(Dense(5, activation='softmax'))\nBILSTMModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nBILSTMModel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BILSTMModelResults = BILSTMModel.fit(x_train, y_train, epochs=5, batch_size=64,validation_split=0.1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning/ Fine Tuning","metadata":{}},{"cell_type":"markdown","source":"Let's try some tweaking parameters here and there and adding more layers to see if we can improve the accuracy. I'll also use the weights of the Embedding layer coming from GloVe vector which was initialized in an earlier step.","metadata":{}},{"cell_type":"code","source":"BILSTMModel_2 = Sequential()\nBILSTMModel_2.add(Input(shape=x_train.shape[1]))\nBILSTMModel_2.add(Embedding(24678,300, weights=[embedding_matrix]))\nBILSTMModel_2.add(Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\nBILSTMModel_2.add(GlobalMaxPooling1D())\nBILSTMModel_2.add(Dense(50, activation='relu'))\nBILSTMModel_2.add(Dropout(0.2))\nBILSTMModel_2.add(Dense(5, activation='softmax'))\nBILSTMModel_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nBILSTMModel_2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BILSTMModel_2Results = BILSTMModel_2.fit(x_train, y_train, epochs=5, batch_size=64,validation_split=0.1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}