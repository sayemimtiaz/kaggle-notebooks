{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Text mining research papers for answers using a modified TF-IDF algorithm"},{"metadata":{},"cell_type":"markdown","source":"This notebook attempts to find answers to the 10 tasks under the CORD-19 dataset challenge.\nThe approach is as follows\n\n* Extract data from json files. The title, abstract and body_text are extracted and converted to csv format (all_sources.csv)\n* Perform TF-IDF over documents in corpus to convert documents to document vectors.\n* TF-IDF is quite an elegant method but it suffers from quite a few limitations. So a modification to cosine similarity is explored during matching documents to search query. The exact details of this modification are explained in the notebook.\n* A search query is provided as input which seeks answers to the 10 tasks. Top N relevent documents (configurable) are retrieved according to the modified cosine similarity match. Top M relevent lines (configurable) per document are also retrieved. These are saved in output files.\n* Apart from outputting csv files TF-IDF models are also saved and are available as output data."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport json  # for reading json data files\nimport os\nimport pickle  # For saving data and models that are frequently used.\nimport sys\nimport time  # For timing execution.\nfrom operator import itemgetter\nimport numpy as np\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom scipy.sparse import lil_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer  # For computing tfidf values of documents\nimport string\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read data and write to csv file \n\nPaper id, title, abstract and body is extracted from json files and written to csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# json files are processed and saved in csv format in this file. The fields considered are paperid,title,abstract,body_text\ndata_file_csv = \"all_sources.csv\"\noutput_data_directory = \"/kaggle/working/\"\n# Mention directories where you put json data files\ndata_directories = \"/kaggle/input/\"\nmetadata_file = \"/kaggle/input/CORD-19-research-challenge/metadata.csv\"\n\n# Return a dictionary \ndef read_json(file_path):\n    with open(file_path) as f:\n        \n        data = json.load(f)\n        d = dict()\n        try:\n            d['paper_id'] = data['paper_id']\n        except:\n             d['paper_id'] = \"\"\n        try:\n            d['title'] = data['metadata']['title']\n        except:\n            d['title'] =\"\"\n        try:\n            text = data['abstract']\n        except:\n            text = \"\"\n            \n        abstract_text = \"\"\n        for content in text:\n            abstract_text = abstract_text + content['text']\n        d['abstract_text'] = abstract_text\n        \n        try:\n            text = data['body_text']\n        except:\n            text = \"\"\n            \n        body_text = \"\"\n        for content in text:\n            body_text = body_text + content['text']\n        d['body_text'] = body_text\n\n        return d\n\n\n# Function for pre processing json file content before writing to csv file.\n#\ndef pre_process(dict_text):\n    processed_text_list = []\n    count = 0\n    for text in dict_text.values():\n        processed_text = text\n        processed_text_list.append(processed_text)\n        # processed_text_list contains paper_id, title, abstract and body_text\n    return processed_text_list\n\n\ndef read_json_files_and_write_to_csv():\n    f = open(output_data_directory + data_file_csv, mode=\"w\")\n    f.close()\n    processed_row_list = []\n    # Counter for number of files read\n    number_of_files_read = 0\n    # Write chunksize lines at a time to csv\n    chunksize = 1000\n\n    for dirname, _, filenames in os.walk(data_directories):\n        for filename in filenames:\n            if filename.endswith(\".json\"):\n                file_path = os.path.join(dirname, filename)\n                # Get parsed dictionary from json data file.\n                dict_text = read_json(file_path)\n                # Preprocess text and append to dataframe file by file\n                processed_text_list_returned = pre_process(dict_text)\n                dict_text = {\"paper_id\": processed_text_list_returned[0], \"title\": processed_text_list_returned[1],\n                             \"abstract\": processed_text_list_returned[2], \"body_text\": processed_text_list_returned[3]}\n                # Append dictionary to list\n                processed_row_list.append(dict_text)\n                number_of_files_read += 1\n\n                # Write chunksize number of lines at a time to csv\n                if number_of_files_read % chunksize == 0:\n                    # Create dataframe out of list of dictionaries\n                    df_csv = pd.DataFrame(processed_row_list)\n                    # Dump dataframe to csv file. Write to csv one row at a time\n                    df_csv.to_csv(output_data_directory + data_file_csv, mode='a', header=False, index=False)\n                    processed_row_list = []\n    # Write any remaining lines\n    # Create dataframe out of list of dictionaries\n    df_csv = pd.DataFrame(processed_row_list)\n    # Dump dataframe to csv file. Write to csv one row at a time\n    df_csv.to_csv(output_data_directory + data_file_csv, mode='a', header=False, index=False)\n\n    print(\"Total number of files read = \" + str(number_of_files_read))\n    print(\"Saved in csv format to \" + data_file_csv)\n    \n    \n# Call read_json_files_and_write_to_csv() to convert json data to csv.\nstart_time = time.time()\nprint(\"Processing ...\")\nread_json_files_and_write_to_csv()\nend_time = time.time()\nprint(\"Time usage \" + str(end_time - start_time) + \" seconds\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Perform TF-IDF\n* Data is retrieved from csv files and put in proper format for TFIDF Vectorizer.\n* TF-IDF of documents in corpus is computed. Document Vectors are made unit in length.\n* The TF-IDF models are saved for future use (fitted_vectorizer.pkl and tfidf_train_vectors.pkl)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# TF-IDF\n# Retrieve data from csv file\ndef retrieve_data_from_csv_files_for_tfidf_input():\n    chunksize = 1000\n    chunklist = []\n    for chunk in pd.read_csv(output_data_directory + data_file_csv, chunksize=chunksize, usecols=[1, 2, 3],\n                             header=None):\n        chunklist.append(chunk)\n    df_concat_chunks = pd.concat(chunklist)\n    tfidf_input = df_concat_chunks[1].map(str) + df_concat_chunks[2].map(str) + df_concat_chunks[3].map(str)\n    tfidf_input = tfidf_input.values\n    return tfidf_input\n\n\nstart_time = time.time()\nprint(\"Retrieving data from \" + output_data_directory + data_file_csv + \" for TF-IDF...\")\ntfidf_input = retrieve_data_from_csv_files_for_tfidf_input()\nend_time = time.time()\nprint(\"Time usage \" + str(end_time - start_time) + \" seconds\")\n\nstart_time = time.time()\n\nprint(\"Calculating TF-IDF for documents in corpus ...\")\n\nmy_stop_words = STOPWORDS.union(\n                {\"preprint\", \"medrxiv\", \"copyright\", \"biorxiv\", \"reserved\", \"author\", \"permission\", \"text\", \"word\",\n                 \"count\", \"rights\", \"count\", \"doi\", \"https\", \"funder\", \"peer\", \"reviewed\", \"org\"})\ntfidf_vectorizer = TfidfVectorizer(use_idf=True, decode_error='ignore',stop_words=my_stop_words,lowercase=True,\n                                   norm='l2')\n\nfitted_vectorizer = tfidf_vectorizer.fit(tfidf_input)\n\n# Dump vectorizer\npickle.dump(fitted_vectorizer, open(output_data_directory+\"fitted_vectorizer.pkl\", \"wb\"))\ntfidf_train_vectors = fitted_vectorizer.transform(tfidf_input)\n\nprint(\"Saving TF-IDF model to fitted_vectorizer.pkl and document vectors to tfidf_train_vectors.pkl...\")\n# Dump train vectors\npickle.dump(tfidf_train_vectors, open(output_data_directory+\"tfidf_train_vectors.pkl\", \"wb\"))\n\nend_time = time.time()\nprint(\"Time usage \" + str(end_time - start_time) + \" seconds\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load TF-IDF model"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\nprint(\"Loading TF-IDF model ...\")\nfitted_vectorizer = pickle.load(open(output_data_directory+\"fitted_vectorizer.pkl\", 'rb'))\ntfidf_train_vectors = pickle.load(open(output_data_directory+\"tfidf_train_vectors.pkl\", 'rb'))\nprint(\"Done ...\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Problems faced due to using cosine similarity measure\n\nTF-IDF is usually used with cosine similarity to determine which documents match a search query the most. While trying to use this simplistic measure some problems were observed. These problems are best explained with help of an example.\n\nSuppose we require the relevant documents to the search query \"diabetes covid-19 risk factors\". The word tokens are diabetes, covid, 19, risk and factors\n\nThe top 5 documents retrieved and the top 5 lines per document only using cosine similarity are displayed below. This illustrates that just cosine similarity is not good enough for our purpose.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load all_sources.csv into a dataframe\nchunksize = 1000\nchunklist = []\nfor chunk in pd.read_csv(output_data_directory + data_file_csv, chunksize=chunksize,\n                         usecols=[0, 1, 2, 3], header=None):\n    chunklist.append(chunk)\ndf_concat_chunks = pd.concat(chunklist)\n\nsearch_query = \"diabetes covid 19 risk factors\"\n# Vectorize query\ntfidf_search_query_vectors = fitted_vectorizer.transform([search_query])\n# Calculate cosine similarity between search query vector and document vectors\ncosine_distance_between_search_query_and_doc_vectors = scipy.sparse.csr_matrix.dot(tfidf_train_vectors,\n                                                       scipy.sparse.csr_matrix.transpose(tfidf_search_query_vectors))\ntop_n_docs = 5\ntop_n_lines_per_doc = 5\n# Get top_n_docs which match the search query the most. The matrix cosine_distance_between_search_query_and_doc_vectors is negated to get document indices \n# in descending order of match\ntop_N_file_indices = np.squeeze(np.array((np.argsort(-cosine_distance_between_search_query_and_doc_vectors.todense().flatten(),axis=1)[0,:top_n_docs])))\n\nprint(\"Showing top matches for search query with just cosine similarity...\")\ndoc_no = 0\nfor index in top_N_file_indices:\n    # Show titles of top documents that match the search query the most\n    doc_no+=1\n    print(\"Document No: \"+str(doc_no))\n    print(\"Title : \"+str(df_concat_chunks.iloc[index, 1]))\n    print()\n    all_lines_in_selected_doc = str(df_concat_chunks.iloc[index, 1]) + \\\n                                    str(df_concat_chunks.iloc[index, 2]) + \\\n                                    str(df_concat_chunks.iloc[index, 3])\n    lines = all_lines_in_selected_doc.split(\". \")\n    # Vectorize lines in current file\n    tfidf_line_vectors = fitted_vectorizer.transform(lines)\n    # Calculate cosine distance between search query and line vectors\n    cosine_distance_between_search_query_and_line_vectors = scipy.sparse.csr_matrix.dot(tfidf_line_vectors,\n                                                            scipy.sparse.csr_matrix.transpose(\n                                                            tfidf_search_query_vectors))\n    # Rank lines according to cosine distance between lines of document and search query vectors\n    top_N_line_indices = np.squeeze(np.array((np.argsort(\n                            -cosine_distance_between_search_query_and_line_vectors.todense().flatten(), axis=1)[0, :top_n_lines_per_doc])))\n    key_lines_in_doc = list(itemgetter(*top_N_line_indices)(lines))\n    print(\"Lines :\")\n    for line in key_lines_in_doc:\n          print(line)\n    print(\"###########################################\")\n    print()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Documents number 2 and 5 have little to no relevance to the search query."},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary_of_words_in_query_and_their_idf_values = dict()\nidf_values = fitted_vectorizer.idf_\nwords_in_search_query = search_query.split()\nfor word in words_in_search_query:\n    try:\n        index_of_word_in_vocab_of_tfidf_vectorizer = fitted_vectorizer.vocabulary_[word]\n        idf_of_word = idf_values[index_of_word_in_vocab_of_tfidf_vectorizer]\n        dictionary_of_words_in_query_and_their_idf_values.update({word:idf_of_word})\n    except:\n        dictionary_of_words_in_query_and_their_idf_values = dictiionary_of_words_in_query_and_their_idf_values\nprint(\"IDF values for words in query ...\")\nprint(dictionary_of_words_in_query_and_their_idf_values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following IDF values for the words in the query was observed at the time of running this simulation.\n\n{'diabetes': 3.453179026688757, 'covid': 3.9598320012919723, '19': 1.4025946028730936, 'risk': 1.8492836902078613, 'factors': 1.6826481323365186}\n\n(These values may change as dataset changes but it should be realized that the values themselves are immaterial for the discussion here)\n\nThe cosine similarity calculates the following metric for a particular file i and the search query vector\n\nCOSINE_SIMILARITY(i) = TFIDFi(diabetes) x TFIDFq(diabetes) + TFIDFi(covid) x TFIDFq(covid) + TFIDFi(19) x TFIDFq(19) + TFIDFi(risk) x TFIDFq(risk) + TFIDFi(factors) x TFIDFq(factors)\n\nwhere TFIDFi and TFIDFq represent TFIDF values for the word in a particular file i and search query respectively\n\nSince covid has a high value of IDF it may produce a high value of cosine similarity for files with high values of term frequency for covid. Thus documents with only a high term frequency for covid might be ranked higher. This poses a problem for information retrieval relevant to all terms of the query.\n\nThe next cell plots the term frequencies of words present in query in top selected documents"},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_of_words_in_query_and_term_frequencies_in_a_document = dict()\ndoc_no = 0\nfor doc_index in top_N_file_indices:\n    doc_no+=1\n    for key in dictionary_of_words_in_query_and_their_idf_values:\n        try:\n            index_of_key_in_vocab_of_tfidf_vectorizer = fitted_vectorizer.vocabulary_[key]\n            idf_of_key = dictionary_of_words_in_query_and_their_idf_values[key]\n            tfidf_value_of_word_in_current_document = tfidf_train_vectors[doc_index,index_of_key_in_vocab_of_tfidf_vectorizer]\n            term_frequency_of_word_in_current_document = tfidf_value_of_word_in_current_document/idf_of_key\n            dict_of_words_in_query_and_term_frequencies_in_a_document.update({key:term_frequency_of_word_in_current_document})\n        except:\n            dict_of_words_in_query_and_term_frequencies_in_a_document = dict_of_words_in_query_and_term_frequencies_in_a_document\n            \n    print(\"Term frequencies for words in document: \"+str(doc_no)+\" (Showing term frequencies for words which are present in query)\")\n    print()\n    print(dict_of_words_in_query_and_term_frequencies_in_a_document)\n    k = dict_of_words_in_query_and_term_frequencies_in_a_document.keys()\n    v = dict_of_words_in_query_and_term_frequencies_in_a_document.values()\n    plt.bar(k,v)\n    plt.title(\"Term frequencies for words in document: \"+str(doc_no)+\" (Showing term frequencies for words which are present in query)\")\n    plt.show()\n    print()\n\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modification to cosine similarity measure\n\nAs speculated document number 2 and 5 have term frequency of diabetes = 0. Still those documents popped up in the top 5 results for search query \"diabetes covid 19 risk factors\". From the plotted bar graphs it is evident that we will get more relevant documents to our search query if documents contain as many words in the query as possible.\n\nRewriting the cosine similarity metric from before \n\nCOSINE_SIMILARITY(i) = TFIDFi(diabetes) x TFIDFq(diabetes) + TFIDFi(covid) x TFIDFq(covid) + TFIDFi(19) x TFIDFq(19) + TFIDFi(risk) x TFIDFq(risk) + TFIDFi(factors) x TFIDFq(factors)  ---- (1)\n\nwhere TFIDFi and TFIDFq represent TFIDF values for the word in a particular file i and search query respectively.\nCOSINE_SIMILARITY(i) denotes how much file i matches with search query\n\nWe define component of cosine similarity as \n\nCOMPONENT_OF_COSINE_SIMILARITY = TFIDFi(word) x TFIDFq(word)\n\nSo equation (1) has 5 components of cosine similarity. One for \"diabetes\" one for \"covid\" etc.\n\nSo we need documents where each component of cosine similarity contributes as much as possible to the final cosine similarity metric.\n\nIn short we would prefer documents where more number of cosine similarity components are non-zero\n\nFor e.g we would prefer a document where\n\nTFIDFi(diabetes) x TFIDFq(diabetes) != 0 and TFIDFi(covid) x TFIDFq(covid) !=0 and TFIDFi(risk) x TFIDFq(risk) !=0\n\nover a document where\n\nTFIDFi(covid) x TFIDFq(covid) !=0 and TFIDFi(risk) x TFIDFq(risk) !=0\n\neven though the latter may have a higher cosine similarity value.\n\nA way to accomplish this is to calculate a metric similar to entropy over the components of cosine similarity\n\nFor e.g. \n\nCOSINE_SIMILARITY(i) = 0.7 + 0 + 0 + 0 + 0 \n\nAfter normalizing so that all components add to 1 we get \n\nNORMALIZED_COSINE_SIMILARITY(i) = 1 + 0 + 0 + 0 + 0\n\nThe components of normalized cosine similarity are (1,0,0,0,0) . If we consider this to be a probability distribution then information entropy of this distribution is 0.\n\nConsider another cosine similarity measure\n\nCOSINE_SIMILARITY(i) = 0.3 + 0.2 + 0 + 0.2 + 0\n\nNORMALIZED_COSINE_SIMILARITY(i) = 0.428 + 0.285 + 0 + 0.285 + 0\n\nComponents of normalized cosine similarity are (0.428,0.285,0,0.285,0). Information entropy of this distribution is about 1.56\n\nSo we would like to pick the latter document over the former even if they have the same cosine similarity measure of 0.7.\n\nThe idea is to multiply the information entropy with cosine similarity and use it as a final metric to rank results of a search query. Therefore if cosine similarity is high just because of one word (such as \"covid\" in the example above) the resulting metric will still be low. Thus this method will prefer documents which have more words of the search query.\n\nNote that in practice we will use the following formula for calculating information entropy of components of normalized cosine similarity\n\nENTROPY_OF_NORMALIZED_COSINE_SIMILARITY_TERMS = SUM (Pi x log (1 + 1/Pi)) where Pi are components of normalized cosine similarity\n\nThis formula will prevent entropy from becoming 0 when search query is a single word.\n\nThe calculated entropy metric can be refined further. \nIf entropy of normalized cosine components of 2 documents is the same then the document which has more important words should be favoured. IDF is a measure of importance of words. A higher IDF means the word is rrer and hence more important in the context of the query.\nThus entropy of normalized cosine components of documents can be weighed by the idf values of words in query.\n\nFor e.g. If a query has 3 words namely w1,w2 and w3 then \n\nSUM_OF_IDF_VALUES = IDF(w1) + IDF(w2) + IDF(w3)\n\nIf a document has w1 and w2 then WEIGHTED_ENTROPY_OF_NORMALIZED_COSINE_COMPONENTS = (IDF(w1) + IDF(w2)) * ENTROPY_OF_NORMALIZED_COSINE_SIMILARITY_TERMS/SUM_OF_IDF_VALUES\nThis metric will improve the entropy measure of a document if more important words are present in document. \n\nThe next few cells implement the explained functionality."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper functions to preprocess query , get data from metedata.csv and write search results to output files.\n\ndef write_to_output_csv_file(data,output_filename):\n    if output_filename !=\"\":\n        df = pd.DataFrame(data, columns=['search_query','paper_id', 'title','key_lines','doi'])\n        df.to_csv(output_data_directory + output_filename,mode = 'a',header=True,index=False)\n\ndef get_metadata(paper_id):\n    chunksize = 1000\n    chunklist = []\n    for chunk in pd.read_csv(metadata_file, chunksize=chunksize):\n        chunklist.append(chunk)\n    df_concat_chunks = pd.concat(chunklist)\n\n    # filtering data\n    matching_row = df_concat_chunks.loc[df_concat_chunks['sha'] == paper_id]\n    if  matching_row.empty == False:\n\n        title = matching_row.iloc[0]['title']\n        doi = 'http://doi.org/'+str(matching_row.iloc[0]['doi'])\n    else:\n        title = None\n        doi = None\n    return title,doi\n\ndef preprocess_query(query):\n    \n    # Preprocess query\n    # # Sanitize and preprocess queries\n    my_stop_words = STOPWORDS\n    # Replace punctuation with space\n    words = query.split()\n    # Replace punctuation with space\n    punctuation_to_be_replaced_with_space = string.punctuation\n    line1 = query.maketrans(punctuation_to_be_replaced_with_space,\n                                     ' ' * len(punctuation_to_be_replaced_with_space))\n    stripped = [w.translate(line1) for w in words]\n    stripped = \" \".join(stripped)\n    search_queries_processed = stripped\n    words = search_queries_processed.split()\n    query_new = ''\n    for word in words:\n        if word not in my_stop_words:\n            query_new = query_new + \" \" + word\n    return query_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \n# This function receives a query, top N number of documents , top M number of lines per document and output filename to write results of query\ndef search_corpus(query,top_n_docs,top_n_lines_per_doc,output_filename):\n    csv_data_search_results = []\n    \n    query_new = preprocess_query(query)\n    \n    dict_of_words_in_query_and_their_idf = dict()\n    words = query_new.split()\n    \n    for word in words:\n        word = word.lower()\n        \n        try:\n            index_of_word_in_vectorizer_vocab = fitted_vectorizer.vocabulary_[word]\n            # Store words in query and their IDFs\n            dict_of_words_in_query_and_their_idf.update({word:idf_values[index_of_word_in_vectorizer_vocab]})\n                                        \n        except:\n            dict_of_words_in_query_and_their_idf = dict_of_words_in_query_and_their_idf\n            \n    \n    # Vectorize query\n\n    tfidf_search_query_vectors = fitted_vectorizer.transform([query_new])\n    \n    # Calculate cosine similarity components for each document in corpus. cosine_similarity_components will be a sparse matrix.\n    # For e.g. if tfidf_search_query_vectors is a sparse csr matrix and non zero elements are (0,2453) and (0,35654) then this matrix will be multiplied with all \n    # tfidf_train_vectors (document vectors) . The broadcasted multiplication will be non zero for those documents where either 2453 or 35654 or both are nonzero.\n        \n    cosine_similarity_components = scipy.sparse.csr_matrix.multiply(tfidf_train_vectors,tfidf_search_query_vectors)\n    \n    \n    # Make cosine similarity components add to one for all documents. Sum of numbers in axis 1 will be one.\n    normalized_cosine_similarity_components = scipy.sparse.csr_matrix(cosine_similarity_components). \\\n    multiply(scipy.sparse.csr_matrix.power(scipy.sparse.csr_matrix(scipy.sparse.csr_matrix.sum(cosine_similarity_components,axis=1)),-1))\n    \n    # idf_sparse_matrix is a csr matrix. It has shape equivalent to corpus of tfidf document vectors.\n    # For e.g suppose cosine_similarity_components  =              (0,20776) = 0.5\n    #                                                              (0,31212) = 0.3\n    #                                                                    .\n    #                                                                    .\n    #                                                              (29200,20776) = 0.4                                                       \n    # where axis 0 of csr matrix represents document number and axis 1 represents word index. idf_sparse_matrix will contain idf values of features in the same format.\n    # For e,g  idf_sparse_matrix                    =              (0,20776) = idf_of_20776\n    #                                                              (0,31212) = idf_of_31212 and so on.\n    \n    idf_sparse_matrix = scipy.sparse.lil_matrix(tfidf_train_vectors.shape)\n\n    for row in cosine_similarity_components.nonzero()[0]:\n        non_zero_columns = cosine_similarity_components[row].nonzero()[1]\n        data = idf_values[non_zero_columns]\n        data_index = 0\n        for column in non_zero_columns:\n            idf_sparse_matrix[row,column] = data[data_index]\n            data_index+=1\n            \n    idf_sparse_matrix = idf_sparse_matrix.tocsr()\n    \n    \n    # Compute the reciprocal of elements in normalized_cosine_similarity_components.\n    # For e.g. if normalized_cosine_similarity_components = (0,20776) = 0.625\n    #                                                       (0,31212) = 0.375\n    #                                                           .\n    #                                                           .\n    #                                                       (29200,20776) = 1\n    \n    # then one_by_pi                                      = (0,20776) = 1/0.625\n    #                                                       (0,31212) = 1 / 0.375 and so on.\n    one_by_pi = scipy.sparse.csr_matrix(scipy.sparse.csr_matrix.power(normalized_cosine_similarity_components,-1))\n    # Take log1p of terms in one_by_pi\n    log_one_by_pi = scipy.sparse.csr_matrix(scipy.sparse.csr_matrix.log1p(one_by_pi))\n    \n    # Calculate entropy (sum(Pi * lop1p(1/Pi))) of normalized cosine similarity components \n    entropy_of_normalized_cosine_similarity_components = scipy.sparse.csr_matrix(scipy.sparse.csr_matrix.sum(\n                                                         scipy.sparse.csr_matrix.multiply(normalized_cosine_similarity_components,log_one_by_pi),\n                                                         axis=1))\n    \n    # sum of idfs of words in query\n    sum_of_idfs_of_words_in_query = sum(list(dict_of_words_in_query_and_their_idf.values()))\n    \n    weights_of_entropy_terms = scipy.sparse.csr_matrix.sum(idf_sparse_matrix,axis=1)/sum_of_idfs_of_words_in_query\n    \n\n    cosine_distance_between_search_query_and_doc_vectors = scipy.sparse.csr_matrix.dot(tfidf_train_vectors,\n                                                           scipy.sparse.csr_matrix.transpose(tfidf_search_query_vectors))\n\n    weighted_entropy_of_normalized_cosine_similarity_components = scipy.sparse.csr_matrix.multiply(\n                                                                  entropy_of_normalized_cosine_similarity_components,\n                                                                  weights_of_entropy_terms)\n    total_match_between_search_query_and_doc_vectors = scipy.sparse.csr_matrix.multiply(\n                                                       cosine_distance_between_search_query_and_doc_vectors ,\n                                                       weighted_entropy_of_normalized_cosine_similarity_components)\n\n    current_query_match = total_match_between_search_query_and_doc_vectors\n\n    top_N_file_indices = np.squeeze(np.array((np.argsort(-current_query_match.todense().flatten(),axis=1)[0,:top_n_docs])))\n\n    for index in top_N_file_indices:\n        paper_id = df_concat_chunks.iloc[index, 0]\n        title = df_concat_chunks.iloc[index, 1]\n        \n\n        all_lines_in_selected_doc = str(df_concat_chunks.iloc[index, 1]) + \\\n                                    str(df_concat_chunks.iloc[index, 2]) + \\\n                                    str(df_concat_chunks.iloc[index, 3])\n        lines = all_lines_in_selected_doc.split(\". \")\n\n        # Match query with all lines of selected document\n        tfidf_line_vectors = fitted_vectorizer.transform(lines)\n\n        # Repeat the same algorithm as before for top M lines per document in top N documents\n        \n        cosine_similarity_components = scipy.sparse.csr_matrix.multiply(tfidf_search_query_vectors, tfidf_line_vectors)\n        normalized_cosine_similarity_components = scipy.sparse.csr_matrix(cosine_similarity_components). \\\n                                                  multiply(scipy.sparse.csr_matrix.power(\n                                                  scipy.sparse.csr_matrix(scipy.sparse.csr_matrix.sum(cosine_similarity_components, axis=1)), -1))\n\n        one_by_pi = scipy.sparse.csr_matrix(scipy.sparse.csr_matrix.power(normalized_cosine_similarity_components, -1))\n        log_one_by_pi = scipy.sparse.csr_matrix(scipy.sparse.csr_matrix.log1p(one_by_pi))\n        entropy_of_normalized_cosine_similarity_components = scipy.sparse.csr_matrix(\n                                                             scipy.sparse.csr_matrix.sum(\n                                                             scipy.sparse.csr_matrix.multiply(\n                                                             normalized_cosine_similarity_components, log_one_by_pi), axis=1))\n        # Rank lines in a file according to match with search query\n        cosine_distance_between_search_query_and_line_vectors = scipy.sparse.csr_matrix.dot(tfidf_line_vectors,\n                                                                scipy.sparse.csr_matrix.transpose(\n                                                                tfidf_search_query_vectors))\n\n        total_match_between_search_query_and_line_vectors_in_curent_file = scipy.sparse.csr_matrix.multiply(\n                                                        cosine_distance_between_search_query_and_line_vectors,\n                                                        entropy_of_normalized_cosine_similarity_components)\n        current_line_match = total_match_between_search_query_and_line_vectors_in_curent_file\n\n        top_N_line_indices = np.squeeze(\n                             np.array((np.argsort(\n                            -current_line_match.todense().flatten(), axis=1)[0, :top_n_lines_per_doc])))\n        key_lines = list(itemgetter(*top_N_line_indices)(lines))\n       \n        # Search metadata.csv to get doi,title,links to document\n        title1, doi = get_metadata(paper_id)\n        if title1 != \"\" and title1 != None:\n            title = title1\n\n        d_line = [query,paper_id, title,\". \\n\".join(key_lines), doi]\n        csv_data_search_results.append(d_line)\n    write_to_output_csv_file(csv_data_search_results, output_filename)\n    return top_N_file_indices\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing after modification to cosine similarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_N_file_indices = search_corpus(query=\"diabetes covid 19 risk factors \", top_n_docs=5, top_n_lines_per_doc=5,output_filename=\"\")\n\ndict_of_words_in_query_and_term_frequencies_in_a_document = dict()\ndoc_no = 0\nfor doc_index in top_N_file_indices:\n    doc_no+=1\n    for key in dictionary_of_words_in_query_and_their_idf_values:\n        try:\n            index_of_key_in_vocab_of_tfidf_vectorizer = fitted_vectorizer.vocabulary_[key]\n            idf_of_key = dictionary_of_words_in_query_and_their_idf_values[key]\n            tfidf_value_of_word_in_current_document = tfidf_train_vectors[doc_index,index_of_key_in_vocab_of_tfidf_vectorizer]\n            term_frequency_of_word_in_current_document = tfidf_value_of_word_in_current_document/idf_of_key\n            dict_of_words_in_query_and_term_frequencies_in_a_document.update({key:term_frequency_of_word_in_current_document})\n        except:\n            dict_of_words_in_query_and_term_frequencies_in_a_document = dict_of_words_in_query_and_term_frequencies_in_a_document\n            \n    print(\"Term frequencies for words in document: \"+str(doc_no)+\" (Showing term frequencies for words which are present in query)\")\n    print()\n    print(dict_of_words_in_query_and_term_frequencies_in_a_document)\n    k = dict_of_words_in_query_and_term_frequencies_in_a_document.keys()\n    v = dict_of_words_in_query_and_term_frequencies_in_a_document.values()\n    plt.bar(k,v)\n    plt.title(\"Term frequencies for words in document: \"+str(doc_no)+\" (Showing term frequencies for words which are present in query)\")\n    plt.show()\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected we see that all of the top 5 documents have a nonzero term frequency for important words in the search query (like diabetes,covid etc). Thus the modification gives somewhat more relevant results than cosine similarity."},{"metadata":{},"cell_type":"markdown","source":"# Pros and Cons\n\nPros\n* TF-IDF with modified cosine similarity measure gives more relevant results to search queries as demonstrated than using simple cosine similarity.\n* TF-IDF is quite easy to understand and implement. It is also quite fast.\n\nCons\n* TF-IDF is a bag of words model and is unable to capture semantics of language.\n* The entropy based modification to cosine similarity is a heuristic measure based in intuition. Although it gives improved results there is no baseline to compare its performance"},{"metadata":{},"cell_type":"markdown","source":"# Queries for various tasks"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task 1\n\nquestions = [\"range of incubation periods of covid-19\",\n             \"how incubation period of covid-19 varies with age\",\n             \"how long individuals are contagious even after recovery from covid-19\"]\noutput_file_name = \"incubation_task1.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n    \nquestions = [\"asymptomatic shedding and transmission of covid-19 sars-cov-2\",\n             \"asymptomatic shedding and transmission of covid-19 sars-cov-2 in children\",\n             \"seasonality of transmission of covid-19\",\n             \"role of the environment in transmission covid-19\",\n             \"effectiveness of movement control strategies to prevent secondary transmission in health care and community settings covid-19\"]\noutput_file_name = \"transmission_task1.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n    \nquestions = [\"physical science of the coronavirus sars-cov-2 charge distribution\",\n             \"physical science of the coronavirus sars-cov-2 adhesion to hydrophilic and hydrophobic surfaces\",\n             \"persistence and stability of coronavirus sars-cov-2 in nasal discharge\",\n             \"persistence and stability of coronavirus sars-cov-2 in sputum\",\n             \"persistence and stability of coronavirus sars-cov-2 in urine\",\n             \"persistence and stability of coronavirus sars-cov-2 in fecal matter\",\n             \"persistence and stability of coronavirus sars-cov-2 in blood\",\n             \"persistence of coronavirus sars-cov-2 on inanimate surfaces\"]\noutput_file_name = \"persistence_task1.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n\nquestions = [\"natural history of the coronavirus sars-cov-2\",\n             \"shedding of coronavirus sars-cov-2 from an infected person\",\n             \"implementation of diagnostics and products to improve clinical processes for covid-19\"]\n\noutput_file_name = \"diagnostics_task1.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n\nquestions = [\"disease models including animal models for infection covid-19\",\n             \"disease models including animal models for transmission covid-19\"]\noutput_file_name = \"disease_models_task1.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n\nquestions = [\"tools and studies to monitor phenotypic change sars-cov-2\",\n             \"tools and studies to monitor potential adaptation sars-cov-2\"]\noutput_file_name = \"tools_task1.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n    \nquestions = [\"immune response and immunity against covid-19\"]\noutput_file_name = \"immunity_task1.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n    \nquestions = [\"effectiveness of personal protective equipment (PPE) covid-19\",\n             \"usefulness of personal protective equipment (PPE) to reduce risk of transmission in health care and community settings\"]\noutput_file_name = \"ppe_task1.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Task-2\n\nquestions = [\"Smoking risk factors covid-19\",\n             \"pre-existing pulmonary disease risk factors covid-19\",\n             \"whether co-existing respiratory or viral infections make the sars-cov-2 virus more transmissible or virulent\",\n             \"risk factors diabetes covid-19\",\n             \"risk factors hypertension covid-19\",\n             \"risk factors cardiac problems covid-19\",\n             \"neonates and pregnant women risk factors covid-19\",\n             \"socio-economic and behavioral risk factors covid-19\"]\noutput_file_name = \"risk_factors_task2.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n    \nquestions = [\"transmission dynamics of the sars-cov-2 the basic reproductive number\",\n             \"incubation period of covid-19\",\n             \"serial interval covid-19\",\n             \"modes of transmission of sars-cov-2 covid-19\",\n             \"environmental factors in transmission sars-cov-2 covid-19\",]\noutput_file_name = \"transmission_dynamics_task2.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n    \nquestions = [\"risk of fatality among symptomatic hospitalized patients covid-19\",\n             \"high risk patient groups covid-19\",\n             \"susceptibility of various populations covid-19\"]\noutput_file_name = \"severity_of_disease_task2.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n\nquestions = [\"public health mitigation measures that could be effective for control covid-19 sars-cov-2\"]\n\noutput_file_name = \"mitigation_task2.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task-3\n\nquestions = [\"real-time tracking of whole genomes sars-cov-2\",\n             \"coordinating the rapid dissemination of information for the development of diagnostics and therapeutics and to track variations of sars-cov-2 over time\",\n             \"geographic distribution and genomic differences of sars-cov-2\",\n             \"number of strains of sars-cov-2 in circulation\",\n             \"multi-lateral agreements such as the nagoya protocol sars-cov-2\"]\noutput_file_name = \"virus_genetics_task3.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n    \nquestions = [\"evidence that livestock could be infected with sars-cov-2 field surveillance genetic sequencing receptor binding\",\n             \"whether livestock can serve as a reservoir of sars-cov-2\",\n             \"evidence of whether farmers are infected with sars-cov-2 covid-19 and whether they could have played a role in the origin\",\n             \"surveillance of mixed wildlife livestock farms for SARS-CoV-2 and other coronaviruses in southeast asia\",\n             \"Experimental infections to test host range for sars-cov-2\",\n             \"evidence of continued spill-over of sars-cov-2 from animals to humans\",\n             \"socioeconomic and behavioral risk factors for spill-over of sars-cov-2 from animals to humans\"\n             ]\noutput_file_name = \"livestock_task3.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n\nquestions = [\"sustainable risk reduction strategies for covid-19 sars-cov-2\"]\noutput_file_name = \"sustainable_risk_reduction_task3.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task-4\n\nquestions = [\"effectiveness of drugs being developed and tried to treat COVID-19 patients\",\n             \"clinical and bench trials to investigate viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocycline\",\n             \"methods evaluating potential complication of Antibody Dependent Enhancement (ADE) in vaccine recipients covid-19\",\n             \"exploration of use of best animal models and their predictive value for a human vaccine covid-19\",\n             \"capabilities to discover  therapeutics for covid-19\",\n             \"clinical effectiveness studies to discover therapeutics, to include antiviral agents covid-19\",\n             \"models  to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics against covid-19\",\n             \"identifying approaches for expanding production capacity of vaccines and therapeutics against covid-19 to ensure equitable and timely distribution to populations in need\",\n             \"efforts targeted at a universal coronavirus vaccine\",\n             \"efforts to develop animal models and standardize challenge studies covid-19\",\n             \"efforts to develop prophylaxis clinical studies and prioritize in healthcare workers covid-19\",\n             \"approaches to evaluate risk for enhanced disease after vaccination covid-19\",\n             \"assays to evaluate vaccine immune response covid-19\",\n             \"process development for vaccines alongside suitable animal models covid-19\"]\noutput_file_name = \"vaccines_and_therapeutics_task4.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task-5\n\nquestions = [\"resources to support skilled nursing facilities and long term care facilities\",\n             \"mobilization of surge medical staff to address shortages in overwhelmed communities\",\n             \"application of regulatory standards (EUA, CLIA) and ability to adapt care to crisis standards of care level\",\n             \"approaches for encouraging and facilitating the production of elastomeric respirators\",\n             \"best telemedicine practices, barriers and faciitators\",\n             \"guidance on the simple things people can do at home to take care of sick people and manage covid-19\",\n             \"use of AI or artificial intelligence in real-time health care delivery to evaluate interventions, risk factors, and outcomes\",\n             \"innovative solutions and technologies in hospital flow and organization\",\n             \"supply chain management to enhance capacity, efficiency, and outcomes\"]\noutput_file_name = \"medical_resources_task5.csv\"\n\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n\nquestions = [\"age adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) for viral etiologies\",\n             \"extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients\",\n             \"outcomes data for COVID-19 after mechanical ventilation adjusted for age\",\n             \"frequency, and course of extrapulmonary manifestations of COVID-19\",\n             \"cardiomyopathy and cardiac arrest in covid-19\",\n             \"oral medications that might potentially work covid-19\",\n             \"determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (steroids, high flow oxygen) covid-19\"]\noutput_file_name = \"clinical_charecterization_of_virus_task5.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task-6\nquestions = [\"guidance on ways to scale up NPIs or non pharmaceutical interventions to handle covid-19 outbreak\",\n             \"establish funding, infrastructure and authorities to give us time to enhance our health care delivery system capacity to respond to an increase in cases\",\n             \"rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches\",\n             \"methods to control the spread of covid-19 in communities\",\n             \"barriers to compliance and how these vary among different populations\",\n             \"models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status\",\n             \"Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs or non pharmaceutical interventions\",\n             \"why people fail to comply with public health advice, even if they want to do so (social or financial costs may be too high)\",\n             \"economic impact of covid-19 outbreak or any pandemic\",\n             \"identifying policy that lessen/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses,treatment\"]\noutput_file_name = \"non_pharmaceutical_interventions_task6.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task-7\nquestions = [\"immediate policy recommendations on mitigation measures covid-19\",\n             \"denominators for testing covid-19\",\n             \"mechanism for rapidly sharing testing data,including demographics for covid-19\",\n             \"sampling methods to determine asymptomatic disease (use of serosurveys such as convalescent samples)\",\n             \"sampling methods for early detection of disease (use of screening of neutralizing antibodies such as ELISAs)\",\n             \"efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms\",\n             \"leverage universities and private laboratories for testing purposes covid-19\",\n             \"development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests\",\n             \"efforts to track the evolution of the virus (genetic drift or mutations)\",\n             \"use of diagnostics such as host response markers (cytokines) to detect early disease or predict progression\",\n             \"policies and protocols for screening and testing\",\n             \"policies to mitigate the effects on supplies associated with mass testing\",\n             \"barriers to developing and scaling up new diagnostic tests\",\n             \"New platforms and technology (CRISPR) in covid-19\",\n             \"rapid sequencing and bioinformatics to target regions of genome of sars-cov-2\",\n             \"explore capabilities for distinguishing naturally-occurring pathogens from intentional\"]\noutput_file_name = \"diagnostics_and_surveilance_task7.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task-8\nquestions = [\"existing ethical principles and standards to salient issues in COVID-19\",\n             \"support sustained education, access, and capacity building in the area of ethics\",\n             \"physical and psychological health of healthcare workers for Covid-19 patients\",\n             \"underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media\",\n             \"identification of the secondary impacts of prevention control measures covid-19\"]\noutput_file_name = \"ethical_considerations_task8.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task-9\nquestions = [\"Methods for coordinating data-gathering with standardized nomenclature covid-19\",\n             \"Understanding and mitigating barriers to information-sharing covid-19\",\n             \"Integration of federal state local public health surveillance systems\",\n             \"modes of communicating with target high-risk populations (elderly, health care workers)\",\n             \"Risk communication and guidelines that are easy to understand and follow covid-19\",\n             \"Misunderstanding around containment and mitigation covid-19\",\n             \"Understanding coverage policies (barriers and opportunities) related to testing, treatment, and care covid-19\",\n             \"Measures to reach marginalized and disadvantaged populations covid-19\",\n             \"mitigating threats to incarcerated people from COVID-19\"]\noutput_file_name = \"info_sharing_and_intersectoral_collaboration_task9.csv\"\nfor question in questions:\n    search_corpus(query=question, top_n_docs=10, top_n_lines_per_doc=5,output_filename=output_file_name)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}