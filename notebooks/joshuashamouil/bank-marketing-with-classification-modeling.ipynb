{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h1>Bank Marketing Analysis with Classification Modeling</h1></center>","metadata":{}},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Marketing campaigns are characterized by focusing on the customer needs and their overall satisfaction. Nevertheless, there are different variables that determine whether a marketing campaign will be successful or not. There are certain variables that we need to take into consideration when making a marketing campaign.\n\nThe goal of this notebook is to use the data to develop a strong model in order to predict which people the bank should market to for their marketing campain to get people to sign up for a term deposit.","metadata":{}},{"cell_type":"markdown","source":"## What is a Term Deposit?\n\nA **Term deposit** is a deposit that a bank or a financial institurion offers with a fixed rate (often better than just opening deposit account) in which your money will be returned back at a specific maturity time. For more information with regards to Term Deposits please click on this link from Investopedia: https://www.investopedia.com/terms/t/termdeposit.asp","metadata":{}},{"cell_type":"markdown","source":"# About the data","metadata":{}},{"cell_type":"markdown","source":"Unfortunately, not much about the data is given besides for the below information. More information about where this data came from can be found at this website: ","metadata":{}},{"cell_type":"markdown","source":"## Bank client data:<br>\n1 - **age:** (numeric)<br>\n2 - **job:** type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')<br>\n3 - **marital:** marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)<br>\n4 - **education:** (categorical: primary, secondary, tertiary and unknown)<br>\n5 - **default:** has credit in default? (categorical: 'no','yes','unknown')<br>\n6 - **housing:** has housing loan? (categorical: 'no','yes','unknown')<br>\n7 - **loan:** has personal loan? (categorical: 'no','yes','unknown')<br>\n8 - **balance:** Balance of the individual.\n\n## Related with the last contact of the current campaign:\n8 - **contact:** contact communication type (categorical: 'cellular','telephone') <br>\n9 - **month:** last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')<br>\n10 - **day:** last contact day of the week (categorical: 'mon','tue','wed','thu','fri')<br>\n11 - **duration:** last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.<br>\n## Other attributes:<br>\n12 - **campaign:** number of contacts performed during this campaign and for this client (numeric, includes last contact)<br>\n13 - **pdays:** number of days that passed by after the client was last contacted from a previous campaign (numeric; -1 means client was not previously contacted)<br>\n14 - **previous:** number of contacts performed before this campaign and for this client (numeric)<br>\n15 - **poutcome:** outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')<br>\n\nOutput variable (desired target):<br>\n21 - **deposit** - has the client subscribed a term deposit? (binary: 'yes','no')","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"markdown","source":"## Libraries for plotting and analysis","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nfrom matplotlib import pyplot\nfrom numpy import where\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Libraries for Modeling","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import SVMSMOTE\nfrom lightgbm import LGBMClassifier\nfrom sklearn import metrics\nfrom sklearn.datasets import make_classification\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/bank-marketing-dataset/bank.csv')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check for any Null values\nnulls = []\nfor i in data.columns:\n    nulls.append(data[i].isnull().sum())\nprint(nulls)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check the column data types\ntypes=[]\nfor i in data.columns:\n    types.append(type(data[i][0]))\nprint(types)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning: Numeric Variables","metadata":{}},{"cell_type":"markdown","source":"Let's start by cleaning the numeric variables and viewing the basic statistics. ","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before moving on we will remove \"duration\" from our dataset because of target leakage, or the idea that the field gives information into the future. To put it in another way, this is information that we should not have yet. As noted above this field highly affects the output because if a person had a phone call duration of 0, we can assume the caller didn't pick up the phone and therefore had no chance of a sale.","metadata":{}},{"cell_type":"code","source":"data.pop(\"duration\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Becase \"day represents the day of the week we should really treat this as a catigorical variable and not a numeric. Additionally, we will combine these values into 3 buckets: beginning of the month, middle of the month, and end of the month. This will prevent us from having 31 different one-hot encoded variables and generalize the variable so that it makes sense to the business problem at hand. After transforming \"day\" into a catigorical variable we will one-hot encode it with the other categoriccal variables.","metadata":{}},{"cell_type":"code","source":"#start = beginning of the month\n#middle = middle of the month\n#end = end of the month\n\ntemp_day = []\nfor i in data['day']:\n    if i<8:\n        temp_day.append(\"start\")\n    elif i>23:\n        temp_day.append(\"end\")\n    else:\n        temp_day.append(\"middle\")\n\ndata['day'] = temp_day","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The variable \"pdays\" represents the number of days that passed by after the client was last contacted from a previous campaign. We need to also note that if the client was never contacted the value of this field is -1. Becuase a value of -1 doesn't make sense in this field and won't properly capture that a client wasn't contacted I would normally create a new binary variable but \"poutcome\" = nonexistant captures this for us.","metadata":{}},{"cell_type":"markdown","source":"Below are some interesting insights we can find from the data:","metadata":{}},{"cell_type":"code","source":"neg_bal = float(len(data['balance'][data['balance']<0]))/float(len(data['balance']))\nprint(\"{:.2%}\".format(neg_bal)+ \" percent of people or \"+str(int(neg_bal*11162))+ \" people have a negative balance\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first_contact = float(len(data['pdays'][data['pdays']==-1]))/float(len(data['pdays']))\nprint(\"Percentage of people that is being contacted for the first time: \"+\"{:.2%}\".format(first_contact));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn-whitegrid')\n\ndata.hist(bins=20, figsize=(14,10), color='#E14906')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we saw from the above statistics of the numeric variables there are potential outliars. Let's talk about some of the max values for some of our variables: <br>\n\n1 - **Balance:** It's not really unheard of to have $81,000 in your checking account. For example, this would make sense from one with a high salary, intersted in saving.<br> \n\n2 - **pdays:** It's a bit unusual to contact a lead after 854 days (almost 2.5 years) although it is plausable. For instance, maybe after 100 days they were removed from the marketing list but a flag was raised that might indicate that the person is likly will open an account and therefore be put back on the marketing list. Although they were removed then added back, it's possible that they never stopped counting the days in between.<br>\n\n3 - **previous:** Like the previous variables that we discussed this one is also plausable. If a client was in multiple campaigns spanning of many quarters or even years can certainly add up to over 50 contacts.<br>\n\n4 - **Campaign:** Being contated over 60 time for one marketing campaign is a bit unheard of even for a campaign that lasts a year. Think about it, after reaching out to a person 20 don't you think they probably aren't interested? Also, most companies won't waste money like that. This will need more investigating. <br>\n","metadata":{}},{"cell_type":"markdown","source":"# Outliar Analysis","metadata":{}},{"cell_type":"code","source":"data.nlargest(20,\"campaign\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the above data there is no indication of an error in the above data. It looks like all of the above data all have that they were previously not contacted before. I could be that possibly that the company wanted to take an aggesive approach to marketing to fresh leads. Because of this reasoning I will not take out any observations although I will make sure to keep this in mind. Although, I decided not to throw out any variables, there is certainly a strong case to be made for tossing rows or transforming the columns to adjust for the outliers.","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning: Catigorical Variables","metadata":{}},{"cell_type":"code","source":"#populate a list with string-type columns\nstr_columns = []\nfor i in data.columns:\n    if(type(data[i][0])==str):\n        str_columns.append(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(str_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adjusting the Binary Categorical Variables","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2,2)\nfig.tight_layout(h_pad=2)\n\nbin_list = [3,4,5,10]\n\nfor i in range(1,5):\n    plt.subplot(2, 2, i)\n    n = bin_list[i-1]\n    counts = data[str_columns[n]].value_counts()\n    plt.barh(y=counts.index, width=counts)\n    plt.title('Distribution of '+str_columns[n])\n    plt.ylabel(str_columns[n])\n    plt.xlabel('Count')\n\nplt.subplots_adjust(left=.25, bottom=2, right=2.75, top=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# change the character values in the above columns to 0/1 binary values\n# 0=\"no\" and 1=\"yes\"\n\ndata['default'] = [0 if i == \"no\" else 1 for i in data['default']]\ndata['housing'] = [0 if i == \"no\" else 1 for i in data['housing']]\ndata['loan']    = [0 if i == \"no\" else 1 for i in data['loan']   ]\ndata['deposit'] = [0 if i == \"no\" else 1 for i in data['deposit']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check\nprint(data['default'].value_counts())\nprint(data['housing'].value_counts())\nprint(data['loan'].value_counts())\nprint(data['deposit'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#adjust our string list and remove the four variables above\nfor i in ['default','housing','loan','deposit']:\n    str_columns.remove(i)\n    \nprint(str_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adjusting the other categorical variables","metadata":{}},{"cell_type":"code","source":"print(str_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we adjusted the binary categorical variables and turned them into numeric variables that we can work with. Now we can transform the categorical variables that contain more than one unique value using one-hot encoding. Before we transform the variables we must view the distribtion to get an idea of the behavior for each variable.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(4,2)\nfig.tight_layout(h_pad=.25)\n\nj = 0\nk = 0 \nfor i in range(1,8):\n    if k==4:\n        k=0\n        j=1\n    plt.subplot2grid((4, 2), (k,j))\n    n = i-1\n    counts = data[str_columns[n]].value_counts()\n    plt.barh(y=counts.index, width=counts)\n    plt.title('Distribution of '+str_columns[n])\n    plt.ylabel(str_columns[n])\n    plt.xlabel('Count')\n    k=k+1\n                     \nplt.subplots_adjust(left=.25, bottom=.5, right=2.75, top=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A few notes from the above distributions:\n\n+ It's interesting that the largest volume of calls happen in the summer months and weakest in the colder months.\n+ Remember that \"poutcome\" reflects how they responded to a previous campaign. We should note that about 75% in this category is labeled as \"unknown\" which is consistant with what we found earlier.\n+ based on the demographic variables, it seems like this campaign targets middle class individuals which supports the statistics from our \"balance\" variable.  \n\nThis information will be valuable when we review the results of our final model","metadata":{}},{"cell_type":"code","source":"# let's use one-hot encodin to change the categorical variables to numericals\ndata = pd.get_dummies(data)\nprint(len(data.columns))\nprint(data.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"style.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,30))\n## Plotting heatmap. Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(data.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(data.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, );\nplt.title(\"Heatmap of all the Features of Train data set\", fontsize = 25);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the above correlation table the categorical variables that got encoded with exactly 3 unique values appear to have multicolinnearity. This can probably be attributed to the fact that 2 of the encoded values will together have information on the third. For example, if there are three choices between pcoucome (success, failure, unknown) and it wasn't a success or a failure, the data will assume it was unknown. Therefore we can remove one of these encoded variables from each of the categorical varible with three unique values.","metadata":{}},{"cell_type":"code","source":"#delete: poutcome_unknown, marital_single, day_end, contact_unknown\n\ndata.pop(\"poutcome_unknown\")\ndata.pop(\"marital_single\")\ndata.pop(\"day_end\")\ndata.pop(\"contact_unknown\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"Below are a few functions to streamline our modeling","metadata":{}},{"cell_type":"code","source":"def Models(models, X_train, X_test, y_train, y_test, title):\n    model = models\n    model.fit(X_train,y_train)\n    \n    X, y = Definedata()\n    train_matrix = pd.crosstab(y_train, model.predict(X_train), rownames=['Actual'], colnames=['Predicted'])    \n    test_matrix = pd.crosstab(y_test, model.predict(X_test), rownames=['Actual'], colnames=['Predicted'])\n    matrix = pd.crosstab(y, model.predict(X), rownames=['Actual'], colnames=['Predicted'])\n    \n    f,(ax1,ax2,ax3) = plt.subplots(1,3,sharey=True, figsize=(20, 3))\n    #f = plt.figure(figsize=(20, 3))\n    \n    g1 = sns.heatmap(train_matrix, annot=True, fmt=\".1f\", cbar=False,ax=ax1)\n    g1.set_title(title)\n    g1.set_ylabel('Total Deposit = {}'.format(y_train.sum()), fontsize=14, rotation=90)\n    g1.set_xlabel('Accuracy score for Training Dataset = {}'.format(accuracy_score(model.predict(X_train), y_train)))\n    g2 = sns.heatmap(test_matrix, annot=True, fmt=\".1f\",cbar=False,ax=ax2)\n    g2.set_title(title)\n    g2.set_ylabel('Total Deposit = {}'.format(y_test.sum()), fontsize=14, rotation=90)\n    g2.set_xlabel('Accuracy score for Testing Dataset = {}'.format(accuracy_score(model.predict(X_test), y_test)))\n    g3 = sns.heatmap(matrix, annot=True, fmt=\".1f\",cbar=False,ax=ax3)\n    g3.set_title(title)\n    g3.set_ylabel('Total Deposit = {}'.format(y.sum()), fontsize=14, rotation=90)\n    g3.set_xlabel('Accuracy score for Total Dataset = {}'.format(accuracy_score(model.predict(X), y)))\n    \n    plt.show()\n    return y, model.predict(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\ndef Featureimportances(models, X_train, y_train):\n    model = models\n    model.fit(X_train,y_train)\n    importances = model.feature_importances_\n    features = data.columns\n    if len(importances)<len(features): \n        features = data.columns[:len(importances)]\n    else:\n        importances = model.feature_importances_[:len(features)]\n    imp = pd.DataFrame({'Features': features, 'Importance': importances})\n    imp = imp.sort_values(by = 'Importance', ascending=False)[:15]\n    imp['Sum Importance'] = imp['Importance'].cumsum()\n    \n    fig = go.Figure()\n    fig.add_trace(go.Bar(x=imp.Features,y=imp.Importance, marker=dict(color=list(range(20)), colorscale=\"Sunsetdark\")))\n\n    fig.update_layout(title=\"Feature Importance\",\n                                 xaxis_title=\"Features\", yaxis_title=\"Importance\",title_x=0.5, paper_bgcolor=\"mintcream\",\n                                 title_font_size=20)\n    fig.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Definedata():\n    # define dataset\n    X=data.drop(columns=['deposit']).values\n    y=data['deposit'].values\n    return X, y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define dataset\ndata.dropna(inplace=True)\nX=data.drop(columns=['deposit']).values\ny=data['deposit'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'LogisticRegression'\n%time Models(LogisticRegression(),X_train, X_test, y_train, y_test, title) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'GaussianNB'\n%time Models(GaussianNB(),X_train, X_test, y_train, y_test, title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'KNeighborsClassifier'\n%time Models(KNeighborsClassifier(n_neighbors=1),X_train, X_test, y_train, y_test, title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'DecisionTreeClassifier'\n%time Models(DecisionTreeClassifier(max_depth=14),X_train, X_test, y_train, y_test, title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'RandomForestClassifier'\n%time Models(RandomForestClassifier(),X_train, X_test, y_train, y_test, title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'GradientBoostingClassifier'\n%time Models(GradientBoostingClassifier(n_estimators=500, learning_rate=1, max_features=2, max_depth=2, random_state=0),X_train, X_test, y_train, y_test, title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'XGBClassifier'\n%time Models(XGBClassifier(),X_train, X_test, y_train, y_test, title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'LGBMClassifier'\n%time Models(LGBMClassifier(),X_train, X_test, y_train, y_test, title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'LinearDiscriminantAnalysis'\n%time Models(LinearDiscriminantAnalysis(),X_train, X_test, y_train, y_test, title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyze the Strongest Model","metadata":{}},{"cell_type":"markdown","source":"Let's take a closer look at the Random Forrest Model since that was one of our most predictive models","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,auc,roc_curve\n\ntitle = 'RandomForestClassifier'\ny, ypred =  Models(RandomForestClassifier(),X_train, X_test, y_train, y_test, title)\n\nfpr, tpr, thresholds = roc_curve(y, ypred)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we have our 3 confustion matricies that were seen before along with an ROC curve.To briefly analyze the confusion matricies, we have a strong Accuracy for our test set with a value of about 0.71 and a strong Accuacy for our full dataset with an Accuracy of about 0.91. \n\nBelow the confustion matricies, we can see the ROC curve with a value of 0.904 which is very strong. The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 â€“ FPR). Classifiers that give curves closer to the top-left corner indicate a better performance. As a baseline, a random classifier is expected to give points lying along the diagonal (FPR = TPR). The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test. In other words, the 45-degree line represents randomly choosing an outcome.","metadata":{}},{"cell_type":"code","source":"Featureimportances(RandomForestClassifier(), X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Right off the bat we can point out that our two most important features in our model are balance and age. Athough this chart doesn't tell us the direction of the feature (if it's positively or negativly affecting the outcome taking everything else constant) we can imply a few things. First, the larger the balance the more money is saved in the bank. The more money that's saved in the bank the more likely you'll have extra money to put towards a term deposti. Second, the older a person is, the more time they have had accumulating wealth. Therefore, they are more likely to have extra money to put toward a term deposit. It might be important to point out that the next three features are attributed to the campaign. \n\nIt's also important to note that although there are two powerful features, there is a healthy amount of importance distributed among the top 10 features in our model. ","metadata":{}},{"cell_type":"markdown","source":"# Conclusion & Future Work\n\nIn total of 9 algorithms were used to classify whether a person would open a term deposit with the bank. Three metrics were used in evaluating their performances. Performance improvement could be achieved through developing an ensambled model, a combination of different algorithms. Although we focused on the Random Forest Model, it could be observed that the 3 following algorithms outperformed other models in terms of the Accuracy.\n\n1. RandomForestClassifier \n2. GradientBoostingClassifier\n3. XGBClassifier\n\nIt is worth noting that, understanding the business problem, how the dataset is built, and deeply understanding the variables at hand is essential for building an acccurate and significant model. \n\nFuture work will include a comprehensive tuning of these 4 algorithms. ","metadata":{}},{"cell_type":"markdown","source":"# References","metadata":{}},{"cell_type":"markdown","source":"[1] https://www.kaggle.com/janiobachmann/bank-marketing-campaign-opening-a-term-deposit \n\n[2] https://www.kaggle.com/winternguyen/credit-card-100-fraud-detection\n\n[3] https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n","metadata":{}}]}