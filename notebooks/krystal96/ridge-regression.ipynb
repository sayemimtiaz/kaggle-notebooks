{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nOur goal here is to find the prediction model with lowest RMSE. Let's have fun in adjusting parameters!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" # 1. Data Processing","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import package\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1  Delete useless columns\nThere ae so many columns in the data, however, it's impossible to include all of them into the model. So.. The first step would be manually drop the columns which is obviously meaningless: like URL,ID numbers. In fact, there are still plenty of information like 'require license?','host_since', which we have no idea  about their relationship with pricing yet. However, I think it's fine to remain them in the data set at this moment. *","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list_df = pd.read_csv('../input/boston/listings.csv')\nlist_df.head()\n# Manually drop some columns that are useless\ndropcol=['listing_url','scrape_id','jurisdiction_names','license','thumbnail_url','medium_url','picture_url','xl_picture_url'\n        ,'host_thumbnail_url','host_picture_url','neighbourhood_group_cleansed']\nlist_df.drop(dropcol, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Missing data\nThere are two criterion for it:\n    1. The column has more than 20% of the data is missing\n    2. The column is less important to our target feature: Price\nIf column meet both of criterion, just drop it!  It could be tough to determine whether the column is important or not.Personally, I make this descion by my prior knowledge on house pricing problem.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a missing value summary table\ntotal = list_df.isnull().sum().sort_values(ascending=False)\npercent = (list_df.isnull().sum()/list_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop columns base on former criterion\nmissing= ['has_availability','square_feet','monthly_price','weekly_price','security_deposit','notes','interaction',\n          'access','neighborhood_overview','host_about','transit','house_rules']\nlist_df.drop(missing, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Change data type\n\nThe most annoyng and time-consuming part of data processing!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fix some datatype errors, extract numbers and change to int type\ncols = ['host_response_rate', 'host_acceptance_rate', 'price', 'cleaning_fee', 'extra_people']\nfor col in cols:\n    list_df[col] = list_df[col].str.extract(r'(\\d+)')\n    list_df[col] = list_df[col].astype('float128').astype('Int64')\nlist_df.columns\n# Extract the number of amenities \nlist_df['n_amenities'] = list_df['amenities'].apply(lambda x: len(x.replace('{', '').\\\n                        replace('{', '').replace('\"', '').split(',')))\nlist_df.drop('amenities', axis=1, inplace=True)\nnewlist= list_df.select_dtypes(include=['int', 'float'])\n# Use Median to replace with NA\nint_fillmean = lambda x: x.fillna(round(x.mean()))\nnewlist = newlist.apply(int_fillmean, axis=0)\nnewlist = newlist.drop(['id', 'host_id', 'latitude', 'longitude'], axis=1).astype(float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Describle Analysis\n## 2.1 'Price' data analysis\n\n**Several Insights:**\n* Mean is \\$168. The price of Boston Airbnb is not cheap :( \n* Variance is pretty large(12523.80). Price fluctuates so widely.\n* Deviate from the normal distribution.\n* Have appreciable positive skewness.\n* Show peakedness\n* Some outliers (min = \\$1, defintely impossible!) Need further Anomaly Detection.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary \ndef status(x) : \n    return pd.Series([x.count(),x.min(),x.quantile(.25),x.median(),\n                      x.quantile(.75),x.mean(),x.max(),x.var(),x.std(),x.skew(),x.kurt()],\n                     index=['Size','MIN','25% quantile','Median','75% quantile','Mean','MAX','Var','STD','skew','kurt'])\nsummary=pd.DataFrame(round(status(list_df.price),2))\nprint(summary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#histogram\nsns.distplot(list_df['price'])\nplt.title('Price Distribution', fontsize=12)\nplt.ylabel('Percentage', fontsize=10)\nplt.xlabel('Price (dollar)', fontsize=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Numberical variables Analysis\n\n\n\n### 2.2.1 Heatmap for each numberical variables\n1.Those features ae most positive correlated with price:\n    * accommodates\n    * beds\n    * bedrooms\n    * guest_include\n    * cleaning fee\n    * host_listing_count\n    \n\n2.There is another interesting finding, host accept rate has negative correlated with price to some degree which could be considered into our model.\n\n\n3.The correlation between of host_listing_count and host_total_listing_count close to 1. We can drop one column for reducing dimension\n   The same as availability_30/60/90. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=newlist.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(16, 14))\n    ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True, annot=True, fmt='.2f',cmap='YlGnBu')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2.2 Boxplot for beds\n\nInteresting! Zero bed has higher average than others! Basically, mean of beds follow rising trend. However, there's no big difference between beds of 3 to 5.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y='price', x='beds', data=list_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Text data analysis\n\nIn this part, I want to find the most popular words in title and description. \n* Not surprisingly, the high frequence words in title are location, room type and adjective words. \n* For description, common words are \"close to\",\"access to\",\"located in\", also displaying the location and convenience.\n\nFor future Airbnb host, it could be a reference for you to name your house! \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform data structure\nimport string\np_words={}\nwords=list_df.loc[:,['name','price','description']]\nwords=words[words.price!=1]\nwords=words[words.price < (220+1.5*(220-85))]\n\n##Key part!! Transform from list to string\ndef transfer(x):\n    name=[]\n    text=' '\n    for i in range(len(x)):\n        name += x.iloc[i].split(' ')\n    for word in name:\n        text +=word +' '\n    return text\nname=transfer(words.name)\ndescription=transfer(words.description)\n#Draw the plot\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(name)\n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(description)                       \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Categorical variables Analysis\n### 2.4.1 Boxplot for neighbourhood\n\n1. Not surprisingly, top 4 neighbourhoods are between the Financial District and Chinatown. Super near to south station. A great location for business person or tourists.\n2. There are some outliers(extreme expensive) houses lie in Back Bay,Brighton, Mission hill.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deal with outlier\nlist_df = list_df[list_df['price'] > 10] \norder_neigh=list_df.groupby('neighbourhood_cleansed')['price'].median().sort_values(ascending=False).index\nsns.boxplot(y=list_df.price, x=list_df.neighbourhood_cleansed,order=order_neigh)\nax = plt.gca()\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4.2 Boxplot for property type\n\nMean price of Villa,Loft,Condo,Apartment are pretty close. However,there are many outliers in Apartment . Luxury Apartment may has a higher price,which could be a reasonable reason for it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"order_room_type=list_df.groupby('property_type')['price'].median().sort_values(ascending=False).index\nsns.boxplot(y=list_df.price, x=list_df.property_type,order=order_room_type)\nax = plt.gca()\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4.3 Boxplot for room type\n\nReasonable data. Entire room has higher price than share room.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y='price', x='room_type', data=list_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4.4 Boxplot for bed type\n\nReasonable data. Real bed has higher price than others. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y='price', x='bed_type', data=list_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4.5 Boxplot for cancellation_policy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y='price', x='cancellation_policy', data=list_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Modeling\nIn this case, we have so many features and want to find a simple model to predict price problem. We'll introduce **Ridge regression**, which\nis the first of two shrinkage methods. Then ,I’ll try the **lasso**, which is better but difficult to compute.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### ### 3.1 Normalze, Split training and test set\n\nAs mentioned before, there are so many variables in the data. Personally, I only choose the top 15 numberical variables with highest correlation with Price and several categorical data.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deal with outliers\n\nnewlist = newlist[newlist['price'] >10]\nnewlist = newlist[newlist['price'] < (220+1.5*(220-85))]\nlist_df = list_df[list_df['price'] >10]\nlist_df = list_df[list_df['price'] < (220+1.5*(220-85))]\n\n#Rank by correlation and only choose top 10  correlation.abs(),delete variables share high correlation with others\nX = corr['price'].abs().sort_values(ascending=False).drop(['price','host_total_listings_count'])[:10]\nx= newlist[X.index] \ny = newlist['price']\n\n#Create dummy varaibles\ndummy_room_type=pd.get_dummies(list_df['room_type'])\ndummy_cancellation_policy=pd.get_dummies(list_df['cancellation_policy'])\ndummy_bed_type=pd.get_dummies(list_df['bed_type'])\ndummy_property_type=pd.get_dummies(list_df['property_type'])\ndummy_neighbourhood_cleansed=pd.get_dummies(list_df['neighbourhood_cleansed'])\nx=pd.concat([x,dummy_room_type,dummy_cancellation_policy,dummy_bed_type,dummy_neighbourhood_cleansed], axis=1)\n\n# Normalize all features\ndef normalize(df):\n    result = df.copy()\n    for feature_name in df.columns:\n        max_value = df[feature_name].max()\n        min_value = df[feature_name].min()\n        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n    return result\n\nx=normalize(x)\ny= (y-min(y))/(max(y)-min(y))\n\n# split training and testing model\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Ridge regression\n\nAs heatmap shows before, our variables are correlated to each other to some degree. Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors. It is hoped that the net effect will be to give estimates that are more reliable. We can use the alpha value to impose a maximum value on the sum of all our weight. By imposing this penalty, we can decrease unimportant parameters. \n\n\nThe key step of Ridge regression is to choosing suitable alpha. The metrics we use here is RMSE(Root Mean Square Error)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas = 10**np.linspace(-3,3,100)\nfrom sklearn.metrics import mean_squared_error\nridge_cofficients = []\nfor alpha in alphas:\n    ridge = Ridge(alpha = alpha, normalize=True)\n    ridge.fit(x_train, y_train)\n    ridge_cofficients.append(ridge.coef_)\n    \nplt.rcParams['axes.unicode_minus'] = False\nplt.style.use('ggplot')\nplt.plot(alphas, ridge_cofficients)\nplt.xscale('log')\nplt.axis('tight')\nplt.title('Ridge trace')\nplt.xlabel('Log Alpha')\nplt.ylabel('Cofficients')\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the right side, the coefficients are all zero. Somewhere in the middle, we have some coefficient values that will give you better prediction results. To find satisfactory answers, we’d need to do **cross-validation testing.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"BTW..We could change scorng by different metrics. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# coss-validation test\nridge_cv = RidgeCV(alphas = alphas, normalize=True, cv = 10,scoring=\"neg_mean_squared_error\")\nridge_cv.fit(x_train, y_train)\n# best lambda\nridge_cv.alpha_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use best alpha to build model. Our final result is show below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_ridge=Ridge(alpha = ridge_cv.alpha_)\nmodel_ridge.fit(x_train,y_train)\npred_ridge= model_ridge.predict(x_test)\n\n# evaluate the result\nrmse = np.sqrt(metrics.mean_squared_error(y_test,pred_ridge))\nr_square = metrics.r2_score(y_test, pred_ridge)\nprint('For test data set our final result is:')\nprint('RMSE {}'.format(rmse ))\nprint('R^2 is {}'.format(r_square))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize our result! To see how R^2 and RMSE change with aphla.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"RR_square = []\nRR_train = []\nalpha = [0.0001, 0.001, 0.1, 1, 2,5,8,10, 15,20]\nfor a in alpha:\n    RigeModel = Ridge(alpha=a) \n    RigeModel.fit(x_train, y_train)\n    RR_square.append(RigeModel.score(x_test, y_test))\n    RR_train.append(RigeModel.score(x_train, y_train))\n    \n# visualize\nplt.figure(figsize=(8, 5))\nplt.plot(alpha,RR_square,'b' ,label='validation data')\nplt.plot(alpha,RR_train, 'r', label='training Data')\nplt.xlabel('alpha')\nplt.ylabel('R2')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE_valid = []\nRMSE_train = []\nalpha = [0.0001, 0.001, 0.1, 1, 2,5,8,10, 15,20]\nfor a in alpha:\n    RigeModel = Ridge(alpha=a) \n    RigeModel.fit(x_train, y_train)\n    RMSE_valid.append(np.sqrt(mean_squared_error(y_test, RigeModel.predict(x_test))))\n    RMSE_train.append(np.sqrt(mean_squared_error( y_train, RigeModel.predict(x_train))))    \n    \n    \n# visualize\nplt.figure(figsize=(8, 5))\nplt.plot(alpha,RMSE_valid,'b' ,label='validation data')\nplt.plot(alpha,RMSE_train, 'r', label='training Data')\nplt.xlabel('alpha')\nplt.ylabel('RMSE')\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize model prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize model prediction\ndef DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):\n    plt.figure(figsize=(10, 8))\n    ax1 = sns.distplot(RedFunction, hist=False, color=\"r\", label=RedName)\n    ax2 = sns.distplot(BlueFunction, hist=False, color=\"b\", label=BlueName, ax=ax1)\n    plt.title(Title)\n    plt.xlabel('Price (dollars)')\n    plt.show()\n    plt.close()\nDistributionPlot(y_test, pred_ridge, 'Actual Values (Train)', 'Predicted Values (Train)', \n                 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# residual scatter plot\nmatplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\npreds = pd.DataFrame({\"preds\":model_ridge.predict(x_train), \"true\":y_train})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.corrcoef(model_ridge.predict(x_train),y_train)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}