{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Installing NVIDIA Rapids (CUDF & CUML) for rapid PCA and T-SNE analysis later."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import sys\n!cp ../input/rapids/rapids.0.11.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.6/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.6\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction"},{"metadata":{},"cell_type":"markdown","source":"In this project, I am going to build a model to predict whether the hotel will be booked or not."},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Plotting for association"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"../input/hotel-booking-demand/hotel_bookings.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that some features have \"119390\" counts while other features such as \"agent\" or \"company\" have less counts, which indicates these features have null values."},{"metadata":{},"cell_type":"markdown","source":"Let's see how many hotels have been checked out based on a histogram."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(x='reservation_status',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the distribution plot for two typees of stays (weekend and weekdays nights) just to see how they are distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nsns.distplot(df['stays_in_weekend_nights'],label=\"Weekend nights stays\",axlabel=False, kde=False)\nsns.distplot(df['stays_in_week_nights'],label=\"Week nights stays\",axlabel=False, kde=False)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's change the bin sizes and axis limits to show the plot better."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nsns.distplot(df['stays_in_weekend_nights'],label=\"Weekend nights stays\",axlabel=False, kde=False,bins=50)\nsns.distplot(df['stays_in_week_nights'],label=\"Week nights stays\",axlabel=False, kde=False,bins=150)\nplt.xlim(0,12)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create box plots for each stay category (weekdays and weekends) showing the relationship between # of stay nights and reservation status"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='reservation_status',y='stays_in_weekend_nights',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='reservation_status',y='stays_in_week_nights',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not a super clear relationship.."},{"metadata":{},"cell_type":"markdown","source":"What about the relationship with \"lead time\"?"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='reservation_status',y='lead_time',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I can see that the canceled reservation had longer lead time than the checked out reservation."},{"metadata":{},"cell_type":"markdown","source":"Will there be any month effect? Let's see."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(x='arrival_date_month',data=df,hue='reservation_status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ah, interesting. It seems people canel less during the winter season (November - January). It could be due to the seasonal effect where (1) people generally travel less (so less cancelling) and (2) hotel prices go up so people do not want to pay additional fees when cancelling."},{"metadata":{},"cell_type":"markdown","source":"We can check the relationship between the number of guests and reservation status. For this, let's create a column that sums up the number of adults, children and babies in our dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['total_guests']=df['adults']+df['children']+df['babies']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(x='total_guests',data=df,hue='reservation_status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No linear correlation was found between the total # of guests and reservation status, but it seems like there was a very large number of guests that had booked a room, which makes sense because the \"df.describe\" table above shows that the max of adults was 55."},{"metadata":{},"cell_type":"markdown","source":"Let's look at this side by side for each category of guests (adults, children and babies)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(nrows=1, ncols=3, figsize=(20,10))\ng=sns.countplot(x='adults',data=df,hue='reservation_status',ax=ax[0])\ng.legend_.remove()\ng=sns.countplot(x='children',data=df,hue='reservation_status',ax=ax[1])\ng.legend_.remove()\ng=sns.countplot(x='babies',data=df,hue='reservation_status',ax=ax[2])\ng.legend(loc='right', bbox_to_anchor=(1.4, 0.5), ncol=1)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Cramer's V for categorical features"},{"metadata":{},"cell_type":"markdown","source":"Aside from the bruteforce way of checking each attribute looking at the plots as shown below, is there a mathematical way to select the parameters that are strongly correlated with each other? The issue here is that reservation_status are linked with some categorical attributes so we cannot simply use \"df.corr()\" to accurately get the correlation matrix."},{"metadata":{},"cell_type":"markdown","source":"There is Cramer's V model based on the chi squared satistic that can show how strongly nominal variables are associated with one another. This is very similar to correlation coefficient where 0 means no linear correlation and 1 means strong linear correlation.\n\nI refered two posts on this : (1) https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9 (2) https://sonj.me/projects/2018/09/05/poisonous-mushroom-classification.html"},{"metadata":{},"cell_type":"markdown","source":"I will first create a new dataframe that contains only categorical attributes. Up to this point, I used \"reservation_status\" to figure out whether the room has been booked or not. But actually I can use \"is_cancelled\" attribute which simply shows 0 for not canceled and 1 for canceled.\n\nLet's drop \"reservation_status\" column from the original data frame, turn these 0 and 1 to categorical values (no and yes), and run Cramer's V analysis again."},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.drop('reservation_status',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['is_canceled']=df['is_canceled'].replace([0,1],[\"no\",\"yes\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extracting categorical feature columns\ncols = df.columns\nnum_cols = df._get_numeric_data().columns\ncat_cols=list(set(cols) - set(num_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cat=df[cat_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cat.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating functions to run Cramer's V analysis for categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\nimport scipy\n\ndef cramers_corrected_stat(confusion_matrix):\n    \"\"\"\n    Calculates the corrected Cramer's V statistic\n    \n    Args:\n        confusion_matrix: The confusion matrix of the variables to calculate the statistic on\n    \n    Returns:\n        The corrected Cramer's V statistic\n    \"\"\"\n    \n    #chi2, _, _, _ = scipy.stats.chi2_contingency(confusion_matrix)\n    chi2 = scipy.stats.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r, k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n    rcorr = r - ((r-1)**2)/(n-1)\n    kcorr = k - ((k-1)**2)/(n-1)\n    \n    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n\n#Getting the list of columns in the dataframe\ncols = list(df_cat.columns.values)\n\n#Creating an empty array to append to as we will go through iteratations to calculate correlations of every combination of variables.\nemp_arr = np.zeros((len(cols),len(cols)))\n\n#Iteraiting dataframe using itertools\n#itertools.combinations() : Given an array of size n, generate and print all possible combinations of r elements in array.\nfor col1, col2 in itertools.combinations(cols, 2):\n    A, B = df_cat[col1], df_cat[col2]\n    idx1, idx2 = cols.index(col1), cols.index(col2)\n    conf_mat = pd.crosstab(A,B) \n    #appending results to emp_array\n    emp_arr[idx1, idx2] = cramers_corrected_stat(conf_mat.values)\n    emp_arr[idx2, idx1] = emp_arr[idx1, idx2]\n\n#creating a correlation matrix\ncorr = pd.DataFrame(emp_arr, index=cols, columns=cols)\n\n# Mask to get lower triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\ncmap = sns.cubehelix_palette(light=1, as_cmap=True)\n\n# Draw the heatmap with the mask \nfig = plt.figure(figsize=(12, 6))\n\nsns.heatmap(corr, mask=mask, cmap=cmap, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, let's take a look at \"deposit_type\" attribute as it showed the highest correlation with the target variable. The reservation_status_date effect was already looked at in the previous section where we saw an intersting trend that people cancel less during the winter time."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(x='deposit_type',data=df,hue='is_canceled')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ah, we can see that with non-refundable deposits guests tend to cancel less than they would do with the no deposit cases."},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Correlation for numerical features"},{"metadata":{},"cell_type":"markdown","source":"Let's quickly glance at numerical features's correlations. Before that we need to re-convert \"is_canceled\" attribute to numerical values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['is_canceled']=df['is_canceled'].replace([\"no\",\"yes\"],[0,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(df.corr(),annot=True,cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The leadtime attribute had the strongest linear correlation with cancellation."},{"metadata":{},"cell_type":"markdown","source":"Let's look at the correlation values in a bar graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()['is_canceled'][:-1].sort_values().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can more clearly see that both lead_time and total_of_special_requests had the strongest linear correlations with is_canceled target variable."},{"metadata":{},"cell_type":"markdown","source":"# 3. Dimensionality reduction : PCA & TSNE"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Categorical features only"},{"metadata":{},"cell_type":"markdown","source":"Now I want to run PCA analysis on categorical features to see if we can really reduce our dataset dimensionality. For PCA to run effectively, let's convert categorical features to numerical ones using Scikit-learn. This requires running integer encoding first follwed by OneHotEncoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate features and predicted value\nX_cat = df_cat.drop(\"is_canceled\", axis=1)\ny_cat = df_cat[\"is_canceled\"].eq('yes').mul(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quickly looking at X_cat.."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cat.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We saw during our data exploration that the \"country\" feature had some missing values. Let's take care of this before moving along."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cat['country'].fillna(\"No Country\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Running labelencoder and onehotencoder to convert to numerical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\ndef labelencode(df):\n    le = LabelEncoder()\n    return df.apply(le.fit_transform)\n\ndef onehotencode(df):\n    onehot = OneHotEncoder()\n    return onehot.fit_transform(df).toarray()\n\nX_2 = labelencode(X_cat)\nonehotlabels = onehotencode(X_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at X_2 to see how the labels have been converted to numerical digits instead."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_2.head().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's first choose 3 principal components to try PCA."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=3)\n\n# Perform PCA on the one-hot encoded labels\nX_pca = pca.fit_transform(onehotlabels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can find the explained_variance_ratio, which shows the amount of information or variance each principal component holds after projecting the data to a lower dimensional subspace."},{"metadata":{"trusted":true},"cell_type":"code","source":"ex_variance=np.var(X_pca,axis=0)\nex_variance_ratio = ex_variance/np.sum(ex_variance)\nprint(ex_variance_ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means that the principal component 1 holds 44.2% of the information while the principal component 2 holds only 32.9% of the information. Summing them up, we will have ~77% of information."},{"metadata":{},"cell_type":"markdown","source":"Let's run PCA analysis when the number of principal components is equal to the number of all attributes in the categorical feature dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=len(df_cat.columns))\n\n# Perform PCA on the one-hot encoded labels\nX_pca = pca.fit_transform(onehotlabels)\n\n# Rebuild it in its original dimension\nX_pca_reconst = pca.inverse_transform(X_pca)\n\n#plotting\n\nplt.figure(figsize=(12,12))\n\nplt.scatter(X_pca[y_cat==0, 0], X_pca[y_cat==0, 1], color='red', alpha=0.5,label='Not canceled')\nplt.scatter(X_pca[y_cat==1, 0], X_pca[y_cat==1, 1], color='blue', alpha=0.5,label='Canceled')\n\nplt.title(\"PCA\")\nplt.ylabel('Principal component Y')\nplt.xlabel('Principal component X')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ideal pattern would be distinct clusters where the clusters are not overlapping with each other a lot."},{"metadata":{},"cell_type":"markdown","source":"Then, how many principal components we would have needed to represent the whole dataset? Let's plot the cumuluative explained variances as a function of principal components, when the # of principal components is set equal to the number of total categorical features we have in the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=len(df_cat.columns))\nX_pca = pca.fit_transform(onehotlabels)\n\npercentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_);\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we need about 8 components to represent 90% of the dataset."},{"metadata":{},"cell_type":"markdown","source":"There is TSNE analysis that is known to improve distinction although it is quite resource-heavy. This is why I installed CUDF and CUML at the beginning of this kernel to utilize GPU."},{"metadata":{},"cell_type":"markdown","source":"This medium post (https://towardsdatascience.com/dimensionality-reduction-toolbox-in-python-9a18995927cd) explains pretty well about TSNE (and other dimension reduction features too): T-SNE reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space."},{"metadata":{},"cell_type":"markdown","source":"To make sure we donâ€™t burden our machine in terms of memory and power/time we will only use the 20,000 samples to run the algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"import cudf, cuml\nfrom cuml.manifold import TSNE\nimport time\n\ntime_start = time.time()\n\ntsne = TSNE(n_components=2,verbose=1, learning_rate=300,perplexity = 50,early_exaggeration = 24,init = 'random',  random_state=2019)\n\nn = 20000  # for 20000 random indices\nindex = np.random.choice(onehotlabels.shape[0], n, replace=False)  \n\nX_tsne=tsne.fit_transform(onehotlabels[index])\ny_tsne=y_cat[index]\n\nprint('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12,12))\n\nplt.scatter(X_tsne[y_tsne==0, 0], X_tsne[y_tsne==0, 1], color='red', alpha=0.5,label='Not canceled')\nplt.scatter(X_tsne[y_tsne==1, 0], X_tsne[y_tsne==1, 1], color='blue', alpha=0.5,label='Canceled')\nplt.title(\"TSNE\")\nplt.ylabel('Principal component Y')\nplt.xlabel('Principal component X')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The overlapping between the two clusters is smaller in TSNE than in PCA but still not super distinct unfortunately. This could be attributed to the sample size, learning rate or perplexity we chose."},{"metadata":{},"cell_type":"markdown","source":"## 3.2 What about numerical features?"},{"metadata":{},"cell_type":"markdown","source":"Let's look at the numerical features the same way we looked at the categorical features for the PCA analysis.\n\nFirst, I will choose the numerical features only and go through standardization."},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting the numerical feature columns one more time\ncols = df.columns\nnum_cols = df._get_numeric_data().columns\n\n#selecting numerical features\ndf_num=df[num_cols].drop('is_canceled',axis=1)\n\n#selecting target ('is_canceled' column)\ny_num=y_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the missing numerical data, let's Panda's fillna feature. We will fill in with the median values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num=df_num.fillna(df_num.median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Running standardization on this dataframe using StandardScaler and replacing the orignial dataframe with the standardized one."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n# Standardizing the features\ndf_num_standard = StandardScaler().fit_transform(df_num.values)\n\n#replacing the X_num dataframe with the standardized dataframe\ndf_num[:] = df_num_standard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num.head().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's run PCA on this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=len(df_num.columns))\n\n# Perform PCA on the one-hot encoded labels\ndf_pca_num = pca.fit_transform(df_num)\n\n#plotting\n\nplt.figure(figsize=(12,12))\n\nplt.scatter(df_pca_num[y_num==0, 0], df_pca_num[y_num==0, 1], color='red', alpha=0.5,label='Not canceled')\nplt.scatter(df_pca_num[y_num==1, 0], df_pca_num[y_num==1, 1], color='blue', alpha=0.5,label='Canceled')\n\nplt.title(\"PCA\")\nplt.ylabel('Principal component Y')\nplt.xlabel('Principal component X')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Still the two clusters are overlapping with each other a lot."},{"metadata":{},"cell_type":"markdown","source":"What about TSNE analysis on the numerical features?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import cudf, cuml\nfrom cuml.manifold import TSNE\nimport time\n\ntime_start = time.time()\n\ntsne = TSNE(n_components=2,verbose=1, learning_rate=300,perplexity = 50,early_exaggeration = 24,init = 'random',  random_state=2019)\n\nn = 20000  # for 2 random indices\nindex = np.random.choice(df_num_standard.shape[0], n, replace=False)  \n\nX_tsne=tsne.fit_transform(df_num_standard[index])\ny_tsne=y_num[index]\n\nprint('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\n\nplt.scatter(X_tsne[y_tsne==0, 0], X_tsne[y_tsne==0, 1], color='red', alpha=0.5,label='Not canceled')\nplt.scatter(X_tsne[y_tsne==1, 0], X_tsne[y_tsne==1, 1], color='blue', alpha=0.5,label='Canceled')\nplt.title(\"TSNE\")\nplt.ylabel('Principal component Y')\nplt.xlabel('Principal component X')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, the two are overlapping with each other a lot so I am not entirely sure if our classification will work accurately. Let's give it a shot with our ANN models anyway for my practice :)"},{"metadata":{},"cell_type":"markdown","source":"# 4. Running ANN classification algorithm"},{"metadata":{},"cell_type":"markdown","source":"Despite of not so ideal results we got from PCA and TSNE analysis on both categorical and numerical features, let's try to predict cancellation using the ANN claffisication algorithm for practice :)"},{"metadata":{},"cell_type":"markdown","source":"Let's first concatanate our numerically converted categorical features (those that went through onehotencoder) and standardized numerical features toegether. Since we will need arrays for our ANN, we will concatanate the arrays, not the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"#concatenating numerically converted categorical and numerical feature arrays\nX_arr=np.concatenate((onehotlabels, df_num_standard), axis=1)\ny_arr = df['is_canceled'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the data into train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_arr,y_arr,test_size=0.25,random_state=2019)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Model creation : layer setup + compiling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the train feature shape to see if the splitting was done as intended"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create layers to run our ANN! I am going to use \"relu\" as my activation function, \"binary_crossentropy\" as a loss function and \"adam\" as an optimizer during compiling."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\n#adding dropout layers for improved learning\nmodel.add(Dense(units=30,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=20,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=10,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=1,activation='sigmoid'))\n\n# For a binary classification problem\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Putting early_stop in to prevent overfitting\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n\nmodel.fit(x=X_train, \n          y=y_train, \n          epochs=100,\n          validation_data=(X_test, y_test), verbose=1,callbacks=[early_stop]\n          )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot our model history (accuracy and loss) to see how our epochs worked."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_loss = pd.DataFrame(model.history.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_loss.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright. Looks like we really did not overfit based on the trend of the validation loss (didn't swing back up)"},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Make predictions & Model evaluation"},{"metadata":{},"cell_type":"markdown","source":"Creating predctions."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict_classes(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the confusion matrix to evalute our classificatio work."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our f1-score is not bad at all :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this confusion matrix on test set, we can see that both false negative and false positive cases are fairly small compared to the size of the whole test dataset."},{"metadata":{},"cell_type":"markdown","source":"As the additional means of verification, let's look at the very first prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that it expected \"0\" which means no cancellation."},{"metadata":{},"cell_type":"markdown","source":"Examining the test label shows that this classification is correct:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What about another prediction? Like.. 12345th prediction? Let's compare the predicted value and actual test value."},{"metadata":{"trusted":true},"cell_type":"code","source":"if predictions[12345] == y_test[12345]:\n    print(\"Prediction and test value match\")\nelse:\n    print(\"Prediction and test value do NOT match\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice! Even though the clustering did not look ideal in our PCA and TSNE analysis, deep learning was able to figure it out!\nWe can use this trained model to predict whether a hotel room will be canceled or not given various input features :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}