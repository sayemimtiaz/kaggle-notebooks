{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ESRB Rating Classification"},{"metadata":{},"cell_type":"markdown","source":"Coded by Luna McBride"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt #Plotting\n%matplotlib inline\n\nplt.rcParams[\"figure.figsize\"] = (10,10) #Make the plots bigger by default\nplt.rcParams[\"lines.linewidth\"] = 2 #Setting the default line width\nplt.style.use(\"ggplot\") #Define the style of the plot\n\nfrom sklearn.model_selection import train_test_split #Split the data into train and test\nfrom sklearn.ensemble import RandomForestClassifier #Forest for prediction and regression\nfrom sklearn.metrics import mean_squared_error #Error testing\nfrom sklearn.metrics import classification_report #Report of Classification\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"esrbTrain = pd.read_csv(\"../input/video-games-rating-by-esrb/Video_games_esrb_rating.csv\") #Load the training data\nesrbTest = pd.read_csv(\"../input/video-games-rating-by-esrb/test_esrb.csv\") #Load the testing data\n\nesrbTrain.head() #Take a peek at the training data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Check for Null Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(esrbTrain.isnull().any()) #Check for null values in the training set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(esrbTest.isnull().any()) #Check for null values in the testing set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values in either dataset."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"colTrain = esrbTrain.columns #Load the training set's columns\nplt.rcParams[\"figure.figsize\"] = (10,4) #Change the plot size\nplt.rcParams.update({'figure.max_open_warning': 0}) #Stop the warning from appearing, since there are 20+ columns\n\n#For each column, plot the values for a quick visualization\nfor col in colTrain:\n    #Ignore the title column\n    if col != \"title\":\n        plt.figure() #Reload the figure to have items on separate plots\n        esrbTrain[col].value_counts().plot.bar(title = col + \" train\") #Plot the columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset is heavily weighted toward more age appropriate descriptors (or rather no violence, no blood, etc). This could make the model have more difficulty in making predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"colTest = esrbTest.columns #Load the testing set's columns\n\n#For each column, plot the values for a quick visualization\nfor col in colTest:\n    #Ignore the title column\n    if col != \"title\":\n        plt.figure() #Reload the figure to have items on separate plots\n        esrbTest[col].value_counts().plot.bar(title = col + \" test\") #Plot the columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test set looks to have similar proportions, to the training set, but there is a heavy skew toward blood and violence. This may give some difficulty to a classification algorithm. Everything else seems like it will be fine."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Prepare the Data for Classification"},{"metadata":{},"cell_type":"markdown","source":"## Create a Validation Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings = esrbTrain[\"esrb_rating\"].copy() #Get the ratings into their own variable\nrates = pd.get_dummies(ratings) #Encode the ESRB ratings\nprint(rates[:5]) #Show how the data is encoded\n\n#Drop ratings and titles from the characteristics. We are testing on rating and the title only adds processing time and dummies\ncharacteristics = esrbTrain.drop(columns = {\"title\", \"esrb_rating\"}).copy()\ntrainChara, valChara, trainRating, valRating = train_test_split(characteristics, rates, test_size = 0.25) #Split out a validation set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set Up the Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"testRate = esrbTest[\"esrb_rating\"].copy() #Get the ratings\ntestTitles = esrbTest[\"title\"] #Extract the titles for later\ntestChara = esrbTest.drop(columns = {\"title\", \"esrb_rating\"}).copy() #Drop the title from the test set for consistency\ntestChara.head() #Take a peek at the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Build the Model"},{"metadata":{},"cell_type":"markdown","source":"Build and test based on the validation set. The test data will be used for classification and not verification in this case."},{"metadata":{"trusted":true},"cell_type":"code","source":"#GetChara: Get the most important characteristics to the classification\n#Input: the characteristics, the model, what the dataset pertains to\n#Output: None\ndef getChara(characteristics, forest, subject):\n    attributes = characteristics.columns #Get the tested attributes\n    attributes = list(zip(attributes, forest.feature_importances_)) #Zip the attributes together with their coefficient\n    sortAtt = sorted(attributes, key = lambda x: x[1], reverse = True) #Sort the zipped attributes by their coefficients\n\n    print(\"According to the Random Forest, the most important factors for {} are: \".format(subject)) #Start printing the most important labels\n    i = 0 #Counter variable so only the top five are printed\n\n    #For each attribute in the sorted attributes\n    for label, coef in sortAtt:\n        if i < 7: #If there has not been five printed yet\n            print(label) #Print the label as an important factor\n        i += 1 #Increase i by 1\n        \n#ReportClassification: analyze accuracy based on the validation set\n#Input: The model, the validation metrics, and the labels\n#Output: Classification metrics\ndef reportClassification(forest, charaVal, rateVal, labels):\n    predict = forest.predict(charaVal) #Create predictions off the validation set\n    accuracy = forest.score(charaVal, rateVal) #Get the accuracy of the validation\n    error = np.sqrt(mean_squared_error(rateVal, predict)) #Get the root mean square error\n    report = classification_report(rateVal, predict, target_names = labels) #Get the classification report\n    \n    return accuracy, error, report #Return the classification metrics\n        \n#BuildModel: Build a random forest model based on the split values\n#Input: The train_test_split training results\n#Output: the trained model\ndef buildModel(charaTrain, rateTrain):\n    forest = RandomForestClassifier(n_estimators = 100) #Initialize the forest model\n    forest.fit(charaTrain, rateTrain) #Train the model\n    return forest #Return the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = buildModel(trainChara, trainRating) #Build the forest model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ratings.unique() #Get the ESRB ratings\n\nacc, err, rep = reportClassification(model, valChara, valRating, labels) #Get the classification reports\nprint(\"Accuracy: {}\".format(acc)) #Print the accuracy\nprint(\"Root Mean Square Error: {}\".format(err)) #Print the error\nprint(\"Classification Report\\n: {}\".format(rep)) #Print the classification report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I tested the validation with various split sizes. The accuracy was highest with a lower validation split, but higher splits lead to a higher recall. The recall is more important in a case like this, so I will stick with a 25% validation split. \n\nE and M are the best accounted for despite their smaller sizes, meaning they are likely more clearly defined when it comes to characteristics to make a game E or M. E10+ and T must has more nuance and human error, hence the lower rates despite their high counts."},{"metadata":{"trusted":true},"cell_type":"code","source":"getChara(characteristics, model, \"ESRB Rating\") #Get the most important characteristics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears violence, language, blood, and themes are the most important attributes in classifying ESRB Ratings. No descriptors means that a game did not get any of the other descriptors, meaning it has no problem content. This likely applies to giving a more family friendly rating like E, hence its importance. "},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Test the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = model.predict(testChara) #Create predictions off the test set\nrate = [] #Create a list to hold converted predictions\n\n#For each prediction, translate it into ESRB Ratings\nfor pre in predict:\n    #Try to get a value from the predictions\n    try:\n        #Take the predictions and multiply the labels to make unused ones null. Then filter out the new nulls to leave\n        #    a single rating. Then convert that into a list to be able to extract the values. Finally, get the only value.\n        rate.append(list(filter(None, pre * labels))[0])\n        \n    #Add the most common item if the model did not predict anything\n    except:\n        rate.append(\"T\") #Give the most common value\n        \ncorr = (rate == testRate) #Compare the ratings to see where it was correct\ncomp = {\"Title\" : testTitles, \"Predicted\" : rate, \"Actual\" : testRate, \"Correct\": corr} #Build a dictionary for the ratings and correctness lists\n    \ntest = pd.DataFrame(comp) #Put the dictionary into a pandas dataframe. This could be converted into a further CSV from here. \ntest.head() #Take a peek at the dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test[\"Correct\"].value_counts() / len(test)) #Check the amount correct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.loc[test[\"Title\"] == \"Yooka-Laylee\"]) #Print Yooka-Laylee as an example of how this could be searchable in a program","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I had gone into this assuming the test set did not have the ratings, so the validation step was not inherently necessary. I would have just seen what it predicted without confirmation if it actually did not have the ratings. The way I did it still makes a visual representation of what it got right and wrong, which is interesting in itself. The main issues I am seeing are ones that were classified as one off of the actual rating, which could be the ESRB's humans causing some variance.\n\nI have let the full dataset load below so you can go through it (if desired). I saved it for the end as to not bog down the rest of the content."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None) #Disable limits\ntest #Load the full dataset","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}