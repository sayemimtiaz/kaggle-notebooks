{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Figuring Out Which Customers May Leave - Churn Analysis\n\n### A Notebook By Aneesh Chopra","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dataset Context \n\nEach row represents a customer, each column contains customer’s attributes described on the column Metadata.\n\nThe data set includes information about:\n\n* Customers who left within the last month – the column is called Churn\n* Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n* Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n* Demographic info about customers – gender, age range, and if they have partners and dependents","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Features/Column/Attributes Explanation\n\n* **customerID **- Customer ID\n* **gender** - Whether the customer is a male or a female\n* **SeniorCitizen** -Whether the customer is a senior citizen or not (1, 0)\n* **Partner** -Whether the customer has a partner or not (Yes, No)\n* **Dependents** -Whether the customer has dependents or not (Yes, No)\n* **tenure** -Number of months the customer has stayed with the company\n* **PhoneService** -Whether the customer has a phone service or not (Yes, No)\n* **MultipleLines** -Whether the customer has multiple lines or not (Yes, No, No phone service)\n* **InternetService** -Customer’s internet service provider (DSL, Fiber optic, No)\n* **OnlineSecurity** -Whether the customer has online security or not (Yes, No, No internet service)\n* **OnlineBackup** -Whether the customer has online backup or not (Yes, No, No internet service)\n* **DeviceProtection** -Whether the customer has device protection or not (Yes, No, No internet service)\n* **TechSupport** -Whether the customer has tech support or not (Yes, No, No internet service)\n* **StreamingTV** -Whether the customer has streaming TV or not (Yes, No, No internet service)\n* **StreamingMovies** -Whether the customer has streaming movies or not (Yes, No, No internet service)\n* **Contract** -The contract term of the customer (Month-to-month, One year, Two year)\n* **PaperlessBilling** -Whether the customer has paperless billing or not (Yes, No)\n* **PaymentMethod** -The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n* **MonthlyCharges** -The amount charged to the customer monthly\n* **TotalCharges** -The total amount charged to the customer\n* **Churn** -Whether the customer churned or not (Yes or No)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as  sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading in the Dataset","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"org=pd.read_csv(\"/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\norg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning Process\n\n- First, we will make a copy of the dataset and work on that, so we make sure that we have the original data set to fallback to incase of any mistakes\n- From the first glimpse at the data, the ID column seems redundant as we already have index to provide us unique identifiers for each customer and it gives no additional info \n- For Standardization purposes you can divide every cleaning operation into **Define** , **Code** and **Test** Sections\n\nLet me show you an example**** ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Cleaning Operation 1\n\n### Define\n- Removing the CustomerID column since it is redundant\n\n### Code","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#making a copy of the original dataset\ndf=org.copy()\n\n#Dropping the customerID column \ndf.drop('customerID', axis=1, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test\n\n- Always make sure to test your operations have worked after every step of cleaning \n- you can make use of **assert** statements for testing\n- if your **assert** statements are false, they will throw an error\n- if it's true, rest of the code present in the cell will be executed without a problem\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"assert 'customerID' not in df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- See, No error is thrown, which means customerID is no longer present in our dataset and we have succesfully completed our first cleaning operation\n- Let's continue toying with the data to see what other cleaning operations we need to perform on our data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- **describe()** and **info()** are some of the most basic and useful methods of assessing our data and find faults in them such as missing values, incorrect datatypes, outliers , etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - There seems to be no missing data as of now, let's continue","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There are supposed to be 4 numeric columns according to the data context given to us\n- **TotalCharges** seems to be missing, lets try to understand why","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Cleaning Operation 2\n\n### Define\n\n- Converting **TotalCharges** Column into numeric \n\n### Code","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting all possible values in the TotalCharges Column into numeric and converting rest of them into missing values\ndf['TotalCharges']=df['TotalCharges'].apply(pd.to_numeric, errors='coerce')\n\n#Checking if there are any null values now\ndf.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- there seem to be 11 missing values now, we will take care of them in the next cleaning operation\n- all the missing values have tenure=0, which means this is the customer's first month. In this case, we can substitute TotalCharges missing values with MonthlyCharge values itself\n\n### Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"assert df['TotalCharges'].dtype=='float64'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Column has been succesfully converted into a numerical column\n\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- As we can see, there is an average of about **3%**  and a median of **1.995%** difference between the predicted and actual values\n- Thus, these predicted values seem like a very sensible amount to impute our missing values with ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Cleaning Operation 3\n\n### Define\n\n- Impute missing values with MonthlyCharges\n\n### Code","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[(pd.isnull(df.TotalCharges)), 'TotalCharges'] = df.MonthlyCharges","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"assert len(df.loc[(pd.isnull(df.TotalCharges))])==0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - we have successfully imputed the values\n \n - now lets make sure all categorical columns only have the values among the given range","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracting all categorical columns\ncategorical_columns=df.select_dtypes(include='object').columns.tolist()\nprint(categorical_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Made a list of number of unique values allowed in each column according to the details provided to us\nx=[2,2,2,2,3,3,3,3,3,3,3,3,3,2,4,2]\ny=[]\n\n#Number of unique elements present in each column\nfor i in categorical_columns:\n    y.append(len(df[i].unique()))\n\n#Comparing Values\nfor i,j in zip(x,y):\n    print(i,j)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- All Columns Seem to have values present in the given range\n- Looks like no more cleaning needs to be done, lets move onto exploration","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summarize our dataset \nprint (\"Rows     : \" ,df.shape[0])\nprint (\"Columns  : \" ,df.shape[1])\nprint (\"\\nFeatures : \\n\" ,df.columns.tolist())\nprint (\"\\nMissing values :  \", df.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",df.nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1-D Exploration\n- Lets get the Class Label(Customer Churn) Breakdown of the dataset given to us","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Churn'].value_counts()/df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = df['Churn'].value_counts(sort = True).index\nsizes = df['Churn'].value_counts(sort = True)\n\ncolors = [\"green\",\"red\"]\nexplode = (0.05,0)  # explode 1st slice\n \nplt.figure(figsize=(7,7))\n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90,)\n\nplt.title('Customer Churn Breakdown')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Side Note : Try to avoid pie charts if possible. It’s a common visualization joke that Pie Charts are the worst. \n\n- Only use them when there are 2-4 different unique values. More the variables, the harder it will be for us to understand what the pie chart is trying to convey\n- Pie charts can be used to manipulate or hide facts as well. For eg, We get to know the distribution of the the Customer Churning thorugh the pie chart but we don't get to know how believable this is since we are unable to figure out the sample size taken for this chart\n- This wouldnt be the case in bar plots since we would have the value counts on the x-axis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(20, 5))\nsns.distplot( df[\"tenure\"] , color=\"skyblue\", ax=axes[0])\nsns.distplot(df['MonthlyCharges'],color='orange',ax=axes[1])\nsns.distplot(df['TotalCharges'],color='green',ax=axes[2])\nfig.suptitle('Histogram of Numerical Columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Obervations \n\n* There seems to be a somewhat uniform distribution of tenure of customers except for two peaks at the extremes which suggest that there are atleast 2 customer segments: \n 1. Loyalists: Which have remained with the given telco for >70 months \n 2. Newcomers: Which have started using the given telco's services\n\n\n* Unable to make any clear observations from the MonthlyCharges Histogram but there seems to be a base plan of $20 being offered by the telco which many customers seem to be using \n\n*  The TotalCharges ditribution sort of mimics the long-tail distribution where as the TotalCharge increases, the number of customer goes on further decreasing ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(20, 20))\nfig1=sns.countplot( df[\"gender\"] ,ax=axes[0,0])\nfig2=sns.countplot( df[\"SeniorCitizen\"] , ax=axes[0,1])\nfig3=sns.countplot( df[\"Contract\"] , ax=axes[2,0])\nfig4=sns.countplot( df[\"PaymentMethod\"] , ax=axes[2,1])\nfig5=sns.countplot( df[\"Partner\"] , ax=axes[1,0])\nfig6=sns.countplot( df[\"Dependents\"] , ax=axes[1,1])\n\nfigures=[fig1,fig2,fig3,fig4,fig5,fig6]\n\nfor graph in figures:\n    graph.set_xticklabels(graph.get_xticklabels(),rotation=90)\n    \n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()/2., height,height ,ha=\"center\")\n\n\nfig.suptitle('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n\n- I only plotted categorical features which seem important to me at the moment\n- Gender and Partner features seem to be evenly distributed among the dataset with each unique value being equally represented \n- There are less number of customers who have dependents as well as less number of customers who are senior citizens\n- A huge majority of customers are tied to the telco services on a month to month basis, which gives them alot of flexibility to move around to try out other competitors\n- Alot of customers also prefer electonic check when it comes to payment method. Maybe due to the ease of the process, no other inference can be made from this solely","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2-D Plots","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### A) Numerical Columns vs Churn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(12, 7))\ng = sns.violinplot(x=\"Churn\", y = \"MonthlyCharges\",data = df, palette = \"Pastel1\",ax=axes[0])\ng = sns.violinplot(x=\"Churn\", y = \"tenure\",data = df, palette = \"Pastel1\",ax=axes[1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n\n- Many people who have chosen the base plan of $20 seem to be sticking to the telco's services wheres as the Monthly Charges go on increasing a huge number of customers seem to have left the services as seen in the ranges 60-120\n\n- There is no pattern observable among customers who have stayed when it comes to tenure but new-comer Customers seem to take up portion of the customers who have churned\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### B) Categorical Columns vs Churn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(20, 20))\nfig1=sns.countplot( x=df[\"Churn\"],hue=df[\"gender\"] ,ax=axes[0,0])\nfig2=sns.countplot( x=df[\"Churn\"],hue=df[\"SeniorCitizen\"] , ax=axes[0,1])\nfig3=sns.countplot( x=df[\"Churn\"],hue=df[\"Contract\"] , ax=axes[2,0])\nfig4=sns.countplot( x=df[\"Churn\"],hue=df[\"PaymentMethod\"] , ax=axes[2,1])\nfig5=sns.countplot( x=df[\"Churn\"],hue=df[\"Partner\"] , ax=axes[1,0])\nfig6=sns.countplot( x=df[\"Churn\"],hue=df[\"Dependents\"] , ax=axes[1,1])\n\nfigures=[fig1,fig2,fig3,fig4,fig5,fig6]\n\nfor graph in figures:\n    graph.set_xticklabels(graph.get_xticklabels(),rotation=90)\n    \n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()/2., height,round((height/7043)*100,2) ,ha=\"center\")\n\n\nfig.suptitle('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n- The gender doesn't seem to play a role in Customer Churn as the distribution remains the same in both cases\n- We see a shift in distribution in the partners category among Cutomers who left and who stayed.\n1. Among Customers who stayed ,There are slighlty more Customers who have partners (38.8%) than those who don't (34.66%)\n2. When it comes to customers who left, Customers who don't have partners (17.04%) are almost twice as much as those who do (9.5%)\n- We already know that majority of customers have a month-to-month contract but as we can see,there is a huge difference in the ratios of Churned Customers where Month-to-Month Contract Customers take up a huge chunk\n- There is a similar case when it comes to PaymentMethods, where Electronic Check replaces Month-to-Month Contracts\n- Using these barplots and annotations, we are able to see the absolute percentage each bar represents in the whole dataset, but when it comes knowing the relative percentages or probabilities each variable has when related to Customer Churning, we will have to use a better alternative i.e in this case, a cross-tabulation\n- Cross-tab will in a sense, help us in calculating probabilities such as **P(Churn = 'Yes'|Contract='Month-to-Month')** which play a key role in Naive Bayes Classifier as well as understanding the impact a variable has on the outcome","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#First Create a dataset which only has categorical columns for the cross-tab\n\n#Method 1: Dropping All Numerical Columns or adding All Categorical Columns MANUALLY\ndf_cat=df.drop(['MonthlyCharges', 'TotalCharges', 'tenure'], axis=1)\nprint(df_cat.shape)\n\n#Method 2: Create a Method to automatically parse through all columns and recognise categorical columns\ncat_cols = df.nunique()[df.nunique() < 6].keys().tolist()\ncat_cols = [x for x in cat_cols]\ndf_cat=df[cat_cols]\nprint(df_cat.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = pd.concat([pd.crosstab(df_cat[x], df_cat.Churn) for x in df_cat.columns[:-1]], keys=df_cat.columns[:-1])\nsummary['Churn_Percentage'] = summary['Yes'] / (summary['No'] + summary['Yes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets check cases where more than 1/3rd of the customers have left\nsummary[summary['Churn_Percentage']>0.33]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing and Feature Engineering\n\n- In the Data Preprocessing Phase, we manipulate and transform our data into a format which can be used by our model to train on. This involves various forms of encoding and scaling of numerical values \n\n- In the Featue Engineering section we will try to build new features or modify exisiting features which in turn will help our model to perform better\n\n- Generally to do this, you require domain knowledge as well as undertanding of common Feature Engineering techniques used \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### A) New Features\n\n- we could multiply the **tenure** column values with the **MonthlyCharges** , lets compare the predicted values (tenure * MonthlyCharges) with actual **TotalCharges** in the dataset. If the values differ much, it means the Customer had his plan changed at some point. \n\n1. If **tenure * MonthlyCharges** == **TotalCharges** -> Consistent Customer: He/She is probably satisfied with the service being provided so far\n2. If **tenure * MonthlyCharges** < **TotalCharges** -> Profitable Customer: A customer would only increase his/her plan if he requires more services and/or his/her income level has increased\n3. If **tenure * MonthlyCharges** > **TotalCharges** -> Declining Customer: The Customer's income level as probably decreased or he/she is dissatified with certain services of the Telco and is trying out the competitor's services\n\n- These are just mere assumptions I have made to justify the reason behind the changes. I could very easily be wrong, but let's check out if we can find anything new about our data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a Predicted Values Column\ndf['PCharges']=df['MonthlyCharges']*df['tenure']\n#Creating a Column to calculate Absolute Percentage Difference between predicted and actual values\ndf['PDifference']=(((df['PCharges']-df['TotalCharges'])/df['TotalCharges'])*100)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(10, 5))\nsns.distplot( df[df['Churn']==\"No\"][\"PDifference\"] , color=\"green\",ax=axes[0])\nsns.distplot( df[df['Churn']==\"Yes\"][\"PDifference\"] , color=\"red\",ax=axes[1])\ndf['PDifference'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"PCharges\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\ntarget_col = [\"Churn\"]\n\n#numerical columns\nnum_cols = [x for x in df.columns if x not in cat_cols + target_col]\n\n#Binary columns with 2 values\nbin_cols = df.nunique()[df.nunique() == 2].keys().tolist()\n\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n\n#Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    df[i] = le.fit_transform(df[i])\n    \n#Duplicating columns for multi value columns\ndf = pd.get_dummies(data = df, columns = multi_cols )\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scaling Numerical columns\nstd = StandardScaler()\n\n# Scale data\nscaled = std.fit_transform(df[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\n\n#dropping original values merging scaled values for numerical columns\ndf_telcom_og = df.copy()\ndf = df.drop(columns = num_cols,axis = 1)\ndf = df.merge(scaled, left_index=True, right_index=True, how = \"left\")\n\n#churn_df.info()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# We remove the label values from our training data\nX = df.drop(['Churn'], axis=1).values\n\n# We assigned those label values to our Y dataset\ny = df['Churn'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=109)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\n\nprint(\"Gaussian Naive Bayes Classifier Results\")\n#Create a Gaussian Classifier\ngnb = GaussianNB()\n\n#Train the model using the training sets\ngnb.fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred = gnb.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nprint(metrics.accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nparameters = {'max_depth':[1, 5, 10, 50],'min_samples_split':[5, 10, 100, 500]}\ndec = DecisionTreeClassifier()\nclf = GridSearchCV(dec, parameters, cv=3, scoring='accuracy',return_train_score=True)\nclf.fit(X_train, y_train)\nresults = pd.DataFrame.from_dict(clf.cv_results_)\nresults_sort = results.sort_values(['mean_test_score'])\nresults_sort.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Decision Tree Classifier Results\")\n#Create a Gaussian Classifier\ndec = DecisionTreeClassifier(max_depth=5,min_samples_split=500)\n\n#Train the model using the training sets\ndec.fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred = dec.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nprint(metrics.accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost.sklearn import XGBClassifier\n\nparameters = {'max_depth':[1, 5, 10, 50],'min_child_weight':range(1,6,2)}\ndec = XGBClassifier()\nclf = GridSearchCV(dec, parameters, cv=3, scoring='accuracy',return_train_score=True)\nclf.fit(X_train, y_train)\nresults = pd.DataFrame.from_dict(clf.cv_results_)\nresults_sort = results.sort_values(['mean_test_score'])\nresults_sort.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"XGBoost Classifier Results\")\n#Create a Gaussian Classifier\ndec = XGBClassifier(max_depth=1,min_child_weight=3)\n\n#Train the model using the training sets\ndec.fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred = dec.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nprint(metrics.accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We have got our accuracy to peak at 0.80265 by using our XGBoost Classifier\n- Though we got only .6994 accuracy from our Naive Bayes Classifier it was better at predicting the Churned Customers since it had great recall but poor precision\n- Our main objective is to be better at predicting customers who leave/churn thus, we should try to create an ensemble of these two models in a way that we can predict the customers who leave better while mainaining high accuracy","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\n\ngnb=CalibratedClassifierCV(gnb,method='isotonic')\ngnb.fit(X_train,y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pred=[]\nfor dp in X_test:\n    dp=dp.reshape(1, -1)\n    gnb_prob=gnb.predict(dp)\n    xgb_prob=dec.predict(dp)\n    if gnb_prob[0]!=xgb_prob[0]:\n        if gnb_prob[0]==0:\n            final_pred.append(1)\n        else:\n            prob_1=gnb.predict_proba(dp)[0][1]\n            prob_0=dec.predict_proba(dp)[0][0]\n            if prob_1>=prob_0:\n                final_pred.append(1)\n            else:\n                final_pred.append(0)\n    else:\n        final_pred.append(gnb_prob[0])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=np.asarray(final_pred)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nprint(metrics.accuracy_score(y_test,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As you can see, we were only able to increase our accuracy by .001 but we were able to predict more customers who left at the cost of wrongly predicted customers who stayed\n- This happened because I created a model in such a way where higher loss was associated with not bring able to predict customers who left\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n\n- A huge majority of customers are tied to the telco services on a month to month basis, which gives them alot of flexibility to move around to try out other competitors\n\n- The TotalCharges ditribution sort of mimics the long-tail distribution where as the TotalCharge increases, the number of customer goes on further decreasing\n\n- There seems to be a somewhat uniform distribution of tenure of customers except for two peaks at the extremes which suggest that there are atleast 2 customer segments:\n 1. Loyalists: Which have remained with the given telco for >70 months\n 2. Newcomers: Which have started using the given telco's services\n\n\n- The gender doesn't seem to play a role in Customer Churn as the distribution remains the same in both cases\n\n- We see a shift in distribution in the partners category among Cutomers who left and who stayed.\n 1. Among Customers who stayed ,There are slighlty more Customers who have partners (38.8%) than those who don't (34.66%)\n 2. When it comes to customers who left, Customers who don't have partners (17.04%) are almost twice as much as those who do (9.5%)\n \n \n- We were succesfully able to built an ensemble model with 80.36% accuracy using XGBoost and Naive Bayes Classifier. For further improvement in the model, we could acquire more data, more features or make an even more complex model but we will have to take care of not overfitting our model\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}