{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CORD-19 Metadata Enrichment [1/x]: Filling in Missing Keys/Identifiers (DOI/PMCID/PMID)"},{"metadata":{},"cell_type":"markdown","source":"# Goals and Motivation\n\nMetadata serves as a critical feature in any data understanding effort. Across the tasks presented in the CORD-19 challenge, metadata has the chance to provide required context in order to best leverage the text. While the text of the various documents allows the researcher to group ideas, without accurate and precise metadata that understanding is limited. This will be a series of notebooks that seek to clean, augment, and enrich the provided metadata in order to help bolster the research efforts of all utilizing this dataset to address the tasks presented. \n\n## Other Enrichment can be found here:\n#### [CORD19-Metadata Enrich: Microsoft Academic API](https://www.kaggle.com/dannellyz/cord19-metadata-enrich-microsoft-academic-api)\n#### [CORD-19 Metadata Enrich [2/x]: Altmetric API](https://www.kaggle.com/dannellyz/cord19-metadata-enrich-altmetric-api)\n\nThe below chart shows the data which is present in the metadata:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import pandas and the read in the metadata csv to a Dataframe\nimport pandas as pd\nmetadata = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Utilizing matplotlib for display\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n# Show the percentage of each column that is present or not NULL\ncol_present_pct = metadata.notnull().sum() / len(metadata)\n\n#Bar Plot\ncol_present_pct.sort_values().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Methodology\n\nIn order to fill in any missing data we must be able to reference the works by thier various IDs. DOI is the most prevelant and so the goal will be to turn all IDs into DOIs.\n\n## [DOI](https://www.doi.org/):\nA digital object identifier (DOI) is a persistent identifier or handle used to identify objects uniquely, standardized by the International Organization for Standardization (ISO). They can be thought of as a url or latter part of a url with resolutions coming from  https://doi.org/[DOI]\n\n## [PMCID & PMID](https://publicaccess.nih.gov/include-pmcid-citations.htm#Difference):\nThe PubMed Central reference number (PMCID) is different from the PubMed reference number (PMID). PubMed Central is an index of full-text papers, while PubMed is an index of abstracts. The PMCID links to full-text papers in PubMed Central, while the PMID links to abstracts in PubMed. PMIDs have nothing to do with the NIH Public Access Policy.\n\nThe provided data had the following breakdown of DOI/PMCID/PMID identifiers for the articles."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"id_col_list = ['doi','pmcid', 'pubmed_id']\ndef null_ids_graph(df):\n    #Group by the various IDs and count their permutations\n    id_types_present = df.notnull().groupby(id_col_list).size()\n    #Plot with bar chart\n    chart = id_types_present.plot.bar()\n    for p in chart.patches:\n        chart.annotate('{:,}'. format(p.get_height()), (p.get_x() * 1.00, p.get_height() * 1.01))\n    return chart\nnull_ids_graph(metadata)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  PMCID -> DOI\n\nSince PMCID is the most prevelant identifier type we will utilize it in order to try and fill in the other items. Converting these to PMID or DOI will allow them to be used later in looking up addition information and features. \n\n## NCBI API\nThe [National Center for Biotechnology Information(NCBI)](https://www.ncbi.nlm.nih.gov/) advances science and health by providing access to biomedical and genomic information. They offer an [API](https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/) to do conversions from PMCID to other journal identifiers.\n\n## Get vs Load\nThe code below has two methods for getting the NCBI data. \n\n### get_ncbi_results\nDoes an API call to get the full list of NCBI results and saves them to a csv.\n\n### load_ncbi_results\nThis loads the csv from the CORD-19 Metadata Enrichment Kaggle public dataset. Can be loaded by clicking \"Add Data\" on the right and using the url:\nhttps://www.kaggle.com/dannellyz/cord19-metadata-enrichment"},{"metadata":{"trusted":true},"cell_type":"code","source":"#URL Lib to query API\nfrom urllib.request import urlopen\n\n#ElementTree to parse XML response\nimport xml.etree.ElementTree as ET\n\n#Import Tracker\nfrom tqdm.notebook import tqdm\n    \ndef chunks(lst, n):\n    \"\"\"\n    Yield successive n-sized chunks from lst.\n    src: https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n    \n    \"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\n\ndef ncbi_api(pmcids):\n    #Base string is the API end point\n    api_base_string = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?tool=my_tool&email=my_email@example.com&ids=\"\n    #List of PMCIDS to send to API end point\n    ids_string = \",\".join(list(pmcids))\n    #Call API with query\n    api_query = api_base_string + ids_string\n\n    #Get API response which is a list of dictionaries\n    with urlopen(api_query) as response:\n        response_content = response.read()\n    root = ET.fromstring(response_content)\n    \n    #Return\n    return [child.attrib for child in root[1:]]\n\ndef get_ncbi_results(file_name, pmcids):\n    #Batch the results as API can only take 10 at a time\n    batch_pmcids = chunks(pmcids, 10)\n    batch_results = []\n\n    #For each batch run against API\n    for batch in tqdm(batch_pmcids):\n        batch_results.extend(ncbi_api(batch))\n\n    #Collect results into a Dataframe\n    ncbi_results = pd.DataFrame(batch_results).drop(\"requested-id\", axis=1)\n\n    #Send dataframe to csv\n    ncbi_results.to_csv(file_name)\n    return ncbi_results\n\ndef load_ncbi_results(file_name):\n    ncbi_results = pd.read_csv(file_name, usecols=[\"doi\", \"pmcid\", \"pubmed_id\"])\n    return ncbi_results\n\ndef get_pmcids(metadata):\n    has_pmcid_no_doi = metadata[metadata.pmcids.notnull() & metadata.doi.isnull()]\n\n#Get the NCBI Results\nfile_name = \"ncbi_metadata.csv\"\nhas_pmcid_no_doi = metadata[metadata.pmcid.notnull() & metadata.doi.isnull()]\npmcids_list = list(has_pmcid_no_doi.pmcid)\n\n#Get from API\n#You must also enable internet in the options to the right\nncbi_results = get_ncbi_results(file_name, pmcids_list)\n\n#Load from Public Data Set\n#ncbi_results = load_ncbi_results(file_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\nAs the below dataframe and graph depict the query to NCBI has filled out an additional 10,339 results for the missing IDS."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Update metadata with new values from NCBI results\nmetadata_v2 = metadata.copy()\nmetadata_v2.update(ncbi_results)\n\n#Graph update\nid_count_v1 = metadata.notnull().groupby(id_col_list).size()\nid_count_v2 = metadata_v2.notnull().groupby(id_col_list).size()\nupdated_counts = id_count_v2 - id_count_v1\nupdated_counts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Before and After"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([pd.DataFrame(id_count_v1, columns = [\"Before\"]).T,\n           pd.DataFrame(updated_counts, columns = [\"After\"]).T]).T.plot.bar(stacked=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Updated Null IDs Graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"null_ids_graph(metadata_v2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Output for future work\nmetadata_v2.to_csv(\"metadata_v2.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Please upvote if this is useful!\n\nPlan to make a couple more notebooks continuing the enrichment of the metadata and if you have any suggestions please leave a comment."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}