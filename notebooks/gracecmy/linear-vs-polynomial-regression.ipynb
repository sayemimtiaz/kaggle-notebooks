{"cells":[{"metadata":{},"cell_type":"markdown","source":"Diamonds - the most valuable gemstone in the world. But what is the relationship between the actual stone and its price? Is it as simple as a linear regression or perhaps a non-linear regression?.."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"]=12,6","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the data, there are 53940 diamonds which can be described in 10 attributes:\n1. carat (i.e. weight)\n1. quality of the cut (5 categories in ascending order of Fair, Good, Very Good, Premium, Ideal)\n1. colour (7 categories with J being the worst to D being the best)\n1. clarity (8 categories in ascending order of I1, SI2, SI1, VS2, VS1, VVS2, VVS1, IF)\n1. depth out of 100 (i.e. how deep the diamond is relative to its width)\n1. table out of 100 (i.e. the ratio between the flat surface at the top of the diamond to its average width)\n1. price in USD\n1. length (x) in mm\n1. width (y) in mm \n1. depth (z) in mm"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/diamonds/diamonds.csv\",index_col=0)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the cut, colour and clarity are categorical variables (i.e. object), there is an order/ranking and so can therefore be transformed into numeric values."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#CUT\ndef cut_codes(cut):\n    if cut==\"Ideal\":\n        return 5\n    elif cut==\"Premium\":\n        return 4\n    elif cut==\"Very Good\":\n        return 3\n    elif cut==\"Good\":\n        return 2\n    else:\n        return 1\ndf[\"cut\"]=df[\"cut\"].apply(lambda x:cut_codes(x))\n\n#COLOUR\ndef color_codes(color):\n    if color==\"D\":\n        return 7\n    elif color==\"E\":\n        return 6\n    elif color==\"F\":\n        return 5\n    elif color==\"G\":\n        return 4\n    elif color==\"H\":\n        return 3\n    elif color==\"I\":\n        return 2\n    else:\n        return 1\ndf[\"color\"]=df[\"color\"].apply(lambda x:color_codes(x))\n\n#CLARITY\ndef clarity_codes(clarity):\n    if clarity==\"I1\":\n        return 8\n    elif clarity==\"SI2\":\n        return 7\n    elif clarity==\"SI1\":\n        return 6\n    elif clarity==\"VS2\":\n        return 5\n    elif clarity==\"VS1\":\n        return 4\n    elif clarity==\"VVS2\":\n        return 3\n    elif clarity==\"VVS1\":\n        return 2\n    else:\n        return 1\ndf[\"clarity\"]=df[\"clarity\"].apply(lambda x:clarity_codes(x))\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values but when we take a look at the data there are some 0s for the x, y and z which should not be mistaken as a measurement of 0 mm because without a length, width or depth a diamond cannot be a 3-dimensional object. Instead this 0 value should be treated as missing data. Our options to deal with missing data are to either:\n1. drop the rows; or\n1. replace with 0s with a new value such as the mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"The number of rows with a value of 0 for x are \",(df[\"x\"]==0).sum(),\".\")\nprint(\"The number of rows with a value of 0 for y are \",(df[\"y\"]==0).sum(),\".\")\nprint(\"The number of rows with a value of 0 for z are \",(df[\"z\"]==0).sum(),\".\")\nprint(\"The total number of rows with a value of 0 are \",((df[\"x\"]==0)|(df[\"y\"]==0)|(df[\"z\"]==0)).sum(),\".\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the number of rows with a value of 0 is only 20 (0.04%) out of the total 53940, we shall just drop said rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[(df[\"x\"]==0)|(df[\"y\"]==0)|(df[\"z\"]==0)].index,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But now when we take a look at our data, there are some visually obvious outliers (especially in y, z, depth and table)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig=plt.figure(figsize=(16,16))\n\nax1=plt.subplot2grid((4,3),(0,0),colspan=2,rowspan=2)\nax2=plt.subplot2grid((4,3),(0,2))\nax3=plt.subplot2grid((4,3),(1,2))\nax4=plt.subplot2grid((4,3),(2,0))\nax5=plt.subplot2grid((4,3),(2,1))\nax6=plt.subplot2grid((4,3),(2,2))\nax7=plt.subplot2grid((4,3),(3,0))\nax8=plt.subplot2grid((4,3),(3,1))\nax9=plt.subplot2grid((4,3),(3,2))\n\nsns.scatterplot(x=df[\"carat\"],y=df[\"price\"],color=\"lavender\",ax=ax1)\nsns.scatterplot(x=df[\"x\"],y=df[\"price\"],color=\"powderblue\",ax=ax2)\nsns.scatterplot(x=df[\"y\"],y=df[\"price\"],color=\"lightblue\",ax=ax3)\nsns.scatterplot(x=df[\"depth\"],y=df[\"price\"],color=\"palegreen\",ax=ax4)\nsns.scatterplot(x=df[\"table\"],y=df[\"price\"],color=\"lightgreen\",ax=ax5)\nsns.scatterplot(x=df[\"z\"],y=df[\"price\"],color=\"skyblue\",ax=ax6)\nsns.scatterplot(x=df[\"cut\"],y=df[\"price\"],color=\"lightsalmon\",ax=ax7)\nsns.scatterplot(x=df[\"color\"],y=df[\"price\"],color=\"palevioletred\",ax=ax8)\nsns.scatterplot(x=df[\"clarity\"],y=df[\"price\"],color=\"gold\",ax=ax9)\n\nplt.tight_layout(pad=1,h_pad=1,w_pad=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So let's filter out the outliers."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"outlier=df[(df[\"y\"]>30)|(df[\"z\"]>10)|(df[\"depth\"]<50)|(df[\"depth\"]>75)|(df[\"table\"]<45)|(df[\"table\"]>75)]\nprint(\"There are {num} rows of visual outliers where y >30, z >10, depth <50 and >75, and table <45 and >75.\".format(num=outlier.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.drop(outlier.index,inplace=True)\n\nprint(\"After dropping the Visual Outliers, the number of rows are now\",df.shape[0],\".\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time to explore the diamonds!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.set_style(\"ticks\")\nsns.distplot(a=df[\"price\"],kde=False,bins=100,color=\"#AE9CCD\")\nplt.xlabel(\"Price in USD\")\nplt.ylabel(\"Count\")\nplt.title(\"Price Distribution\",size=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"The highest price is USD \",df[\"price\"].max())\nprint(\"The lowest price is USD \",df[\"price\"].min())\nprint(\"The average price is USD \",round(df[\"price\"].mean()))\nprint(\"The most common price is USD \",df[\"price\"].value_counts().idxmax())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mask=np.zeros_like(df.corr(),dtype=np.bool)\nmask[np.triu_indices_from(mask)]=True\nsns.heatmap(data=df.corr(),annot=True,square=True,mask=mask,cmap=\"Pastel1\",linewidths=1,linecolor=\"white\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of the 9 attributes, the weight (i.e. carat) of a diamond has the highest correlation with the price followed by its dimensions x, y and z."},{"metadata":{},"cell_type":"markdown","source":"But just by looking at these numbers, is it safe to assume a linear relationship? What if there is a non-linear relationship?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx=df.drop([\"price\"],axis=1)\ny=df[\"price\"]\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=5)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\nrelationship=[\"Linear\",\"Squared\",\"Cubic\"]\nrmse=[]\nr2score=[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear Relationship**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"lr=LinearRegression()\nlr.fit(x_train,y_train)\ny1_predict=lr.predict(x_test)\n\nprint(\"MAE: %.2f\"%mean_absolute_error(y_test,y1_predict))\nprint(\"MSE: %.2f\"%mean_squared_error(y_test,y1_predict))\nprint(\"RMSE: %.2f\"%np.sqrt(mean_absolute_error(y_test,y1_predict)))\nprint(\"R2: %.2f\"%r2_score(y_test,y1_predict))\n\nrmse.append(np.sqrt(mean_absolute_error(y_test,y1_predict)))\nr2score.append(r2_score(y_test,y1_predict))\n\nsns.distplot(y_test,hist=True,bins=50,kde=True,color=\"lightskyblue\",label=\"Actual Values\")\nsns.distplot(y1_predict,hist=True,bins=70,kde=True,color=\"plum\",label=\"Predicted Values\")\nplt.legend()\nplt.xlabel(\"Price\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y1_predict.min()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"The R Squared Score was not bad at 0.91, but the model predicted negative prices with a min of {predmin} to the max {predmax}, while the actual price only ranged from {actualmin} to {actualmax}. The price distribution did follow the general trend but the medians were not too far off by almost 400 with the predicted at {predmed} and the actual at {actualmed}.\".format(predmin=int(y1_predict.min()),predmax=int(y1_predict.max()),actualmin=y_test.min(),actualmax=y_test.max(),predmed=int(np.median(y1_predict)),actualmed=int(np.median(y_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Squared Relationship**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"poly2=PolynomialFeatures(degree=2)\nx2_train=poly2.fit_transform(x_train)\nx2_test=poly2.transform(x_test)\n\nlr2=LinearRegression()\nlr2.fit(x2_train,y_train)\ny2_predict=lr2.predict(x2_test)\n\nprint(\"MAE: %.2f\"%mean_absolute_error(y_test,y2_predict))\nprint(\"MSE: %.2f\"%mean_squared_error(y_test,y2_predict))\nprint(\"RMSE: %.2f\"%np.sqrt(mean_absolute_error(y_test,y2_predict)))\nprint(\"R2: %.2f\"%r2_score(y_test,y2_predict))\n\nrmse.append(np.sqrt(mean_absolute_error(y_test,y2_predict)))\nr2score.append(r2_score(y_test,y2_predict))\n\nsns.distplot(y_test,hist=True,bins=50,kde=True,color=\"lightskyblue\",label=\"Actual Values\")\nsns.distplot(y2_predict,hist=True,bins=70,kde=True,color=\"Plum\",label=\"Predicted Values\")\nplt.legend()\nplt.xlabel(\"Price\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"The R Squared Score improved to 0.96 and the model better followed the price distribution but still predicted negative prices with a min of {predmin} to the max {predmax}, while the actual price only ranged from {actualmin} to {actualmax}. The median prices were much closer and only off by 10 (predicted {predmed} and actual {actualmed}).\".format(predmin=int(y2_predict.min()),predmax=int(y2_predict.max()),actualmin=y_test.min(),actualmax=y_test.max(),predmed=int(np.median(y2_predict)),actualmed=int(np.median(y_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cubic Relationship**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"poly3=PolynomialFeatures(degree=3)\nx3_train=poly3.fit_transform(x_train)\nx3_test=poly3.transform(x_test)\n\nlr3=LinearRegression()\nlr3.fit(x3_train,y_train)\ny3_predict=lr3.predict(x3_test)\n\nprint(\"MAE: %.2f\"%mean_absolute_error(y_test,y3_predict))\nprint(\"MSE: %.2f\"%mean_squared_error(y_test,y3_predict))\nprint(\"RMSE: %.2f\"%np.sqrt(mean_absolute_error(y_test,y3_predict)))\nprint(\"R2: %.2f\"%r2_score(y_test,y3_predict))\n\nrmse.append(np.sqrt(mean_absolute_error(y_test,y3_predict)))\nr2score.append(r2_score(y_test,y3_predict))\n\nfig=plt.figure(figsize=(10,6))\nax1=fig.add_axes([0,0,1,1])\nax2=fig.add_axes([0.15,0.3,0.5,0.5])\nsns.distplot(y_test,hist=True,bins=50,kde=True,color=\"lightskyblue\",label=\"Actual Values\",ax=ax1)\nsns.distplot(y3_predict,hist=True,bins=70,kde=True,color=\"plum\",label=\"Predicted Values\",ax=ax1)\nsns.distplot(y_test,hist=True,bins=50,kde=True,color=\"lightskyblue\",label=\"Actual Values\",ax=ax2)\nsns.distplot(y3_predict,hist=True,bins=1000,kde=True,color=\"plum\",label=\"Predicted Values\",ax=ax2)\nax1.legend(loc=\"upper right\")\nax1.set_xlabel(\"Price\")\nax2.set_xlabel(\"Price\")\nax2.set_xlim([-1000,25000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"The worst R Squared Score at 0.42. The model predicted well between the prices 0 to 2000, but predicted a much larger range of prices with a min of {predmin} to the max {predmax}, while the actual price only ranged from {actualmin} to {actualmax}. Although the predicted price range was way off, the median prices were not too bad with the predicted at {predmed} and the actual at {actualmed}).\".format(predmin=int(y3_predict.min()),predmax=int(y3_predict.max()),actualmin=y_test.min(),actualmax=y_test.max(),predmed=int(np.median(y3_predict)),actualmed=int(np.median(y_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So what is the relationship??**\n\nThe root mean squared error and the r-squared scores for each model will be compared to determine the model with the best performance (i.e. the smallest RMSE and largest R2)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"scores=pd.DataFrame({\"Relationship\":relationship,\"RMSE\":rmse,\"R2-Scores\":r2score})\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,ax1=plt.subplots()\n\nax1.plot(scores[\"Relationship\"],scores[\"RMSE\"],color=\"sandybrown\",marker=\"o\")\nax1.set_ylabel(\"Root Mean Square Error\",fontsize=12,color=\"sandybrown\")\nfor label in ax1.get_yticklabels():\n    label.set_color(\"sandybrown\")\n    \nax2=ax1.twinx()\nax2.plot(scores[\"Relationship\"],scores[\"R2-Scores\"],color=\"yellowgreen\",marker=\"^\")\nax2.set_ylabel(\"R Squared\",fontsize=12,color=\"yellowgreen\")\nfor label in ax2.get_yticklabels():\n    label.set_color(\"yellowgreen\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The squared regression did the best job with the highest R Squared Score of 0.96 and lowest RMSE of 21. And although all three models predicted negative prices, the squared regression model was able to predict the most accurate range of positive prices and the closest median price. \n\nThe linear regression did slightly poorer than the squared regression but still a good job, at least much much better than the cubic regression. The cubic regression did have the best RMSE but at the cost of the worse R Squared Score, which may be due to overfitting."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}