{"cells":[{"metadata":{},"cell_type":"markdown","source":"cord19q: COVID-19 Open Research Dataset (CORD-19) Analysis\n======\n\n![CORD19](https://pages.semanticscholar.org/hs-fs/hubfs/covid-image.png?width=300&name=covid-image.png)\n\n***NOTE: There is a [Report Builder Notebook](https://www.kaggle.com/davidmezzetti/cord-19-report-builder) that runs on a prebuilt model. If you just want to try this out without a full build, this is the best choice. [Task Reports](#List-of-task-reports) are available in both Notebook and Excel formats.***\n\nCOVID-19 Open Research Dataset (CORD-19) is a free resource of scholarly articles, aggregated by a coalition of leading research groups, about COVID-19 and the coronavirus family of viruses. The dataset can be found on [Semantic Scholar](https://pages.semanticscholar.org/coronavirus-research) and there is a research challenge on [Kaggle](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).\n\nThis project builds an index over the CORD-19 dataset to assist with analysis and data discovery. A series of tasks were explored to identify relevant articles and help find answers to key scientific questions on a number of COVID-19 research topics. \n\n*An example result snippet using this method is shown below for one of the task questions. For each task question, a highlights section is built with a summary of the results along with each matching article and each article's text matches.*\n\n>## Effectiveness of drugs being developed and tried to treat COVID-19 patients.\n>\n>#### Highlights<br/>\n>- Currently, there are no approved drugs to treat the infection. [Arya et al](https://doi.org/10.26434/chemrxiv.11860011.v2)<br/>\n>- Antiviral drugs: lopinavir/ritonavir and ribavirin had been tried to treat SARS disease with apparent favorable clinical response. [Wu et al](https://doi.org/10.1097/jcma.0000000000000270)<br/>\n>- Although antiviral drugs, including osehamivir and ribavirin had been applied to our patients, to date no effective antiviral to treat COVID-19 has been identified. [Xu et al](https://doi.org/10.1101/2020.03.03.20030668)<br/>\n>\n>#### Articles<br/>\n>|Date|Authors|Title|LOE & Sample|Matches|\n>|----|----|----|----|----|\n>|2020|Wang et al|[Science in the fight against the novel coronavirus disease](https://doi.org/10.1097/cm9.0000000000000777)<br/>Chin Med J (Engl)|II. Randomized Controlled Trial<br/><br/>They performed a metagenomic analysis of respiratory tract specimens obtained from five patients suffering from the pneumonia in question and identified the virus now known as 2019-nCoV as the causative agent.|As specific therapies targeting 2019-nCoV are lacking, it may be useful to repurpose drugs already licensed for marketing or clinical trials to treat COVID-19 patients in an emergency response; researchers are actively working to identify such drugs.<br/><br/> Clinical trials are also underway to validate the effectiveness of various other licensed drugs against COVID-19.|"},{"metadata":{},"cell_type":"markdown","source":"# Install from GitHub\n\n![](http://)Full source code for [cord19q](https://github.com/neuml/cord19q) is on GitHub and be installed into this notebook as follows:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"# Install cord19q project\n!pip install git+https://github.com/neuml/cord19q\n\n# Install scispacy model\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook requires Internet connectivity to be enabled. If this notebook is copied, the GitHub project could also be forked for an edited notebook to modify the Python code. Would simply just need to update the pip install command above to the new repository location."},{"metadata":{},"cell_type":"markdown","source":"# Build SQLite articles database\n\nThe raw CORD-19 data is stored across a metadata.csv file and json files with the full text. This project uses [SQLite](https://www.sqlite.org/index.html) to aggregate and store the merged content.\n\nThe ETL process transforms the csv/json files into a SQLite database. The process iterates over each row in metadata.csv, extracts the column data and ensures it is not a pure duplicate (using the sha hash). This process will also load the full text if available. \n\n## Tagging\nArticles are tagged based on keyword matches. The only tag at this time is COVID-19 and articles are tagged with this if the article text contains any of the following regular expressions. \n\n>2019[\\-\\s]?n\\s?cov, 2019 novel coronavirus, coronavirus 2019, coronavirus disease (?:20)?19, covid(?:[\\- ]19)?, n\\s?cov 2019, sars-cov-2, wuhan (?:coronavirus|pneumonia)\n\nCredit to [@ajrwhite](https://www.kaggle.com/ajrwhite) and his [notebook](https://www.kaggle.com/ajrwhite/covid-19-thematic-tagging-with-regular-expressions) in helping to develop this list.\n\n## Study Design\nAdditional metadata is parsed out of the article to derive information on the study design.\n\n### Design Type\nThe full text is analyzed to determine a design type for the backing study in the article. This logic is pessimistic and will assign the largest number (lowest level of confidence). The process builds a feature vector of counts for keywords in each design type category. It also has minimum count required to be considered a member of a category. Titles are also analyzed for design type matches.\n\nCredit to [@savannareid](https://www.kaggle.com/savannareid) for developing the keywords to use with this method. The keywords can be found in this [domain dictionary](https://docs.google.com/spreadsheets/d/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E/edit#gid=389064679). More details on deriving a study design can be found in [this discussion](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/discussion/139355). \n\n### Sample Size\nThe full text is also analyzed to determine the sample size for the backing study in the article. This logic uses Natural Lanuage Processing (NLP) to find tokens that look to represent a sample. For example, for a Systematic Review, the logic looks for the word studies and determines if it has a number associated with it. That full sentence is used as the sample to give context to the sample size.\n\n## Grammar Labels\nThe title, abstract and full-text fields are tokenized into sentences. Linguistic rules are used to label each sentence to help identify concise, data-driven statements. \n\nFor the linguistic rules process, it has two basic rules right now.\n\n1. *QUESTION*: Sentence ending in a '?' mark\n2. *FRAGMENT*: Less informative/incomplete statements. Acceptable sentences have the following structure.\n  - At least one nominal subject noun/proper noun AND\n  - At least one action/verb AND\n  - At least 5 words\n\nImportant source files to highlight\n- ETL Process -> [execute.py](https://github.com/neuml/cord19q/blob/master/src/python/cord19q/etl/execute.py)\n- Study Design rules -> [design.py](https://github.com/neuml/cord19q/blob/master/src/python/cord19q/etl/design.py)\n- Sample Size Detection rules -> [sample.py](https://github.com/neuml/cord19q/blob/master/src/python/cord19q/etl/sample.py)\n- Linguistic rules -> [grammar.py](https://github.com/neuml/cord19q/blob/master/src/python/cord19q/etl/grammar.py)\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from cord19q.etl.execute import Execute as Etl\n\n# Build SQLite database for metadata.csv and json full text files\nEtl.run(\"../input/CORD-19-research-challenge\", \"cord19q\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Upon completion, a database named articles.sqlite will be stored in the output directory under a sub-folder named cord19q."},{"metadata":{},"cell_type":"markdown","source":"# Build Embedding Index\n\nAn embeddings index is created with [FastText](https://fasttext.cc/) + [BM25](https://en.wikipedia.org/wiki/Okapi_BM25). Background on this method can be found in this [Medium article](https://towardsdatascience.com/building-a-sentence-embedding-index-with-fasttext-and-bm25-f07e7148d240) and an existing repository using this method [codequestion](https://github.com/neuml/codequestion).\n\nThe embeddings index takes each COVID-19 tagged, non-labeled (not a question/fragment) section, tokenizes the text, and builds a sentence embedding. A sentence embedding is a BM25 weighted combination of the FastText vectors for each token in the sentence. The embeddings index takes the full corpus of these embeddings and builds a [Faiss](https://github.com/facebookresearch/faiss) index to enable similarity searching. \n\nImportant source files to highlight\n\n* Indexing Process -> [index.py](https://github.com/neuml/cord19q/blob/master/src/python/cord19q/index.py)\n* Tokenizer -> [tokenizer.py](https://github.com/neuml/cord19q/blob/master/src/python/cord19q/tokenizer.py)\n* Embeddings Model -> [embeddings.py](https://github.com/neuml/cord19q/blob/master/src/python/cord19q/embeddings.py)\n* BM25 Scoring -> [scoring.py](https://github.com/neuml/cord19q/blob/master/src/python/cord19q/scoring.py)\n\nFastText vectors trained on the full CORD-19 corpus are required. A [dataset with pre-trained vectors](https://www.kaggle.com/davidmezzetti/cord19-fasttext-vectors) is included and used in this notebook. Building the vectors takes a couple of hours when locally trained and would most likely take much longer within a notebook. \n\nVectors can optionally be (re)built by running the following command with the project and articles.sqlite database installed locally:\n\n```\npython -m cord19q.vectors\n```\n\nThe following code builds the embeddings index using fastText vectors trained on the full CORD-19 dataset. Alternatively, any [pymagnitude vector file](https://github.com/plasticityai/magnitude#pre-converted-magnitude-formats-of-popular-embeddings-models) can be used the build the sentence embeddings."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import shutil\n\nfrom cord19q.index import Index\n\n# Copy vectors locally for predictable performance\nshutil.copy(\"../input/cord19-fasttext-vectors/cord19-300d.magnitude\", \"/tmp\")\n\n# Build the embeddings index\nIndex.run(\"cord19q\", \"/tmp/cord19-300d.magnitude\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from cord19q.highlights import Highlights\nfrom cord19q.tokenizer import Tokenizer\n\nfrom wordcloud import WordCloud\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.graph_objects as go\nimport pycountry\n\n# Tokenizes text and removes stopwords\ndef tokenize(text, case_sensitive=False):\n    # Get list of accepted tokens\n    tokens = [token for token in Tokenizer.tokenize(text) if token not in Highlights.STOP_WORDS]\n    \n    if case_sensitive:\n        # Filter original tokens to preserve token casing\n        return [token for token in text.split() if token.lower() in tokens]\n\n    return tokens\n    \n# Country data\ncountries = [c.name for c in pycountry.countries]\ncountries = countries + [c.alpha_3 for c in pycountry.countries]\n\n# Lookup country name for alpha code. If already an alpha code, return value\ndef countryname(x):\n    country = pycountry.countries.get(alpha_3=x)\n    return country.name if country else x\n    \n# Resolve alpha code for country name\ndef countrycode(x):\n    return pycountry.countries.get(name=x).alpha_3\n\n# Tokenize and filter only country names\ndef countrynames(x):\n    return [countryname(token) for token in tokenize(x, True) if token in countries]\n\n# Word Cloud colors\ndef wcolors(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n    colors = [\"#7e57c2\", \"#03a9f4\", \"#011ffd\", \"#ff9800\", \"#ff2079\"]\n    return np.random.choice(colors)\n\n# Word Cloud visualization\ndef wordcloud(df, title = None):\n    # Set random seed to have reproducible results\n    np.random.seed(64)\n    \n    wc = WordCloud(\n        background_color=\"white\",\n        max_words=200,\n        max_font_size=40,\n        scale=5,\n        random_state=0\n    ).generate_from_frequencies(df)\n\n    wc.recolor(color_func=wcolors)\n    \n    fig = plt.figure(1, figsize=(15,15))\n    plt.axis('off')\n\n    if title:\n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wc),\n    plt.show()\n\n# Dataframe plot\ndef plot(df, title, kind=\"bar\", color=\"bbddf5\"):\n    # Remove top and right border\n    ax = plt.axes()\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n\n    # Set axis color\n    ax.spines['left'].set_color(\"#bdbdbd\")\n    ax.spines['bottom'].set_color(\"#bdbdbd\")\n\n    df.plot(ax=ax, title=title, kind=kind, color=color);\n\n# Pie plot\ndef pie(labels, sizes, title):\n    patches, texts = plt.pie(sizes, colors=[\"#4caf50\", \"#ff9800\", \"#03a9f4\", \"#011ffd\", \"#ff2079\", \"#7e57c2\", \"#fdd835\"], startangle=90)\n    plt.legend(patches, labels, loc=\"best\")\n    plt.axis('equal')\n    plt.tight_layout()\n    plt.title(title)\n    plt.show()\n    \n# Map visualization\ndef mapplot(df, title, bartitle):\n    fig = go.Figure(data=go.Choropleth(\n        locations = df[\"Code\"],\n        z = df[\"Count\"],\n        text = df[\"Country\"],\n        colorscale = [(0,\"#fffde7\"), (1,\"#f57f17\")],\n        showscale = False,\n        marker_line_color=\"darkgray\",\n        marker_line_width=0.5,\n        colorbar_title = bartitle,\n    ))\n\n    fig.update_layout(\n        title={\n            'text': title,\n            'y':0.9,\n            'x':0.5,\n            'xanchor': 'center',\n            'yanchor': 'top'},\n        geo=dict(\n            showframe=False,\n            showcoastlines=False,\n            projection_type='equirectangular'\n        )\n    )\n    \n    fig.show(config={\"displayModeBar\": False, \"scrollZoom\": False})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the data\nThe articles database has a copy of all articles that were found in metadata.csv. Pure duplicate articles (based on the sha hash) are filtered out. In addition to the metadata and text fields, a field named tags is added. Each article is tagged based on the topic. The only tag at this time is COVID-19 for articles that directly mention COVID-19 and related terms. This field is important as the embedding index and all model searches will go against the subset of data tagged as COVID-19."},{"metadata":{},"cell_type":"markdown","source":"## Articles Table\nA sample of the articles table is shown below."},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport sqlite3\n\n# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\n# Articles\npd.set_option(\"max_colwidth\", 125)\narticles = pd.read_sql_query(\"select * from articles where tags is not null LIMIT 5\", db)\narticles","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sections Table\nIn addition to the articles table, another table named sections is also created. The full text content is stored here. Each row is a single sentence from an article. Sentences are parsed using [NTLK's](https://www.nltk.org/) sent_tokenize method. The article id and tags are also stored with each section. The sections schema and sample rows are shown below."},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\n# Sections\npd.set_option(\"max_colwidth\", 125)\nsections = pd.read_sql_query(\"select * from sections where tags is not null LIMIT 5\", db)\nsections","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Most Frequent Words in Tagged Articles\nThe following wordcloud shows the most frequent words within the titles of tagged articles."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\n# Select data\narticles = pd.read_sql_query(\"select title from articles where tags is not null\", db)\n\n# Build word frequencies on filtered tokens\nfreqs = pd.Series(np.concatenate([tokenize(x) for x in articles.Title])).value_counts()\nwordcloud(freqs, \"Most frequent words in article titles tagged as COVID-19\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tagged Articles by Country Mentioned\nThe following map shows the Articles by Country mentioned. China is mentioned significantly more and it's count is clipped in this graphic to allow showing distribution across the globe."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\nsections = pd.read_sql_query(\"select text from sections where tags is not null\", db)\n\n# Filter tokens to only country names. Build dataframe of Country, Count, Code\nmentions = pd.Series(np.concatenate([countrynames(x) for x in sections.Text])).value_counts()\nmentions = mentions.rename_axis(\"Country\").reset_index(name=\"Count\")\nmentions[\"Code\"] = [countrycode(x) for x in mentions[\"Country\"]]\n\n# Set max to 250 to allow shading for multiple countries\nmentions[\"Count\"] = mentions[\"Count\"].clip(upper=300)\n\nmapplot(mentions, \"Tagged Articles by Country Mentioned\", \"Articles by Country\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tagged Articles by Source\nThe following graph shows the articles grouped by the source field in the metadata."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\narticles = pd.read_sql_query(\"select source from articles where tags is not null\", db)\n\nfreqs = articles.Source.value_counts().sort_values(ascending=True)\nplot(freqs, \"Tagged Articles by Source\", \"barh\", \"#1976d2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tagged Articles by Publication\nThe graph below shows the articles grouped by publication. Only the Top 20 publications are shown and many articles have no publication."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\narticles = pd.read_sql_query(\"select case when (Publication = '' OR Publication IS NULL) THEN '[None]' ELSE Publication END AS Publication from articles where tags is not null\", db)\n\nfreqs = articles.Publication.value_counts().sort_values(ascending=True)[-20:]\n\nplot(freqs, \"Tagged Articles by Publication\", \"barh\", \"#7e57c2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tagged Articles by Publication Month\nThe following graph shows articles by publication month. All of the articles have a publication date of 2020 or later (or the date is null). Many publication dates only include the year but there is a significant portion of articles this month, which shows the rapid pace things are moving. Also note that some publication dates are in the future. The articles have been released early to help find answers."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\narticles = pd.read_sql_query(\"select strftime('%Y-%m', published) as Published from articles where tags is not null and published >= '2020-01-01' order by published\", db)\n\nfreqs = articles.Published.value_counts().sort_index()\nplot(freqs, \"Tagged Articles by Publication Month\", \"bar\", \"#ff9800\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tagged Articles by Study Design\nThe chart below shows articles grouped by study design type. The study design gives researchers insight into the overall structure and quality of a study. The more rigor and hard data that goes into a study, the more reliable. This is a distinction compared to many other search systems, where we look for the best matching text. Credibility of the information is very important \nin helping judge whether the conclusions are reliable. \n\nThe medical field is rightfully a skeptical field. Many technologists are accustomed to running a web search and quickly trying the top results until you get to something that works. Lets be glad our doctors don't do the same. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Connect to database\ndb = sqlite3.connect(\"cord19q/articles.sqlite\")\n\narticles = pd.read_sql_query('select count(*) as count, case when design=1 then \"meta-analysis\" when design in (2, 3) then \"control trial\" ' + \n                             'when design in (4, 5, 6) then \"prospective studies\" when design=7 then \"retrospective cohort\" ' +\n                             'when design in (8, 9) then \"case-control\" when design=10 then \"case study\" else \"simulation\" end as design from articles ' +\n                             'where tags is not null and design > 0 group by design', db)\n\narticles = articles.groupby([\"design\"]).sum().reset_index()\n\n# Plot a pie chart of study types\npie(articles[\"design\"], articles[\"count\"], \"Tagged Articles by Study Design\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploration Takeaways\nGiven the urgency to find any data to help, many of the tagged articles are recent. Publications by nature put hypothesises and theories through a rigorious scientific method/peer review to ensure accuracy and reliability. It's a balancing act of not holding on to data that can help against making sure decisions are based on accurate data. Given that all searches are against this subset of data, conclusions should be carefully drawn. "},{"metadata":{},"cell_type":"markdown","source":"# Testing the model\n\nNow that both the articles.sqlite database and embeddings index are both created, lets test that everything is working properly."},{"metadata":{},"cell_type":"markdown","source":"## Word Embeddings\nThe foundation of sentence embeddings are word embeddings. As previously explained, sentence embeddings are just word embeddings joined together (each token weighted by a BM25 index). "},{"metadata":{"trusted":true},"cell_type":"code","source":"from cord19q.embeddings import Embeddings\n\nembeddings = Embeddings()\nembeddings.load(\"cord19q\")\n\nvectors = embeddings.vectors\n\npd.DataFrame(embeddings.vectors.most_similar(\"covid-19\", topn=10), columns=[\"key\", \"value\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The vector model is good at identifying near matches, which helps increase the accuracy of the overall model. Notice that the top hits are typos (covid-10 mistyped 0 instead of 9). \n\nBelow shows similarity for a list of terms, numbers look overall as expected, model has learned an association between the various diseases and knows phone is not related."},{"metadata":{"trusted":true},"cell_type":"code","source":"vectors.similarity(\"coronavirus\", [\"sars\", \"influenza\", \"ebola\", \"phone\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sentence Embeddings\nAt the highest level, the model builds embeddings for each sentence in the corpus. For input queries, it compares each sentence against the input query. Faiss enables that similarity search to be fast. An example of how this works at a small level below."},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence1 = \"Range of incubation periods for the disease in humans\"\nsentence2 = \"The incubation period of 2019-nCoV is generally 3-7 days but no longer than 14 days, and the virus is infective during the incubation period\"\n\nembeddings.similarity(Tokenizer.tokenize(sentence1), Tokenizer.tokenize(sentence2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence1 = \"Range of incubation periods for the disease in humans\"\nsentence2 = \"The medical profession is short on facemasks during this period, more are needed\"\n\nembeddings.similarity(Tokenizer.tokenize(sentence1), Tokenizer.tokenize(sentence2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Run a query\nRun a full query to ensure model is working."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from cord19q.query import Query\n\n# Execute a test query\nQuery.run(\"antiviral covid-19 success treatment\", 5, \"cord19q\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from cord19q.report import Report\nfrom IPython.display import display, Markdown\n\ntransmission = \\\n    [\"Range of incubation periods for the disease in humans\",\n     \"How the incubation period varies across age and health status\",\n     \"How long individuals are contagious, even after recovery.\",\n     \"Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\",\n     \"Seasonality of transmission.\",\n     \"Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\",\n     \"Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\",\n     \"Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\",\n     \"Natural history of the virus and shedding of it from an infected person\",\n     \"Implementation of diagnostics and products to improve clinical processes\",\n     \"Disease models, including animal models for infection, disease and transmission\",\n     \"Tools and studies to monitor phenotypic change and potential adaptation of the virus\",\n     \"Immune response and immunity\",\n     \"Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\",\n     \"Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\",\n     \"Role of the environment in transmission\"]\n\nrisk_factors = \\\n    [\"Smoking, pre-existing pulmonary disease\",\n     \"Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities\",\n     \"Neonates and pregnant women\",\n     \"Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\",\n     \"Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\",\n     \"Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\",\n     \"Susceptibility of populations\",\n     \"Public health mitigation measures that could be effective for control\"]\n\nvirus_genome = \\\n    [\"Real-time tracking of whole virus genomes\",\n     \"Mechanism for coordinating the rapid dissemination of that genome information to inform the development of diagnostics and therapeutics\",\n     \"Track variations of the virus over time\",\n     \"Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences\",\n     \"Determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.\",\n     \"Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding)\",\n     \"Evidence that livestock could serve as a reservoir after the epidemic appears to be over.\",\n     \"Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.\",\n     \"Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\",\n     \"Experimental infections to test host range for this pathogen.\",\n     \"Animal host(s) and any evidence of continued spill-over to humans\",\n     \"Socioeconomic and behavioral risk factors for this spill-over\",\n     \"Sustainable risk reduction strategies\"]\n\ninterventions = \\\n    [\"Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.\",\n     \"Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments.\",\n     \"Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.\",\n     \"Methods to control the spread in communities, barriers to compliance and how these vary among different populations..\",\n     \"Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.\",\n     \"Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs.\",\n     \"Research on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high).\",\n     \"Research on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.\"]\n\nvaccines = \\\n    [\"Effectiveness of drugs being developed and tried to treat COVID-19 patients.\",\n     \"Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\",\n     \"Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\",\n     \"Exploration of use of best animal models and their predictive value for a human vaccine.\",\n     \"Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\",\n     \"Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\",\n     \"Efforts targeted at a universal coronavirus vaccine.\",\n     \"Efforts to develop animal models and standardize challenge studies\",\n     \"Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\",\n     \"Approaches to evaluate risk for enhanced disease after vaccination\",\n     \"Assays to evaluate vaccine immune response and process development for vaccines [in conjunction with therapeutics]\",\n     \"Suitable animal models for vaccine development\"]\n\nmedical_care = \\\n    [\"Resources to support skilled nursing facilities and long term care facilities.\",\n     \"Mobilization of surge medical staff to address shortages in overwhelmed communities\",\n     \"Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with/without other organ failure – particularly for viral etiologies\",\n     \"Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients\",\n     \"Outcomes data for COVID-19 after mechanical ventilation adjusted for age.\",\n     \"Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest.\",\n     \"Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level.\",\n     \"Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks.\",\n     \"Best telemedicine practices, barriers and faciitators, and specific actions to remove/expand them within and across state boundaries.\",\n     \"Guidance on the simple things people can do at home to take care of sick people and manage disease.\",\n     \"Oral medications that might potentially work.\",\n     \"Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually.\",\n     \"Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes.\",\n     \"Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials\",\n     \"Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials\",\n     \"Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)\"]\n\nsharing = \\\n    [\"Methods for coordinating data-gathering with standardized nomenclature.\",\n     \"Sharing response information among planners, providers, and others.\",\n     \"Understanding and mitigating barriers to information-sharing.\",\n     \"How to recruit, support, and coordinate local (non-Federal) expertise and capacity relevant to public health emergency response (public, private, commercial and non-profit, including academic).\",\n     \"Integration of federal/state/local public health surveillance systems.\",\n     \"Value of investments in baseline public health response infrastructure preparedness\",\n     \"Modes of communicating with target high-risk populations (elderly, health care workers).\",\n     \"Risk communication and guidelines that are easy to understand and follow (include targeting at risk populations’ families too).\",\n     \"Communication that indicates potential risk of disease to all population groups.\",\n     \"Misunderstanding around containment and mitigation.\",\n     \"Action plan to mitigate gaps and problems of inequity in the Nation’s public health capability, capacity, and funding to ensure all citizens in need are supported and can access information, surveillance, and treatment.\",\n     \"Measures to reach marginalized and disadvantaged populations.\",\n     \"Data systems and research priorities and agendas incorporate attention to the needs and circumstances of disadvantaged populations and underrepresented minorities.\",\n     \"Mitigating threats to incarcerated people from COVID-19, assuring access to information, prevention, diagnosis, and treatment.\",\n     \"Understanding coverage policies (barriers and opportunities) related to testing, treatment, and care\"]\n\nethics = \\\n    [\"Efforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019\",\n     \"Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight\",\n     \"Efforts to support sustained education, access, and capacity building in the area of ethics\",\n     \"Efforts to establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences.\",\n     \"Efforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control.\",\n     \"Secondary impacts of public health measures (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures)\",\n     \"Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients\",\n     \"Identify the immediate physical and psychological health of medical staff that must be addressed.\",\n     \"Efforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.\"]\n\ndiagnostics = \\\n    [\"How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures.\",\n     \"Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible.\",\n     \"Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples)\",\n     \"Early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs).\",\n     \"Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.\",\n     \"Recruitment, support, and coordination of local expertise and capacity (public, private—commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.\",\n     \"National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).\",\n     \"Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.\",\n     \"Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity.\",\n     \"How surveillance experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded).\",\n     \"Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.\",\n     \"Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance/detection schemes.\",\n     \"Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.\",\n     \"Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions.\",\n     \"Policies and protocols for screening and testing.\",\n     \"Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.\",\n     \"Technology roadmap for diagnostics.\",\n     \"Barriers to developing and scaling up new diagnostic tests (e.g., market forces)\",\n     \"How future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.\",\n     \"New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.\",\n     \"Coupling genomics and diagnostic testing on a large scale.\",\n     \"Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.\",\n     \"Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.\",\n     \"One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.\"]\n   \n# Writes queries out to a local file for processing\ndef queries(file, queries):\n    with open(file + \".txt\", \"w\") as output:\n        for query in queries:\n            output.write(\"%s\\n\" % query)\n\n# Builds Markdown and Excel reports\ndef report(file):\n    Report.run(file + \".txt\", 50, \"md\", \"cord19q\")\n    Report.run(file + \".txt\", 50, \"xlsx\", \"cord19q\")\n\n# Save queries to file\nqueries(\"transmission\", transmission)\nqueries(\"risk-factors\", risk_factors)\nqueries(\"virus-genome\", virus_genome)\nqueries(\"interventions\", interventions)\nqueries(\"vaccines\", vaccines)\nqueries(\"medical-care\", medical_care)\nqueries(\"sharing\", sharing)\nqueries(\"ethics\", ethics)\nqueries(\"diagnostics\", diagnostics)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building task reports\nTask reports are an aggregation of each question within a task in the challenge. For each question, a query is run and the top articles are returned. For each article, text matches are shown as bulleted points and these are the best matching sentences within the article. The full list of result sentences are also analyzed and run through a [textrank algorithm](https://en.wikipedia.org/wiki/Automatic_summarization#TextRank_and_LexRank). Highlights or top sentences within the results are also shown within the report. \n\nImportant source files to highlight\n* Report process -> [report.py](https://github.com/neuml/cord19q/blob/master/src/python/cord19q/report.py)\n* Textrank algorithm to highlight best sentences -> [highlights.py](https://github.com/neuml/cord19q/blob/master/src/python/cord19q/highlights.py)"},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true},"cell_type":"code","source":"# Generate report for each task list. \nfor name in [\"transmission\", \"risk-factors\", \"virus-genome\", \"interventions\", \"vaccines\", \"medical-care\", \"sharing\", \"ethics\", \"diagnostics\"]:\n    report(name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Render a Markdown report\n\nThe following code can be used to render a Markdown file for a task report within a notebook.\n\n```python\nfrom IPython.display import display, Markdown\n\n# Where file is the full-path to the Markdown file (transmission, risk-factors, ...)\nfile = \"transmission.md\"\n\ndisplay(Markdown(filename=file))\n```"},{"metadata":{},"cell_type":"markdown","source":"# List of task reports\n\nThe following is a list of Notebooks covering each task report. The reports are also available in Excel (links below each Notebook). The Notebook and Excel files both have the same data and formatting.\n\n- [What is known about transmission, incubation, and environmental stability?](https://www.kaggle.com/davidmezzetti/cord-19-transmission-incubation-environment)\n  - [XLSX](transmission.xlsx)\n- [What do we know about COVID-19 risk factors?](https://www.kaggle.com/davidmezzetti/cord-19-risk-factors)\n  - [XLSX](risk-factors.xlsx)\n- [What do we know about virus genetics, origin, and evolution?](https://www.kaggle.com/davidmezzetti/cord-19-virus-genetics-origin-and-evolution)\n  - [XLSX](virus-genome.xlsx)\n- [What do we know about vaccines and therapeutics?](https://www.kaggle.com/davidmezzetti/cord-19-vaccines-and-therapeutics)\n  - [XLSX](vaccines.xlsx)\n- [What do we know about non-pharmaceutical interventions?](https://www.kaggle.com/davidmezzetti/cord-19-non-pharmaceutical-interventions)\n  - [XLSX](interventions.xlsx)\n- [What has been published about medical care?](https://www.kaggle.com/davidmezzetti/cord-19-medical-care)\n  - [XLSX](medical-care.xlsx)\n- [What do we know about diagnostics and surveillance?](https://www.kaggle.com/davidmezzetti/cord-19-diagnostics-and-surveillance)\n  - [XLSX](diagnostics.xlsx)\n- [What has been published about information sharing and inter-sectoral collaboration?](https://www.kaggle.com/davidmezzetti/cord-19-sharing-and-collaboration)\n  - [XLSX](sharing.xlsx)\n- [What has been published about ethical and social science considerations?](https://www.kaggle.com/davidmezzetti/cord-19-ethical-and-social-science-considerations)\n  - [XLSX](ethics.xlsx)\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}