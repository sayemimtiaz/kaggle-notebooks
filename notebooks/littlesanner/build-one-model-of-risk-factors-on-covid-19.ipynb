{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 Risk Factors Analysis\n\n![](https://upload.wikimedia.org/wikipedia/commons/9/96/3D_medical_animation_coronavirus_structure.jpg)\n\n <h1 align=\"left\" style=\"color:blue;\">\n <br><font color=\"red\"> Every life counts.</font>ðŸ˜·\nBest wishes. <br>  \n</h1> \n\n**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n1. [Step 1: Define the Problem and give approach](#ch1)\n1. [Step 2: Install dependencies & Load Data Modelling Libraries](#ch2)\n1. [Step 3: Prepare the data](#ch3)\n1. [Step 4: Meet and greet the data (including cleaning)](#ch4)\n1. [Step 5: Explore the Data(Feature Selection)](#ch5)\n1. [Step 6: Try Different Model](#ch6)\n1. [Step 7: Evaluate Model Performance](#ch7)\n1. [Step 8: Conclusion](#ch8)\n1. [Step 9: Appendix](#ch9)\n1. [Acknowledgements and Reference](#ch10)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch1\"></a>\n# Step 1: Define the Problem and give approach","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Aim of this Notebook\nTo understand the risk factors surrounding COVID-19\nMain goal of this research is to analyze the data and find Risk Factors of COVID-19\n\n**Task Details (Taken from [Task Home Page](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=558))**  \nWhat do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\n\n1. Data on potential risks factors  \n    **a.** Smoking, pre-existing pulmonary disease  \n    **b.** Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other            co-morbidities  \n    **c.** Neonates and pregnant women  \n    **d.** Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.  \n2. Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n3. Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n4. Susceptibility of populations\n5. Public health mitigation measures that could be effective for control\n\nWhen a new virus is discovered and causes a pandemic, it is important for scientists to get information coming from all scientific sources that may help them combat the pandemic.  The challenege, however, is that the number of scientific papers created is large and the papers are published very rapidly, making it nearly impossible for scientists to digest and understand important data in this mass of data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Try to define the problem\n\n**[Here are the symptoms associated with COVID-19 and how they compare with symptoms of the common cold, the flu, and allergies](https://www.businessinsider.com/coronavirus-symptoms-compared-to-flu-common-cold-and-allergies-2020-3):**\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.insider.com/5e6a58e684159f61963287a2?width=1000&format=jpeg&auto=webp\" width=\"600px\" align=\"left\"> \nSome risk factor described in paper is below.\n\n - history of smoking\n - maximum body temperature at admission\n - respiratory failure\n - Higher respiratory rate\n - Higher C-reactive protein\n - Lower Albumin\n \nRisk factor described in paper which describe age and underlying disease is below.\n\n - Higher age\n - Higher number of chronic underlying diseases\n\nCRP is an indicator of the inflammatory response and may indicate the risk of severity. In addition, it is highly likely that a healthy body is less likely to be severely ill.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://ichef.bbci.co.uk/news/640/cpsprodpb/16F8F/production/_111059049_corona_virus_symptoms_short_v4_640-nc.png\" width=\"600px\" align=\"left\"> \n \nOther papers describe cardiovascular risk and severity.\n\n - Higher C-reactive protein\n - Higher creatinine\n - Higher N-terminal pro B-type natriuretic peptide(NT-proBNP; only men)\n - Higher cardiac troponin-I (cTnl; only men)\n - history of smoking\n - maximum body temperature at admission\n - respiratory failure\n - Higher respiratory rate\n - Higher C-reactive protein\n - Lower Albumin\n \n ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## **Analysis**:\n \n<ol>\n    <li>Unsupervised Learning task, because we don't have labels for the articles</li>\n    <li>Dimensionality Reduction(like PCA) and Clustering(K-Means++ classify) task </li>\n    <li>Try different models, like: </li>\n    <li>Finding the Necessary Papers based on simple \"Keyword\" search: can try K-nearest</li>\n    <li>Analyzing papers that contain specific words like \"pulmonary\", \"smoking\", \"pregnancy\" and \"newborns\"</li>\n    <li>There is no continuous flow of data, no need to adjust to changing data, and the data is small enough to fit in memmory(Batch Learning)</li>\n    <li>Altough, there is no continuous flow of data, our approach has to be scalable as there will be more literature later</li>\n</ol>\n\n**Pros**:\n* Doesn't only use the title or meta-data, but the actual content (text body) of the articles \n* Once trained, the model is easy and fast to apply\n* Helps to discover latent relationships between articles that might drive innovation\n\n**Cons**:\n* Unsupervised learning of topics is hard to verify","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch2\"></a>\n# Step 2: Install dependencies & Load Data Modelling Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install pyecharts\n# !pip install Pillow\n\n# !pip install numpy torchvision_nightly\n# !pip install covid19_tools\n\n# !pip install spacy # Uncomment this if spacy package is not installed.\n# !pip uninstall spacy # Uncomment this if installed version of spacy fails.\n# !python -m spacy download en # Uncomment this if en language is not loaded in spacy package. \n\n# !pip install bert-tensorflow\n# !pip install  tensorflow-gpu==1.15.0\n\n!pip install spacy-langdetect\n!pip install language-detector\n!pip install symspellpy\n!pip install sentence-transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### the usual suspects","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport re\n\nimport json\nimport math\nimport glob\nimport time\n\nimport string\nimport random\nimport pickle\n\nimport functools\nimport collections\n\nfrom tqdm import tqdm\nfrom PIL import Image\nimport seaborn as sns\n\nfrom nltk import PorterStemmer\nimport torch.nn.functional as F\n\n\n# from pyecharts.charts import Graph\n# from pyecharts import options as opts\nfrom keras.preprocessing import sequence\nfrom scipy.spatial.distance import cdist\nfrom nltk.corpus import stopwords \nfrom nltk.corpus import wordnet\nfrom IPython.core.display import display, HTML\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom torch.utils.data import Dataset,TensorDataset,DataLoader\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer, WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check version \nimport sys #access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\nimport tensorflow as tf\nprint(\"tensorflow version: {}\". format(tf.__version__))\n\nimport torch #collection of machine learning algorithms\nprint(\"torch version: {}\". format(torch.__version__))\n\n\n# ignore warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\nprint('-'*71)\n\n\nfrom subprocess import check_output\nprint('input file is:', check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GPU and TPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# detect and init the TPU\n\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\n# with tpu_strategy.scope():\n#     model = tf.keras.Sequential( â€¦ ) # define your model normally\n#     model.compile( â€¦ )\n\n# # train model normally\n# model.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=â€¦)\n\n\n\n## try to use the tpu of Kaggle\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE\nfrom kaggle_datasets import KaggleDatasets\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch3\"></a>\n# Step 3: Prepare Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input/CORD-19-research-challenge/  # the content of input files\nroot_path = '/kaggle/input/CORD-19-research-challenge/'\nmetadata_path = f'{root_path}/metadata.csv'  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### about the README","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/CORD-19-research-challenge/metadata.readme', 'r') as f:\n    data = f.read()\n    print(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch4\"></a>\n# Step 4: Meet and greet the data (including cleaning)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## load the meta data from the CSV file \ndf = pd.read_csv(metadata_path,header = 0,  # you can specify the header where it is from\n                 usecols=['title','abstract','authors','doi','publish_time','source_x'],   # select use columns\n                 dtype={\n                        'Microsoft Academic Paper ID': str,\n                        'pubmed_id': str,\n                        'doi': str,                       \n                       },\n                 low_memory=False)\n\nprint (f'The shape of the input data:\\n{df.shape[0]} articles, every article has {df.shape[1]} features')\nprint(df.info())\nprint(f'None data:\\n{df.isnull().sum()}')\ndf.sample(3)\n\n# before 2020, maybe publication about COVID-2019 is not out.\ndf_2020 = df.query(\"'2020' in publish_time\")\n\n# I think Confidence Interval (CI) is used epidemiological evaluation\ndf_2020_ci = df_2020.loc[df_2020[\"abstract\"].str.contains(\"CI\").fillna(False), :]\n\nprint(df_2020_ci.shape)\ndf_2020_ci.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## about the biorxiv_dir","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_dir = '../input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/'\nfilenames = os.listdir(biorxiv_dir)\nprint(\"Number of articles retrieved from biorxiv:\", len(filenames))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nfrom tqdm import tqdm_notebook\n\nstopwords = set(STOPWORDS)\n#https://www.kaggle.com/gpreda/cord-19-solution-toolbox\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=30, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(20,5))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n    \n    \nshow_wordcloud(df['abstract'], title = 'metadata - papers Abstract - frequent words (400 sample)')\n\n## Convert abstract to list\n# data = df.abstract.dropna().values.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### deal with the JSON file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Fetch All of JSON File Path\n# all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n# len(all_json)\n# print(all_json)\n\n# # Checking JSON Schema Structure\n# with open(all_json[0]) as file:\n#     first_entry = json.load(file)\n#     print(json.dumps(first_entry, indent=4))\n    \n    \n# load the json file in the directory\ndirs_=[\"/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json\",\n\"/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/pdf_json\",\n\"/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pdf_json\",\n\"/kaggle/input/CORD-19-research-challenge/custom_license/custom_license/pdf_json\"]\n \n\ndata_ = list()\nfor dir_ in dirs_:\n\tfor filename in tqdm(os.listdir(dir_)):\n\n\t\tx=str(dir_)+'/'+str(filename)\n        \n\t\twith open(x) as file:\n\t\t\tdata=json.loads(file.read())\n\t\t\n\t\t#take out the data from the json format\n\t\tpaper_id=data['paper_id']\n\t\tmeta_data=data['metadata']\n\t\tabstract=data['abstract']\n\t\tabstract_text=\"\"\n\t\tfor text in abstract:\n\t\t\tabstract_text+=text['text']+\" \"\n\t\tbody_text=data['body_text']\n\t\tfull_text=\"\"\n\t\tfor text in body_text:\n\t\t\tfull_text+=text['text']+\" \"\n\t\tback_matter=data['back_matter']\n\t\t#store everything to a dataframe\n\t\tdata_.append([paper_id,abstract_text,full_text])\n\ndf_json = pd.DataFrame(data_,columns=['paper_id','abstract','full_text'])\nprint(df.head())\n#save as a csv\n#df.to_csv('biorxiv_medrxiv.csv', index = True)\ndf_json.to_csv('train.csv', index = True)\n#a data frame for my complete body.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_json.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean the data\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-docs/stable/indexing.html)\n* [pandas.isnull](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html)\n* [pandas.DataFrame.sum](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html)\n* [pandas.DataFrame.mode](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mode.html)\n* [pandas.DataFrame.copy](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.copy.html)\n* [pandas.DataFrame.fillna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html)\n* [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)\n* [pandas.Series.value_counts](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html)\n* [pandas.DataFrame.loc](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy = df.copy() #remember python assignment or equal passes by reference vs values, so use copy()\n# data_cleaner = [data1, df]  # however passing by reference is convenient, because we can clean both datasets at once\n\n# df = df.drop_duplicates(subset='abstract', keep=\"first\")  # df=df.drop_duplicates() # df_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\n# df_covid.describe(include='all')\n\ndf = df.dropna()  # drop NANs \n\ndf[\"abstract\"] = df[\"abstract\"].str.lower()  # convert abstracts to lowercase\n\nprint (f'after preproccess: the shape :{df.shape}')   # print (f'head input data information: {df.head()}')   # show 5 lines of the new dataframe\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch5\"></a>\n# Sep 5: Explore the Data(Feature Selection)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Literature source distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sourceDic = collections.defaultdict(int)\nfor s in df[\"source_x\"][df[\"source_x\"].notnull()]:\n    sourceDic[s] += 1\nsizes, explode, labels = [], [], []\nfor s in sourceDic:\n    sizes.append(sourceDic[s])\n    explode.append(0)\n    labels.append(s)\n    \ncolors = ['red', 'gold', 'lightcoral', 'violet', 'lightskyblue', 'green']\nfig = plt.gcf()\nfig.set_size_inches(8, 8)\nplt.pie(sizes, explode=explode, labels=labels, colors = colors, autopct='%1.2f%%', shadow=True, startangle=140)\nplt.title('Literature source distribution')\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Publish year distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"yearList = []\nfor y in df[\"publish_time\"][df[\"publish_time\"].notnull()]:\n    yearList.append(int(re.split(' |-', y)[0]))\n\nsns.distplot(yearList, bins = 50)\nplt.title(\"Publish year distribution\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Abstract distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The number of articles with abstract: \" + str(sum(df[\"abstract\"].notnull())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we try to normalize the abstracts to learn the distribution of abstract length and tokens. The following steps will also be the steps to preprocess the abstracts for extracting tf-idf features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch6\"></a>\n# Step 6: Try Different Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Model word2count","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords \nfrom nltk.corpus import wordnet\n\nstartTime = time.time()\nabsLength = []\nword2count = {}\nfor abstract in df[\"abstract\"][df[\"abstract\"].notnull()]:\n    ## Remove web links\n    abstract = re.sub('https?://\\S+|www\\.\\S+', '', abstract) \n\n    ## Lowercase\n    abstract = abstract.lower()\n    \n    ## Remove punctuation\n    abstract = re.sub('<.*?>+', ' ', abstract)\n    abstract = re.sub('[%s]' % re.escape(string.punctuation), ' ', abstract)\n    \n    ## Remove number\n    abstract = re.sub(r'\\d+', '', abstract)\n    \n    ## Tokenize\n    words = word_tokenize(abstract)\n    \n    ## Remove stop words\n    nltk_stop_words = stopwords.words('english')\n    words = [word for word in words if word not in nltk_stop_words]\n    \n    ## Stem\n    stemmer = SnowballStemmer('english')\n    words = [stemmer.stem(word) for word in words]\n    \n    ## Lematize verbs\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n    \n    ## Record length\n    absLength.append(len(words))\n    \n    ## Get word count\n    for word in words:\n        count = word2count.get(word, 0)\n        word2count[word] = count + 1\nprint(\"Time spent: \" + str(round((time.time() - startTime) / 60, 3)) + \"min.\")\nprint(\"The number of tokens: \" + str(len(word2count)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of abstract length\nThere are 20 extremely long abstracts. We excluded them from the histogram.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(sorted(absLength)[:-20], bins = 50) # There are 20 extremely long abstracts\nplt.xlabel(\"Abstract token count\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 50 mostly frequent tokens","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_word_count = pd.DataFrame(sorted(word2count.items(), key=lambda x: x[1])[::-1])\nsns.set(rc={'figure.figsize':(12,10)})\nsns.barplot(y = df_word_count[0].values[:50], x = df_word_count[1].values[:50], color='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model textNormalize","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def textNormalize(rawString):\n    \"\"\"\n    Function for text normalization.\n    Text normalization includes:\n    1. removing web links\n    2. converting all letters to lower or upper case\n    3. removing punctuationsz\n    4. removing numbers\n    5. tokenization\n    6. removing stopwords\n    7. stemming\n    8. lemmatization\n    Input:\n        rawString: a string contains the text to be normaized. \n    Output:\n        normText: a string contains the normalized text where the tokens extracted from rawString are joined by space.\n    \"\"\"\n    if rawString == np.nan:\n        return rawString\n    ## Remove web links\n    rawString = re.sub('https?://\\S+|www\\.\\S+', '', rawString) \n\n    ## Lowercase\n    rawString = rawString.lower()\n    \n    ## Remove punctuation\n    rawString = re.sub('<.*?>+', ' ', rawString)\n    rawString = re.sub('[%s]' % re.escape(string.punctuation), ' ', rawString)\n    \n    ## Remove number\n    rawString = re.sub(r'\\d+', '', rawString)\n    \n    ## Tokenize\n    words = word_tokenize(rawString)\n    \n    ## Remove stop words\n    nltk_stop_words = stopwords.words('english')\n    words = [word for word in words if word not in nltk_stop_words]\n    \n    ## Stem\n    stemmer = SnowballStemmer('english')\n    words = [stemmer.stem(word) for word in words]\n    \n    ## Lematize verbs\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n    \n    normText = \" \".join(words)\n    \n    return normText\n\n\nstartTime = time.time()\ndf[\"clean_abstract\"] = float(\"NaN\")\ndf.loc[df[\"abstract\"].notnull(), \"clean_abstract\"] = \\\ndf[\"abstract\"][df[\"abstract\"].notnull()].apply(lambda x: textNormalize(x))\nprint(\"Time spent: \" + str(round((time.time() - startTime) / 60, 3)) + \"min.\")\n\ndf.to_csv(\"Metadata_clean.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Deep Dive\n\nThe author used: \n\n* LDA for probabilistic topic assignment vector.\n* Bert for sentence embedding vector.\n\n1. Concatenated both LDA and Bert vectors with a weight hyperparameter to balance the relative importance of information from each source.\n2. Used autoencoder to learn a lower dimensional latent space representation of the concatenated vector.\n\n* The assumption is that the concatendate vector shoul have a manifold shaep in the high dimensional space. \n* USed clustering on the latent space representations to get topics. \n\n![Contextual Topic Identification model design](https://miro.medium.com/max/1410/1*OKCYnB-JbGq1NDwNSKd5Zw.png)\n\n**Data pipeline (from development to deployment**)\n\n![Data pipeline (from development to deployment)](https://miro.medium.com/max/1348/1*Cdp4y1tfMxqoj96o6lUdFg.png)\n\n\nSource:Shoa ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Utils\nfrom collections import Counter\nfrom sklearn.metrics import silhouette_score\nimport umap\nfrom wordcloud import WordCloud\nfrom gensim.models.coherencemodel import CoherenceModel\n\n\ndef get_topic_words(token_lists, labels, k=None):\n    \"\"\"\n    get top words within each topic from clustering results\n    \"\"\"\n    if k is None:\n        k = len(np.unique(labels))\n    topics = ['' for _ in range(k)]\n    for i, c in enumerate(token_lists):\n        topics[labels[i]] += (' ' + ' '.join(c))\n    word_counts = list(map(lambda x: Counter(x.split()).items(), topics))\n    # get sorted word counts\n    word_counts = list(map(lambda x: sorted(x, key=lambda x: x[1], reverse=True), word_counts))\n    # get topics\n    topics = list(map(lambda x: list(map(lambda x: x[0], x[:10])), word_counts))\n\n    return topics\n\ndef get_coherence(model, token_lists, measure='c_v'):\n    \"\"\"\n    Get model coherence from gensim.models.coherencemodel\n    :param model: Topic_Model object\n    :param token_lists: token lists of docs\n    :param topics: topics as top words\n    :param measure: coherence metrics\n    :return: coherence score\n    \"\"\"\n    if model.method == 'LDA':\n        cm = CoherenceModel(model=model.ldamodel, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n                            coherence=measure)\n    else:\n        topics = get_topic_words(token_lists, model.cluster_model.labels_)\n        cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n                            coherence=measure)\n    return cm.get_coherence()\n\ndef get_silhouette(model):\n    \"\"\"\n    Get silhouette score from model\n    :param model: Topic_Model object\n    :return: silhouette score\n    \"\"\"\n    if model.method == 'LDA':\n        return\n    lbs = model.cluster_model.labels_\n    vec = model.vec[model.method]\n    return silhouette_score(vec, lbs)\n\ndef plot_proj(embedding, lbs):\n    \"\"\"\n    Plot UMAP embeddings\n    :param embedding: UMAP (or other) embeddings\n    :param lbs: labels\n    \"\"\"\n    n = len(embedding)\n    counter = Counter(lbs)\n    for i in range(len(np.unique(lbs))):\n        plt.plot(embedding[:, 0][lbs == i], embedding[:, 1][lbs == i], '.', alpha=0.5,\n                 label='cluster {}: {:.2f}%'.format(i, counter[i] / n * 100))\n    plt.legend(loc = 'best')\n    plt.grid(color ='grey', linestyle='-',linewidth = 0.25)\n\n\ndef visualize(model):\n    \"\"\"\n    Visualize the result for the topic model by 2D embedding (UMAP)\n    :param model: Topic_Model object\n    \"\"\"\n    if model.method == 'LDA':\n        return\n    reducer = umap.UMAP()\n    print('Calculating UMAP projection ...')\n    vec_umap = reducer.fit_transform(model.vec[model.method])\n    print('Calculating UMAP projection. Done!')\n    plot_proj(vec_umap, model.cluster_model.labels_)\n    dr = '/kaggle/working/contextual_topic_identification/docs/images/{}/{}'.format(model.method, model.id)\n    if not os.path.exists(dr):\n        os.makedirs(dr)\n    plt.savefig('/kaggle/working/2D_vis')\n\ndef get_wordcloud(model, token_lists, topic):\n    \"\"\"\n    Get word cloud of each topic from fitted model\n    :param model: Topic_Model object\n    :param sentences: preprocessed sentences from docs\n    \"\"\"\n    if model.method == 'LDA':\n        return\n    print('Getting wordcloud for topic {} ...'.format(topic))\n    lbs = model.cluster_model.labels_\n    tokens = ' '.join([' '.join(_) for _ in np.array(token_lists)[lbs == topic]])\n\n    wordcloud = WordCloud(width=800, height=560,\n                          background_color='white', collocations=False,\n                          min_font_size=10).generate(tokens)\n\n    # plot the WordCloud image\n    plt.figure(figsize=(8, 5.6), facecolor=None)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad=0)\n    dr = '/kaggle/working/{}/{}'.format(model.method, model.id)\n    if not os.path.exists(dr):\n        os.makedirs(dr)\n    plt.savefig('/kaggle/working' + '/Topic' + str(topic) + '_wordcloud')\n    print('Getting wordcloud for topic {}. Done!'.format(topic))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Preprocessing \n\nfrom stop_words import get_stop_words\nfrom nltk.stem.porter import PorterStemmer\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom language_detector import detect_language\n\nimport pkg_resources\nfrom symspellpy import SymSpell, Verbosity\n\nsym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\ndictionary_path = pkg_resources.resource_filename(\n    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\nif sym_spell.word_count:\n    pass\nelse:\n    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n\n\n###################################\n#### sentence level preprocess ####\n###################################\n\n# lowercase + base filter, some basic normalization\ndef f_base(s):\n    \"\"\"\n    :param s: string to be processed\n    :return: processed string: see comments in the source code for more info\n    \"\"\"\n    # normalization 1: xxxThis is a --> xxx. This is a (missing delimiter)\n    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # before lower case\n    # normalization 2: lower case\n    s = s.lower()\n    # normalization 3: \"&gt\", \"&lt\"\n    s = re.sub(r'&gt|&lt', ' ', s)\n    # normalization 4: letter repetition (if more than 2)\n    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n    # normalization 5: non-word repetition (if more than 1)\n    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n    # normalization 6: string * as delimiter\n    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)\n    # normalization 7: stuff in parenthesis, assumed to be less informal\n    s = re.sub(r'\\(.*?\\)', '. ', s)\n    # normalization 8: xxx[?!]. -- > xxx.\n    s = re.sub(r'\\W+?\\.', '.', s)\n    # normalization 9: [.?!] --> [.?!] xxx\n    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n    # normalization 10: ' ing ', noise text\n    s = re.sub(r' ing ', ' ', s)\n    # normalization 11: noise text\n    s = re.sub(r'product received for free[.| ]', ' ', s)\n    # normalization 12: phrase repetition\n    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n\n    return s.strip()\n\n\n# language detection\ndef f_lan(s):\n    \"\"\"\n    :param s: string to be processed\n    :return: boolean (s is English)\n    \"\"\"\n    \n    # some reviews are actually english but biased toward french\n    return detect_language(s) in {'English', 'French','Spanish','Chinese'}\n\n\n###############################\n#### word level preprocess ####\n###############################\n\n# filtering out punctuations and numbers\ndef f_punct(w_list):\n    \"\"\"\n    :param w_list: word list to be processed\n    :return: w_list with punct and number filter out\n    \"\"\"\n    return [word for word in w_list if word.isalpha()]\n\n\n# selecting nouns\ndef f_noun(w_list):\n    \"\"\"\n    :param w_list: word list to be processed\n    :return: w_list with only nouns selected\n    \"\"\"\n    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n\n\n# typo correction\ndef f_typo(w_list):\n    \"\"\"\n    :param w_list: word list to be processed\n    :return: w_list with typo fixed by symspell. words with no match up will be dropped\n    \"\"\"\n    w_list_fixed = []\n    for word in w_list:\n        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n        if suggestions:\n            w_list_fixed.append(suggestions[0].term)\n        else:\n            pass\n            # do word segmentation, deprecated for inefficiency\n            # w_seg = sym_spell.word_segmentation(phrase=word)\n            # w_list_fixed.extend(w_seg.corrected_string.split())\n    return w_list_fixed\n\n\n# stemming if doing word-wise\np_stemmer = PorterStemmer()\n\n\ndef f_stem(w_list):\n    \"\"\"\n    :param w_list: word list to be processed\n    :return: w_list with stemming\n    \"\"\"\n    return [p_stemmer.stem(word) for word in w_list]\n\n\n# filtering out stop words\n# create English stop words list\n\nstop_words = (list(\n    set(get_stop_words('en'))\n    |set(get_stop_words('es'))\n    |set(get_stop_words('de'))\n    |set(get_stop_words('it'))\n    |set(get_stop_words('ca'))\n    #|set(get_stop_words('cy'))\n    |set(get_stop_words('pt'))\n    #|set(get_stop_words('tl'))\n    |set(get_stop_words('pl'))\n    #|set(get_stop_words('et'))\n    |set(get_stop_words('da'))\n    |set(get_stop_words('ru'))\n    #|set(get_stop_words('so'))\n    |set(get_stop_words('sv'))\n    |set(get_stop_words('sk'))\n    #|set(get_stop_words('cs'))\n    |set(get_stop_words('nl'))\n    #|set(get_stop_words('sl'))\n    #|set(get_stop_words('no'))\n    #|set(get_stop_words('zh-cn'))\n))\n\n\n\n\ndef f_stopw(w_list):\n    \"\"\"\n    filtering out stop words\n    \"\"\"\n    return [word for word in w_list if word not in stop_words]\n\n\ndef preprocess_sent(rw):\n    \"\"\"\n    Get sentence level preprocessed data from raw review texts\n    :param rw: review to be processed\n    :return: sentence level pre-processed review\n    \"\"\"\n    s = f_base(rw)\n    if not f_lan(s):\n        return None\n    return s\n\n\ndef preprocess_word(s):\n    \"\"\"\n    Get word level preprocessed data from preprocessed sentences\n    including: remove punctuation, select noun, fix typo, stem, stop_words\n    :param s: sentence to be processed\n    :return: word level pre-processed review\n    \"\"\"\n    if not s:\n        return None\n    w_list = word_tokenize(s)\n    w_list = f_punct(w_list)\n    w_list = f_noun(w_list)\n    w_list = f_typo(w_list)\n    w_list = f_stem(w_list)\n    w_list = f_stopw(w_list)\n\n    return w_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Autoencoder\nimport keras\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nclass Autoencoder:\n    \"\"\"\n    Autoencoder for learning latent space representation\n    architecture simplified for only one hidden layer\n    \"\"\"\n\n    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.autoencoder = None\n        self.encoder = None\n        self.decoder = None\n        self.his = None\n\n    def _compile(self, input_dim):\n        \"\"\"\n        compile the computational graph\n        \"\"\"\n        input_vec = Input(shape=(input_dim,))\n        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n        decoded = Dense(input_dim, activation=self.activation)(encoded)\n        self.autoencoder = Model(input_vec, decoded)\n        self.encoder = Model(input_vec, encoded)\n        encoded_input = Input(shape=(self.latent_dim,))\n        decoder_layer = self.autoencoder.layers[-1]\n        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n\n    def fit(self, X):\n        if not self.autoencoder:\n            self._compile(X.shape[1])\n        X_train, X_test = train_test_split(X)\n        self.his = self.autoencoder.fit(X_train, X_train,\n                                        epochs=200,\n                                        batch_size=128,\n                                        shuffle=True,\n                                        validation_data=(X_test, X_test), verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom gensim import corpora\nimport gensim\n#from Autoencoder import *\n#from preprocess import *\nfrom datetime import datetime\n\n\ndef preprocess(docs, samp_size=None):\n    \"\"\"\n    Preprocess the data\n    \"\"\"\n    if not samp_size:\n        samp_size = 100\n\n    print('Preprocessing raw texts ...')\n    n_docs = len(docs)\n    sentences = []  # sentence level preprocessed\n    token_lists = []  # word level preprocessed\n    idx_in = []  # index of sample selected\n    #     samp = list(range(100))\n    samp = np.random.choice(n_docs, samp_size)\n    for i, idx in enumerate(samp):\n        sentence = preprocess_sent(docs[idx])\n        token_list = preprocess_word(sentence)\n        if token_list:\n            idx_in.append(idx)\n            sentences.append(sentence)\n            token_lists.append(token_list)\n        print('{} %'.format(str(np.round((i + 1) / len(samp) * 100, 2))), end='\\r')\n    print('Preprocessing raw texts. Done!')\n    return sentences, token_lists, idx_in\n\n\n# define model object\nclass Topic_Model:\n    def __init__(self, k=10, method='TFIDF'):\n        \"\"\"\n        :param k: number of topics\n        :param method: method chosen for the topic model\n        \"\"\"\n        if method not in {'TFIDF', 'LDA', 'BERT', 'LDA_BERT'}:\n            raise Exception('Invalid method!')\n        self.k = k\n        self.dictionary = None\n        self.corpus = None\n        #         self.stopwords = None\n        self.cluster_model = None\n        self.ldamodel = None\n        self.vec = {}\n        self.gamma = 15  # parameter for reletive importance of lda\n        self.method = method\n        self.AE = None\n        self.id = method + '_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n\n    def vectorize(self, sentences, token_lists, method=None):\n        \"\"\"\n        Get vecotr representations from selected methods\n        \"\"\"\n        # Default method\n        if method is None:\n            method = self.method\n\n        # turn tokenized documents into a id <-> term dictionary\n        self.dictionary = corpora.Dictionary(token_lists)\n        # convert tokenized documents into a document-term matrix\n        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n\n        if method == 'TFIDF':\n            print('Getting vector representations for TF-IDF ...')\n            tfidf = TfidfVectorizer()\n            vec = tfidf.fit_transform(sentences)\n            print('Getting vector representations for TF-IDF. Done!')\n            return vec\n\n        elif method == 'LDA':\n            print('Getting vector representations for LDA ...')\n            if not self.ldamodel:\n                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n                                                                passes=20)\n\n            def get_vec_lda(model, corpus, k):\n                \"\"\"\n                Get the LDA vector representation (probabilistic topic assignments for all documents)\n                :return: vec_lda with dimension: (n_doc * n_topic)\n                \"\"\"\n                n_doc = len(corpus)\n                vec_lda = np.zeros((n_doc, k))\n                for i in range(n_doc):\n                    # get the distribution for the i-th document in corpus\n                    for topic, prob in model.get_document_topics(corpus[i]):\n                        vec_lda[i, topic] = prob\n\n                return vec_lda\n\n            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n            print('Getting vector representations for LDA. Done!')\n            return vec\n\n        elif method == 'BERT':\n\n            print('Getting vector representations for BERT ...')\n            from sentence_transformers import SentenceTransformer\n            model = SentenceTransformer('bert-base-nli-max-tokens')\n            vec = np.array(model.encode(sentences, show_progress_bar=True))\n            print('Getting vector representations for BERT. Done!')\n            return vec\n\n             \n        elif method == 'LDA_BERT':\n        #else:\n            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n            self.vec['LDA_BERT_FULL'] = vec_ldabert\n            if not self.AE:\n                self.AE = Autoencoder()\n                print('Fitting Autoencoder ...')\n                self.AE.fit(vec_ldabert)\n                print('Fitting Autoencoder Done!')\n            vec = self.AE.encoder.predict(vec_ldabert)\n            return vec\n\n    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n        \"\"\"\n        Fit the topic model for selected method given the preprocessed data\n        :docs: list of documents, each doc is preprocessed as tokens\n        :return:\n        \"\"\"\n        # Default method\n        if method is None:\n            method = self.method\n        # Default clustering method\n        if m_clustering is None:\n            m_clustering = KMeans\n\n        # turn tokenized documents into a id <-> term dictionary\n        if not self.dictionary:\n            self.dictionary = corpora.Dictionary(token_lists)\n            # convert tokenized documents into a document-term matrix\n            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n\n        ####################################################\n        #### Getting ldamodel or vector representations ####\n        ####################################################\n\n        if method == 'LDA':\n            if not self.ldamodel:\n                print('Fitting LDA ...')\n                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n                                                                passes=20)\n                print('Fitting LDA Done!')\n        else:\n            print('Clustering embeddings ...')\n            self.cluster_model = m_clustering(self.k)\n            self.vec[method] = self.vectorize(sentences, token_lists, method)\n            self.cluster_model.fit(self.vec[method])\n            print('Clustering embeddings. Done!')\n\n    def predict(self, sentences, token_lists, out_of_sample=None):\n        \"\"\"\n        Predict topics for new_documents\n        \"\"\"\n        # Default as False\n        out_of_sample = out_of_sample is not None\n\n        if out_of_sample:\n            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n            if self.method != 'LDA':\n                vec = self.vectorize(sentences, token_lists)\n                print(vec)\n        else:\n            corpus = self.corpus\n            vec = self.vec.get(self.method, None)\n\n        if self.method == \"LDA\":\n            lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n                                                     key=lambda x: x[1], reverse=True)[0][0],\n                                    corpus)))\n        else:\n            lbs = self.cluster_model.predict(vec)\n        return lbs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Training\n#from model import *\n#from utils import *\nimport pickle\nimport argparse\nwarnings.filterwarnings('ignore', category=Warning)  # import warnings\n\n\ndef main():  #def model(): #:if __name__ == '__main__':  \n    \n    method = \"BERT\"\n    samp_size = 50000\n    ntopic = 20\n    \n    #parser = argparse.ArgumentParser(description='contextual_topic_identification tm_test:1.0')\n    #parser.add_argument('--fpath', default='/kaggle/working/train.csv')\n    #parser.add_argument('--ntopic', default=10,)\n    #parser.add_argument('--method', default='TFIDF')\n    #parser.add_argument('--samp_size', default=20500)\n    \n    #args = parser.parse_args()\n\n    data = pd.read_csv('/kaggle/working/train.csv')\n    data = data.fillna('')  # only the comments has NaN's\n    rws = data.abstract\n    sentences, token_lists, idx_in = preprocess(rws, samp_size=samp_size)\n    # Define the topic model object\n    #tm = Topic_Model(k = 10), method = TFIDF)\n    tm = Topic_Model(k = ntopic, method = method)\n    # Fit the topic model by chosen method\n    tm.fit(sentences, token_lists)\n    # Evaluate using metrics\n    with open(\"/kaggle/working/{}.file\".format(tm.id), \"wb\") as f:\n        pickle.dump(tm, f, pickle.HIGHEST_PROTOCOL)\n\n    print('Coherence:', get_coherence(tm, token_lists, 'c_v'))\n    print('Silhouette Score:', get_silhouette(tm))\n    # visualize and save img\n    visualize(tm)\n    for i in range(tm.k):\n        get_wordcloud(tm, token_lists, i)\n\n    \nmain()  # the model training now need to take more than 6 hours\n\n### TODO:\n\n# * Implement models.ldamulticore â€“ parallelized Latent Dirichlet Allocation using all CPU cores to parallelize and speed up model training.\n# * Switch from BERT/RoBERTa to SciBERT, BART, and or other models. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Model Different MAB Algorithms for Exploration and Exploitation\n\ntry to get one of paper in many papers and use the MAB algorithms to improve the probability that we can get the related paper.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# class Bandit:  # Reference : https://www.wikiwand.com/en/Algorithms_for_calculating_variance\n#   def __init__(self, m):\n#     \"\"\"\n#     :param m  : the real mean reward of the bandit. (not visible to the algorithm)\n#     \"\"\"\n#     self.m = m   # the real mean reward\n#     self.mean = 0  \n#     self.var = float('inf')  # naive algorithm to calculate the estimated variance \n#     self.N = 0  # arm_count\n#     self.sum = 0\n#     self.sumSq = 0\n#     # self.alpha = 0.5  # win weight/times\n#     # self.beta = 0.5  # lose weight\n\n#   def pull(self):\n#     \"\"\"Generate the random numbers\n#         observes the reward. The reward should be the real mean reward \n#         +/- a random value \n#     \"\"\"\n#     if self. N < 0:\n#       return False\n#     p = np.random.randn(5)\n#     # reward = self.m + np.random.normal(self.N, 1)\n#     reward = self.m + p[0] * 10  # larger variance\n#     return reward\n\n#   def update(self, x):\n#     \"\"\" update the stats\n#     \"\"\"\n#     self.N += 1\n#     self.sum += x\n#     self.sumSq += x * x\n#     self.mean = self.sum / self.N\n#     self.var = (self.sumSq - self.sum * self.sum / self.N) / (self.N - 1)\n\n#     # self.alpha +=  self.mean\n#     # self.beta +=  (1 - self.mean)\n\n#     return x\n\n\n\n# def calculate_delta(T, item, chosen_count):\n#     return 1 if chosen_count[item] == 0 else np.sqrt(2 * np.log(T) / chosen_count[item])\n    \n# def ucb(bandits, n):\n#   \"\"\" select the bandit to pull using upper confident bound\n#   :param bandits - the list of bandit objects to choose from\n#   :param n - the number of repeated experiments\n#   :return j - the index of the bandit to pull\n#   \"\"\"\n#   d = len(bandits)\n#   estimated_rewards = []  # estimated rewards of each bandit\n#   chosen_count = np.zeros(d) \n\n#   for i in bandits:\n#     estimated_rewards.append(i.mean)\n\n#   upper_bound_probs = [estimated_rewards[item] + calculate_delta(n, item, chosen_count) for item in range(d)]\n        \n#   return np.argmax(upper_bound_probs)\n\n\n# def epsilon_greedy(bandits, eps):\n#   \"\"\" select the bandit to pull using epsilon greedy\n#   :param bandits - the list of bandit objects to choose from\n#   :param eps - epsilon probability of random action 0 < eps < 1 (float)\n#   :return j - the index of the bandit to pull\n#   \"\"\"\n#   # Mean reward for each arm\n#   k_reward = []\n#   for i in bandits:\n#     k_reward.append(i.m)\n\n#   p = np.random.rand()\n\n#   if p < eps:\n#     j = np.random.choice(len(bandits))\n#   else:  # greedy action\n#     j = np.argmax(k_reward) # pick up the max meal reward for each bandit\n#   return j\n\n\n# import  pymc\n\n# def thompson_sampling(bandits):\n#   \"\"\" select the bandit to pull using Thompson Sampling\n#   :param bandits - the list of bandit objects to choose from\n#   :param n - the number of repeated experiments\n#   :return j - the index of the bandit to pull\n#   \"\"\"\n#   thetas = []\n#   for i in bandits:\n#     thetas.append(random.normalvariate(i.mean, math.sqrt(i.var)))\n#   return   np.argmax(thetas) \n\n\n# def run_experiment(bandits, eps, N, strategy='epsilon_greedy'):\n#   data = np.empty(N)\n  \n#   for i in range(N):\n#     # epsilon greedy\n#     if strategy == 'epsilon_greedy':\n#       j = epsilon_greedy(bandits, eps)\n#     elif strategy == 'ucb':\n#       j = ucb(bandits, i)\n#     elif strategy == 'thompson_sampling':\n#       j = thompson_sampling(bandits)\n\n#     else:\n#       j = np.random.choice(len(bandits))\n#     x = bandits[j].pull()\n#     bandits[j].update(x)\n\n#     # for the plot\n#     data[i] = x\n#   cumulative_average = np.cumsum(data) / (np.arange(N) + 1)\n#   return cumulative_average\n\n\n# # a =  [Bandit(1.1), Bandit(1.2), Bandit(1.3)]\n# # bandits = [Bandit(1), Bandit(3), Bandit(5)]\n# # c_0 = run_experiment(bandits, 0.0, 1000)\n\n# # bandits = [Bandit(1), Bandit(3), Bandit(5)]\n# # c_1 = run_experiment(bandits, 0.1, 1000)\n\n# # bandits = [Bandit(1), Bandit(3), Bandit(5)]\n# # c_05 = run_experiment(bandits, 0.05, 1000)\n\n# # bandits = [Bandit(1), Bandit(3), Bandit(5)]\n# # c_01 = run_experiment(bandits, 0.01, 1000)\n\n# # bandits = [Bandit(1), Bandit(3), Bandit(5)]\n# # c_1 = run_experiment(bandits, 1, 1000)\n\n# # plt.plot(c_0, label='eps = 0')\n# # plt.plot(c_1, label='eps = 0.1')\n# # plt.plot(c_05, label='eps = 0.05')\n# # plt.plot(c_01, label='eps = 0.01')\n# # plt.plot(c_1, label='eps = 1')\n# # plt.legend()\n# # plt.show()\n\n\n# # bandits = [Bandit(1.1), Bandit(1.2), Bandit(1.3),Bandit(1.4), Bandit(1.5), Bandit(1.6), Bandit(1.7), Bandit(1.8), Bandit(1.9)]\n# # c_eps = run_experiment(bandits, 0.05, 1000, strategy='epsilon_greedy')\n\n# # bandits = [Bandit(1.1), Bandit(1.2), Bandit(1.3),Bandit(1.4), Bandit(1.5), Bandit(1.6), Bandit(1.7), Bandit(1.8), Bandit(1.9)]\n# # c_ucb = run_experiment(bandits, 0.05, 1000, strategy='ucb')\n\n# # bandits = [Bandit(1.1), Bandit(1.2), Bandit(1.3),Bandit(1.4), Bandit(1.5), Bandit(1.6), Bandit(1.7), Bandit(1.8), Bandit(1.9)]\n# # c_tp = run_experiment(bandits, 0.05, 1000, strategy='thompson_sampling')\n\n# # plt.plot(c_eps, label='eps = 0.05')\n# # plt.plot(c_ucb, label='ucb')\n# # plt.plot(c_tp, label='thompson sampling')\n# # plt.legend()\n# # plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model KMeans","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Manual list for highlighting\n# Need to look at automated / updateable approach to identifying these\nrisk_factors = [\n    'diabetes',\n    'hypertension',\n    'smoking',\n    'cardiovascular disease',\n    'chronic obstructive pulmonary disease',\n    'cerebrovascular disease',\n    'kidney disease',\n    ' age ',\n    ' aged',\n    'blood type',\n    'hepatitis',\n    ' male ',\n    ' female ',\n    ' males ',\n    ' females ',\n    'arrhythmia',\n    ' sex ',\n    ' gender ',\n    'acute respiratory distress syndrome',\n    'sepsis shock',\n    'cardiac injury',\n    'acute kidney injury',\n    'liver dysfunction',\n    'gastrointestinal haemorrhage',\n    'conjunctivitis',\n    'comorbidity',\n    'comorbidities',\n    'co-morbidity',\n    'co-morbidities',\n    ' smoker',\n    'non-smoker'\n]\n\n# Reference: https://www.kaggle.com/maksimeren/covid-19-literature-clustering#Unsupervised-Learning:-Clustering-with-K-Means\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.feature_extraction.text import HashingVectorizer\n\n# hash vectorizer instance\nhvec = HashingVectorizer(lowercase=False, analyzer=lambda l:l, n_features=2**12)\nn_gram_all = []\n\ndict = pd.read_csv('/kaggle/working/train.csv').fillna('')  # only the comments has NaN's\ndictlist = key = value = []\nfor key, value in dict.items():\n    temp = [key,value]\n    dictlist.append(temp)\n    \nfor word in key:\n    # get n-grams for the instance\n    n_gram = []\n    for i in range(len(word)-2+1):\n        n_gram.append(\"\".join(word[i:i+2]))\n    n_gram_all.append(n_gram)\n    \n# features matrix X\nX = hvec.fit_transform(n_gram_all)\n\nfrom sklearn.model_selection import train_test_split\n\n# test set size of 20% of the data and the random seed 42 <3\nX_train, X_test = train_test_split(X.toarray(), test_size=0.2, random_state=42)\n\nprint(\"X_train size:\", len(X_train))\nprint(\"X_test size:\", len(X_test), \"\\n\")\n\nk = 7 \nkmeans = KMeans(n_clusters=k, n_jobs=4, verbose=10)\ny_pred = kmeans.fit_predict(X_train)\n\n# add labels\ny_train = y_pred  # Labels for the training set:\ny_test = kmeans.predict(X_test)  # Labels for the test set:","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne = TSNE(verbose=1, perplexity=5)   # Dimensionality Reduction with t-SNE\nX_embedded = tsne.fit_transform(X_train)\n\n# plot the t-SNE. scatterplot again and see if we have any obvious clusters after we have labels\n# sns settings \nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"bright\", len(set(y_pred)))\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title(\"t-SNE Covid-19 Articles - Clustered\")\n# plt.savefig(\"plots/t-sne_covid19_label.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"bright\", len(set(y_pred)))\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title(\"t-SNE Covid-19 Articles - Clustered\")\n# plt.savefig(\"plots/t-sne_covid19_label.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch7\"></a>\n# Step 7: Evaluate Model Performance","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"after comaring three different models, we pick up the Model Deep Dive","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch8\"></a>\n# Step 8: Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**If someone in Kaggle can see:**\n    \nIt take me a lot time to run the different model, one draft session can only run 9 hours, and will only \nsave the session 1 hour after disconnected. So every time failed, it cost me a lot of time. \n\nAnd it will restart after it execced the memory limit, which will lost all the draft session. \n\nFeel the document for GPU and TPU are not friendly especially for beginners.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## export the result to file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# export to .csv file\n# risk_factors_df = pd.DataFrame(csv_data)\n# risk_factors_df.to_csv('risk_factors.csv', index=False)\n# risk_factors_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch9\"></a>\n# Step 9: Appendix","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nEvidence of an impact of legislative bans on smoking prevalence and tobacco consumption is inconsistent, with some studies not detecting additional longâ€term change in existing trends in smoking prevalence or impact on respiratory and perinatal health outcomes (PMC_ID: PMC6486282). Coronavirus infection should be considered in differential diagnosis of respiratory tract illness in adults including those that require hospitalization, have a history of smoking and have pulmonary comorbidities (PMC_ID: PMC5631947). Pulmonary function testing procedures have been associated with an increasing risk of COVID-19 transmission among patients/subjects and medical staffs. It is strongly recommended to suspend the pulmonary function test for the confirmed or suspected cases of COVID-19 during the contagious stage, and to postpone the test for other patients if it is not imperative. H1N1 patients coinfected with rhinovirus had less severe disease while non-rhinovirus co-infections were associated with substantially higher severity without changes in influenza viral titer (PMC_ID: PMC3153592). There was a significant decrease in the duration of influenza A viral shedding in co-infections (of any respiratory viruses) compared to single infections (PMC_ID: PMC6625191). Respiratory viruses rarely reach their epidemic peak concurrently and there appears to be a negative association between infection with one respiratory virus and co-infection with another (PMC_ID: PMC6625191). Co-detections of specific clusters of viruses were observed in 9% of acute respiratory illness cases particularly in children, were less frequent in households without children, and were less symptomatic (e.g., lower fever) than single infections (PMC_ID: PMC4344779). Most frequently co-detected viruses were coronavirus, respiratory syncytial virus, and influenza A virus (PMC_ID: PMC4344779). Co-infection was less likely with increasing age, which may be a consequence of pre-existing immunity or decreased viral shedding with increasing age (PMC_ID: PMC6625191). Infection by rhinovirus may result in temporary immunity of the host to infection by other respiratory viruses due to the production of cytokines, thus resulting in a negative association between rhinovirus infection and co-infection with another virus (PMC_ID: PMC6625191). So far, no maternal deaths have been reported for women with COVID-19. There appears to be some risk of premature rupture of membranes, preterm delivery, fetal tachycardia and fetal distress when the infection occurs in the third trimester of pregnancy. There is no evidence suggesting transplacental transmission based on very limited data, as the analysis of amniotic fluid, cord blood, neonatal throat swab, and breast milk samples available from six of the nine patients were found to be negative for COVID-19. Whether virus shedding occurs vaginally is also not known. In general, the clinical characteristics of the pregnant women with COVID-19 pneumonia were similar to those of non-pregnant adult patients who developed COVID-19 pneumonia. Another study reported that the impact of COVID-19 infection on pregnant women appears to be less severe. Both SARS coronavirus and H7N9 viruses presented a global epidemic threat, but the social and economic impacts of H7N9 were not as serious as in the case of SARS because the response to H7N9 was more effective (PMC_ID: PMC6046118). A mathematical model for MERS-CoV transmission dynamics has been used to estimate the transmission rates in two periods due to the implementation of intensive interventions (PMC_ID: PMC4776270). The serial interval of COVID-19 is close to or shorter than its median incubation period. This suggests that a substantial proportion of secondary transmission may occur prior to illness onset. The COVID-19 serial interval is also shorter than the serial interval of severe acute respiratory syndrome (SARS), indicating that calculations made using the SARS serial interval may introduce bias. Among patients infected with a respiratory virus, risk of hospitalization was higher among underweight adult patients, and obese patients had a longer mean length of stay once hospitalized (PMC_ID: PMC6809817). Underweight children were not at increased risk of hospitalization (PMC_ID: PMC6809817). For MERS-CoV, increased age and underlying comorbidity were risk factors for both death and severe disease, while cases arising in Saudi Arabia were more likely to be severe (PMC_ID: PMC5023790). There is no significant association between alleles or genotypes of the MASP2 tagSNP and susceptibility to SARS-CoV in both Beijing and Guangzhou populations (PMC_ID: PMC2683852). MASP2 polymorphisms is not directly related to SARS-CoV susceptibility in northern and southern Chinese (PMC_ID: PMC2683852). Strong infection control and response measures were effective in controlling the outbreak of MERS-CoV in the Republic of Korea (PMC_ID: PMC4957609). Two of the most effective policy procedures to prevent infections of SARS-CoV would be to apply stringent precautionary measures and to impose quicker and more effective quarantine of the exposed populace (PMC_ID: PMC1732706).\n\n\nDiscussion: Potential risk factors include pulmonary comorbidities and age. It is still not clear whether coinfection with other viruses or pregnancy is a risk factor of COVID_19. Based on our results for other tasks, health-care workers also have a higher risk of infection due to close contact with patients.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch10\"></a>\n# Acknowledgements and Reference\n\n- This notebook is mainly contributed by the [littlesanner](https://www.kaggle.com/littlesanner), [HangkaiWang](https://www.kaggle.com/hangkaiwang) and [maysa](https://www.kaggle.com/maysawittayanontawet).\n\n#### Cite:  \n1. [COVID-19 Literature Clustering | Kaggle](https://www.kaggle.com/littlesanner/covid-19-literature-clustering/edit) \n1. [COVID EDA: Initial Exploration Tool | Kaggle](https://www.kaggle.com/ivanegapratama/covid-eda-initial-exploration-tool)<br>\n1. [Mining COVID-19 scientific papers | Kaggle](https://www.kaggle.com/mobassir/mining-covid-19-scientific-papers)\n1. [Topic Modeling BERT+LDA | Kaggle](https://www.kaggle.com/dskswu/topic-modeling-bert-lda)\n1. [Contextual Topic Identification | Medium](https://blog.insightdatascience.com/contextual-topic-identification-4291d256a032)\n1. [COVID-19 what is risk? | Kaggle](https://www.kaggle.com/nishimoto/covid-19-what-is-risk)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}