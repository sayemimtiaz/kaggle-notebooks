{"cells":[{"metadata":{},"cell_type":"markdown","source":"### About the data\n\nThis dataset comprises of sales transactions captured at a retail store. It’s a classic dataset to explore and expand your feature engineering skills and day to day understanding from multiple shopping experiences. This is a regression problem.The idea and dataset is taken from AnalyticsVidhya where the project is a part of a hackathon.\n\n### Data Description\nVariable\t:                Definition\n\nUser_ID\t:                    User ID\n\nProduct_ID :                 Product ID\n\nGender :                     Sex of User\n\nAge\t :                       Age in bins\n\nOccupation  :                Occupation (Masked)\n\nCity_Category  :             Category of the City (A,B,C)\n\nStay_In_Current_City_Years:\tNumber of years stay in current city\n\nMarital_Status:\t            Marital Status\n\nProduct_Category_1:\t        Product Category (Masked)\n\nProduct_Category_2\t:        Product may belongs to other category also (Masked)\n\nProduct_Category_3:\t        Product may belongs to other category also (Masked)\n\nPurchase\t:                Purchase Amount (Target Variable)"},{"metadata":{},"cell_type":"markdown","source":"### Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/black-friday/test.csv\")\nsales = pd.read_csv(\"../input/black-friday/train.csv\")\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for submission\n\nsubmission = pd.DataFrame()\nsubmission['Purchase'] = []\nsubmission['User_ID'] = test['User_ID']\nsubmission['Product_ID'] = test['Product_ID']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sales.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the data, we can conclude that our set possesses 12 different parameters: 7 numerical (integer and float) and 5 object variables.\n\nLooking into the summary statistics for these 7numercal features"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 12 features, looking into each of these features:\n    \n1. User ID: Each user has been provided a unique ID. Lets see how many unique users we have in our dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.User_ID.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 5891 unique users in our dataset and none of the value in this feature is null"},{"metadata":{},"cell_type":"markdown","source":"2. Product_ID: Each product that is available for sales has a specific/ unique product id associated with it. Lets look into the number of unique products available for sale."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.Product_ID.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there are a total of 3631 products available for sales."},{"metadata":{},"cell_type":"markdown","source":"3. Gender: Gender is a categorical variables with 2 categries: Male(M) and Female(F)."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.Gender.value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values in this feature and Males constitute 75% of the data."},{"metadata":{},"cell_type":"markdown","source":"4. Age: Age is again a categorical data with age divide in particular range."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.Age.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The age is divide in 7 categories i.e 0-17,18-25,26-35,36-45,46-50,51-55,55+ . The bins size here is varing."},{"metadata":{},"cell_type":"markdown","source":"5. Occupation: The Occupation number is the ID number of occupation type of each customer. We can see that around 21 different occupations exist."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.Occupation.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. City_Category : The city has been categorised into 3 categories i.e A,B,C."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.City_Category.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7. Stay_In_Current_City_Years : This depects the numbers of year from which a person is residing in that particular city. It has been divided into 5 categories "},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.Stay_In_Current_City_Years.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"8. Marital_Status: This features shows if a person is married or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.Marital_Status.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The products have been categorised into three categories with represent three different features:\n    \n9. Product_Category_1 \n    \n10. Product_Category_2 \n    \n11. Product_Category_3"},{"metadata":{},"cell_type":"markdown","source":"12. Purchase: This is our final feature which is our dependent variables whose value we want to predict, the purchase amount. It is a contionous variable so this makes it a regression problem."},{"metadata":{},"cell_type":"markdown","source":"### Missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets combine the data for data prep\n\ntest['Purchase']=np.nan\nsales['data']='train'\ntest['data']='test'\ntest=test[sales.columns]\ncombined=pd.concat([sales,test],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"combined.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.isna().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that there are 2 features which contain the missing values i.e Product_Category_2 and Product_Category_3."},{"metadata":{"trusted":true},"cell_type":"code","source":"#percent of missing data relevant to all data\npercent = (sales.isnull().sum()/sales.isnull().count()).sort_values(ascending=False)\npercent[[0,1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The feature Product_Category_3 has 70% data missing so imputing this much data is not feasible , so it is better to drop this feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.drop('Product_Category_3',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The feature Product_Category_2 has 30% data missing so we can impute values into this using an appromiate method."},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.Product_Category_2.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are almost 18 categories in which these Product_Category_2 is divided ,imputing the mean value does make sense because that gives a decimal term 9.8 which is not a product category here. So, there are 2 possible ways median or mode."},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputed missing values with random values in the same probability distribution as given feature already had\n\nvc = combined.Product_Category_2.value_counts(normalize = True)\nmiss = combined.Product_Category_2.isna()\ncombined.loc[miss, 'Product_Category_2'] = np.random.choice(vc.index, size = miss.sum(), p = vc.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.Product_Category_2.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values left in the data set.The purchase null values are because of the test data that needs to be predicted"},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#using the train data part from combined dataset for eda\n\nsales_1 = combined[combined['data']=='train']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Univariate Analysis:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(sales_1['Gender'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph shows that there are almost 3 times more male customers than female customers."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(sales_1['Age'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph shows that the majority of the customers that purchase things during the sales season mainly belong to the age group of 26-35 and 36-45."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(sales_1['Occupation'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph shows that top three Occupations with the majority of buyers is 4,0,7."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(sales_1['City_Category'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph shows that people from city B buy majorly during the sale  "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(sales_1['Stay_In_Current_City_Years'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph shows that majority people buying during sales have lived in the current city for an year."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(sales_1['Marital_Status'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graphs shows that single people tend to buy more things during sales."},{"metadata":{},"cell_type":"markdown","source":"#### Bivariate Analysis / Multivariate Analysis:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avearge amount spend by different age groups\n\ndata = sales_1.groupby('Age')['Purchase'].mean()\nplt.plot(data.index,data.values,marker='o',color='g')\nplt.xlabel('Age group');\nplt.ylabel('Average_Purchase amount in $');\nplt.title('Age group vs average amount spent');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average amount spend by age group 51-55 is most during the festive season sales."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avearge amount spend based on the time of stay in the current city\n\ndata = sales_1.groupby('Stay_In_Current_City_Years')['Purchase'].mean()\nplt.plot(data.index,data.values,marker='o',color='y')\nplt.xlabel('Stay_In_Current_City_Years');\nplt.ylabel('Average_Purchase amount in $');\nplt.title('Stay_In_Current_City_Years vs average amount spent');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The people who have been living in the current city for 2 or more years are on an average spending more amount in the black friday sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avearge purchase based on Marital_Status\n\ndata = sales_1.groupby('Marital_Status')['Purchase'].mean()\nplt.bar(data.index,data.values)\nplt.xlabel('Marital_Status');\nplt.ylabel('Average_Purchase amount in $');\nplt.title('Avearge purchase based on Marital_Status');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Purchasers who married or not, have almost same average of purchase."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 10 products which made the highest sales\n\ndata = sales_1.groupby(\"Product_ID\").sum()['Purchase']\n\nplt.figure(figsize=(10,5))\ndata.sort_values(ascending=False)[0:10].plot(kind='bar')\nplt.xticks(rotation=90)\nplt.xlabel('Product ID')\nplt.ylabel('Total amount purchased in Million $')\nplt.title('Top 10 Products with highest sales')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#comparing based on Marital_Status and Gender\n\nsns.countplot(x='Marital_Status',data=sales_1,hue='Gender')\nplt.title('Comparing based on Marital_Status and Gender')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Males tend to purchase more and Unmarried Males are around 45% in the data and they show to purchase 9000$ on average."},{"metadata":{},"cell_type":"markdown","source":"Products that are most purchased by each of the age group:"},{"metadata":{"trusted":true},"cell_type":"code","source":"a =pd.crosstab(sales_1['Age'],sales_1['Product_ID'])\na.idxmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Occupations and City Category\n\nplt.figure(figsize=(15,5))\nsns.countplot(x='Occupation',data=sales_1,hue='City_Category')\nplt.title('Comparing Occupations and City Category')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People from Occupation 4,0,7 are buying the most and most of the people from these occupations belong to City_Category B."},{"metadata":{"trusted":true},"cell_type":"code","source":"#the purchase habits of different genders across the different city categories.\n\ng = sns.FacetGrid(sales_1,col=\"City_Category\")\ng.map(sns.barplot, \"Gender\", \"Purchase\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For City_categories B and C, Males tend to dominate the purchasing, whereas it is the opposite for City Category_C, where Females tend to puchase more than men."},{"metadata":{},"cell_type":"markdown","source":"### Data preprocessing."},{"metadata":{"trusted":true},"cell_type":"code","source":"# for datapreprocessing again working with the combined dataset\ncombined.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. User_ID and Product_ID: "},{"metadata":{"trusted":true},"cell_type":"code","source":"# User_ID data preprocess. e.g. 1000002 -> 2\n\ncombined['User_ID'] = combined['User_ID'] - 1000000\n\n# Product_ID preprocess e.g. P00069042 -> 69042\n\ncombined['Product_ID'] = combined['Product_ID'].str.replace('P00', '')\n\n#object to int\ncombined['Product_ID'] = pd.to_numeric(combined['Product_ID'],errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Product_Category_2 :\n\nAll the unique values in product category 2 are integers. But the data type shown in info is float so we can change it by converting the numbers in float to integers."},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.Product_Category_2 = combined.Product_Category_2.astype('int64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# features with datatype object\n\ncat_cols = combined.select_dtypes(['object']).columns\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Stay_In_Current_City_Years\n\nFor Stay in current city years we need to convert the object datatype to int.\nIt contains a category which has '4+' that needs to be altered."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4+ to 4\ncombined['Stay_In_Current_City_Years'] =np.where(combined['Stay_In_Current_City_Years'].str[:2]==\"4+\",4,combined['Stay_In_Current_City_Years'])\n\n#object to int\ncombined['Stay_In_Current_City_Years'] = pd.to_numeric(combined['Stay_In_Current_City_Years'],errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Gender: \n\n    Gender 'F' for female are represented by the value fo 0.\n\n    Gender 'M' for male are represented by the value fo 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Gender'] = combined['Gender'].map({'F':0, 'M':1}).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. Age"},{"metadata":{},"cell_type":"raw","source":"from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\ncombined[\"Age\"] = lb_make.fit_transform(combined[\"Age\"])\ncombined['Age'].value_counts()\n\n#this can be used but I preferred the mean of the ages for each grp"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modify age column\n\ncombined['Age'] = combined['Age'].map({'0-17': 9,\n                               '18-25': 22,\n                               '26-35': 31,\n                               '36-45': 42,\n                               '46-50': 48,\n                               '51-55': 53,\n                               '55+': 60})\ncombined['Age'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6. City_Category : dummy variables for this feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"combined = pd.get_dummies(combined,columns=['City_Category'],drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting the data back into train and test as it was already provided\n\nsales = combined[combined['data']=='train']\ndel sales['data']\ntest_input = combined[combined['data']=='test']\ntest_input.drop(['Purchase','data'],axis=1,inplace=True)\n\ndel combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heatmap to show the correlation between various variables of the train data set\n\nplt.figure(figsize=(12, 5))\ncor = sales.corr()\nax = sns.heatmap(cor,annot=True)\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variables which show a significant correlation in the data are:\n\n1. Marital_status and Age\n2. Product_Category_1 and Purchase\n3. City_Category_B and City_category_A"},{"metadata":{},"cell_type":"markdown","source":"### Model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting the data into X and y\nX = sales.drop('Purchase',axis=1)\ny = sales['Purchase']\n\n#train test split for model building\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"#standardization\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\n\nX_test = sc.fit_transform(X_test)\n\ntest_input = sc.fit_transform(test_input)\n\n#used the standardized outputs for Linear Regression and Ridge Regression but did not create much difference"},{"metadata":{},"cell_type":"markdown","source":"LinearRegression :\n    \nLinearRegression fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linear regression\n\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train,y_train) # training the algorithm\n\n# Getting the coefficients and intercept\n\nprint('coefficients:\\n', lr.coef_)\nprint('\\n intercept:', lr.intercept_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting on the test data\n\ny_pred = lr.predict(X_test)\n\nfrom sklearn import metrics\n\nprint('r2_score:', metrics.r2_score(y_test,y_pred)) \nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"# for submission\n\n#submitted with basic linear regression model\n\nlin_reg= LinearRegression()\nlin_reg.fit(X,y)\npredict = lin_reg.predict(test_input)\n\nsubmission['Purchase'] = predict\n\nsubmission.to_csv('Sample_Submission_3.csv',index=False)\n\nTried models:\n\nlin_reg = LinearRegression() # rmse = 5238.936"},{"metadata":{},"cell_type":"markdown","source":"The score generated with the Linear Regression model was very low so used Regularized Linear model i.e Ridge Regression\n\nRidge Regression: This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ridge Regression\n\nfrom sklearn.linear_model import Ridge\n\nRR = Ridge(alpha=0.05,normalize=True)\nRR.fit(X_train, y_train)\n\ny_pred = RR.predict(X_test)\n\nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"RR = Ridge(alpha=0.05,normalize=True)\nRR.fit(X,y)\npredict = RR.predict(test_input)\n\nsubmission['Purchase'] = predict\n\nsubmission.to_csv('Sample_Submission_ridge.csv',index=False)\n\n#Tried model score: 4670"},{"metadata":{},"cell_type":"markdown","source":"Linear Regressiom models were not giving that much improvement so tried non linear regression models.\n\nDecision Tree: "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree Model\n\nfrom sklearn.tree import DecisionTreeRegressor\nDT = DecisionTreeRegressor(max_depth=15, min_samples_leaf=100)\n\nDT.fit(X_train, y_train)\n\ny_pred = DT.predict(X_test)\n\nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"DT = DecisionTreeRegressor(max_depth=15, min_samples_leaf=100)\n\nDT.fit(X,y)\n\npredict = DT.predict(test_input)\n\nsubmission['Purchase'] = predict\nsubmission.to_csv('Sample_Submission_DT.csv',index=False)\n\n#score: 2743.697"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree 2\n\nDT2 = DecisionTreeRegressor(max_depth=8, min_samples_leaf=150)\n\nDT2.fit(X_train, y_train)\n\ny_pred = DT2.predict(X_test)\n\nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"DT2 = DecisionTreeRegressor(max_depth=8, min_samples_leaf=150)\n\nDT2.fit(X,y)\n\npredict = DT2.predict(test_input)\n\nsubmission['Purchase'] = predict\nsubmission.to_csv('Sample_Submission_DT2.csv',index=False)\n\nscore: 2892.200"},{"metadata":{},"cell_type":"markdown","source":"Random Forest Regressor: \n\nRandom Forest is an ensemble machine learning algorithm that follows the bagging technique. The base estimators in random forest are decision trees.It randomly selects a set of features which are used to decide the best split at each node of the decision tree."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the model\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state = 3,max_depth=10,n_estimators=25)\n\nrf.fit(X_train,y_train)\n\ny_pred = rf.predict(X_test)\n\nprint('r2_score:', metrics.r2_score(y_test,y_pred)) \nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"# for submission\n\n#submitted with Random Forest model\n\nrf = RandomForestRegressor(random_state = 3,max_depth=10,n_estimators=25)\nrf.fit(X,y)\npredict = rf.predict(test_input)\n\nsubmission['Purchase'] = predict\n\nsubmission.to_csv('Sample_Submission_rf.csv',index=False)\n\n#Tried models: \nRandom Forest score : 6190"},{"metadata":{"trusted":true},"cell_type":"code","source":"# another random forest\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf3 = RandomForestRegressor(random_state=3,max_depth=10,min_samples_split=500,oob_score=True)\n\n\nrf3.fit(X_train,y_train)\n\ny_pred = rf3.predict(X_test)\n\nprint('r2_score:', metrics.r2_score(y_test,y_pred)) \nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"rf3 = RandomForestRegressor(random_state=3,max_depth=10,min_samples_split=500,oob_score=True)\nrf3.fit(X,y)\npredict = rf3.predict(test_input)\n\nsubmission['Purchase'] = predict\n\nsubmission.to_csv('Sample_Submission_rf3.csv',index=False)\n\nSCORE: 2817.313"},{"metadata":{"trusted":true},"cell_type":"code","source":"# random forest 4\n\nrf4 = RandomForestRegressor(n_estimators=30,random_state=3,max_depth=15,min_samples_split=100,oob_score=True)\n\n\nrf4.fit(X_train,y_train)\n\ny_pred = rf4.predict(X_test)\n\nprint('r2_score:', metrics.r2_score(y_test,y_pred)) \nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"rf4 = RandomForestRegressor(n_estimators=30,random_state=3,max_depth=15,min_samples_split=100,oob_score=True)\nrf4.fit(X,y)\npredict = rf4.predict(test_input)\n\nsubmission['Purchase'] = predict\nsubmission.to_csv('Sample_Submission_rf4.csv',index=False)\n\nSCORE:2708.849"},{"metadata":{},"cell_type":"markdown","source":"ExtraTreesRegressor : \n\nThe main difference between random forests and extra trees (usually called extreme random forests) lies in the fact that, instead of computing the locally optimal feature/split combination (for the random forest), for each feature under consideration, a random value is selected for the split (for the extra trees)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting the model\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nrf = ExtraTreesRegressor()\n\nrf.fit(X_train,y_train)\n\ny_pred = rf.predict(X_test)\n\nprint('r2_score:', metrics.r2_score(y_test,y_pred)) \nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"ER = ExtraTreesRegressor()\nER.fit(X,y)\npredict = ER.predict(test_input)\n\nsubmission['Purchase'] = predict\n\nsubmission.to_csv('Sample_Submission_ER.csv',index=False)\n\n#SCORE: 2847.775"},{"metadata":{},"cell_type":"markdown","source":"XG BRegressor :\n    \nXGBoost (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as ‘regularized boosting‘ technique."},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoost Model1\nfrom xgboost import XGBRegressor\n\n\nxgb1 = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n\nxgb1.fit(X_train,y_train)\n\ny_pred = xgb1.predict(X_test)\n\nprint('r2_score:', metrics.r2_score(y_test,y_pred)) \nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"xgb1 = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nxgb1.fit(X,y)\npredict = xgb1.predict(test_input)\n\nsubmission['Purchase'] = predict\n\nsubmission.to_csv('Sample_Submission_XGB1.csv',index=False)\n\nScore: 2591.5417"},{"metadata":{"trusted":true},"cell_type":"code","source":"## XGBoost2\nfrom xgboost import XGBRegressor\n\nxgb2 = XGBRegressor(n_estimators=500,max_depth=10,learning_rate=0.05)\n\nxgb2.fit(X_train,y_train)\n\ny_pred = xgb2.predict(X_test)\n\nprint('r2_score:', metrics.r2_score(y_test,y_pred)) \nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"xgb2 = XGBRegressor(n_estimators=500,max_depth=10,learning_rate=0.05)\n\nxgb2.fit(X,y)\npredict = xgb2.predict(test_input)\n\nsubmission['Purchase'] = predict\n\nsubmission.to_csv('Sample_Submission_XGB2.csv',index=False)\n\nscore: 2522.743"},{"metadata":{"trusted":true},"cell_type":"code","source":"## XGBoost3\n\nxgb3 = XGBRegressor(n_estimators=6,max_depth=500)\n\nxgb3.fit(X_train,y_train)\n\ny_pred = xgb3.predict(X_test)\n\nprint('r2_score:', metrics.r2_score(y_test,y_pred)) \nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"xgb3 = XGBRegressor(n_estimators=6,max_depth=500)\n\nxgb3.fit(X,y)\npredict = xgb3.predict(test_input)\n\nsubmission['Purchase'] = predict\n\nsubmission.to_csv('Sample_Submission_XGB3.csv',index=False)\n\nscore: 3081.743"},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoost4\n\nxgb4 = XGBRegressor(learning_rate=1.0, max_depth=6, min_child_weight=40, seed=0)\n\nxgb4.fit(X_train,y_train)\n\ny_pred = xgb4.predict(X_test)\n\nprint('r2_score:', metrics.r2_score(y_test,y_pred)) \nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"xgb4 = XGBRegressor(learning_rate=1.0, max_depth=6, min_child_weight=40, seed=0)\n\nxgb4.fit(X,y)\npredict = xgb4.predict(test_input)\n\nsubmission['Purchase'] = predict\n\nsubmission.to_csv('Sample_Submission_XGB4.csv',index=False)\n\n#score: 2584.6452"},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoost5\nfrom xgboost import XGBRegressor\n\nxgb5 = XGBRegressor(n_estimators=450,max_depth=8,learning_rate=0.076)\n\nxgb5.fit(X_train,y_train)\n\ny_pred = xgb5.predict(X_test)\n\nprint('r2_score:', metrics.r2_score(y_test,y_pred)) \nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"xgb5 = XGBRegressor(n_estimators=450,max_depth=8,learning_rate=0.076)\n\nxgb5.fit(X,y)\npredict = xgb5.predict(test_input)\n\nsubmission['Purchase'] = predict\n\nsubmission.to_csv('Sample_Submission_XGB5.csv',index=False)\nscore:\t2547.915"},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoost6\nfrom xgboost import XGBRegressor\n\nxgb6 = XGBRegressor(n_estimators=470,max_depth=9,learning_rate=0.06)\n\nxgb6.fit(X_train,y_train)\n\ny_pred = xgb6.predict(X_test)\n\nprint('r2_score:', metrics.r2_score(y_test,y_pred)) \nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"xgb6 = XGBRegressor(n_estimators=470,max_depth=9,learning_rate=0.06)\n\nxgb6.fit(X,y)\npredict = xgb6.predict(test_input)\n\nsubmission['Purchase'] = predict\n\nsubmission.to_csv('Sample_Submission_XGB6.csv',index=False)\n\nscore:2532.73"},{"metadata":{},"cell_type":"markdown","source":"Light GBM:\n\nLight GBM beats all the other algorithms when the dataset is extremely large. Compared to the other algorithms, Light GBM takes lesser time to run on a huge dataset.LightGBM is a gradient boosting framework that uses tree-based algorithms and follows leaf-wise approach while other algorithms work in a level-wise approach pattern"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\n\nlgbm1 = LGBMRegressor(n_estimators=500,max_depth=10,learning_rate=0.05)\n\nlgbm1.fit(X_train,y_train)\n\ny_pred = lgbm1.predict(X_test)\n\nprint('r2_score:', metrics.r2_score(y_test,y_pred)) \nprint('rmse:', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n\nComparing all the models, we conclude that the XGBRegressor model is the best model to be able to predict purchase amount from our dataset.\n\nParameters and score: \n\nXGBRegressor(n_estimators=500,max_depth=10,learning_rate=0.05)\n\nr2_score: 0.7492767237638949\n\nrmse: 2518.284905633662\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf1 = df.head(25)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Importance\n\nimp = pd.DataFrame(xgb2.feature_importances_,index=X.columns,columns=['importance'])\nimp.sort_values(by='importance',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.plot(kind='bar',figsize=(10,8))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df1.Predicted,df1.Actual)\nplt.plot(y_pred,y_pred,'r')\nplt.xlabel('y predicted')\nplt.ylabel('y actual')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}