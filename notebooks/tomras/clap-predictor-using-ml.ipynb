{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"In this notebook I will try to perform data analysis and evaluate most popular ML algorithms for clap prediction task."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\nfrom nltk.corpus import stopwords\nimport string\nimport re\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8cf28a2bdfad46dd1319b4b4ea56d7f296bff34"},"cell_type":"markdown","source":"First of all let's read data and analyze data types."},{"metadata":{"trusted":true,"_uuid":"e177c0089aa373b65386d5b9ebb8a0ea663aa600"},"cell_type":"code","source":"df = pd.read_csv('../input/articles.csv')\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b23d79b1218b6b54aa0130363b63ee66301f9dee"},"cell_type":"markdown","source":"It seems that claps data are in wrong format. Need to fix that."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"fc88dd8522bd052d54446690f095cbeaf87f02e6"},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6862654a6ea5e6bf37be04da943836fbc47a7d0","scrolled":true},"cell_type":"code","source":"df['claps'] = df['claps'].apply(lambda x: int(float(x[:-1]) * 1000) if x[-1] == 'K' else int(x))\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f602339e83e03ad897105990cbb5cc2c2ecbf01"},"cell_type":"markdown","source":"Lets look if there are any NaN values."},{"metadata":{"trusted":true,"_uuid":"fd3e2c36d27bec5bd811cd1668c3c0400044aecf"},"cell_type":"code","source":"df.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b5423830b6c4b62c7f08729447817b821732ddb"},"cell_type":"markdown","source":"There are no NaN values in this data set so I am moving to the next step: feature engineering. I will add few more fields to my pandas data frame: len_title, len_text, title_clean, text_clean, len_title_clean, len_text_clean. I think those fields are self explainable. What is more before cleaning my text data I will change all text to lower case. After all I will combine author, title and text fields to single column. "},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"96b756c65b93a56fca76ffd24344a43ccaccea04"},"cell_type":"code","source":"df['title_len'] = df['title'].str.len()\ndf['text_len'] = df['text'].str.len()\n\ndf['title'] = df['title'].apply(lambda x: x.lower())\ndf['text'] = df['text'].apply(lambda x: x.lower())\ndf['author'] = df['author'].apply(lambda x: x.lower())\n\ndf['title_clean'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\ndf['text_clean'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\n\ndf['title_clean'] = df['title_clean'].apply(lambda x: re.sub('[' + string.punctuation + 'â€”]', '', x))\ndf['text_clean'] = df['text_clean'].apply(lambda x: re.sub('[' + string.punctuation + 'â€”]', '', x))\n\ndf['title_clean'] = df['title_clean'].apply(lambda x: x.translate(str.maketrans('', '', string.digits)))\ndf['text_clean'] = df['text_clean'].apply(lambda x: x.translate(str.maketrans('', '', string.digits)))\n\ndf['title_clean'] = df['title_clean'].apply(lambda x: re.sub(' +', ' ', x))\ndf['text_clean'] = df['text_clean'].apply(lambda x: re.sub(' +', ' ', x))\n\ndf['title_clean_len'] = df['title_clean'].str.len()\ndf['text_clean_len'] = df['text_clean'].str.len()\n\ndf['full_text'] = df['author'] + ' ' + df['title_clean'] + ' ' + df['text_clean']\n\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea741c8475ab16164fcc9f6b6ad35aea07256502"},"cell_type":"markdown","source":"Now I am going to remove unnecessary data columns and data will be ready for analysis. Lets do that."},{"metadata":{"trusted":true,"_uuid":"9a38d58f1bdcc3e81412a04f111bf4303d859d10","scrolled":true},"cell_type":"code","source":"df = df.drop('link', axis=1)\ndf = df.drop('text', axis=1)\ndf = df.drop('title', axis=1)\ndf = df.drop('title_clean', axis=1)\ndf = df.drop('text_clean', axis=1)\ndf = df.drop('author', axis=1)\n\ndf = df.drop_duplicates()\n\ndf.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"801c5254cafac2bafe12b8d68e9eb28ec98175df"},"cell_type":"markdown","source":"By looking at the data it seems we have some outliers, lets plot blox plot and check them out."},{"metadata":{"trusted":true,"_uuid":"80a15ace2a6cc9fc3cb7b7f238adc10305f7d741"},"cell_type":"code","source":"df.boxplot(column=['claps', \n                   'text_len', \n                   'text_clean_len'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"357fb207a4e852f1d9e644219d5aa657646af878"},"cell_type":"code","source":"df.boxplot(column=['reading_time', \n                   'title_len', \n                   'title_clean_len'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ceefeaa1d692a94a88b8a24991154569074a6ee5"},"cell_type":"markdown","source":"Lets analyze some outliers and check if there are any reasonable grounds to exclude that data from data set."},{"metadata":{"trusted":true,"_uuid":"b9a0e4dd056036529de1934d4e7ed40432b476b9"},"cell_type":"code","source":"sns.pairplot(df[['claps', \n                 'reading_time',\n                 'title_len', \n                 'title_clean_len',\n                 'text_len', \n                 'text_clean_len']], kind='reg')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1ef0a88d6fe87583d857798e0510cdb5292466e"},"cell_type":"markdown","source":"Distributions of claps, reading_time, text_len and text_clean_len are positive skewed. It shows that highest frequencies of particular entities are distributed near small values.\n\nWhat is more claps (dependant variable) has weak positive linear relationship with every independant variable. \n\nOf course we see strong relationship between reading_time and text_len and text_clean_len. Variables text_len, title_len and text_clean_len and title_clean_len are correlated.\n\nBlox plots shows that outliers are detected when claps > 18000, text_len > 28000, text_clean_len > 18500, reading_time > 22, title_len > 95 and title_clean_len > 81. Lets look at those data points."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"5ade046f4eb0cd1817bb6913224b73032a6ce27a"},"cell_type":"code","source":"df[df['claps'] > 18000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a720bf01cf2776d718380341d3ff7d8030cc40a0"},"cell_type":"code","source":"df[df['text_len'] > 28000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ac3420bdf4bdde93a354638b0f5536c40134614"},"cell_type":"code","source":"df[df['reading_time'] > 22]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e51b31947d2b0fe76fa35f6d76b9aa294f579c1"},"cell_type":"markdown","source":"After analysing the data found no reason to remove outliers (if I can call them so). It is normal data points and because of small data set they look like outliers."},{"metadata":{"_uuid":"bb24079857283b2f8b250ac6c0e067d4f361fea9"},"cell_type":"markdown","source":"As text features I desided to use TF-IDF. "},{"metadata":{"trusted":true,"_uuid":"73486ee64d703c2dd7308316cbd28ee73cd9370c"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=None)\nfull_text_features = vectorizer.fit_transform(df['full_text'])\nfull_text_features.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4043b05395f2886f8657105ef9f74f743cd3e50f"},"cell_type":"markdown","source":"All variables has different scale, so I am using standard scaler to make scale equal."},{"metadata":{"trusted":true,"_uuid":"b5adb582ea76764b88c1115ff613134ee9459c53"},"cell_type":"code","source":"scaler = StandardScaler()\nnum_features = scaler.fit_transform(df[['reading_time', \n                                        'title_len',\n                                        'text_len',\n                                        'title_clean_len',\n                                        'text_clean_len']])\nnum_features.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9ab97380dca79f29781d1a5f817a42205ea5cde"},"cell_type":"markdown","source":"After all text features are concatenated with left text length features."},{"metadata":{"trusted":true,"_uuid":"c4afee0f56b38d62cc16768a1de31b1d016940cc"},"cell_type":"code","source":"full_text_features = np.concatenate([full_text_features.toarray(), num_features], axis=1)\nfull_text_features.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bcaa31f57611f3182fd272233a8d136775dd3ed"},"cell_type":"markdown","source":"Performing train/test split."},{"metadata":{"trusted":true,"_uuid":"2a956082a115c696498103edc48b6be1c417c41e"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(full_text_features, df[['claps']].values, test_size=0.3)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"458f6c5a915f3a579272d6ecde7f99c5f3f1aca9"},"cell_type":"code","source":"y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d2bf73d0884ac4522a0d214634a38256bc01403"},"cell_type":"markdown","source":"Testing linear regression model."},{"metadata":{"trusted":true,"_uuid":"03a8c4aa03dbaced16a43bb163aeab7da16c80c5"},"cell_type":"code","source":"reg = LinearRegression().fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a663d8b4529af1c177e28fc2a78c8da46f41f2ce"},"cell_type":"code","source":"y_pred = reg.predict(X_test)\ny_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"28cc19ce70c1ac510bbd2254196cfd1308c33797"},"cell_type":"code","source":"r2_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81c20387964036c16ce9047922fb84d60498b424"},"cell_type":"markdown","source":"R2 metric shows that it is hard task for linear regression model to learn having so much features (R-squared=1â€“1=0). So I am changing number of claps to categorical values."},{"metadata":{"trusted":true,"_uuid":"e0dcbe81b9bc9d3fc298f050b2be7062edb554f3"},"cell_type":"code","source":"df[['claps']].hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"705b5e96750523eb429c07459939a5eeae80fa5c"},"cell_type":"markdown","source":"Claps are devided into categories:\n0 - 10000: rising start\n10001 - 20000: star\n20001 - all other: super star"},{"metadata":{"trusted":true,"_uuid":"4c4fe86c755aca15ac53c188d0946a281d1c0598"},"cell_type":"code","source":"df['claps_categorical'] = df['claps'].apply(lambda x: 'rising star' if x >= 0 and x <= 10000 else 'star' if x >= 10001 and x <= 20000 else 'super star')\ndf[['claps', 'claps_categorical']].head(15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f468cdd21ed8a9d79ef1756155a16cdcfa992d53"},"cell_type":"markdown","source":"Performing new train.test split."},{"metadata":{"trusted":true,"_uuid":"e6b375906b8d5a649d7670b52c5650108402fbb4"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(full_text_features, df[['claps_categorical']].values, test_size=0.3)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3cdfccf1ac7ade89a88f4574a99e309bf4bc315"},"cell_type":"code","source":"y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff22392f15b5cc356451756d083772bf02421947"},"cell_type":"markdown","source":"Using Random Forest classifier for classification task."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"57311ff59d16b686043af8d17e215705f9d43bfb"},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=1000, max_depth=2, random_state=0)\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e0826946399108c0b33b8542195373e2d1cbc38"},"cell_type":"markdown","source":"Predicting claps category."},{"metadata":{"trusted":true,"_uuid":"a3939c94466e83be0bd493852a1e0efdb92d3e22"},"cell_type":"code","source":"y_pred = clf.predict(X_test)\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5b427aa955040c6f0c7c57a055fd7c86df8a8b7"},"cell_type":"markdown","source":"By looking at accuracy score I can tell that model is performing well, 85 % accuracy."},{"metadata":{"trusted":true,"_uuid":"e3a3e6cf5d37fa0cf7af8156abaa435cd96828d7","scrolled":true},"cell_type":"code","source":"confusion_matrix(y_test, y_pred, labels=['rising star', 'star', 'super star'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"780776183156a6a52a96f44aa0760b6214b9f85c"},"cell_type":"markdown","source":"Well confusion matrix shows that only rising star category was predicted correct. All other classes were predicted incorrectly."},{"metadata":{"trusted":true,"_uuid":"433477720dcaa81e2a15a59ad3e589091fc34c06"},"cell_type":"markdown","source":"In order to increase recognition accuracy need to play more with feature engineering and classifier hyperparameters tunning. What is more this dataset is to small to get good and confident results. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}