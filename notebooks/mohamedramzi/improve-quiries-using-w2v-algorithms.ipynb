{"cells":[{"metadata":{"_uuid":"0f726ea7-5219-400c-9709-0bb673a782b3","_cell_guid":"4eb42bdc-3176-4342-a6c0-8a80d9b0ae2d","trusted":true},"cell_type":"markdown","source":" **Application of Word2Vec Algorithms to Improve Queries of Corona Virus Articles** \nThe project aim to improve the pourcentage of relevent answers by usin word2vec algorithms (Skip-Gram and CBOW) to improve queries after validation on Search-engine by calculating accuracy operation."},{"metadata":{},"cell_type":"markdown","source":"# About the Dataset\n**# Dataset Description**\n\nCORD-19 is a resource of over 44,000 scholarly articles, including over 29,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.\n![CORD19](https://maghrebemergent.info/wp-content/uploads/2020/03/5988555_coronavirus-thumb-img-COVID-01-540x304.jpg)"},{"metadata":{},"cell_type":"markdown","source":"**Accessing of the Dataset**\n\nWe have made this dataset available on Kaggle, and are periodically updating it from its source. To learn more and access the latest copy of the dataset, you can also go here: https://pages.semanticscholar.org/coronavirus-research."},{"metadata":{},"cell_type":"markdown","source":"**Install requirements Libreries using pip**\n* numpy : Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object. \n    >> pip install numpy\n\n* pandas : is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n    >> pip install pandas\n    \n* json : Python has a built-in package called json , which can be used to work with JSON data. \n\n* nltk : NLTK is a leading platform for building Python programs to work with human language data.\n    >> pip install nltk\n    \n* gensim : is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.\n    >> pip install gensim\n\n* mathplotlib :  is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n    >> pip install matplotlib\n"},{"metadata":{"_uuid":"3bd4f193-d2b0-4972-872e-53244fc776c6","_cell_guid":"0468b6c4-b689-43f4-99e5-2a2bd5ca7f66","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA Accesssing\n\nLoad_articles() is a function that load all articles in the path given and it depends on json library to read json files , os to operation systems and pandas for represent in a dictionary to use it as DataFrame."},{"metadata":{"_uuid":"028be7c2-5bdb-425f-aa54-f4d63b203d87","_cell_guid":"3c6c192b-5023-4ba2-8fe2-65e74c579106","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport json\n\npath = \"/kaggle/input/CORD-19-research-challenge/\"\n\ndef Load_articles(path):\n    titles=[]\n    abstracts=[]\n    texts=[]\n    #['biorxiv_medrxiv','comm_use_subset','custom_license','noncomm_use_subset'] u can add all of them\n    for directory in ['biorxiv_medrxiv'] : \n        dir_list = os.listdir(path+directory+'/'+directory+'/')\n        for dr in dir_list:\n            j= json.load(open(path+directory+'/'+directory+'/'+dr,'rb'))\n            titles.append(j['metadata']['title'])\n            abstrs=\"\"\n            for abstract in j['abstract'] :\n                abstrs+=str(abstract['text']+'\\n')\n            abstracts.append(abstrs)\n            txts=\"\"\n            for text in j['body_text'] :\n                txts+=str(text['text']+'\\n')\n            texts.append(txts)\n    print('get_all_articles_texts accomlished')\n    return {'title':titles,'abstract':abstracts,'body_text':texts}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PreProcessing\n\n**Text preprocessing** is traditionally an important step for natural language processing (NLP) tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better.\n\n**DataPreprecessing() :** its'a function for preprocessing the list of ilst given from word_tokenize in create_model function using **nltk.RegexpTokenizer** to save just caracters want its and **nltk.corpus.stopwords** to remove al stop words and the same for **QstPreprocessing()** given in input Question or task to preprocessing after give us main the net words in the question as ouput.\n\n**nltk.RegexpTokenizer** is a tokenizer that splits a string using a regular expression, which matches either the tokens or the separators between tokens.\n*     in this case the regular expression is (r'[a-zA-Z0-9_-]+') and that mean remove all caracters except letters , numbers and '_' , '-' signs and those lasts caracters for objects names like 'Covid-19'.\n    \n**stopwords**: A stop word is a commonly used word (such as “with”, “to”, “for”, “if”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query, nltk library provide all stop words in any language .\n*     -> to check the list of stop words u should to apply this little code\n*         >>from nltk.corpus import stopwords\n*         >>print(set(stopwords.words('english')))\n*         set of english stop words is :\n* ('both', 'myself', 'now', 'couldn', 'were', 'had', 'o', 'between', 'why', 'there', 'aren', 'too', 'to', 's', 'your', 'of', 'ours', 'we', 'below', 'over', 'a', 'against', 'which', 'has', \"that'll\", 'these', 'most', \"don't\", 'on', 'they', 'off', 'its', 'at', 'theirs', 'above', 'if', 'me', 'whom', \"she's\", 'she', 'ain', \"you'd\", 'mightn', 'nor', \"you'll\", 'and', 'am', 'our', 'y', \"didn't\", 'no', 'ma', 'before', 'i', 'with', \"you've\", \"shan't\", 'again', 'hasn', 'more', \"won't\", \"aren't\", 'does', \"doesn't\", 'hadn', \"weren't\", 'each', 'where', 'himself', 'what', 'is', 'as', 'under', 't', 'm', 'wasn', 'doing', 'was', 'here', 'it', \"hadn't\", \"shouldn't\", 'them', 'being', 'the', 'he', 'very', 'isn', 'be', 'or', 'should', 'd', 'hers', \"you're\", 'are', \"wasn't\", 'all', 'yourselves', 'further', 'won', 'itself', 'needn', 'their', \"needn't\", \"haven't\", 'any', 'because', 've', 'from', 'in', \"couldn't\", 'for', \"wouldn't\", 'once', 'out', 'until', 'did', 'while', \"isn't\", 'than', 'not', 'didn', 'those', 'such', 'but', 'that', 'shouldn', 'shan', 'few', 'this', 'can', 'haven', 'wouldn', 'doesn', 'will', 'my', 'themselves', \"it's\", 'her', 'do', 'having', 'll', 'an', \"mightn't\", \"should've\", \"hasn't\", 'through', 'herself', 'who', 'so', 'mustn', 'ourselves', 'by', 'after', 'then', 'own', 'just', 'have', 'you', 'only', 'same', \"mustn't\", 're', 'when', 'down', 'yourself', 'up', 'don', 'him', 'about', 'into', 'weren', 'other', 'yours', 'been', 'how', 'during', 'some', 'his')\n\n* As Example the preprocessing using QstPreprocessing of the question \"What is known about transmission, incubation, and environmental stability?\" -> \" known transmission incubation environmental stability \""},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\n\ndef DataPreprecessing(lst_of_lst):\n    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9_-]+')\n    stop_words = set(stopwords.words('english'))\n    lst_result=[]\n    for lst in lst_of_lst:\n        txt=' '.join(lst)\n        lst_no_punc=tokenizer.tokenize(txt)\n        lst_no_punc_no_SW = [word for word in lst_no_punc if word not in stop_words]\n        lst_result.append(lst_no_punc_no_SW )\n    print('No_StopWords_No_punctuation accomplshed')\n    return (lst_result)\n\ndef QstPreprocessing(txt):\n    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9_-]+')\n    stop_words = set(stopwords.words('english'))\n    items = [word for word in [ item.lower() for item in tokenizer.tokenize(txt)] if word not in stop_words]\n    return (\" \".join(items))\n\n#print(QstPreprocessing(\"What is known about transmission, incubation, and environmental stability?\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Embedding\n**Text Embedding : ** is the mathematical representations of words as vectors. They are created by analyzing a body of text and representing each word, phrase, or entire document as a vector in a high dimensional space (similar to a multi-dimensional graph).the embedding approach have many algorithms as Word2Vec (used in this analysis) , Fasttext, Glove ... .\n\n**Word2Vec** : is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov,It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW) .\n* **CBOW Model**: This method takes the context of each word as the input and tries to predict the word corresponding to the context. \n* **Skip-Gram model:** This looks like multiple-context CBOW model just got flipped. To some extent that is true.\n\n**gensim library :** provide all word2vec methods and functions, The creation of the model by gensim.models.Word2Vec() function with configuration of the models creation as Example : \n gensim.models.Word2Vec(tok_corp,min_count=6,size=32, iter=10, sg=1, workers=14)\n*     tok_corp is the a list of list generated by nltk.word_tokenize() function\n*     min_count (int, optional) – Ignores all words with total frequency lower than this.\n*     size (int, optional) – Dimensionality of the word vectors.\n*     iter (int, optional) – Number of iterations (epochs) over the corpus.\n*     sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n*     workers (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n\nNote : The trained word vectors can also be stored/loaded from a format compatible with the original word2vec implementation via self.wv.save_word2vec_format and gensim.models.keyedvectors.KeyedVectors.load_word2vec_format().\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\nimport os\nimport gensim\n\ndef creat_model(path,Model_Name1,Model_Name2):\n#     if not(os.path.isfile(path+Model_Name)) :\n    df=pd.DataFrame(Load_articles(path)) #DataDrame presenting\n    corpus=df[\"title\"].values.tolist()+df[\"abstract\"].values.tolist()+df[\"body_text\"].values.tolist()\n    del df\n    tok_corp=[nltk.word_tokenize(str(sent).lower()) for sent in corpus] #tokenize the full text\n    del corpus\n    print('step tokenize accomplished')\n    tok_corp=DataPreprecessing(tok_corp) #Data PreProcessing\n    print('step remove Stop Words with Punctuation accomplished')\n    model1=gensim.models.Word2Vec(tok_corp,min_count=6,size=32, iter=10, sg=0, workers=14)\n    model1.save(\"../working/\"+Model_Name1)#save the first model in \"../working/\" path\n    del model1\n    model2=gensim.models.Word2Vec(tok_corp,min_count=6,size=32, iter=10, sg=1, workers=14)\n    model2.save(\"../working/\"+Model_Name2) #save the second model in \"../working/\" path\n    del model2\n    del tok_corp\n    \ncreat_model(\"/kaggle/input/CORD-19-research-challenge/\",\"model_CBOW\",\"model_SG\") # Models Creation\n\nsg_model = gensim.models.Word2Vec.load(\"../working/model_SG\") # load Skip-Gram model from \"../working/model_SG\" path after saved there\ncbow_model = gensim.models.Word2Vec.load(\"../working/model_CBOW\") # load CBOW model from \"../working/model_CBOW\" path ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation\n\nmodel validation is the task of confirming that the outputs of a statistical model are acceptable with respect to the real data-generating process.\n\nto validate our models we used model.wv.most_similar(word,topn=10) function to return the similar words to the word from the model given as input.\n\nmost_similar(model,word) it's a function return a dataframe of similar words with score of each one used model.wv.most_similar() function.\n\nWe can see an example of similar words of covid-19 as DataFrame with the both models (Skip-Gram and CBOW):"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load model\ndef most_similar(model,word):\n    similarities = model.wv.most_similar(word,topn=10)\n    key=[similar_word for (similar_word , score) in similarities]\n    value=[score for (similar_word , score) in similarities]\n    return pd.DataFrame({'key':key,'value':value}, columns=[\"key\", \"value\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_similar(sg_model,'covid-19') \n\"\"\"exmample of applying the similar word function for the 'covid-19' word on the skip-gram model \"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Similar words meaning return from Skip-Gram model**\n* ncp : novel coronavirus pneumonia\n* 2019n-cov : 2019 novel coronavirus\n* feidian : \n* ard : \n* covid : similar name to covid-19\n* nation-wide : existing or happening in all parts of a particular country.\n* single-centered : \n* coronavirus-infected : infection of corona virus\n* municipality : a city or town with its own local government, or the local government itself.\n* covid-2019 : similar name to covid-19"},{"metadata":{"trusted":true},"cell_type":"code","source":"most_similar(cbow_model,'covid-19') \n\"\"\"exmample of applying the similar word function for the 'covid-19' word on the skip-gram model \"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Similar words meaning return from CBOW model**\n* ncp : novel coronavirus pneumonia.\n* 2019-ncov : 2019 novel coronavirus.\n* sars-cov-2 : SARS-CoV-2 belongs to the coronavirus family.\n* ncip : novel coronavirus-infected pneumonia.\n* china : country of corona virus starting .\n* wuhan : city of corona virus starting.\n* covid-2019 : similar name .\n* diagnosed : recognize the name of character of a disease or a problem, by examining it.\n* suspected : believed to be true or to exist.\n* situation : the set of things that are happening and the conditions that exist at a particular time and place."},{"metadata":{},"cell_type":"markdown","source":"# Questions Enhancement as Quiries\n\nQuestion is a sentence in an interrogative form addressed to reply by answer or more ,why need a preprocessing to be able to enhancing as well as the dataset and put the similar words after that the quiries it's will be prepared to entring in search engine.\n\nQst_Enhancement function use QstPreprocessing(qst) to preprocessing the question and after that , it add a similar words of each word nearets to same word in the quiries using the selected model.\n\n**Example 1** using skip-gram model with threshold of 0.8 : the enhacement of the question \"What is known about transmission, incubation, and environmental stability?\" is : **'known uncharacterized bacteriophages auxiliary transmission transmissions human-to-human spillover superspreading animal-to-human zoonotic onward person-to-person spread undocumented incubation serial period latent erlang periods 4-day environmental climatic abiotic stability'**\n\n**Example 2** using CBOW model with threshold of 0.8: the enhacement of the question \"What is known about transmission, incubation, and environmental stability?\" is : **'known transmission transmissions incubation serial environmental natural influencing stability'**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Qst_Enhancement(model,qst,threshold):\n    qst=QstPreprocessing(qst)\n    items=[]\n    for word in qst.split():\n        items.append(word)\n        try:\n            similarities = model.wv.most_similar(word,topn=10)\n            for similar_word , score in similarities:\n                if score > threshold:\n                    items.append(similar_word)\n        except : \n            pass\n    return \" \".join(items)\n       \n#Qst_Enhancement(sg_model,\"What is known about transmission, incubation, and environmental stability?\",0.80)\nQst_Enhancement(cbow_model ,\"What is known about transmission, incubation, and environmental stability?\",0.80)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** **Implementation (answer the questions)** **\nre_search(qst) designed to answer questions by given as output the articles that have the question words in the document as well as the search engine.\n\nthe list of tasks is :\n* What is known about transmission, incubation, and environmental stability?\n* What do we know about COVID-19 risk factors?\n* What do we know about virus genetics, origin, and evolution?\n* What do we know about virus genetics, origin, and evolution?\n* What do we know about vaccines and therapeutics?\n* What do we know about non-pharmaceutical interventions?\n* Sample task with sample submission\n* What do we know about diagnostics and surveillance?\n* What has been published about medical care?\n* Sample task with sample submission\n\nthe function return articles of any question ranking  by the number of question words in the article."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\npath=\"/kaggle/input/CORD-19-research-challenge/\"\ndef re_search(model,qst):\n    qst=Qst_Enhancement(model,qst,0.8)\n    lst=qst.split()\n    df=pd.DataFrame(Load_articles(path))\n    columns = list(df)\n    articles=[]\n    for i in range(len(df)):\n        article = (df['title'][i])+\"\\n\"+(df['abstract'][i])\n        rank=2\n        if(re.search(\"^.*\"+\"*.*|\".join(lst)+\"*$\",article)):\n            for item in lst:\n                if len(re.findall(item,article)) > 0:\n                    rank+= 1\n                    articles.append([rank,article])\n    articles.sort(reverse=True)\n    for i,j in articles :\n        print('score ======'+str(i))\n        print(j)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Result\n\nthe result of implementation of the first task or question as Example by using re_search() function is a set of articles that give us multi informations with the score of the number of words in question have answers in the article with the Skip-gram model firstly after that the CBOW model as shown below."},{"metadata":{"trusted":true},"cell_type":"code","source":"re_search(sg_model,'What is known about transmission, incubation, and environmental stability?')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re_search(cbow_model,'What is known about transmission, incubation, and environmental stability?')","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}