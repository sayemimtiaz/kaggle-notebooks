{"cells":[{"metadata":{"id":"PklANi32vU6h"},"cell_type":"markdown","source":"# Bank Loan Exploratory Data Analysis\n****\nBy: Santh Raul and Ramlal Naik","execution_count":null},{"metadata":{"id":"_NzirmyAva_B"},"cell_type":"markdown","source":"# I. Problem Statement:\n* If the applicant is likely to repay the loan, then not approving the loan results in a loss of business to the company\n* If the applicant is not likely to repay the loan, i.e. he/she is likely to default, then approving the loan may lead to a financial loss for the company.\n\nThe company wants to understand the driving factors (or driver variables) behind the loan default. i.e the variables which are strong in loan default.\n","execution_count":null},{"metadata":{"id":"PChDJTxR6stZ"},"cell_type":"markdown","source":"###  II. Import Libraries and set required parameters","execution_count":null},{"metadata":{"id":"CnX_FOMcvU6w","outputId":"dcd6ee47-1bee-4039-b5f4-be03cae6cdea","trusted":true},"cell_type":"code","source":"# Import libraries\nimport numpy as np\nprint('numpy version\\t:', np.__version__)\nimport pandas as pd\nprint('pandas version\\t:', pd.__version__)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint('seaborn version\\t:', sns.__version__)\nfrom scipy import stats\n\nimport os\n\npd.set_option('display.max_columns', 200) # to display all the columns\npd.set_option('display.max_rows',150) # to display all rows of df series\npd.options.display.float_format = '{:.4f}'.format #set it to convert scientific noations such as 4.225108e+11 to 422510842796.00\n\nimport warnings\nwarnings.filterwarnings('ignore') # if there are any warning due to version mismatch, it will be ignored\n\nimport random","execution_count":null,"outputs":[]},{"metadata":{"id":"Cvo7dO632O3o"},"cell_type":"markdown","source":"###  1. Data Importing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Sample data to overcome Memory Error\n# # Less RAM: Reduce the data: It's completely fine to take a sample of the data to work on this case study\n# # Random Sampling to get a random sample of data from the complete data\n# filename = \"application_data.csv\"# This file is available is the same location as the jupyter notebook\n\n# # Count the number of rows in my file\n# num_lines = sum(1 for i in open(filename))\n# # The number of rows that I wanted to load\n# size = num_lines//2\n\n# # Create a random indices between these two numbers\n\n# random.seed(10)\n# skip_id = random.sample(range(1, num_lines), num_lines-size)\n\n# df_app = pd.read_csv(filename, skiprows = skip_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ndf_app = pd.read_csv('../input/credit-card/application_data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get some insights of data","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# get shape of data (rows, columns)\nprint(df_app.shape)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_app.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"Rc1SFw3T4EYv","outputId":"05536e66-be7c-44e8-81ad-909728afbb89","scrolled":false,"trusted":true},"cell_type":"code","source":"# get some insights of data\ndf_app.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_app.info()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# get the count, size and Unique value in each column of application data\ndf_app.agg(['count','size','nunique'])","execution_count":null,"outputs":[]},{"metadata":{"id":"t5CAfQKj1Pnj"},"cell_type":"markdown","source":"### 2. Data Quality Check and Missing Values","execution_count":null},{"metadata":{"id":"uprs1zyuVxXo"},"cell_type":"markdown","source":"#### 2.a. Find the percentage of missing values of the columns","execution_count":null},{"metadata":{"id":"TRyT5sCDvU7E","trusted":true},"cell_type":"code","source":"# funcion to get null value\ndef column_wise_null_percentage(df):\n    output = round(df.isnull().sum()/len(df.index)*100,2)\n    return output","execution_count":null,"outputs":[]},{"metadata":{"id":"YjqF5m1rFW9m","outputId":"e03740f2-dbc1-4c04-f9b0-29515ca6a20f","scrolled":false,"trusted":true},"cell_type":"code","source":"# get missign values of all columns\nNA_col = column_wise_null_percentage(df_app)\nNA_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# identify columns only with null values\nNA_col = NA_col[NA_col>0]\nNA_col","execution_count":null,"outputs":[]},{"metadata":{"id":"omzyJ6i_6u4f","outputId":"2c38e802-61ba-48e3-ba91-82afb9dd3f24","trusted":true},"cell_type":"code","source":"# grafical representation of columns having % null values\nplt.figure(figsize= (20,4),dpi=300)\nNA_col.plot(kind = 'bar')\nplt.title (' columns having null values')\nplt.ylabel('% null values')\nplt.show()\n# plt.savefig('filename.png', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"id":"dpQlESJiF1Jj"},"cell_type":"markdown","source":"#### 2.b. Identify and remove columns with high missing percentage (>50%)","execution_count":null},{"metadata":{"id":"_7ooniNfvU7H","outputId":"84a2d800-e4f4-4ff5-bd96-3c437f91058f","scrolled":false,"trusted":true},"cell_type":"code","source":"# Get the column with null values more than 50%\nNA_col_50 = NA_col[NA_col>50]\nprint(\"Number of columns having null value more than 50% :\", len(NA_col_50.index))\nprint(NA_col_50)","execution_count":null,"outputs":[]},{"metadata":{"id":"nB_GlrF7eJOI"},"cell_type":"markdown","source":"* Droped all columns from Dataframe for which missing value percentage are more than 50%.\n\n`````````````\n       'OWN_CAR_AGE', 'EXT_SOURCE_1', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG',\n       'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG',\n       'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG',\n       'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG',\n       'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BUILD_MODE',\n       'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMIN_MODE',\n       'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE',\n       'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI',\n       'BASEMENTAREA_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI',\n       'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI',\n       'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI',\n       'NONLIVINGAREA_MEDI', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE',\n       'WALLSMATERIAL_MODE'\n```````````","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# removed 41 columns having null percentage more than 50%.\ndf_app = df_app.drop(NA_col_50.index, axis =1)\ndf_app.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"tKpzhTe6GhqP"},"cell_type":"markdown","source":"#### 2.c. identify columns with less missing missing values (<15%)","execution_count":null},{"metadata":{"id":"6MiBOGc2lE-w","outputId":"8cfbcb71-06db-45f2-b6ba-bd1bf385e3a5","scrolled":false,"trusted":true},"cell_type":"code","source":"# Get columns having <15% null values\nNA_col_15 = NA_col[NA_col<15]\nprint(\"Number of columns having null value less than 15% :\", len(NA_col_15.index))\nprint(NA_col_15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NA_col_15.index","execution_count":null,"outputs":[]},{"metadata":{"id":"AqS53YGNayZe"},"cell_type":"markdown","source":"* The columns having null values less than 15% are,\n\n> 'AMT_GOODS_PRICE', 'NAME_TYPE_SUITE', 'EXT_SOURCE_2','OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE',\n       'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE','AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY',\n       'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR'\n\n* These columns shall be imputed with suitable values which shall be explained subsequently. ","execution_count":null},{"metadata":{"id":"FvzSE_1Ip_yk","outputId":"6a460d5b-945e-438f-ee9a-3c985793cb95","scrolled":true,"trusted":true},"cell_type":"code","source":"# understand the insight of missing columns having <15% null values\ndf_app[NA_col_15.index].describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"ykw3S0ITcIKS","outputId":"62e0e24e-52e0-43ff-8be0-17f9126b1b52","scrolled":true,"trusted":true},"cell_type":"code","source":"# identify unique values in the colums having <15% null value \ndf_app[NA_col_15.index].nunique().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **For analysis of imputation selecetd 7 varibles.**\n<br>Continuious variables:\n``````\n> 'EXT_SOURCE_2','AMT_GOODS_PRICE'\n``````\nCategorical variables:\n`````````\n> 'OBS_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','NAME_TYPE_SUITE'\n`````````\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Continous variable:","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Box plot for continuious variable\nplt.figure(figsize=(12,4))\nsns.boxplot(df_app['EXT_SOURCE_2'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,4))\nsns.boxplot(df_app['AMT_GOODS_PRICE'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inference from box plot:\n* for 'EXT_SOURCE_2' there is no outliers present. And there is no significant diffence observed between mean and median. However data look to be right skewed. So missing values can be imputed with median value: 0.565\n* for 'AMT_GOODS_PRICE' there is significant number of outlier present in the data. SO data should be imputed with median value: 450000\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Categorical variables:","execution_count":null},{"metadata":{"id":"WOfpEDxtJsJO","outputId":"e6b69340-06be-400f-f460-06b37fb545f3","trusted":true},"cell_type":"code","source":"# identify maximum frequency values\nprint('Maximum Frequncy categorical values are,')\nprint('NAME_TYPE_SUITE: ',df_app['NAME_TYPE_SUITE'].mode()[0])\nprint('OBS_30_CNT_SOCIAL_CIRCLE:', df_app['OBS_30_CNT_SOCIAL_CIRCLE'].mode()[0])\nprint('DEF_30_CNT_SOCIAL_CIRCLE:', df_app['DEF_30_CNT_SOCIAL_CIRCLE'].mode()[0])\nprint('OBS_60_CNT_SOCIAL_CIRCLE:', df_app['OBS_60_CNT_SOCIAL_CIRCLE'].mode()[0])\nprint('DEF_60_CNT_SOCIAL_CIRCLE:', df_app['DEF_60_CNT_SOCIAL_CIRCLE'].mode()[0])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"AXUo_7FTMNF6"},"cell_type":"markdown","source":"For categorical vriable the value which should be imputed with maximum in frequency.<br>\nSo the value to be imputed are:<br>\nNAME_TYPE_SUITE:  Unaccompanied<br>\nOBS_30_CNT_SOCIAL_CIRCLE: 0.0 <br>\nDEF_30_CNT_SOCIAL_CIRCLE: 0.0<br>\nOBS_60_CNT_SOCIAL_CIRCLE: 0.0<br>\nDEF_60_CNT_SOCIAL_CIRCLE: 0.0<br>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove unwanted columns from application dataset for better analysis.\n\nunwanted=['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE','FLAG_PHONE', 'FLAG_EMAIL',\n          'REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY','FLAG_EMAIL','CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT',\n          'REGION_RATING_CLIENT_W_CITY','FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3','FLAG_DOCUMENT_4',\n          'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6','FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9','FLAG_DOCUMENT_10',\n          'FLAG_DOCUMENT_11','FLAG_DOCUMENT_12','FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15',\n          'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18','FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20',\n          'FLAG_DOCUMENT_21','EXT_SOURCE_2','EXT_SOURCE_3','YEARS_BEGINEXPLUATATION_AVG','FLOORSMAX_AVG','YEARS_BEGINEXPLUATATION_MODE',\n          'FLOORSMAX_MODE','YEARS_BEGINEXPLUATATION_MEDI','FLOORSMAX_MEDI','TOTALAREA_MODE','EMERGENCYSTATE_MODE']\n\ndf_app.drop(labels=unwanted,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_app.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"WV5Niyk97N8_","outputId":"ae591b73-c83a-4347-f591-a34fe71483d8","scrolled":false,"trusted":true},"cell_type":"code","source":"df_app.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some columns where the value is mentioned as 'XNA' which means 'Not Available'. So we have to find the number of rows and columns.","execution_count":null},{"metadata":{"id":"WV5Niyk97N8_","outputId":"ae591b73-c83a-4347-f591-a34fe71483d8","trusted":true},"cell_type":"code","source":"# For Code Gender column\n\nprint('CODE_GENDER: ',df_app['CODE_GENDER'].unique())\nprint('No of values: ',df_app[df_app['CODE_GENDER']=='XNA'].shape[0])\n\nXNA_count = df_app[df_app['CODE_GENDER']=='XNA'].shape[0]\nper_XNA = round(XNA_count/len(df_app.index)*100,3)\n\nprint('% of XNA Values:',  per_XNA)\n\nprint('maximum frequency data :', df_app['CODE_GENDER'].describe().top)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, Female is having the majority and only 2 rows are having XNA values, we can impute those with Gender 'F' as there will be no impact on the dataset. Also there will no impact if we drop those rows.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Dropping the XNA value in column 'CODE_GENDER' with \"F\" for the dataset\n\ndf_app = df_app.drop(df_app.loc[df_app['CODE_GENDER']=='XNA'].index)\ndf_app[df_app['CODE_GENDER']=='XNA'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Organization column\nprint('No of XNA values: ', df_app[df_app['ORGANIZATION_TYPE']=='XNA'].shape[0])\n\nXNA_count = df_app[df_app['ORGANIZATION_TYPE']=='XNA'].shape[0]\nper_XNA = round(XNA_count/len(df_app.index)*100,3)\n\nprint('% of XNA Values:',  per_XNA)\n\ndf_app['ORGANIZATION_TYPE'].describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, for column 'ORGANIZATION_TYPE', we have total count of 153755 rows of which 27737 rows are having 'XNA' values. Which means 18% of the column is having this values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Dropping the rows have 'XNA' values in the organization type column\n\n# df_app = df_app.drop(df_app.loc[df_app['ORGANIZATION_TYPE']=='XNA'].index)\n# df_app[df_app['ORGANIZATION_TYPE']=='XNA'].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.d. Check the data type of all the columns and changed the data type.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df_app.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Casting variable into numeric in the dataset\n\nnumeric_columns=['TARGET','CNT_CHILDREN','AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','REGION_POPULATION_RELATIVE',\n                 'DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH','HOUR_APPR_PROCESS_START',\n                 'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY','REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY',\n                'DAYS_LAST_PHONE_CHANGE']\n\ndf_app[numeric_columns]=df_app[numeric_columns].apply(pd.to_numeric)\ndf_app.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"PHluYviL8jW1"},"cell_type":"markdown","source":"Following age/days columns are having -ve value, which needs to converted to  +ve value.\n\n```\n'DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH','DAYS_LAST_PHONE_CHANGE',\n```","execution_count":null},{"metadata":{"id":"MIagjjIm7N9C","trusted":true},"cell_type":"code","source":"# Converting '-ve' values into '+ve' Values\ndf_app['DAYS_BIRTH'] = df_app['DAYS_BIRTH'].abs()\ndf_app['DAYS_EMPLOYED'] = df_app['DAYS_EMPLOYED'].abs()\ndf_app['DAYS_REGISTRATION'] = df_app['DAYS_REGISTRATION'].abs()\ndf_app['DAYS_ID_PUBLISH'] = df_app['DAYS_ID_PUBLISH'].abs()\ndf_app['DAYS_LAST_PHONE_CHANGE'] = df_app['DAYS_LAST_PHONE_CHANGE'].abs()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.e Checking the outlier for numerical variables:","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# describe numeric columns\ndf_app[numeric_columns].describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Box plot for selected columns\nfeatures = ['CNT_CHILDREN', 'AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','DAYS_EMPLOYED', 'DAYS_REGISTRATION']\n\nplt.figure(figsize = (20, 15), dpi=300)\nfor i in enumerate(features):\n    plt.subplot(3, 2, i[0]+1)\n    sns.boxplot(x = i[1], data = df_app)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above box plot and descibe analysis we found that following are the numeric columns are having outliers:\n~~~~~~~~~\nCNT_CHILDREN, AMT_INCOME_TOTAL,AMT_CREDIT,AMT_ANNUITY,DAYS_EMPLOYED, DAYS_REGISTRATION\n~~~~~~~~","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* The first quartile almost missing for CNT_CHILDREN that means most of the data are present in the first quartile.\n\n* There is single high value data point as outlier present in AMT_INCOME_TOTAL and Removal this point will dtrasticaly impact the box plot for further analysis.\n\n* The first quartiles is slim compare to third quartile for AMT_CREDIT,AMT_ANNUITY, DAYS_EMPLOYED, DAYS_REGISTRATION. This mean data are skewed towards first quartile.","execution_count":null},{"metadata":{"id":"uvLyd-FHA9JW","outputId":"c4072bae-b61d-45ef-947c-9ecd361b2f9b"},"cell_type":"markdown","source":"#### 2.f. Bin Creation","execution_count":null},{"metadata":{"id":"cwYOiH3gOrNF"},"cell_type":"markdown","source":"Creating bins for continous variable categories column 'AMT_INCOME_TOTAL' and 'AMT_CREDIT'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = [0,100000,200000,300000,400000,500000,10000000000]\nslot = ['<100000', '100000-200000','200000-300000','300000-400000','400000-500000', '500000 and above']\n\ndf_app['AMT_INCOME_RANGE']=pd.cut(df_app['AMT_INCOME_TOTAL'],bins,labels=slot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = [0,100000,200000,300000,400000,500000,600000,700000,800000,900000,10000000000]\nslot = ['<100000', '100000-200000','200000-300000','300000-400000','400000-500000', '500000-600000',\n        '600000-700000','700000-800000','850000-900000','900000 and above']\n\ndf_app['AMT_CREDIT_RANGE']=pd.cut(df_app['AMT_CREDIT'],bins,labels=slot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Analysis:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing the dataset into two dataset of  target=1(client with payment difficulties) and target=0(all other)\n\ntarget0_df=df_app.loc[df_app[\"TARGET\"]==0]\ntarget1_df=df_app.loc[df_app[\"TARGET\"]==1]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# insights from number of target values\n\npercentage_defaulters= round(100*len(target1_df)/(len(target0_df)+len(target1_df)),2)\n\npercentage_nondefaulters=round(100*len(target0_df)/(len(target0_df)+len(target1_df)),2)\n\nprint('Count of target0_df:', len(target0_df))\nprint('Count of target1_df:', len(target1_df))\n\n\nprint('Percentage of people who paid their loan are: ', percentage_nondefaulters, '%' )\nprint('Percentage of people who did not paid their loan are: ', percentage_defaulters, '%' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating Imbalance percentage\n    \n# Since the majority is target0 and minority is target1\n\nimb_ratio = round(len(target0_df)/len(target1_df),2)\n\nprint('Imbalance Ratio:', imb_ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Imbalance ratio is 11.48","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 3.a Univariate analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Categorical Univariate Analysis in logarithmic scale for target=0 (client with no payment difficulties)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count plotting in logarithmic scale\n\ndef uniplot(df,col,title,hue =None):\n    \n    sns.set_style('whitegrid')\n    sns.set_context('talk')\n    plt.rcParams[\"axes.labelsize\"] = 14\n    plt.rcParams['axes.titlesize'] = 16\n    plt.rcParams['axes.titlepad'] = 14\n    \n    \n    temp = pd.Series(data = hue)\n    fig, ax = plt.subplots()\n    width = len(df[col].unique()) + 7 + 4*len(temp.unique())\n    fig.set_size_inches(width , 8)\n    plt.xticks(rotation=45)\n    plt.yscale('log')\n    plt.title(title)\n    ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index,hue = hue) \n        \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categoroical Univariate Analysis in logarithmic scale\n\nfeatures = ['AMT_INCOME_RANGE', 'AMT_CREDIT_RANGE','NAME_INCOME_TYPE','NAME_CONTRACT_TYPE']\nplt.figure(figsize = (20, 15))\n\nfor i in enumerate(features):\n    plt.subplot(2, 2, i[0]+1)\n    plt.subplots_adjust(hspace=0.5)\n    sns.countplot(x = i[1], hue = 'TARGET', data = df_app)\n    \n    plt.rcParams['axes.titlesize'] = 16\n    \n    plt.xticks(rotation = 45)\n    plt.yscale('log')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Insights:<br>\n\n> AMT_INCOME_RANGE : \n    * The people having 100000-200000 are havign higher number of loan and also having higher in defaulter\n    * The income segment having >500000 are having less defaulter.\n\n> AMT_CREDIT_RANGE:\n    * The people having <100000 loan are less defaulter.\n    * income having more thatn >100000 are almost equal % of loan defaulter\n\n> NAME_INCOME_TYPE:\n    * Student pensioner and business have higher percentage of loan repayment.\n    * Working, State servent and Commercial associates have higher default percentage.\n    * Maternity category is significantly higher problem in replayement.\n\n> NAME_CONTRACT_TYPE\n    * For contract type ‘cash loans’ is having higher number of credits than ‘Revolving loans’ contract type.\n    * From the above graphs we can see that the Revolving loans are small amount compared to Cash loans but the % of non payment for the revolving loans are comapritvely high.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Categoroical Univariate Analysis in Value scale\n\nfeatures = ['CODE_GENDER','FLAG_OWN_CAR']\nplt.figure(figsize = (20, 10))\n\nfor i in enumerate(features):\n    plt.subplot(2, 2, i[0]+1)\n    plt.subplots_adjust(hspace=0.5)\n    sns.countplot(x = i[1], hue = 'TARGET', data = df_app)\n     \n    plt.rcParams['axes.titlesize'] = 16\n    plt.xticks(rotation = 45)\n#     plt.yscale('log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Insights: \n> CODE_GENDER:\n    * The % of defaulters are more in Male than Female\n\n\n> FLAG_OWN_CAR:\n    * The person owning car is having higher percentage of defaulter.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Univariate analysis Continuious variables:","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Univariate Analysis for continous variable\n\nfeatures = ['AMT_ANNUITY','AMT_GOODS_PRICE','DAYS_BIRTH','DAYS_EMPLOYED','DAYS_LAST_PHONE_CHANGE','DAYS_ID_PUBLISH']\nplt.figure(figsize = (15, 20))\n\nfor i in enumerate(features):\n    plt.subplot(3, 2, i[0]+1)\n    plt.subplots_adjust(hspace=0.5)\n    sns.boxplot(x = 'TARGET', y = i[1], data = df_app)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inference:\n* Days_Birth: The people having higher age are having higher probability of repayment.\n* Some outliers are observed in In 'AMT_ANNUITY','AMT_GOODS_PRICE','DAYS_EMPLOYED', DAYS_LAST_PHONE_CHANGE in the dataset.\n* Less outlier observed in Days_Birth and DAYS_ID_PUBLISH\n* 1st quartile is smaller than third quartile in In 'AMT_ANNUITY','AMT_GOODS_PRICE', DAYS_LAST_PHONE_CHANGE.\n* In DAYS_ID_PUBLISH: people changing ID in recent days are relativelty prone to be default.\n* There is single high value data point as outlier present in DAYS_EMPLOYED. Removal this point will drastically impact the box plot for further analysis. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 3.b. Bivariate analysis for numerical variables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**For Target 0**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plotting for Credit amount\n\nplt.figure(figsize=(16,12))\nplt.xticks(rotation=45)\nsns.boxplot(data =target0_df, x='NAME_EDUCATION_TYPE',y='AMT_CREDIT', hue ='NAME_FAMILY_STATUS',orient='v')\nplt.title('Credit amount vs Education Status')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Family status of 'civil marriage', 'marriage' and 'separated' of Academic degree education are having higher number of credits than others. \n* Also, higher education of family status of 'marriage', 'single' and 'civil marriage' are having more outliers. Civil marriage for Academic degree is having most of the credits in the third quartile.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plotting for Income amount in logarithmic scale\n\nplt.figure(figsize=(16,12))\nplt.xticks(rotation=45)\nplt.yscale('log')\nsns.boxplot(data =target0_df, x='NAME_EDUCATION_TYPE',y='AMT_INCOME_TOTAL', hue ='NAME_FAMILY_STATUS',orient='v')\nplt.title('Income amount vs Education Status')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In Education type 'Higher education' the income amount is mostly equal with family status. It does contain many outliers. \n* Less outlier are having for Academic degree but there income amount is little higher that Higher education. \n* Lower secondary of civil marriage family status are have less income amount than others.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**For Target 1**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plotting for credit amount\n\nplt.figure(figsize=(15,10))\nplt.xticks(rotation=45)\nsns.boxplot(data =target0_df, x='NAME_EDUCATION_TYPE',y='AMT_CREDIT', hue ='NAME_FAMILY_STATUS',orient='v')\nplt.title('Credit Amount vs Education Status')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Observations are Quite similar with Target 0 \n* Family status of 'civil marriage', 'marriage' and 'separated' of Academic degree education are having higher number of credits than others. \n* Most of the outliers are from Education type 'Higher education' and 'Secondary'. \n* Civil marriage for Academic degree is having most of the credits in the third quartile.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plotting for Income amount in logarithmic scale\n\nplt.figure(figsize=(16,12))\nplt.xticks(rotation=45)\nplt.yscale('log')\nsns.boxplot(data =target0_df, x='NAME_EDUCATION_TYPE',y='AMT_INCOME_TOTAL', hue ='NAME_FAMILY_STATUS',orient='v')\nplt.title('Income amount vs Education Status')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is also have some similarity with Target0, \n* Education type 'Higher education' the income amount is mostly equal with family status. \n* Less outlier are having for Academic degree but there income amount is little higher that Higher education. \n* Lower secondary are have less income amount than others.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3.c. Correlation:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Getting top 10 correlation between variables","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Top 10 correlated variables: target 0 dataaframe\n\ncorr = target0_df.corr()\ncorrdf = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\ncorrdf = corrdf.unstack().reset_index()\ncorrdf.columns = ['Var1', 'Var2', 'Correlation']\ncorrdf.dropna(subset = ['Correlation'], inplace = True)\ncorrdf['Correlation'] = round(corrdf['Correlation'], 2)\ncorrdf['Correlation'] = abs(corrdf['Correlation'])\ncorrdf.sort_values(by = 'Correlation', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Top 10 correlated variables: target 1 dataaframe\n\ncorr = target1_df.corr()\ncorrdf = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\ncorrdf = corrdf.unstack().reset_index()\ncorrdf.columns = ['Var1', 'Var2', 'Correlation']\ncorrdf.dropna(subset = ['Correlation'], inplace = True)\ncorrdf['Correlation'] = round(corrdf['Correlation'], 2)\ncorrdf['Correlation'] = abs(corrdf['Correlation'])\ncorrdf.sort_values(by = 'Correlation', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n* From the above correlation analysis it is infered that the highest corelation (1.0) is between (OBS_60_CNT_SOCIAL_CIRCLE with OBS_30_CNT_SOCIAL_CIRCLE) and (FLOORSMAX_MEDI with FLOORSMAX_AVG) which is same for both the data set.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4. Read Previous Application data and merging with application data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the dataset of previous application\n\ndf_prev=pd.read_csv('../input/credit-card/previous_application.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#explore the dataset\ndf_prev.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get shape of data (rows, columns)\ndf_prev.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the type of dataset\ndf_prev.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# displaying the informtion of previous application dataset\ndf_prev.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describing the previous application dataset\ndf_prev.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding percentage of null values columns\nNA_col_pre = column_wise_null_percentage(df_prev)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# identify columns only with null values\nNA_col_pre = NA_col_pre[NA_col_pre>0]\nNA_col_pre","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grafical representation of columns having % null values\nplt.figure(figsize= (20,4),dpi=300)\nNA_col_pre.plot(kind = 'bar')\nplt.title (' columns having null values')\nplt.ylabel('% null values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the column with null values more than 50%\nNA_col_pre = NA_col_pre[NA_col_pre>50]\nprint(\"Number of columns having null value more than 50% :\", len(NA_col_pre.index))\nprint(NA_col_pre)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Droped all columns from Dataframe for which missing value percentage are more than 50%.\n``````    \n    'AMT_DOWN_PAYMENT', 'RATE_DOWN_PAYMENT', 'RATE_INTEREST_PRIMARY','RATE_INTEREST_PRIVILEGED'\n``````","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# removed 4 columns having null percentage more than 50%.\ndf_prev = df_prev.drop(NA_col_pre.index, axis =1)\ndf_prev.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the Application dataset with previous appliaction dataset\n\ndf_comb = pd.merge(left=df_app,right=df_prev,how='inner',on='SK_ID_CURR',suffixes='_x')\ndf_comb.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_comb.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the column names after merging from combined df\n\ndf_comb = df_comb.rename({'NAME_CONTRACT_TYPE_' : 'NAME_CONTRACT_TYPE','AMT_CREDIT_':'AMT_CREDIT','AMT_ANNUITY_':'AMT_ANNUITY',\n                         'WEEKDAY_APPR_PROCESS_START_' : 'WEEKDAY_APPR_PROCESS_START',\n                         'HOUR_APPR_PROCESS_START_':'HOUR_APPR_PROCESS_START','NAME_CONTRACT_TYPEx':'NAME_CONTRACT_TYPE_PREV',\n                         'AMT_CREDITx':'AMT_CREDIT_PREV','AMT_ANNUITYx':'AMT_ANNUITY_PREV',\n                         'WEEKDAY_APPR_PROCESS_STARTx':'WEEKDAY_APPR_PROCESS_START_PREV',\n                         'HOUR_APPR_PROCESS_STARTx':'HOUR_APPR_PROCESS_START_PREV'}, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing unwanted columns from cmbined df for analysis\n\ndf_comb.drop(['SK_ID_CURR','WEEKDAY_APPR_PROCESS_START', 'HOUR_APPR_PROCESS_START','REG_REGION_NOT_LIVE_REGION', \n              'REG_REGION_NOT_WORK_REGION','LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY',\n              'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY','WEEKDAY_APPR_PROCESS_START_PREV',\n              'HOUR_APPR_PROCESS_START_PREV', 'FLAG_LAST_APPL_PER_CONTRACT','NFLAG_LAST_APPL_IN_DAY'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Performing univariate analysis**","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Distribution of contract status in logarithmic scale\n# Distribution of contract status in logarithmic scale\n\nsns.set_style('whitegrid')\nsns.set_context('talk')\n\nplt.figure(figsize=(10,25),dpi = 300)\nplt.rcParams[\"axes.labelsize\"] = 20\nplt.rcParams['axes.titlesize'] = 22\nplt.rcParams['axes.titlepad'] = 30\nplt.xticks(rotation=90)\nplt.xscale('log')\nplt.title('Distribution of contract status with purposes')\nax = sns.countplot(data = df_comb, y= 'NAME_CASH_LOAN_PURPOSE', \n                   order=df_comb['NAME_CASH_LOAN_PURPOSE'].value_counts().index,hue = 'NAME_CONTRACT_STATUS') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Points to be concluded from above plot:\n\nMost rejection of loans came from purpose 'repairs'.\nFor education purposes we have equal number of approves and rejection\nPayign other loans and buying a new car is having significant higher rejection than approves.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of contract status\n\nsns.set_style('whitegrid')\nsns.set_context('talk')\n\nplt.figure(figsize=(10,30),dpi = 300)\nplt.rcParams[\"axes.labelsize\"] = 20\nplt.rcParams['axes.titlesize'] = 22\nplt.rcParams['axes.titlepad'] = 30\nplt.xticks(rotation=90)\nplt.xscale('log')\nplt.title('Distribution of purposes with target ')\nax = sns.countplot(data = df_comb, y= 'NAME_CASH_LOAN_PURPOSE', \n                   order=df_comb['NAME_CASH_LOAN_PURPOSE'].value_counts().index,hue = 'TARGET') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Few points we can conclude from abpve plot:\n\nLoan purposes with 'Repairs' are facing more difficulites in payment on time.\nThere are few places where loan payment is significant higher than facing difficulties. They are 'Buying a garage', 'Business developemt', 'Buying land','Buying a new car' and 'Education' Hence we can focus on these purposes for which the client is having for minimal payment difficulties.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Bivariate analysis**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plotting for Credit amount in logarithmic scale\n\nplt.figure(figsize=(20,15),dpi = 300)\nplt.xticks(rotation=90)\nplt.yscale('log')\nsns.boxplot(data =df_comb, x='NAME_CASH_LOAN_PURPOSE',hue='NAME_INCOME_TYPE',y='AMT_CREDIT_PREV',orient='v')\nplt.title('Prev Credit amount vs Loan Purpose')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we can conclude some points-\n\nThe credit amount of Loan purposes like 'Buying a home','Buying a land','Buying a new car' and'Building a house' is higher.\nIncome type of state servants have a significant amount of credit applied\nMoney for third person or a Hobby is having less credits applied for.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plotting for Credit amount prev vs Housing type in logarithmic scale\n\nplt.figure(figsize=(15,15),dpi = 150)\nplt.xticks(rotation=90)\nsns.barplot(data =df_comb, y='AMT_CREDIT_PREV',hue='TARGET',x='NAME_HOUSING_TYPE')\nplt.title('Prev Credit amount vs Housing type')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here for Housing type, office appartment is having higher credit of target 0 and co-op apartment is having higher credit of target 1. So, we can conclude that bank should avoid giving loans to the housing type of co-op apartment as they are having difficulties in payment. Bank can focus mostly on housing type with parents or House\\appartment or miuncipal appartment for successful payments.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 6. Conclusion/Recomendation:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**1. Banks should focus more on contract type ‘Student’ ,’pensioner’ and ‘Businessman’ with housing ‘type other than ‘Co-op apartment’ for successful payments.**\n\n**2. Banks should focus less on income type ‘Working’ as they are having most number of unsuccessful payments.**\n\n**3. In loan purpose ‘Repairs’:**\n\n> a. Although having higher number of rejection in loan purposes with 'Repairs' there are observed difficulties in payment on time.<br> \nb. There are few places where loan payment is delay is significantly high.<br> \nc. Bank should keep continue to caution while giving loan for this purpose.\n\n**4. Bank should avoid giving loans to the housing type of co-op apartment as they are having difficulties in payment.**\n\n**5. Bank can focus mostly on housing type ‘with parents’ , ‘House\\apartment’ and ‘municipal apartment’ for successful payments.**\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}