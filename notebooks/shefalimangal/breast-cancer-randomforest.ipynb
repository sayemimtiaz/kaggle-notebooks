{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#import modules\n%matplotlib inline\n\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier \nfrom urllib.request import urlopen \n\n\nplt.style.use('ggplot')\npd.set_option('display.max_columns', 500) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LOAD DATA\n\nload data into panda dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"breast_cancer = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['id', 'diagnosis', 'radius_mean', \n         'texture_mean', 'perimeter_mean', 'area_mean', \n         'smoothness_mean', 'compactness_mean', \n         'concavity_mean','concave_points_mean', \n         'symmetry_mean', 'fractal_dimension_mean',\n         'radius_se', 'texture_se', 'perimeter_se', \n         'area_se', 'smoothness_se', 'compactness_se', \n         'concavity_se', 'concave_points_se', \n         'symmetry_se', 'fractal_dimension_se', \n         'radius_worst', 'texture_worst', \n         'perimeter_worst', 'area_worst', \n         'smoothness_worst', 'compactness_worst', \n         'concavity_worst', 'concave_points_worst', \n         'symmetry_worst', 'fractal_dimension_worst']\n\ndx = ['Benign', 'Malignant']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning:\n    We do some minor cleanage like setting the id_number to be the data frame index, along with converting the diagnosis to the standard binary 1, 0 representation using the map() function."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting 'id_number' as our index\n\nbreast_cancer.set_index(['id'], inplace=True)\n\n# Converted to binary to help later on with models and plots\n\nbreast_cancer['diagnosis'] = breast_cancer['diagnosis'].map({'M': 1, 'B': 0})\nbreast_cancer.diagnosis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Values\nGiven context of the data set, I know that there is no missing data, but I ran an apply method utilizing a lambda expression that checks to see if there was any missing values through each column. Printing the column name and total missing values for that column, iteratively."},{"metadata":{"trusted":true},"cell_type":"code","source":"breast_cancer.apply(lambda x: x.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to delete this extra column since it doesn't contain any data.\n\nThis following code will be used for the random forest model, where the id_number won't be relevant."},{"metadata":{},"cell_type":"markdown","source":"Here we're deleting the extra column"},{"metadata":{"trusted":true},"cell_type":"code","source":" #For later use in CART models\nnames_index = names[2:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del breast_cancer['Unnamed: 32']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's preview the data set utilizing the head() function which will give the first 5 values of our data frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"breast_cancer.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we'll give the dimensions of the data set; where the first value is the number of patients and the second value is the number of features.\n\nWe print the data types of our data set this is important because this will often be an indicator of missing data, as well as giving us context to anymore data cleanage."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Here's the dimensions of our data frame:\\n\", \n     breast_cancer.shape)\nprint(\"Here's the data types of our columns:\\n\",\n     breast_cancer.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"breast_cancer.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Training and Test Sets\nLet's split the data set into our training and test sets, which will be pseudo-randomly selected to create a 80-20% split. You will use the training set to train the model and perform some optimization. You will use the test set, which will act as unseen data, to assess model performance.\n\nWhen using this method for machine learning, always be wary of utilizing your test set to create models. Data leakage is a common problem that can result in overfitting. "},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_space = breast_cancer.iloc[:,breast_cancer.columns != 'diagnosis']\nfeature_class = breast_cancer.iloc[:,breast_cancer.columns == 'diagnosis']\n\ntraining_set, test_set, class_set, test_class_set = train_test_split(feature_space,feature_class,test_size = 0.20,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # Cleaning test sets to avoid future warning messages\nclass_set = class_set.values.ravel() \ntest_class_set = test_class_set.values.ravel() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting Random Forest\nNow, let's create the model, starting with parameter tuning. Here are the parameters we will be tuning in this tutorial:\n\nmax_depth: The maximum splits for all trees in the forest. bootstrap: An indicator of whether or not we want to use bootstrap samples when building trees.\nmax_features: The maximum number of features that will be used in node splitting — the main difference I previously mentioned between bagging trees and random forest. Typically, you want a value that is less than p, where p is all features in your data set.\ncriterion: This is the metric used to asses the stopping criteria for the decision trees.\nOnce we've instantiated our model, we will go ahead and tune our parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_rf = RandomForestClassifier(random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameters Optimization\nUtilizing the GridSearchCV functionality, let's create a dictionary with parameters we are looking to optimize to create the best model for our data. Setting the n_jobs to 3 tells the grid search to run three jobs in parallel, reducing the time the function will take to compute the best parameters. I included the timer to see how long different jobs took; that led me to ultimately decide to use three parallel jobs.\n\nThis will help set the parameters we will use to tune one final parameter: the number of trees in our forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)\nstart = time.time()\nparam_dist = {'max_depth':[2,3,4],\n              'bootstrap':[True,False],\n              'max_features':['auto','sqrt','log2',None],\n              'criterion': ['gini','entropy']}\ncv_rf = GridSearchCV(fit_rf,cv=10,\n                    param_grid = param_dist,\n                    n_jobs = 3)\ncv_rf.fit(training_set,class_set)\n\nprint('Best Parameters using grid search: \\n', \n      cv_rf.best_params_)\nend = time.time()\nprint('Time taken in grid search: {0: .2f}'.format(end - start))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once we are given the best parameter combination, we set the parameters to our model.\n\nNotice how we didn't utilize the bootstrap: True parameter, this will make sense in the following section."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set best parameters given by grid search \nfit_rf.set_params(criterion = 'entropy',\n                  max_features = 'log2', \n                  max_depth = 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# OOB Rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_rf.set_params(warm_start=False, \n                  oob_score=True)\n\nmin_estimators = 15\nmax_estimators = 1000\n\nerror_rate = {}\n\nfor i in range(min_estimators, max_estimators + 1):\n    fit_rf.set_params(n_estimators=i)\n    fit_rf.fit(training_set,class_set)\n\n    oob_error = 1 - fit_rf.oob_score_\n    error_rate[i] = oob_error\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert dictionary to a pandas series for easy plotting \noob_series = pd.Series(error_rate)\n\nprint(oob_series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(figsize=(10,10))\n\nax.set_facecolor('#fafafa')\noob_series.plot(kind = 'line',\n               color = 'red')\nplt.axhline(0.055,\n           color = '#875FDB',\n           linestyle = '--')\nplt.xlabel('n_estimators')\nplt.ylabel('OOB Error Rate')\nplt.title('OOB Error Rate Across various Forest sizes \\n(From 15 to 1000 trees)')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The OOB error rate starts to oscilate at around 400 trees, so I will go ahead and use my judgement to use 400 trees in my forest. Using the pandas series object I can easily find the OOB error rate for the estimator as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('OOB error rate for 400 tree is: {0:5f}'.format(oob_series[400]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Utilizing the OOB error rate that was created with the model gives us an unbiased error rate. Since OOB can be calculated with the model estimation, it's helpful when cross validating and/or optimizing hyperparameters prove to be too computationally expensive.\n\nFor the sake of this tutorial, I will go over other traditional methods for optimizing machine learning models, including the training and test error route and cross validation metrics."},{"metadata":{},"cell_type":"markdown","source":"# Traditional Training and Test Set Split¶\nIn order for this methodology to work we will set the number of trees calculated using the OOB error rate, and removing the warm_start and oob_score parameters. Along with including the bootstrap parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_rf.set_params(n_estimators = 400,\n                 bootstrap = True,\n                 warm_start = False,\n                 oob_score = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Algorithm\nNext we train the algorithm utilizing the training and target class set we had made earlier."},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_rf.fit(training_set, class_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Variable Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"def variable_importance(fit):\n    try:\n        if not hasattr(fit, 'fit'):\n            return print(\"'{0}' is not an instantiated model from scikit-learn\".format(fit)) \n                # Captures whether the model has been trained\n        if not vars(fit)[\"estimators_\"]:\n            return print(\"Model does not appear to be trained.\")\n    except KeyError:\n        print(\"Model entered does not contain 'estimators_' attribute.\")\n\n    importances = fit.feature_importances_\n    indices = np.argsort(importances)[::-1]\n    return {'importance': importances,\n            'index': indices}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_imp_rf = variable_importance(fit_rf)\n\nimportances_rf = var_imp_rf['importance']\n\nindices_rf = var_imp_rf['index']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_var_importance(importance, indices, name_index):\n    \n    print(\"Feature ranking:\")\n    \n\n    for f in range(0, indices.shape[0]):\n        i = f\n        print(\"{0}. The feature '{1}' has a Mean Decrease in Impurity of {2:.5f}\"\n              .format(f + 1,\n                      names_index[indices[i]],\n                      importance[indices[f]]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_var_importance(importances_rf, indices_rf, names_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def variable_importance_plot(importance, indices, name_index):\n    \"\"\"\n    Purpose\n    ----------\n    Prints bar chart detailing variable importance for CART model\n    NOTE: feature_space list was created because the bar chart\n    was transposed and index would be in incorrect order.\n\n    Parameters\n    ----------\n    * importance: Array returned from feature_importances_ for CART\n                models organized by dataframe index\n    * indices: Organized index of dataframe from largest to smallest\n                based on feature_importances_\n    * name_index: Name of columns included in model\n\n    Returns:\n    ----------\n    Returns variable importance plot in descending order\n    \"\"\"\n    index = np.arange(len(names_index))\n\n    importance_desc = sorted(importance)\n    feature_space = []\n    for i in range(indices.shape[0] - 1, -1, -1):\n        feature_space.append(names_index[indices[i]])\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n\n    #ax.set_axis_bgcolor('#fafafa')\n    plt.title('Feature importances for Random Forest Model\\\n    \\nBreast Cancer (Diagnostic)')\n    plt.barh(index,\n             importance_desc,\n             align=\"center\",\n             color = '#875FDB')\n    plt.yticks(index,\n               feature_space)\n\n    plt.ylim(-1, 30)\n    plt.xlim(0, max(importance_desc) + 0.01)\n    plt.xlabel('Mean Decrease in Impurity')\n    plt.ylabel('Feature')\n\n    plt.show()\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variable_importance_plot(importances_rf, indices_rf, names_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cross_val_metrics(fit, training_set, class_set, estimator, print_results = True):\n    \"\"\"\n    Purpose\n    ----------\n    Function helps automate cross validation processes while including \n    option to print metrics or store in variable\n\n    Parameters\n    ----------\n    fit: Fitted model \n    training_set:  Data_frame containing 80% of original dataframe\n    class_set:     data_frame containing the respective target vaues \n                      for the training_set\n    print_results: Boolean, if true prints the metrics, else saves metrics as \n                      variables\n\n    Returns\n    ----------\n    scores.mean(): Float representing cross validation score\n    scores.std() / 2: Float representing the standard error (derived\n                from cross validation score's standard deviation)\n    \"\"\"\n    my_estimators = {\n    'rf': 'estimators_',\n    'nn': 'out_activation_',\n    'knn': '_fit_method'\n    }\n    try:\n        # Captures whether first parameter is a model\n        if not hasattr(fit, 'fit'):\n            return print(\"'{0}' is not an instantiated model from scikit-learn\".format(fit)) \n\n        # Captures whether the model has been trained\n        if not vars(fit)[my_estimators[estimator]]:\n            return print(\"Model does not appear to be trained.\")\n\n    except KeyError as e:\n        print(\"'{0}' does not correspond with the appropriate key inside the estimators dictionary. \\\n\\nPlease refer to function to check `my_estimators` dictionary.\".format(estimator))\n        raise\n\n    n = KFold(n_splits=10)\n    scores = cross_val_score(fit, \n                         training_set, \n                         class_set, \n                         cv = n)\n    if print_results:\n        for i in range(0, len(scores)):\n            print(\"Cross validation run {0}: {1: 0.3f}\".format(i, scores[i]))\n        print(\"Accuracy: {0: 0.3f} (+/- {1: 0.3f})\"\\\n              .format(scores.mean(), scores.std() / 2))\n    else:\n        return scores.mean(), scores.std() / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_metrics(fit_rf, \n                  training_set, \n                  class_set, \n                  'rf',\n                  print_results = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Set Metrics\nUsing the test set that was created earlier, let's examine another metric for the evaluation of our model. You'll recall that that we didn't touch the test set until now — after we had completed hyperparamter optimization — to avoid the problem of data leakage."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_rf = fit_rf.predict(test_set)\nprint(predictions_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix\nHere we create a confusion matrix visual with seaborn and transposing the matrix when creating the heatmap."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(test_class_set, predictions):\n    \"\"\"Function returns confusion matrix comparing two arrays\"\"\"\n    if (len(test_class_set.shape) != len(predictions.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (test_class_set.shape != predictions.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = test_class_set,\n                                        columns = predictions)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = create_conf_mat(test_class_set, predictions_rf)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_rf = fit_rf.score(test_set, test_class_set)\n\nprint(\"Here is our mean accuracy on the test set:\\n {0:.3f}\"\\\n      .format(accuracy_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we calculate the test error rate!\ntest_error_rate_rf = 1 - accuracy_rf\nprint(\"The test error rate for our model is:\\n {0: .4f}\"\\\n      .format(test_error_rate_rf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ROC Curve Metrics\nA receiver operating characteristic (ROC) curve calculates the false positive rates and true positive rates across different thresholds. Let's graph these calculations.\n\nIf our curve is located in the top left corner of the plot, that indicates an ideal model; i.e., a false positive rate of 0 and true positive rate of 1. On the other hand, a ROC curve that is at 45 degrees is indicative of a model that is essentially randomly guessing.\n\nWe will also calculate the area under the curve (AUC). The AUC is used as a metric to differentiate the prediction power of the model for patients with cancer and those without it. Typically, a value closer to 1 means that our model was able to differentiate correctly from a random sample of the two target classes of two patients with and without the disease."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We grab the second array from the output which corresponds to\n# to the predicted probabilites of positive classes \n# Ordered wrt fit.classes_ in our case [0, 1] where 1 is our positive class\npredictions_prob = fit_rf.predict_proba(test_set)[:, 1]\n\nfpr2, tpr2, _ = roc_curve(test_class_set,\n                          predictions_prob,\n                          pos_label = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auc_rf = auc(fpr2, tpr2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(fpr, tpr, auc, estimator, xlim=None, ylim=None):\n    \"\"\"\n    Purpose\n    ----------\n    Function creates ROC Curve for respective model given selected parameters.\n    Optional x and y limits to zoom into graph\n\n    Parameters\n    ----------\n    * fpr: Array returned from sklearn.metrics.roc_curve for increasing\n            false positive rates\n    * tpr: Array returned from sklearn.metrics.roc_curve for increasing\n            true positive rates\n    * auc: Float returned from sklearn.metrics.auc (Area under Curve)\n    * estimator: String represenation of appropriate model, can only contain the\n    following: ['knn', 'rf', 'nn']\n    * xlim: Set upper and lower x-limits\n    * ylim: Set upper and lower y-limits\n    \"\"\"\n    my_estimators = {'knn': ['Kth Nearest Neighbor', 'deeppink'],\n              'rf': ['Random Forest', 'red'],\n              'nn': ['Neural Network', 'purple']}\n\n    try:\n        plot_title = my_estimators[estimator][0]\n        color_value = my_estimators[estimator][1]\n    except KeyError as e:\n        print(\"'{0}' does not correspond with the appropriate key inside the estimators dictionary. \\\n\\nPlease refer to function to check `my_estimators` dictionary.\".format(estimator))\n        raise\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.set_facecolor('#fafafa')\n\n    plt.plot(fpr, tpr,\n             color=color_value,\n             linewidth=1)\n    plt.title('ROC Curve For {0} (AUC = {1: 0.3f})'\\\n              .format(plot_title, auc))\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=2) # Add Diagonal line\n    plt.plot([0, 0], [1, 0], 'k--', lw=2, color = 'black')\n    plt.plot([1, 0], [1, 1], 'k--', lw=2, color = 'black')\n    if xlim is not None:\n        plt.xlim(*xlim)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.show()\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(fpr2, tpr2, auc_rf, 'rf',\n               xlim=(-0.01, 1.05), \n               ylim=(0.001, 1.05))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curve(fpr2, tpr2, auc_rf, 'rf', \n               xlim=(-0.01, 0.2), \n               ylim=(0.85, 1.01))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_class_report(predictions, alg_name):\n    \"\"\"\n    Purpose\n    ----------\n    Function helps automate the report generated by the\n    sklearn package. Useful for multiple model comparison\n\n    Parameters:\n    ----------\n    predictions: The predictions made by the algorithm used\n    alg_name: String containing the name of the algorithm used\n    \n    Returns:\n    ----------\n    Returns classification report generated from sklearn. \n    \"\"\"\n    print('Classification Report for {0}:'.format(alg_name))\n    print(classification_report(predictions, \n            test_class_set, \n            target_names = dx))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_report = print_class_report(predictions_rf, 'Random Forest')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}