{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nWelcome to my Kernel! This is my first Kaggle project that I am creating for my class final project this semester. \n\nThe dataset is about the **housing prices in King County, WA,** whose county seat is Seattle. It is a predominantly urban and suburban county. \n\nMore information on the place : \n\n[https://en.wikipedia.org/wiki/King_County,_Washington](http://)\n\n![](http://upload.wikimedia.org/wikipedia/commons/b/bc/Seattle_-_King_County_Courthouse_and_King_County_Administration_Building_01.jpg)\n\nI have first tried to **visualize the data, clean it and ultimately build a linear regression model to analyze it**.\n\nI have used some **python packages** to carryout the analyses and build the regression model.\n\nThis project helps to solve the problem a lot of the potential users face: with the housing market being so diverse and with so many factors influencing the prices coming into play, one can easily be overwhelmed. By the menas of this project, I want to highlight the important components of such a market in a purely unbiased,data-driven way. After looking at this project, one might get a good enough idea about the housing market in King County, WA in 2014-15.\n\nThis kernel will be extremely helpful for **potential buyers, realtors and builders** who will get more insights into what influences prices and help them take more educated data-driven decisions.\n\nI would greatly appreciate any further comments/questions.\n\nSince it is created for academic purposes, detailed inferences/documentation has been provided. Feel free to jump to the conclusion for a snapshot of the findings.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 1.Data Exploration     "},{"metadata":{},"cell_type":"markdown","source":"#  1.1.Importing the relevant modules and the dataset"},{"metadata":{},"cell_type":"markdown","source":"I have imported relevant libraries in order to carry out the analysis.**For the final project,they have been documneted in the final project proposal.**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#override the matplotlib style of graphs wiht seaborn.\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the pandas method to create a dataframe and print the first 5 rows of it. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"raw_data = pd.read_csv(\"../input/housesalesprediction/kc_house_data.csv\")\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get more information on the type of variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get some **descriptive statistics** like mean, standard deviation, minimum and maximum values,etc. The \"include ='all'\" method is used to get data on both categorical and numerical data."},{"metadata":{},"cell_type":"markdown","source":"#  1.2.Describing the preliminary data "},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.describe(include='all')\n#We observe no missing values at first which we confirm later. \n#There seem to be a lot of house with some that are exceptionally priced.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following method checks for any missing values and returns their sum. It is important to get rid of missing values to create an accurate model."},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The price is going to be the dependent variable ****which is influence by other variables like number of bedrooms,bathrooms, condition,grade, waterfront,etc.** \n\nFor the final project, the variables have been explored in more detail in the documentation file supporting the project."},{"metadata":{},"cell_type":"markdown","source":"We drop the columns 'id' and 'date' since they don't give us much information about the price."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = raw_data.drop(['id','date'],axis = 1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  1.3. Correaltion Matrix"},{"metadata":{},"cell_type":"markdown","source":"We use a **heatmap** in order to find the correaltion between the variables of this dataset. The further section,Data Visualization, uses this heatmap primarily."},{"metadata":{"trusted":true},"cell_type":"code","source":"#change the size of the figure using this matplotlib me.thod\nplt.subplots(figsize=(15,10))\n\n#plot a correalation heatmap using seaborn. Border the squares with black color, show the correaltion index and round it up.\nsns.heatmap(data.corr(), annot = True,linewidths=.5,linecolor='black',fmt = '1.1f')\n\n#give a title to the map and display it.\nplt.title('correlation heatmap',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.Data Visualization"},{"metadata":{},"cell_type":"markdown","source":"In this section, we surround all the visualizations and their analyses around the dependent variable, price. \n\n**This data being from a mix of urban and suburban locations, has a lot of outliers or exceptions. These cannot be fully explained because of the disparity in the price of houses across various neighborhoods. \n**\n\nWe can try to reason the existence of such exceptions looking at just the correaltion but we **can never fully explain the causation.**"},{"metadata":{},"cell_type":"markdown","source":"#  2.1. Relationship between price and the number of bedrooms, bathrooms and floors:"},{"metadata":{},"cell_type":"markdown","source":"* For a quick overview, we use the pairplot method of seaborn. It will pairwise relationships of different variable subsets of a dataset on rows and columns.\n* It seems that the **number of bedrooms, bathrooms and floors have a positive correaltion with price and with each other.**\n* This is pretty intuitive because the more the number of these variables, the bigger the house and the costlier it would be.\n* We shouldn't jump to conclusions though since there a lot of outliers that we will deal with on a case-by-case basis."},{"metadata":{"trusted":true},"cell_type":"code","source":"#We use this in-built seaborn method to plot the specified variables and display regression lines to summarize the trends.\nsns.pairplot(data,vars = [\"price\",\"bedrooms\",\"bathrooms\",\"floors\"], kind =\"reg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.2. Number of Bedrooms and Price"},{"metadata":{},"cell_type":"markdown","source":"We obtain the unique entries in this following column."},{"metadata":{"trusted":true},"cell_type":"code","source":"#pandas mehtod to obtain the unique values of this variable to understand which values have been taken.\ndata['bedrooms'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the **no of bedrooms is not continuous, we can plot it using a boxplot.** A box plot (or box-and-whisker plot) shows the distribution of quantitative data in a way that facilitates comparisons between variables or across levels of a categorical variable. The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution, except for points that are determined to be “outliers” using a method that is a function of the inter-quartile range."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12,10))\n\n#seaborn method to plot a boxplot using the specified variables from the dataset.\nsns.boxplot(x=\"bedrooms\", y = \"price\",data= data)\n\nplt.title('price vs no of bedrooms',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most of the houses have an average of **3 to 4 bedrooms**.\n* The horzintal line across the boxes denotes the median price, the lower half and the upper half represent the 25th and the 75th quartile respectively. The vertical line is the typical range and outliers above the range are represented as individual points.\n* There are **a lot of outliers** interestingly. A house with no bedrooms might be a studio apartment and there is one with 33 bedrooms. Surprizingly, it is modestly priced in comparision to some other houses.\n* For any given number of bedrooms, there are a lot of outliers which donn't fall under the range. This can be because c**ertain houses in the Greater Seattle Area might naturally be more expensive than a suburban house with more no of bedrooms.**\n* **Thus, this is not the only variable that explains price. We have to take into account the zipcode, size and other variables as well.**"},{"metadata":{},"cell_type":"markdown","source":"# 2.3. Number of Bathrooms and Price"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['bathrooms'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of bathrooms seems to follow a **continuous distribution.** A **histogram** would be an ideal option to denote it.[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12,10))\n\n#The underscore is a dummy variable used for making it 2D.\n_=plt.hist(data['bathrooms'],color='salmon')\n_=plt.xlabel('no of bathrooms')\n_=plt.ylabel('price')\n\nplt.title('price vs no of bathrooms',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Again, as in the case of the number of bedrooms, the number of bathrooms and the price varies a lot. Some houses with 2 bathrooms are priced way more than some wiht 4 or 5.\n* We might be tempted to think that the number of bathrooms are the price show an increasing trend. We have to keep in mind that this dataset primarily comes from an urban and a suburban region. Exceptions are therfore bound to occur."},{"metadata":{},"cell_type":"markdown","source":"# 2.4. Number of Floors and Price"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['floors'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given the distribution of the variable, a **bargraph** would be the simplest way to visualize this variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,10))\n\n#seaborn method for plotting a bargraph.\nsns.barplot(x=\"floors\",y=\"price\",data=data,palette=\"Blues_d\")\n\nplt.title('price vs no of floors',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As expected, although there is a postive correaltion between these two variables, **there isn't an obvious trend.**\n* Penthouses and loft apartments in downtown Seattle might definitely be more expensive than a three-story suburban colonel."},{"metadata":{},"cell_type":"markdown","source":"# 2.5. Living Area, Waterfront, View and Price"},{"metadata":{},"cell_type":"markdown","source":"Given the number of variables involved,a **relplot** is the best option to explore these varibles.\n\nThis function provides access to several different axes-level functions that show the relationship between two variables with semantic mappings of subsets.The relationship between x and y can be shown for different subsets of the data using the hue, size, and style parameters.\n\nWe note that the **waterfront has already been converted into a dummy variable by mapping no waterfront and a waterfront with 0 and 1 respectively.\n**\nTHe view just tells us how good the view of the waterfront is, if any, on a scale of 0 to 4."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#seaborn method for a 'relpot':view acts as a further breakdown dimension. We change the look of the graph by using another color pallate.\nsns.relplot(x=\"sqft_living\",y=\"price\",hue=\"waterfront\",col=\"view\",palette=[\"g\", \"r\"],data=data)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As evident from the graphs above, **a house with a good view of the waterfront definitely costs more on an average.** There is one house which doesn't have as good as a view but still costs more.\n* As expected, the presence of a waterfront is not game-changing because a lot of houses with no such views still continue to priced similarly.\n* But for houses with a similar living area, **a very good view of the waterfront shoots the price up **(See graph no 4 ).\n"},{"metadata":{},"cell_type":"markdown","source":"# 2.6. Living Area,Lot Size, Living Area and Lot Size in the Proximity and Price"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data, vars = [\"sqft_living\",\"sqft_lot\",\"sqft_basement\",\"sqft_living15\",\"sqft_lot15\"], kind =\"reg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is a **lot of correalation between the living area**('sqft_living') and the **other variables **like the size of the lot('sqft_lot'), basement('sqft_basement') and the living area and plot size of the nearest 15 houses ('sqft_living15' and 'sqft_lot15' respectively).\n* **A large living area would definitely mean more room for basement and a larger lot**. As evident from the correlation heatmap, a larger living area is also strongly correlated with the number of bedrooms, bathrooms and floors.\n* Houses that are close to each other are similar in area and similarly priced(refer the heatmap).This can be attributed to the clustering of similar houses into a neighborhood. **More affluent neighborhoods will also have similar prices.**\n* There aren't as many exceptions in this case."},{"metadata":{},"cell_type":"markdown","source":"# 2.7. Grade and Price"},{"metadata":{},"cell_type":"markdown","source":"This tells us how the grade, that is the quality of construction materials used might affet price."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['grade'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given the kind of distribution, using a boxplot makes sense."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12,10))\n\nsns.boxplot(x=\"grade\", y = \"price\",data= data,palette=\"Set3\")\n\nplt.title('price vs grade',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It is easy to see an obvious **upward trend** in the graph.\n* Better construction materials increase and the cost of labor and raw materials which is refected by the price.\n* Although we can't deny some exceptions, they definitely aren't as deviant as some of the variables that we have seen earlier."},{"metadata":{},"cell_type":"markdown","source":"# 2.8. Condition and Price"},{"metadata":{},"cell_type":"markdown","source":"This variable explores how good or bad the condition of the house is on a scale of 1 to 5."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['condition'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12,10))\n\nsns.boxplot(x=\"condition\", y = \"price\",data= data,palette=\"Set1\")\n\nplt.title('price vs condition',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We observe that **a better condition doesn't necessarily imply a higher price.**\n* Again it would be to broad to generalize anything given the number of outliers.\n* It is interesting to note that houses in a mediocre condition(grade=3) have a lot of outliers. It is an intersting point for further exploration by the concerned users."},{"metadata":{},"cell_type":"markdown","source":"# 2.9. Grade, Condition and Price"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,10))\n\n#we use a scatterplot to analyze the relationship between price and grade and further break it down using condiiton.\nsns.scatterplot(x=\"grade\",y=\"price\",hue=\"condition\",size=\"condition\",sizes=(20, 200),data=data)\n\nplt.title('relationship between price,grade and condition',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It is pretty interesting to note that **although better grade of materials used demands a higher price, it doesn't necessarily mean that the house is any better condition.**\n* On an average, **houses that were constructed using quality materials as are in a similar condition as their counterparts built with mediocre materials.**\n* However is that houses in an excellent condition(condition=6) were mainly built using the best grade materials(12 or 13).\n* Again, we have to keep in mind that other variables like the year in which the house was built and the fact that it was renovated or not might play a role.\n* A quick look at the heatmap does indicate **a negative correalation between condition of the house and the year in which it was built.** There is however, no such correlation between condtion and renovation.\n* Some other variables like maintenance and deterioration due to weather conditions might have a hand. These are out of our scope though for the given dataset.\n"},{"metadata":{},"cell_type":"markdown","source":"# 2.10. Year Built"},{"metadata":{},"cell_type":"markdown","source":"This gives us an overview of the years in which the first 50 houses were built."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,15))\n\n#this matplotlib method gives us the distribution of the counts of the first 50 observations and the year in which they were built. \n#the argument passed in displays the percentage upto the first decimal place.\ndata.yr_built.value_counts().head(50).plot.pie(autopct='%1.1f%%')\n\nplt.title('year built pie chart',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.yr_built.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **A lot of houses were built in the 2000s and 2010s.**\n* A quick representation of the count of unique values confirms the fact."},{"metadata":{},"cell_type":"markdown","source":"# 2.11. Location"},{"metadata":{},"cell_type":"markdown","source":"We plot the latitude and longitude coordinates given in the dataset. \n\nThe axes range from negative to positive values because of how directions are plotted on a graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12,10))\n\nplt.scatter(data['long'],data['lat'],color=\"purple\")\n\n#we set the limits according to the cartographical convention.\nplt.xlim(-180,180)\nplt.ylim(-180,180)\n\nplt.xlabel('longitude')\nplt.ylabel('latitude')\n\nplt.title('distribution of houses',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we zoom in for a better picture.\n\nplt.subplots(figsize=(12,10))\n\nplt.scatter(data['long'],data['lat'],color=\"purple\")\n\n#note that the coordinates have been selected based on the output of the previous scatterplot and hence won't be 100% accurate.\nplt.xlim(-121.2,-122.6)\nplt.ylim(47,47.9)\n\nplt.xlabel('longitude')\nplt.ylabel('latitude')\n\nplt.title('distribution of houses closeup',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://www.seattle.gov/Images/Clerk/DistrictsMap.jpg)\n* This yield a very interesting result. **Majority of the houses are located in the Greater Seattle Region.**\n* More location details can be found out by plugging in the values of the longitude and the latitude in online calculators such as this one:\n[https://www.latlong.net/](http://)\n"},{"metadata":{},"cell_type":"markdown","source":"# 2.12.Living Area and Price"},{"metadata":{},"cell_type":"markdown","source":"The **most important relationship** would perhaps be this one. This forms the basis of our next few sections."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12,10))\n\ny=data['price']\nx=data['sqft_living']\n\nplt.scatter(x,y,color='green')\n\nplt.title('price vs living area',size = 18)\nplt.show()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see an **exponential type of relationship**. We will analyze it after dealing with outliers."},{"metadata":{},"cell_type":"markdown","source":"# 3.Dealing with Outliers"},{"metadata":{},"cell_type":"markdown","source":"**We draw the probability density functions of some variables to understand the concentration and distribution of the variable. We can also see the distribuiton of outliers and remove them.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(8,8))\n\n#this in-built seaborn method plot the necessary graph.\nsns.distplot(data['price'],color='crimson')\n\nplt.title('pdf of price',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We drop the 99th percentile since most of them are exceptions."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(8,8))\n\n#we create a new variable to contain the observations in the 99th percentile, that is the most dramatic outliers.\nq = data['price'].quantile(0.99)\n\n#we store it in a new data fram that contains all the observations except for the top 1 percentile. They would normally represent some luxury houses.\ndata_1 = data[data['price']<q]\ndata_1.describe(include = \"all\")\n\nsns.distplot(data_1['price'],color='crimson')\n\nplt.title('pdf of price less than the 99th percentile',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While still there are many outliers, as against the previous case, they are far fewer.Getting rid of all the outliers at the same time will make our model incapable of explaining exceptions altogether and paint an inaccurate and biased image of housing prices."},{"metadata":{},"cell_type":"markdown","source":"We identify some variables with the most significant number of outlier from our previous analyses and drop the 99th percentile.\nEvery time we remove the outliers of a particular variable, we create a new dataframe with updated values."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(8,8))\n\nsns.distplot(data_1['bedrooms'],color='m')\n\nplt.title('pdf of no of bedrooms',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(8,8))\n\np=data_1['bedrooms'].quantile(0.99)\ndata_2 = data_1[data_1['bedrooms']<p]\n\nsns.distplot(data_2['bedrooms'],color='m')\n\nplt.title('pdf of no of bedrooms less than 99th percentile',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12,10))\n\nsns.distplot(data_2['sqft_living'],color='pink')\n\nplt.title('pdf of living area',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(8,8))\np = data_2['sqft_living'].quantile(0.99)\n\ndata_3 = data_2[data_2['sqft_living']<p]\n\nsns.distplot(data_3['sqft_living'],color='pink')\n\nplt.title('pdf of price less than 99th percentile',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(8,8))\n\nsns.distplot(data_2['sqft_basement'],color='cyan')\n\nplt.title('pdf of basement',size = 18)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(8,8))\n\np = data_3['sqft_basement'].quantile(0.99)\ndata_4 = data_3[data_3['sqft_basement']<p]\n\nsns.distplot(data_4['sqft_basement'],color='cyan')\n\nplt.title('pdf of basement with 99th percentile',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.Regression Model"},{"metadata":{},"cell_type":"markdown","source":"# 4.1.New DataFrame"},{"metadata":{},"cell_type":"markdown","source":"We reset the indices of the previous dataframe in order to make the process easier."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model=data_3.reset_index(drop=True)\ndata_model.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2.Checking the Assumptings for the Ordinary Least Squares Method(For final project, see documentation)."},{"metadata":{},"cell_type":"markdown","source":"**We make sure that the assumptions of normality and multicollinearity aren't violated.**"},{"metadata":{},"cell_type":"markdown","source":"# 4.2.1.Normality by Logarithmic Transformation"},{"metadata":{},"cell_type":"markdown","source":"In our previous analysis, we couldn't spot an obvious linear relationship between the living area and price. **We convert the price using logarithms in order to linearize it.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#we can directly use the numpy method to convert all the price datapoints and store it a new variable.\nlog_price=np.log(data_model['price'])\n\n#we store the new variable in a new column in the existing dataframe.\ndata_model['log_price'] = log_price\n\ndata_model.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(8,8))\n\ny=data_model['log_price']\nx=data_model['sqft_living']\n\nplt.scatter(x,y,color='green')\n\nplt.title('log price vs living area',size = 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As evident, the relationship is now much clearer and easier to interpret."},{"metadata":{},"cell_type":"markdown","source":"# 4.2.2.Multicollinearity Using Variable Influence Factor"},{"metadata":{},"cell_type":"markdown","source":"This tells us how much the behavior of a variable is influenced by the other variables. We will use it in tandem with the heatmap to identify **multicollinearity**."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#please note that this method has been directly used as per the statsmodels documentation. There is no inbuilt method to calculate the vif but the algorithm is cited and cab be found in the accompanying documentation.\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvariables = data_model[[ 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot','floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above','sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat','long', 'sqft_living15', 'sqft_lot15']]\n\nvif = pd.DataFrame()\n\nvif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\nvif[\"Features\"] = variables.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The vif for some values is extremely high**, more some being infinity!\n\nWe recall that **some variables related to area were extremely correlated. We drop those variables.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cleaned = data_model.drop(['sqft_living15','sqft_lot15','sqft_above','sqft_lot','sqft_basement',],axis = 1)\ndata_cleaned.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cleaned.columns.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.3.Building the Model"},{"metadata":{},"cell_type":"markdown","source":"**We declare the independent and dependent variables.**\n![](http:miro.medium.com/max/2872/1*k2bLmeYIG7z7dCyxADedhQ.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#In the above equation, x1 is x or the values taken by the x variable and y is the values taken by the y variable.\n\nx1=data_cleaned[[ 'bedrooms', 'bathrooms', 'sqft_living',\n       'floors', 'waterfront', 'view', 'condition', 'grade',\n        'yr_built', 'yr_renovated', 'zipcode', 'lat',\n       'long']]\n\ny=data_cleaned[['log_price']]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.4.Is the Model Significant?"},{"metadata":{},"cell_type":"markdown","source":"**We fit the model using the appropriate Ordinary Least Squares method that comes with the statsmodels.api package.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#again, this method is pre-existing and can be directly used. The citation can be found in the documentation.\n\n#this is b0. We are essentially adding a coulmn consisting of only 1s that is equal in length to the y variable.\nx= sm.add_constant(x1)\n\n#we fit the regression model on x and y using the appropriate method and store it in a variable.\nresults = sm.OLS(y,x).fit()\n\n#we summarize our findings.\nresults.summary()\n\n#note that there is a variable to represent the error in the image. In statistical terms it is the SSE. In easier words, we are trying to minimize the error. The lower the error, the better our model is.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Just creating a model is not enough. We have to gauge **the significance of the model by looking at summary table.(For the final project, see documentation for more inforamtion)**\n  \n  1. **R-squared**: It tells us how close the data is to the fitted line.It is **pretty close at around 74%**.\n  \n  2. **Adjusted R-squared:** It penalizes us for adding variables that have no explanatory power. Looks like it has **passed this test** too.\n  \n  3.**F-statistic:** It tells us if the F-distribution is followed. The higher the value, the better.\n  \n  4.**p-value:**It tells us the lowest value at which the null hypothesis can be rejected. Here the null hypothesis is that the dependent variables don't have any       explanatory power.The lowest value is 0.05. This test is also passed."},{"metadata":{},"cell_type":"markdown","source":"# 4.5.Regression Model "},{"metadata":{},"cell_type":"markdown","source":"Although our last model was pretty significant, we can still achieve a better result by trying to eliminate more correlation. On repeated trials, we can drop the following variables:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_reg=data_model.drop(['long','yr_renovated'],axis=1)\ndata_reg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1=data_cleaned[[ 'bedrooms', 'bathrooms', 'sqft_living',\n       'floors', 'waterfront', 'view', 'condition', 'grade',\n        'yr_built', 'lat','zipcode']]\ny=data_cleaned[['log_price']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= sm.add_constant(x1)\nresults = sm.OLS(y,x).fit()\nresults.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **This model is more statistically significant because without dropping the R-squared much and keeping the adjusted R-squared the same, the F-statistic has increased significantly and the p-value is 0 now.**\n* We still have a warning about strong multicollinearity. This is because certain variables like the number of bedrooms and bathrooms are although not very strongly correlated with the livng area, still have a non-negligible correaltion. Despite this, dropping them would significantly reduce the R-squared or the explanatory power.\n* We have to keep in mind that **no model can fully capture the dataset**(If it does so, we might have an overfitting model).\n* **We can therefore adopt the model at the accuracy level of 74%.**"},{"metadata":{},"cell_type":"markdown","source":"# 5.Predictions"},{"metadata":{},"cell_type":"markdown","source":"# 5.1.Predicting Prices"},{"metadata":{},"cell_type":"markdown","source":"For the OLS Method, we had added a constant,'x' which was equal in size to all the other x1 variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"#the first column displays the constant that we added earlier.\nx\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.2.New Dataframe"},{"metadata":{},"cell_type":"markdown","source":"We create **a new dataframe in order to predict the prices** using some other variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"#we create a new dataframe with some observations.\ndata_with_predictions = pd.DataFrame({'const':1,'bedrooms':[3,3],'bathrooms':[1,2.25],'sqft_living':[1180,2570],'floors':[1,2],'waterfront':[0,0],'view':[0,0],'condition':[3,3], 'grade':[7,7],'yr_built':[1955,1951],'lat':[47.5112,47.7210],'zipcode':[98103,98002]})\n\n#we name the columns and display it.\ndata_with_predictions=data_with_predictions[['const','bedrooms','bathrooms','sqft_living','floors','waterfront','view','condition','grade','yr_built','lat','zipcode']]\n\ndata_with_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The regression lies in the 'results' method.** We will fit the dataframe using this method and add it to our existing dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = results.predict(data_with_predictions)\npredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we store the predictions in a new variable and attach it to the dataframe\ndata_with_predictions['predictions'] = predictions\ndata_with_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.3.Exponential Transformation"},{"metadata":{},"cell_type":"markdown","source":"**We take the exponent of the prices which is still in logarithms and store it in the dataframe.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#using the inbuilt numpy method we take the exponent(inverse of logarthim)of the logarithmic price to get the original prices that we are interested in.\npred_price=np.exp(data_with_predictions['predictions'])\n\npred_price","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.4.Final Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"#again we store it in a new variable and attach it to the dataset.\ndata_with_predictions['predicted_price']=pred_price\ndata_with_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5.5.Remarks"},{"metadata":{},"cell_type":"markdown","source":"* The new dataframe is actually the first two observations of the orginal dataset.\n* The first prediction is within 31% of the observed value.\n* The second prediction is at around 20% from the observed value.\n* The** combined prediciton is within 27%** of the the observed values, our R-squared it at around 25%."},{"metadata":{},"cell_type":"markdown","source":"# 6.Conclusion"},{"metadata":{},"cell_type":"markdown","source":"* We first identified the independent variables and the dependent variable, price.\n* After visualizing the dataset using graphics,we inferred:\n  1. Most of the **houses are located in the Seattle Metropolis.**\n  \n  2. **Most of the houses were built in the 21st century.**\n  3. We observed that the **housing in King County is primarily influenced by the living area and the grade of the construction materials** used to build the house,followed by the number of bedrooms and the number of bathrooms.\n  4. The **neighborhoods are distinctly demarcated** because of the similarity in the sizes and prices of houses in close proximity.\n  5. **When it comes to prices and the number of bedrooms,bathrooms and floors, there are a lot of exceptions because of the unique locations of the houses merged into one large dataset.**\n  6. **Houses built with better construction materials cost more on an average though they do not guarantee a better condition of the house.**\n  7. For houses that have an excellent view of the waterfront, their prices are signifiantly higher than both those houses that totally lack a view or those with a compromised view.\n* We dealt with outliers by plotting the probability density function of some variables. We also linearized the model and identified the regressors keeping in mind the assumptions of the Ordinary Least Squares Method.\n* We built a **regression model using the OLS Method with around 74% of accuracy.**\n* We also predicted the prices of two houses and compared it to their actual values. We found that their **combined accuracy was 73% overall.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}