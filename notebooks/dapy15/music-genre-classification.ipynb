{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Music Genre Classification\nIn this notebook I tried to learn the basic concepts of neural networks and use it to classify the music files in dataset. Majorly this notebook can be divided into 3 parts:\n\n   1) Using ANN \n\n   2) Tackling overfitting with ANN\n\n   3) Using CNN\n\n\nAlso to read the dataset I have used librosa library which only read files <1Mb and one file is greater than the size giving error due to which I have ignored it. The dataset contains the following genres, the keys being the prediction targets\n    \n    0: \"disco\",\n    1: \"metal\",\n    2: \"reggae\",\n    3: \"blues\",\n    4: \"rock\",\n    5: \"classical\",\n    6: \"jazz\",\n    7: \"hiphop\",\n    8: \"country\",\n    9: \"pop\"\n","metadata":{}},{"cell_type":"code","source":"import os\nimport librosa\nimport math\nimport json \nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"_uuid":"f32a1031-9332-42b6-8335-dee2b86310a3","_cell_guid":"01a50213-fe5d-431b-9ade-28c438b3bceb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_path = r\"../input/gtzan-dataset-music-genre-classification/Data/genres_original\"\njson_path = r\"data.json\"\nSAMPLE_RATE = 22050\nDURATION = 30\nSAMPLES_PER_TRACK = SAMPLE_RATE * DURATION","metadata":{"_uuid":"d401d5fb-ea22-4562-bc1b-e882f59016ce","_cell_guid":"e1ba8988-65f9-4697-a731-e71b25dd171b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_mfcc(dataset_path, json_path, n_mfcc=13, n_fft=2048,\n             hop_length=512, num_segments=5):\n    # Data storage dictionary\n    data = {\n        \"mapping\": [],\n        \"mfcc\": [],\n        \"labels\": [],\n    }\n    samples_ps = int(SAMPLES_PER_TRACK/num_segments) # ps = per segment\n    expected_vects_ps = math.ceil(samples_ps/hop_length)\n    \n    # loop through all the genres\n    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n        # ensuring not at root\n        if dirpath is not dataset_path:\n            # save the semantic label\n            dirpath_comp = dirpath.split(\"/\")\n            semantic_label = dirpath_comp[-1]\n            data[\"mapping\"].append(semantic_label)\n            print(f\"Processing: {semantic_label}\")\n            \n            # process files for specific genre\n            for f in filenames:\n                if(f==str(\"jazz.00054.wav\")):\n                    # As librosa only read files <1Mb\n                    continue\n                else:\n                    # load audio file\n                    file_path = os.path.join(dirpath, f)\n                    signal,sr = librosa.load(file_path,sr=SAMPLE_RATE)\n                    for s in range(num_segments):\n                        start_sample = samples_ps * s\n                        finish_sample = start_sample + samples_ps\n\n                        mfcc = librosa.feature.mfcc(signal[start_sample:finish_sample],\n                                                    sr = sr,\n                                                    n_fft = n_fft,\n                                                    n_mfcc = n_mfcc,\n                                                    hop_length = hop_length)\n\n                        mfcc = mfcc.T\n\n                        # store mfcc if it has expected length \n                        if len(mfcc)==expected_vects_ps:\n                            data[\"mfcc\"].append(mfcc.tolist())\n                            data[\"labels\"].append(i-1)\n                            print(f\"{file_path}, segment: {s+1}\")\n\n    with open(json_path,\"w\") as f:\n        json.dump(data,f,indent=4)","metadata":{"_uuid":"c44ee71f-62e4-4100-8e2b-d2a84b01cf8e","_cell_guid":"f0f0a6db-b7e9-468f-890f-36312fe994e7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_mfcc(dataset_path,json_path,num_segments=10)\nclear_output()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filepath = r\"../input/gtzan-dataset-music-genre-classification/Data/genres_original/blues/blues.0000\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(2):\n    audio, sfreq = librosa.load(filepath+str(i)+\".wav\")\n    time = np.arange(0, len(audio))/sfreq\n    plt.plot(time,audio)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Sound Amplitude\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classifier\nThis part uses the concepts of ANN with keras and sequential layers. I have also done splitting in the ratio 70:30\n\nThe model is Sequential and architecture only has Flatten and the Dense layers available in keras for the basic ANN representation. As it is naive model we can expect it to be overfit. Info on the layers can be found [here](https://machinelearningknowledge.ai/different-types-of-keras-layers-explained-for-beginners/)","metadata":{}},{"cell_type":"code","source":"import json\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data\ndef load_data(dataset_path):\n    with open(dataset_path,\"r\") as f:\n        data = json.load(f)\n    \n    # Convert list to numpy arrays\n    inputs = np.array(data[\"mfcc\"])\n    targets = np.array(data[\"labels\"])    \n    \n    return inputs,targets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs,targets = load_data(r\"./data.json\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting the data\nfrom sklearn.model_selection import train_test_split\n\ninput_train, input_test, target_train, target_test = train_test_split(inputs, targets, test_size=0.3)\nprint(input_train.shape, target_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Flatten(input_shape=(inputs.shape[1],inputs.shape[2])))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import optimizers\nadam = optimizers.Adam(lr=1e-4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=adam,\n             loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=adam,\n             loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])\n\nhist = model.fit(input_train, target_train,\n                 validation_data = (input_test,target_test),\n                 epochs = 50,\n                 batch_size = 32)\nclear_output()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_history(hist):\n    plt.figure(figsize=(20,15))\n    fig, axs = plt.subplots(2)\n    # accuracy subplot\n    axs[0].plot(hist.history[\"accuracy\"], label=\"train accuracy\")\n    axs[0].plot(hist.history[\"val_accuracy\"], label=\"test accuracy\")    \n    axs[0].set_ylabel(\"Accuracy\")\n    axs[0].legend(loc=\"lower right\")\n    axs[0].set_title(\"Accuracy eval\")\n    \n    # Error subplot\n    axs[1].plot(hist.history[\"loss\"], label=\"train error\")\n    axs[1].plot(hist.history[\"val_loss\"], label=\"test error\")    \n    axs[1].set_ylabel(\"Error\")\n    axs[1].set_xlabel(\"Epoch\")\n    axs[1].legend(loc=\"upper right\")\n    axs[1].set_title(\"Error eval\")\n    \n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(hist)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_error, test_accuracy = model.evaluate(input_test, target_test, verbose=1)\nprint(f\"Test accuracy: {test_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Overfitting\nThis part deals with the overfitting of the previous model. We can deal with it by majorly doing the following process.\n- Making architecture less complicated \n- Using augmented data\n- Early stopping of training\n- Adding dropout layers\n- Regularization / Standardization  \n\nI have added the dropout layers and kernel_regularizers as compared to previous naive model giving the dropout probability as 30%\nKernel_regularizers is one of the 3 type of regularizer used to impose penalties. More info can be found [here](https://medium.com/@robertjohn_15390/regularization-in-tensorflow-using-keras-api-48aba746ae21)","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras as keras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Overfitting\nmodel = Sequential()\n\nmodel.add(Flatten(input_shape=(inputs.shape[1],inputs.shape[2])))\nmodel.add(Dense(512, activation='relu', kernel_regularizer = keras.regularizers.l2(0.001)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(256, activation='relu', kernel_regularizer = keras.regularizers.l2(0.003)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64, activation='relu', kernel_regularizer = keras.regularizers.l2(0.01)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=adam,\n             loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])\n\nhist = model.fit(input_train, target_train,\n                 validation_data = (input_test,target_test),\n                 epochs = 50,\n                 batch_size = 32)\n\nclear_output()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(hist)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_error, test_accuracy = model.evaluate(input_test, target_test, verbose=1)\nprint(f\"Test accuracy: {test_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the overfitting is greatly reduced but still we are not able to get a good accuracy. Now we will try doing it with Convolutional Neural Network (CNN)","metadata":{}},{"cell_type":"markdown","source":"# CNN\nUsing keras layers of Conv2D, MaxPool2D, BatchNormalization.\n\nCNN layers takes input primarily in 3D shape, so we again have to prepare the dataset in the form and for that, I have used np.newaxis function which adds a column/layer in the data","metadata":{}},{"cell_type":"code","source":"def prepare_dataset(test_size, validation_size):\n    X,y = load_data(r\"./data.json\")\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = validation_size)\n    X_train = X_train[..., np.newaxis]\n    X_val = X_val[..., np.newaxis]\n    X_test = X_test[..., np.newaxis]\n\n    return X_train, X_val, X_test, y_train, y_val, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, X_test, y_train, y_val, y_test = prepare_dataset(0.25, 0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = (X_train.shape[1],X_train.shape[2],X_train.shape[3])\nprint(input_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(64, (3, 3), activation = \"relu\", input_shape = input_shape))\nmodel.add(MaxPool2D((3, 3), strides=(2, 2), padding=\"same\"))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(32, (3, 3), activation = \"relu\"))\nmodel.add(MaxPool2D((3, 3), strides=(2, 2), padding=\"same\"))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(32, (2, 2), activation = \"relu\"))\nmodel.add(MaxPool2D((2, 2), strides=(2, 2), padding=\"same\"))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(16, (1, 1), activation = \"relu\"))\nmodel.add(MaxPool2D((1, 1), strides=(2, 2), padding=\"same\"))\nmodel.add(BatchNormalization())\n\nmodel.add(Flatten())\nmodel.add(Dense(64, activation=\"relu\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(10, activation=\"softmax\"))\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=adam,\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\nhist = model.fit(X_train, y_train,\n                 validation_data = (X_val, y_val),\n                 epochs = 40,\n                 batch_size = 32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(hist)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\nprint(f\"Test accuracy: {test_accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, X, y):\n    X = X[np.newaxis,...]\n    prediction = model.predict(X)\n    predicted_index = np.argmax(prediction, axis=1)\n    print(f\"Expected index: {y}, Predicted index: {predicted_index}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(model, X_test[10], y_test[10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see the accuracy has improved by Significant amount but still the accuracy is not enough, in the future of this notebook I am planning to implement RNN model in this and finally use the ensembling to get a push in accuracy. However any other suggestions are always invited!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}