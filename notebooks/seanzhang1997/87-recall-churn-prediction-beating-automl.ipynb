{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport missingno as msno\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf = pd.read_csv(\"../input/credit-card-customers/BankChurners.csv\")\n\n\n#Remove the ID column and the last two columns\ndf = df.iloc[:,1:-2]\n#Check out the missing values\ndf.replace([\"Unknown\",\"NaN\"],np.nan, inplace=True)\n#Visualize the missing value\nmsno.matrix(df) \n# We have missing values in education, maritial and income randomly distributed\n# We can delete rows with more than 2 missing values\ndf[\"num_missing\"] = df.apply(lambda x: x.isnull().sum(), axis=1)\n# I checked the maximum missing number per row is 2. \ndf = df[df[\"num_missing\"]!=2]\n# I will put the missing value back to Unknown here since they might contain info\ndf.fillna(\"Unknown\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For all the categorical data, we create order for those which has order meaning and encode the rest\neducation_dic = {'Uneducated':0, 'High School':1, 'Unknown':2, 'College':3, 'Graduate':4, 'Post-Graduate':5,\\\n    'Doctorate':6}\nincome_dic = {'Less than $40K':0, '$40K - $60K':1, \"Unknown\":2, '$60K - $80K':3, '$80K - $120K':4,'$120K +':5 }\nchurn_dic = {'Existing Customer':0, 'Attrited Customer':1}\ncard_dic = {'Blue':0, 'Silver':1, 'Gold':2, 'Platinum':3}\n\ndf[\"Education_Level\"].replace(education_dic, inplace=True)\ndf[\"Income_Category\"].replace(income_dic, inplace=True)\ndf[\"Attrition_Flag\"].replace(churn_dic, inplace=True)\ndf[\"Card_Category\"].replace(card_dic, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For sure those numbers might not be in the correct magitude. We can adjust those later after we see the feature importance.\ndf = pd.get_dummies(df)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation Plot\nfrom string import ascii_letters\nsns.set(style=\"white\")\n# Generate a large random dataset\nrs = np.random.RandomState(22)\n# Compute the correlation matrix\ncorr = df.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df,df[\"Attrition_Flag\"], test_size=0.2,\n                                                    random_state=42)\nX_train.shape, X_test.shape # We have 7845 in train and 1962 in test\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Upampling & Model Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\nX = X_train.copy()\n# separate minority and majority classes\nnon_churn = X[X[\"Attrition_Flag\"]==0]\nchurn     = X[X[\"Attrition_Flag\"]==1]\n\n# upsample minority\nchurn_upsampled = resample(churn,\n                          replace=True, # sample with replacement\n                          n_samples=len(non_churn), # match number in majority class\n                          random_state=1) # reproducible results\n\nupsampled = pd.concat([non_churn,churn_upsampled])\n\n# check new class counts\n# print(upsampled[\"Attrition_Flag\"].value_counts()) #6561 for each category","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = upsampled.drop(\"Attrition_Flag\",axis=1)\ny_train = upsampled[\"Attrition_Flag\"].astype(int)\nX_test = X_test.drop(\"Attrition_Flag\", axis=1)\ny_test = y_test.astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets do a simple logistic regression here first \nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(solver = \"lbfgs\",random_state=42).fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n# Model Performance\ndef prediction_result(y_test, y_pred):\n    print(\"Accuracy : \", accuracy_score(y_test, y_pred) *  100)\n    print(\"Recall : \", recall_score(y_test, y_pred) *  100)\n    print(\"Precision : \", precision_score(y_test, y_pred) *  100)\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))\nprediction_result(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators = 50, random_state = 0).fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n# Model Performance\nprediction_result(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nclf = xgb.XGBClassifier( \n    n_estimatoryhs=1000,\n    max_depth=3, \n    learning_rate=0.02, \n    subsample=0.8,\n    colsample_bytree=0.4, \n    missing=-1, \n)\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n# Model Performance\nprediction_result(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SMOTE and Model Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nX_train, X_test, y_train, y_test = train_test_split(df,df[\"Attrition_Flag\"], test_size=0.2,\n                                                    random_state=40)\nprint(X_train.shape, y_train.shape)\nX_train.shape, X_test.shape # We have 7845 in train and 1962 in test\n\noversample = SMOTE()\nX_train_SMOTE, y_train_SMOTE = oversample.fit_resample(X_train,y_train)\n\nprint(\"After Upsampling:-\")\nprint(X_train_SMOTE.shape, y_train_SMOTE.shape)\n\nX_train_SMOTE = X_train_SMOTE.drop(\"Attrition_Flag\",axis=1)\nX_test = X_test.drop(\"Attrition_Flag\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction_result(y_test, y_pred):\n    print(\"Accuracy : \", accuracy_score(y_test, y_pred) *  100)\n    print(\"Recall : \", recall_score(y_test, y_pred) *  100)\n    print(\"Precision : \", precision_score(y_test, y_pred) *  100)\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))\n    \n# Logistic Regression\nclf = LogisticRegression(solver = \"lbfgs\",random_state=42).fit(X_train_SMOTE, y_train_SMOTE)\ny_pred = clf.predict(X_test)\nprint(\"LOGISTIC RESULT\")\nprint(prediction_result(y_test,y_pred))\n\n# Random Forest\nclf = RandomForestClassifier(n_estimators = 50, random_state = 0).fit(X_train_SMOTE, y_train_SMOTE)\ny_pred = clf.predict(X_test)\nprint(\"RANDOM FOREST RESULT\")\nprint(prediction_result(y_test,y_pred))\n\n# XGBoost\nimport xgboost as xgb\nclf = xgb.XGBClassifier( \n    n_estimatoryhs=1000,\n    max_depth=3, \n    learning_rate=0.02, \n    subsample=0.8,\n    colsample_bytree=0.4, \n    missing=-1, \n)\n\nclf.fit(X_train_SMOTE, y_train_SMOTE)\ny_pred = clf.predict(X_test)\n\n# Model Performance\nprint(\"XGBOOST RESULT\")\nprint(prediction_result(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The result is not as good as upsampling(out of my expectation). "},{"metadata":{},"cell_type":"markdown","source":"## Upsampling with H2O AutoML"},{"metadata":{"trusted":true},"cell_type":"code","source":"import h2o\nh2o.init()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I ran into a error because h2o take the task as a regression task. I have no idea what happened since I already specified the target column as \"enum\". No related info online and the error disappeared after I restarted the kernel. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from h2o.automl import H2OAutoML\nh2o_df = h2o.H2OFrame(df)\nh2o_df[\"Attrition_Flag\"] = h2o_df[\"Attrition_Flag\"].asfactor()\n# h2o_df.describe()\ntrain, test = h2o_df.split_frame(ratios=[.8])\n\n# Identify predictors and response\nx = train.columns\ny = \"Attrition_Flag\"\nx.remove(y)\n\ntrain[y] = train[y].asfactor()\ntest[y] = test[y].asfactor()\n\naml = H2OAutoML(max_runtime_secs=600,\n                exclude_algos=['DeepLearning'],\n                seed=1,\n                stopping_metric='AUC',\n                sort_metric='AUC',\n                balance_classes=True,\n                project_name='Churn_Prediction'\n)\n\n%time aml.train(x=x, y=y, training_frame=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=lb.nrows)  # Print all rows instead of default","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = h2o.get_model('StackedEnsemble_AllModels_AutoML_20201222_163314')\nmodel.model_performance(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The recall is 258/(258+36)=87%. It can be improved to 1 with a lower threshold of 0.012851."},{"metadata":{},"cell_type":"markdown","source":"## Variable Importance for Churn Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = h2o.get_model(\"XGBoost_grid__1_AutoML_20201222_163314_model_4\")\nmodel2.varimp_plot(num_of_features=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}