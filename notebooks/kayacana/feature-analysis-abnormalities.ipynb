{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feature Analysis","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom scipy.stats import chi2_contingency\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_df = pd.read_csv(\"/kaggle/input/customer-analytics/Train.csv\")\nraw_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Individual Relations of Shipment Factors with Success\n### Correlations with Numerical Factors","metadata":{}},{"cell_type":"code","source":"raw_df.corr()['Reached.on.Time_Y.N'].sort_values().reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Individual correlations of success with other factors are generally close to zero. However, it can still provide us some insights.\n\n- __ID__: The highest negative correlation. It means nothing since ID is categorical. However, if IDs are given in a chronological order, we may conclude that service quality get lower by the time.\n- __Weight_in_gms__: Negative correlation might be reasonable since it would be harder to handle shipment of heavier products.\n- __Cost_of_the_Product__: Very week negative correlation. It might be because of a possible correlation with weight of the product, which is not calculated yet.\n- __Customer_care_calls__: Very week negative correlation. The problems with shipment may require more calls.\n- __Prior_purchases__: Very week negative correlation. Customer acquisition might be main strategy rather than customer retention. However, magnitude of correlation is too low to make a certain comment on it.\n- __Customer_rating__: No correlation. Personally, I would expect a positive correlation.\n- __Discount_offered__: Positive correlation. Probably, high discount rates are offered to more important customers whose shipments are priortized to be completed on time.","metadata":{}},{"cell_type":"markdown","source":"### Categorical Relations\n\nWe cannot use correlations to describe relations for categorical variables. But we can investigate whether all groups have similar success rate or not for any given categorical feature. So if all groups have similar success rates then we can say that this categorical feature has no effect on shipment success.\n\nFor this reason, I used Chi-square test for independence of two categorical variables.","metadata":{}},{"cell_type":"code","source":"contingency_table = raw_df.groupby(['Warehouse_block','Reached.on.Time_Y.N']).size().reset_index() \\\n    .rename(columns={0:'Counts'}).pivot(columns='Warehouse_block',index='Reached.on.Time_Y.N')\nprint(contingency_table)\n\nstat, p, dof, expected = chi2_contingency(contingency_table)\nprint(f'\\nP value for independence: {p:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"P value is far away from zero which lead us to conclude with that: **There is no relation between warehouse block and shipment success**. You can also have same idea by just looking the contingency table. Success ratio for every warehouse is similar (about 60%).\n\nLets do the same analysis for the other variables.","metadata":{}},{"cell_type":"code","source":"other_cat_cols = ['Mode_of_Shipment','Product_importance','Gender']\nfor cat_col in other_cat_cols:\n    contingency_table = raw_df.groupby([cat_col,'Reached.on.Time_Y.N']).size().reset_index() \\\n        .rename(columns={0:'Counts'}).pivot(columns=cat_col,index='Reached.on.Time_Y.N')\n    print(contingency_table)\n\n    stat, p, dof, expected = chi2_contingency(contingency_table)\n    \n    print(f'\\nP value for independence: {p:.4f}\\n\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Shipment type and gender seems unrelated with the shipment success. However. **the shipment success is not independent from the product importance**. Lets see the success rates by product importances.","metadata":{}},{"cell_type":"code","source":"order = {'high':0,'medium':1,'low':2}\ncontingency_table = raw_df.groupby(['Product_importance','Reached.on.Time_Y.N']).size().reset_index() \\\n        .pivot(columns='Product_importance',index='Reached.on.Time_Y.N')\nsuccess_ratios = contingency_table.div(contingency_table.sum(0), axis=1).iloc[1].reset_index().iloc[:,1:]\nsuccess_ratios = success_ratios.sort_values('Product_importance',key = lambda col: col.map(order)).rename(columns={1:'Success Ratio'})\nsuccess_ratios","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It clearly seems that **high products with importance have more successful history of shipment**. It is reasonable.","metadata":{}},{"cell_type":"markdown","source":"### Further Analysis of Some Features\n\nWe already looked for correlations for those features. But we did not take their distributions into account. It would be a good choice to use scatter plot with regression line. However, since our output value is binary, trying something different might be useful.\n\nI will separate continuous features into equally sized chnuks and will look for success rate of each chunk.","metadata":{}},{"cell_type":"code","source":"columns_to_categorize = ['Discount_offered','Cost_of_the_Product','Weight_in_gms']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_categorical = raw_df.copy()\nn_chunks = 10\nfor col in columns_to_categorize:\n    all_categorical[col] = all_categorical[col]\n    all_categorical = all_categorical.sort_values(col).reset_index(drop=True)\n    all_categorical[col] = all_categorical.index\n    all_categorical[col] = all_categorical[col]/all_categorical[col].max()*(n_chunks-.0001)\n    all_categorical[col] = \"Chunk \" + (all_categorical[col]//1).astype(int).astype(str)\nall_categorical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_categorical.groupby(['Discount_offered'])['Reached.on.Time_Y.N'].mean().plot()\nplt.title(\"Discount Rate (Chunked) vs. Success\")\nplt.show()\n\nall_categorical.groupby(['Cost_of_the_Product'])['Reached.on.Time_Y.N'].mean().plot()\nplt.title(\"Cost of Product (Chunked) vs. Success\")\nplt.show()\n\nall_categorical.groupby(['Weight_in_gms'])['Reached.on.Time_Y.N'].mean().plot()\nplt.title(\"Weight of Product (Chunked) vs. Success\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I cut each of these three features into 10 equally sized chunks. You can see them on X axis. Y axis shows mean success ratio of each chunk\n\n-- **Discount_offered**: *Chunk 0* shows the lowest 10% percentile of discount_offered while *Chunk 9* shows shipments with the highest 10% percentile of discount rates. We already know that it is positively correlated with success. But the effect is only observable on last two chunks.\n\n-- **Cost_of_the_Product**: *Chunk 0* shows the lowest 10% percentile of costs. The graph has a downward slope means that high cost groups have more risk of shipment failure. It is hard to find any direct reason for this. Equal chunk sizes might cause a misleading view.\n\n-- **Weight_in_gms**: It seems like lighter products are easier to ship. Chunk 3 has 100% success, which is interesting.\n\nTo understand data, it is always better to look at it from different aspects. We investigated data with equally sized chunks. It has some disadvantages that may mislead us because each chunk has different value range. So I will also plot histograms of these features by failure and success:","metadata":{}},{"cell_type":"code","source":"\nfor_histogram = raw_df[np.append(columns_to_categorize,'Reached.on.Time_Y.N')]\nfor_histogram['Succes'] = for_histogram['Reached.on.Time_Y.N'].map({0:'Failure',1:'Success'})\nfor_histogram_successAll = for_histogram.copy()\nfor_histogram_successAll['Succes'] = 'All'\n\nfor_histogram = pd.concat([for_histogram,for_histogram_successAll],0)\nfor c in columns_to_categorize:\n    g = sns.FacetGrid(for_histogram, col=\"Succes\")\n    g.map(sns.histplot, c,bins=25)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In plots, the first column indicates success, the second one indictaces failure of the shipment. Third column shows the distribution of all shipments.\n\n-- **Discount_offered**: The data is right skewed and there is no failure after a certain discount rate. This indicates that higher discounts are exceptional,and for the prime customers.This is consistent with line plot of chunks. Last two chunks probably had high variance values.\n\n-- **Cost_of_the_Product**: For success and failure situations, we have similar distributions. However as the value increases, ratio of succes to failure is going down slightly. That is the cause of the downward slope of line plot of chunks above.\n\n-- **Weight_in_gms**: The data has bimodal distribution with insteresting cut off points. There is no failure for a large range of weight values between 2000 and 4000. In the same range, number of successful shippings are also slightly lower.","metadata":{}},{"cell_type":"markdown","source":"# Abnormalities","metadata":{}},{"cell_type":"markdown","source":"### 1. ID vs Success Rate","metadata":{}},{"cell_type":"code","source":"ma_success = raw_df['Reached.on.Time_Y.N'].rolling(100,1).mean()\nma_success.plot()\nplt.title('Moving Average of Success Rate Ordered by ID')\nplt.show()\n\nfirst_fail = np.where(ma_success < 1)[0][0]\nprint(f'First shipment failure is at ID of {first_fail}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If success is ordered by Customer IDs, **there seems no shipment failure until 3135th ID**. Success seems random after that ID. The reason might be one of these:\n1. **There is no failure for a long time at the beginning**. It might be plausible if we assume ID is chronological and workload for the company was less at the beginning. But still, it is a strange pattern since there is NO failure for a long time and it suddenly goes random without any pattern after that point.\n2. **Consecutive succeses occured by chance**. You can check it out. It is not so plausible for a random data that 30% of its entries are ordered like that!\n3. **A change occured in data entry process**. It is possible because of different data sources, different personnel etc.\n4. **First entries are faulty**. Personally, It seems the most likely reason to me.\n\nTherefore, I recommend you to try your prediction model with and without this section of the data. It might differ.","metadata":{}},{"cell_type":"markdown","source":"### 2. Disitribution of Product Weight, Cost, and Importance","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data = raw_df,x='Weight_in_gms',y='Cost_of_the_Product')\nplt.title(\"Weight of Product vs. Cost of Product\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Weight vs cost graph seems so 'synthetic'. **There are some groups that have sharp boundaries and uniform distributions which is not seems natural**. Lets check out product importance in addition the cÄ±rrent features.","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data = raw_df,x='Weight_in_gms',y='Cost_of_the_Product',hue = 'Product_importance')\nplt.title(\"Weight of Product vs. Cost of Product\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I would expect cost of product and product importance to be related. However, **product importance seems distributed randomly**. Maybe product importance indicates something related with customer importance or hazardousness of the product.","metadata":{}},{"cell_type":"markdown","source":"### 3. Distribution of Product Weight for Failed Shipments","metadata":{}},{"cell_type":"code","source":"g = sns.FacetGrid(raw_df, col=\"Reached.on.Time_Y.N\")\ng.map(sns.histplot, 'Weight_in_gms')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For failed shipments distribution of weights has something unnatural. **There is no failure between 2,000-4,000 g weights while before 2,000 and after 4,000 there is a uniform pattern**.\n\nYou can recall the same absence of data in cost vs weight graph:","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(data = raw_df,x='Weight_in_gms',y='Cost_of_the_Product')\nplt.title(\"Weight of Product vs. Cost of Product\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(data = raw_df,x='ID',y='Weight_in_gms',hue='Reached.on.Time_Y.N')\nplt.title(\"ID vs. Weight of Product\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This last one is really interesting. It is combination of our two findings. **We have totally different different data before and after ID 3135**.Before than that we have product weights that uniformly distributed (except outliers) between 1000 and 4000. And interestingly, all of our shipments were made on time! After that ID, we have two distinct weight groups totally different from the former distribution. And we have a random success rate along the IDs and weights. It seems like two different companies data is combined.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}