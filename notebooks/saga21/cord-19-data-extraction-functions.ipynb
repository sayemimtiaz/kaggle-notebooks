{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CORD-19 - Data extraction functions\n\nWithin the arising COVID-19 pandemia, Kaggle has launched the COVID-19 Open Research Dataset Challenge (CORD-19) dataset as a general call for any data scientist that is able to contribute extracting relevant information to deal with the virus. Since we are still in the first stages of this analytics challenge, the idea of this kernel is to provide quality of life functions to extract certain information about COVID-19 papers. The content is far from being particularly creative or perfect, but it will hopefully save time to other people interested in the challenge.\n\nTABLE OF CONTENTS\n\n1. [Filter papers by word occurrences](#section1)\n2. [Extract the conclusions section](#section2)\n3. [Most common words in conclusions by topic](#section3)\n3. [Text summarization](#section4)\n\nDisclaimer: This kernel is still under construction. "},{"metadata":{},"cell_type":"markdown","source":"Import required libraries:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\npd.set_option('display.width', 30) \nimport matplotlib.pyplot as plt\nimport time\nimport warnings \nwarnings.filterwarnings('ignore')\nfrom heapq import nlargest \n\n# NLP libraries\nimport spacy\nfrom spacy.lang.en import English\nnlp = spacy.load('en_core_web_lg')\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom scipy.sparse.csr import csr_matrix #need this if you want to save tfidf_matrix\n\n# There's a large number of input files, it's nice to check out the list\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I load the output files from [xhlulu's kernel](https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv), which contains a useful transformation of the json files in dictionaries to csv readable format. Go check it to give some credit!"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"biorxiv = pd.read_csv(\"/kaggle/input/cord-19-eda-parse-json-and-generate-clean-csv/biorxiv_clean.csv\")\nbiorxiv.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Filter papers by word occurrences <a id=\"section1\"></a>\n\nGeneral studies like word frequency and such do require the full set of scientific papers. However, when dealing with specific tasks or topics, it's useful to select the subset of papers containing only certain words. Despite being very simple, the function defined in this section provides a list of paper_id containing a desired set of words."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter papers containing all words in list\ndef filter_papers_word_list(word_list):\n    papers_id_list = []\n    for idx, paper in biorxiv.iterrows():\n        if all(x in paper.text for x in word_list):\n            papers_id_list.append(paper.paper_id)\n\n    return papers_id_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see an example related to the challenge task [What is known about transmission, incubation, and environmental stability?](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=568), tackling the last bullet: **Role of the environment in transmission**. We look for  papers that contain the words \"coronavirus\", \"environment\" and \"transmission\". (Thanks to [Mar√≠lia](https://www.kaggle.com/mpwolke) for her insightful comment!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option(\"display.max_colwidth\", 100000) # Extend the display width to prevent split functions to not cover full text\n\nbiorxiv_environment = filter_papers_word_list([\"coronavirus\"])\nprint(\"Papers containing coronavirus: \", len(biorxiv_environment))\n\nbiorxiv_environment = filter_papers_word_list([\"environment\"])\nprint(\"Papers containing environment: \", len(biorxiv_environment))\n\nbiorxiv_environmental = filter_papers_word_list([\"environmental\"])\nprint(\"Papers containing environmental: \", len(biorxiv_environmental))\n\nprint(\"Intersection of environment and environmental: \", len(set(biorxiv_environment)-(set(biorxiv_environment)-set(biorxiv_environmental))))\n\nbiorxiv_environment_transmission = filter_papers_word_list([\"coronavirus\", \"environment\", \"transmission\"])\nprint(\"Number of papers containing coronavirus, environment and transmission: \", len(biorxiv_environment_transmission))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**:\n\n* From the 803 biorxiv papers from this challenge, the word \"coronavirus\" appears only in half of them (412). This suggests that a large number of papers are not strictly related to COVID-19, despite they may involve useful information to answer some of the challenge tasks.\n* Papers with \"environmental\" are a subset of those containing \"environment\". However, this could not be the case for other words' families. In the next subsection we cover an alternative to find all papers based on the lemma of words. "},{"metadata":{},"cell_type":"markdown","source":"### 1.1. Alternative filter: word lemmatization\n\nFiltering by literal words may lead to loosing some papers just because the word appeared in an alternative version. For example, above we looked for the word \"environment\", and we had to check if the word \"environmental\" was present in some papers where \"environment\" was not. To deal with this, we can **transform all words into their lemma**, and then filter the papers. \n\nNotice that **this procedure is very time consuming**, since we need to first transform all texts (including the ones that do not contain any information about our word list).\n\nTo do this, we will use the spaCy library (language defined in the import section):"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatizer(text):\n    tokens = [token.lemma_ for token in text]\n    return ' '.join([token for token in tokens])\n\n# Filter papers containing all words in list\ndef filter_papers_word_list_lemma(word_list):\n    papers_id_list = []\n    word_list_lemma = lemmatizer(nlp(str(' '.join([token for token in word_list]))))\n    for idx, paper in biorxiv.iterrows():\n        if all(w in lemmatizer(nlp(paper.text)) for w in word_list_lemma):\n            papers_id_list.append(paper.paper_id)\n\n    return papers_id_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ts = time.time() # I comment this part so that commit time does not skyrocket\n\n#biorxiv_environment_lemma = filter_papers_word_list_lemma([\"coronavirus\",\"environment\"])\n#print(\"Papers containing environment: \", len(biorxiv_environment_lemma))\n\n#print(\"Time spent: \", time.time() - ts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Extract the conclusions section <a id=\"section2\"></a>\n\nMost scientific papers contain a Conclusion section, which consists on a summary of the main observations and results from the study. In order to reduce the amount of data to analyze, it may prove useful to focus on the conclusions instead of performing a full search in the paper. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_conclusion(df, papers_id_list):\n    data = df.loc[df['paper_id'].isin(papers_id_list)]\n    conclusion = []\n    for idx, paper in data.iterrows():\n        paper_text = paper.text\n        if \"\\nConclusion\\n\" in paper.text:\n            conclusion.append(paper_text.split('\\nConclusion\\n')[1])\n        else:\n            conclusion.append(\"No Conclusion section\")\n    data['conclusion'] = conclusion\n        \n    return data\n\npd.reset_option('^display.', silent=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's  now extract the Conclusion section from all papers containing the words \"coronavirus\", \"environment\" and \"transmission\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"environ_trans_conclusion = extract_conclusion(biorxiv, biorxiv_environment_transmission)\nenviron_trans_conclusion.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Done, our DataFrame has now a conclusion column. We are able to process conclusions independently, but remember this is convenient only in certain situations. For example, for different reasons **most papers do not have a section named Conclusion** (i.e. exploratory analysis with no firm conclusions, reviews of the pandemic, papers that have a results or discussion section, etc). A valid alternative is to extract all final sections named either Conclusion, Conclusions, Results or Discussion, and then join them in a column. However, for the sake of simplicity, I'll stick with the original apporach. \n\nWith the help of the word_bar_function from [Paul Mooney](https://www.kaggle.com/paultimothymooney/most-common-words-in-the-cord-19-dataset), let's study which are the most frequent words in the Conclusion section of papers containing \"coronavirus\", \"environment\" and \"transmission\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.width', 100000)\n\ndef word_bar_graph_function(df,column,title):\n    # adapted from https://www.kaggle.com/benhamner/most-common-forum-topic-words\n    topic_words = [ z.lower() for y in\n                       [ x.split() for x in df[column] if isinstance(x, str)]\n                       for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])\n    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))\n    plt.title(title)\n    plt.show()\n\nplt.figure(figsize=(10,10))\nword_bar_graph_function(environ_trans_conclusion, \"conclusion\", \"Most common words in papers with coronavirus, environment & transmission\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that some of these words are merely situational (i.e. conclusion, section, medrxiv), but others may be particularly common in the subset of papers we have filtered. For example, words like \"epicenter\", \"humidity\", \"city\", \"distance\" and \"lockdown\" seem particularly related to the transmission of the virus and the environmental effects, and they probably won't be that frequent in other articles. \n\nLet's compare this case with papers containing the word \"susceptibility\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_susceptibility = filter_papers_word_list([\"coronavirus\", \"susceptibility\"])\nsusceptibility_conclusion = extract_conclusion(biorxiv, biorxiv_susceptibility)\nplt.figure(figsize=(10,10))\nword_bar_graph_function(susceptibility_conclusion, \"conclusion\", \"Most common words in papers with coronavirus and susceptibility\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, in this case there is no trace of the words \"epicenter\", \"humidity\", \"city\", \"distance\" or \"lockdown\". Instead, we now see large frequencies for words like \"reporting\", \"rate\", \"scenarios\", \"diagnostic\" and \"protocol\", which are more related to the contagion suscpetibility of the population."},{"metadata":{},"cell_type":"markdown","source":"# 3. Most common words in conclusions by topic <a id=\"section3\"></a>\n\nIn section 2 we've achieved our objective to extract the conclusion text of papers by their topic, based on a list of words. However, you will notice that there are words that do not provide any useful information, but are just situational in the Conclusion section (i.e. conclusion, section, preprint, license, copyright, etc). It's time to solve this and provide a cleaner list of the most common words in Conclusions filtered by topic. \n\nThe main workflow is:\n1. **Compare lists of words**. Provide a technique to verify if papers containing two different sets of lists are redundant\n2. **Most common words in conclusion**. Except for stopwords, extract the most common words in the Conclusion section for all papers related to coronavirus\n3. **Clean common words in conclusion by topic**. Compute the most common words by topic, but this time without irrelevant information"},{"metadata":{},"cell_type":"markdown","source":"## 3.1. Compare lists of words\n\nSince in many cases choosing the right set of words is not direct, I'll create a function tho verify if two different lists of words are equivalent or not. An example to compare papers with the word \"environment\" vs papers with \"environmental\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compare_papers_by_word(list1, list2):\n    biorxiv_1 = filter_papers_word_list(list1)\n    biorxiv_2 = filter_papers_word_list(list2)\n    if len(biorxiv_2) == len(set(biorxiv_1)-(set(biorxiv_1)-set(biorxiv_2))):\n        answer = \"List 1 contains List2\"\n    else:\n        answer = \"Lists are different\"\n    return answer\n\nlist1 = [\"environment\"]\nlist2 = [\"environmental\"]\ncompare_papers_by_word(list1, list2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Most common words in conclusion\n\nNow we need to know which are the most common words from all papers related to coronavirus, so we can substract them when plotting the most common words from papers of a given topic:"},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_conclusion = filter_papers_word_list([\"coronavirus\"])\nbiorxiv_coronavirus_conclusion = extract_conclusion(biorxiv, biorxiv_conclusion)\n\n# Filter out papers without Conclusions section\nbiorxiv_conclusion_informed = biorxiv_coronavirus_conclusion[biorxiv_coronavirus_conclusion[\"conclusion\"] != \"No Conclusion section\"]\n# Split in words\nlist_of_words = \"\".join([c for c in biorxiv_conclusion_informed.conclusion]).split()\n# Remove stopwords\nall_text_conclusion_nonstop = [w.lower() for w in list_of_words if w not in stopwords.words(\"english\")]\n# List of 100 most common words\nmost_common_words_conclusion = Counter(all_text_conclusion_nonstop).most_common(100)\nmost_common_words_conclusion","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The list contains a lot of words that are just situational; preprint, license, medRxiv or copyright. This words add no value to our analysis, and hence should be removed when showing the most common words from the Conclusion of papers from a certain topic. My personal criteria through manual revision has lead to the following final list of 68 words:"},{"metadata":{"trusted":true},"cell_type":"code","source":"manual_filter = [('preprint', 204),\n ('.', 137),\n ('license', 115),\n ('medrxiv', 108),\n ('the', 107),\n ('copyright', 78),\n ('holder', 78),\n ('peer-reviewed)', 78),\n ('available', 73),\n ('(which', 72),\n ('doi:', 64),\n ('granted', 63),\n ('display', 62),\n ('it', 62),\n ('author/funder,', 59),\n ('made', 53),\n ('4.0', 52),\n ('international', 52),\n ('data', 51),\n ('perpetuity.is', 35),\n ('cc-by-nc-nd', 28),\n ('study', 26),\n ('in', 21),\n ('we', 20),\n ('figure', 19),\n ('using', 18),\n ('author/funder.', 18),\n ('biorxiv', 18),\n ('number', 17),\n ('used', 17),\n ('sars-cov-2', 17),\n ('results', 16),\n ('covid-19', 16),\n ('perpetuity.the', 15),\n ('all', 15),\n ('1', 14),\n ('without', 13),\n ('show', 13),\n ('contributed', 13),\n ('our', 12),\n ('different', 12),\n ('2019-ncov', 12),\n ('also', 11),\n ('table', 11),\n ('coronavirus', 11),\n ('information', 11),\n ('cc-by-nc', 11),\n ('cc-by', 11),\n ('rights', 10),\n ('reserved.', 10),\n ('no', 10),\n ('reuse', 10),\n ('allowed', 10),\n ('based', 10),\n ('may', 10),\n ('consent', 10),\n ('authors', 10),\n ('https://doi.org/10.1101/2020.02.07.20021139', 10),\n ('2', 9),\n ('for', 9),\n ('however,', 9),\n ('this', 9),\n ('virus', 9),\n ('might', 8),\n ('manuscript', 8),\n ('https://doi.org/10.1101/2020.02.', 8),\n ('manuscript.', 8),\n ('reading', 8)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3. Clean common words in conclusion by topic\n\nNow the easy part, use the functions from previous sections to provide  alist of common words in the Conclusion section of a given subtopic: "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modify slightly Paul's function to prevent irrelevant words to appear\ndef word_bar_graph_function_mod(df,column,title,exception):\n    # adapted from https://www.kaggle.com/benhamner/most-common-forum-topic-words\n    topic_words = [ z.lower() for y in\n                       [ x.split() for x in df[column] if isinstance(x, str)]\n                       for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w.lower() for w in popular_words if w not in stopwords.words(\"english\")]\n    popular_words_noexcep = [w.lower() for w in popular_words_nonstop if w not in exception]\n    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_noexcep[0:50])])\n    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_noexcep[0:50]))\n    plt.title(title)\n    plt.show()\n    \n    \n# Create a general function for cleaner\ndef common_words_conclusion_clean(word_list, exception_list):\n    \n    \"\"\"\n    word_list = list of words that should contain the paper\n    exception_list = list of most common words in desired papers, in format [(word1, counter1), (word2, counter2),...]\n    \"\"\"\n\n    # Papers with the word coronavirus (I've verified that it contains all papers with COVID-19 and other key words)\n    biorxiv_coronavirus = filter_papers_word_list([\"coronavirus\"])\n    print(\"Papers containing coronavirus: \", len(biorxiv_coronavirus))\n\n    # Extract Conclusion from these papers, filter out papers without it\n    biorxiv_coronavirus_conclusion = extract_conclusion(biorxiv, biorxiv_coronavirus)\n    biorxiv_coronavirus_conclusion = biorxiv_coronavirus_conclusion[biorxiv_coronavirus_conclusion[\"conclusion\"] != \"No Conclusion section\"]\n\n    exception_words = [x[0] for x in exception_list]\n\n    plt.figure(figsize=(10,10))\n    title = \"Most common words in Conclusions on papers with \", word_list\n    word_bar_graph_function_mod(biorxiv_coronavirus_conclusion, \"conclusion\", title, exception_words)\n    \n    \ncommon_words_conclusion_clean([\"coronavirus\", \"environment\", \"transmission\"], manual_filter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a cleaner list of common words for COVID-19 papers related to environment and transmission. This could help people filter the papers they are interested in, so that there's no need to read the whole literature."},{"metadata":{},"cell_type":"markdown","source":"# 4. Text summarization <a id=\"section4\"></a>\n\nPrevious sections have been focused into reducing the amount of information from papers so that anyone interested in the subject can focus on certain papers, and identify in a simple look which words are meant in the Conclusion section. However, sometimes working with the whole paper may be mandatory, hence requiring an alternative method to reduce the workload when reviewing papers. To tackle this problem we'll use text summarization.\n\nText summarization seeks to extract the most relevant information from a text, and it can be achieved through two different approaches:\n* **Extractive**. Select a subset of words that retain the most important points, then select sentences with higher weights.\n* **Abstractive**. Based on semantic understanding, the model aims to produce a completely new text, shorter than the original document but with the same message.\n\nI'll proceed with extractive summarization given that in most cases we are interested in complete literal sentences from the papers, besides it's faster and the implementation is relatively simple. The main workflow that we will follow is:\n\n1. **Word scoring**. Compute the term frequency - inverse frequency document ([TFIDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)) score for each word in the document\n2. **Split**. Split the paper's text into sentences, using dot and line break as delimiters\n3. **Sentence scoring**. Use text rank, with the sentence score based on word scores. Filter out sentences with >50 words\n4. **Summary**. The summary consists on the top 10 sentences with higher score\n\nWith this approach, the maximum summary length is 500 words, which is a reasonable number given that abstracts usually range from 100 to 500 words. \n\nLet's see an example for the paper [Diagnosis of Acute Respiratory Syndrome Coronavirus 2 Infection by Detection of Nucleocapsid Protein running title: Diagnosis of COVID-19 by N antigen detection](https://www.medrxiv.org/content/10.1101/2020.03.07.20032524v2.full.pdf). The first step is to define the TFIDF function and then store in a dictionary the score of each word."},{"metadata":{"trusted":true},"cell_type":"code","source":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words = 'english', sublinear_tf=True)\n    matrix = tfidf_vectorizer.fit_transform(data)\n    return matrix, tfidf_vectorizer\n\nlist_corpus = list(biorxiv[biorxiv.paper_id == '6d0127f985edbe088bc279865bef25a31f54a066'].text)\n\ntfidf_matrix, tfidf_vectorizer = tfidf(list_corpus)\nword_scores_df = pd.DataFrame(tfidf_matrix.toarray(), columns = tfidf_vectorizer.get_feature_names())   # extract a df with the words' scores\nword_scores = dict(zip(list(word_scores_df.columns), list(word_scores_df.iloc[0])))  # convert to dict\n\nprint(\"Arbitrary slice of 10 words from the dictionary\")\ndict(list(word_scores.items())[100:110])  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point we have a complete dictionary for all words in the paper, with their respective TFIDF scores. To split the paper into sentences, we **replace all line breaks** (\\n, \\r) **by dots** (.), and then split the document using the dot as a delimiter. Each **sentence's score is the sum of all its word's scores**, effectively penalizing short sentences (which I consider a desired criteria)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into sentences\nsentences_list = [nlp(s) for s in list(biorxiv[biorxiv.paper_id == '6d0127f985edbe088bc279865bef25a31f54a066'].text.str.replace('\\n', '.').replace('\\r', '.'))]\nsentences_list = str(sentences_list[0]).split('.')   # Split sentences by .\nsentences_scores = {}\n\n# Define the sentence scoring function\ndef get_sentence_score(sentence: str, word_scores: dict):\n    words = sentence.split()\n    if len(words) < 50:\n        score = sum([word_scores.get(w.lower(),0) for w in words])\n    else:\n        score=0\n    return score\n\n# Assign scores and join the top10 sentences into the final summary\nfor s in sentences_list:\n    sentences_scores[s] = get_sentence_score(s, word_scores)\n    \ntop10_sentences = nlargest(10, sentences_scores, key=sentences_scores.get)\ntop10_sentences = [s for s in top10_sentences ]\nsummary = ' '.join(top10_sentences)\n\nprint(\"Original paper size: \", len(list_corpus[0].split()))\nprint(\"Summary size: \", len(summary.split()))\nprint(\"Reduction percentage: \", 100-round(len(summary.split())/len(list_corpus[0].split())*100, 2), \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This method has reduced the original document to a 18.86% of the original, which considering the increasingly large number of papers in the literature, would save many time to anyone interested into reading them partially. The obtained summary is the following:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(summary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A version of the above code for direct copy-paste purposes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Term frequency - inverse document frequency function\ndef tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words = 'english', sublinear_tf=True)\n    matrix = tfidf_vectorizer.fit_transform(data)\n    return matrix, tfidf_vectorizer\n\n\n# Define the sentence scoring function\ndef get_sentence_score(sentence: str, word_scores: dict):\n    words = sentence.split()\n    if len(words) < 50:\n        score = sum([word_scores.get(w.lower(),0) for w in words])\n    else:\n        score=0\n    return score\n\n\n# Summary extraction function\ndef extract_summary(df, paper_id):\n\n    list_corpus = list(biorxiv[biorxiv.paper_id == paper_id].text)\n    tfidf_matrix, tfidf_vectorizer = tfidf(list_corpus)\n    word_scores_df = pd.DataFrame(tfidf_matrix.toarray(), columns = tfidf_vectorizer.get_feature_names())   # extract a df with the words' scores\n    word_scores = dict(zip(list(word_scores_df.columns), list(word_scores_df.iloc[0])))  # convert to dict\n\n    # Split into sentences\n    sentences_list = [nlp(s) for s in list(biorxiv[biorxiv.paper_id == paper_id].text.str.replace('\\n', '.').replace('\\r', '.'))]\n    sentences_list = str(sentences_list[0]).split('.')   # Split sentences by .\n    sentences_scores = {}\n\n    # Assign scores and join the top10 sentences into the final summary\n    for s in sentences_list:\n        sentences_scores[s] = get_sentence_score(s, word_scores)\n\n    top10_sentences = nlargest(10, sentences_scores, key=sentences_scores.get)\n    top10_sentences = [s for s in top10_sentences ]\n    summary = ' '.join(top10_sentences)\n        \n    return summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that an input DataFrame is required, in order to generate the new summary column. In case you are interested into summarizing only some papers, you can first filter these papers from the dataframe. Results of the the previous case for reports with the words \"coronavirus\", \"environment\" and \"transmision\" are the following:"},{"metadata":{"trusted":true},"cell_type":"code","source":"environ_trans_summary = environ_trans_conclusion.copy()\nenviron_trans_summary['summary'] = environ_trans_summary['paper_id'].apply(lambda x: extract_summary(environ_trans_summary, x))\nenviron_trans_summary.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And that's it! Hopefully this will help to reduce the workload when reviewing tons of papers, or to filter information for data processing purposes. Of course, this summarization technique can be applied to other language projects, so feel free to adapt it to them."},{"metadata":{},"cell_type":"markdown","source":"# 5. Exploration of COVID-19 papers by topic <a id=\"section3\"></a>\n\nTechniques reviewed in the previous sections have proven their utility, but now it's time to apply them and analyze the content of the papers. The main idea is to characterize some topics of the articles, so that we are able to understand which factors are more important in each of them. \n\nRemind that our final objective is to answer some of the open question about the COVID-19 spread, and to help scientists when dealing with the huge set of articles related to this topic. Hence, to facilitate access to the different tasks of the challenge, each of them will be covered in a separated subsection:\n\n1. What is known about transmission, incubation, and environmental stability?\n2. What do we know about COVID-19 risk factors?\n3. What do we know about virus genetics, origin, and evolution?\n4. Sample task with sample submission\n5. What do we know about non-pharmaceutical interventions?\n6. What do we know about vaccines and therapeutics?\n7. What has been published about ethical and social science considerations?\n8. What do we know about diagnostics and surveillance?\n9. What has been published about medical care?\n10. What has been published about information sharing and inter-sectoral collaboration?"},{"metadata":{},"cell_type":"markdown","source":"## 5.1. What is known about transmission, incubation, and environmental stability?"},{"metadata":{},"cell_type":"markdown","source":"* **Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"biorxiv_incub_hum = filter_papers_word_list([\"coronavirus\", \"incubation\", \"human\"])\nbiorxiv_incub_hum_conclusion = extract_conclusion(biorxiv, biorxiv_incub_hum)\n\nbiorxiv_incub_hum_summary = biorxiv_incub_hum_conclusion.copy()\nbiorxiv_incub_hum_summary['summary'] = biorxiv_incub_hum_summary['paper_id'].apply(lambda x: extract_summary(biorxiv_incub_hum_summary, x))\nbiorxiv_incub_hum_summary.head(5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}