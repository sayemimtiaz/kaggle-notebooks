{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2 as cv\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nimport glob\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://realpython.com/traditional-face-detection-python/\nface_cascade = cv.CascadeClassifier('/kaggle/input/haarcascades/haarcascade_frontalface_alt.xml')\n\ndef detect_faces(img):\n    grey_image = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n    detected_faces = face_cascade.detectMultiScale(grey_image)\n    return detected_faces\n\ndef crop_face(img):\n    faces = detect_faces(img)\n    if len(faces) == 0:\n        return img\n    else:\n        column, row, width, height = faces[0]\n        cropped = img[row:row+height, column:column+width]\n        return cv.resize(cropped, img.shape[0:2][::-1])\n\nimage = cv.imread('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/000001.jpg')\n    \nfig, ax = plt.subplots(1,2)\nax[0].imshow(image)\nax[1].imshow(crop_face(image));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Each identity has to have its own folder\nimport pandas as pd\nfrom shutil import copyfile\n\ntrain_size = 1000\ntest_size = train_size // 10\n\nfile = '/kaggle/input/identity-celeba/identity_CelebA.txt'\nidentities = pd.read_csv(file, delimiter=' ', names=['file', 'identity'])\n\n!rm -rf /kaggle/working/train && rm -rf /kaggle/working/train\n\nsource_dir = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/%s'\ntarget_dir_train = '/kaggle/working/train/%d/'\ntarget_dir_test = '/kaggle/working/test/%d/'\n\ndef get_dest_dir(identity):\n    if identity <= train_size:\n        return target_dir_train\n    elif identity <= train_size + test_size:\n        return target_dir_test\n    else:\n        return None\n\nfor _, file, identity in tqdm(list(identities.itertuples())):\n    dest_dir = get_dest_dir(identity)\n    if dest_dir is not None:\n        dest_dir = dest_dir % identity\n        os.makedirs(dest_dir, exist_ok=True)\n        copyfile(source_dir % file, dest_dir + file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_class(class_name, cl='train'):\n    # Check to make sure that all files in each directory are of the same person (sanity check)\n    files = glob.glob('/kaggle/working/%s/%s/*' % (cl, class_name))\n\n    fig, ax = plt.subplots(1, len(files), figsize=(20,10))\n    for index, file in enumerate(files):\n        ax[index].imshow(cv.cvtColor(cv.imread(file), cv.COLOR_BGR2RGB))\n        ax[index].axis('off')\n    plt.show()\n    \nshow_class('1006', 'test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the image preprocessor \nfrom keras.preprocessing.image import ImageDataGenerator\nimport random\n\ndef preprocess_image(img):\n    # image must be an int for cropping and float for Keras\n    return \n\ntrain_directory = '/kaggle/working/train/'\ntest_directory = '/kaggle/working/test/'\nbatch_size = 128\n\nclass DirectoryFlow:\n    def __init__(self, directory, batch_size, in_memory=True):\n        self.directory = directory\n        self.batch_size = batch_size\n        self.in_memory = in_memory\n        if in_memory:\n            self.file_structure = self.load_images(directory)\n        else: \n            self.file_structure = {int(f): [directory + '%s/%s' % (f, file_name) for file_name in os.listdir(directory + f)] for f in os.listdir(directory)}\n        self.classes = list(self.file_structure.keys())\n        print('Loaded %d classes' % len(self.classes))\n        \n    def load_images(self, directory):\n        structure = {}\n        for f in tqdm(os.listdir(directory)):\n            structure[int(f)] = [self.read_image(directory + '%s/%s' % (f, file_name)) for file_name in os.listdir(directory + f)]\n        return structure\n    \n    def read_image(self, file):\n        img = cv.imread(file)\n        img = cv.resize(img, (64, 64))\n        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n        img = crop_face(img.astype('uint8'))\n        img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n        img = img.astype('float32')/255\n        img = img.reshape(*img.shape, 1)\n        return img\n    \n    def next(self, class_name=None):\n        if class_name is None:\n            class_name = random.choice(self.classes)\n        item = random.choice(self.file_structure[class_name])\n        \n        if self.in_memory:\n            return item, class_name\n        else:\n            return self.read_image(item), class_name\n    \n    def next_batch(self):\n        images = []\n        labels = np.zeros((self.batch_size))\n        for i in range(self.batch_size):\n            image, class_name = self.next()\n            images.append(image)\n            labels[i] = class_name\n        return np.array(images), labels\n\ndatagen_train = DirectoryFlow(train_directory, batch_size)\ndatagen_test = DirectoryFlow(test_directory, batch_size)\n\n# datagen_test = DirectoryFlow(test_directory)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/aleju/face-comparer/blob/master/train.py#L252\nfrom keras.layers import Input, Dense, Activation, Dropout, Flatten, Dense, Add, Conv2D, MaxPooling2D, Lambda, BatchNormalization, GaussianNoise\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop\nfrom keras import backend as K\n\ninput_shape = (64, 64, 1)\n\ndef make_facial_feature_cnn():\n    network_input = Input(shape=input_shape)\n    face = Conv2D(32, kernel_size=(5,5), strides=(1,1), activation='relu', padding='same', name='conv1')(network_input)\n    face = Conv2D(32, kernel_size=(5,5), strides=(1,1), activation='relu', padding='same', name='conv2')(face)\n    face = Conv2D(64, kernel_size=(8,8), strides=(3,3), activation='relu', padding='same', name='conv3')(face)\n\n    face = Conv2D(32, kernel_size=(5,5), strides=(1,1), activation='relu', padding='same', name='conv4')(face)\n    face = Conv2D(64, kernel_size=(9,9), strides=(4,4), activation='relu', padding='same', name='conv5')(face)\n    face = Conv2D(32, kernel_size=(5,5), strides=(2,2), activation='relu', padding='same', name='conv6')(face)\n    \n    face = Flatten()(face)\n    face= Dropout(0.15)(face)\n    face = Dense(32, activation='tanh')(face)\n    face = BatchNormalization()(face)\n\n    return Model(network_input, face)\n\nfacial_feature_cnn = make_facial_feature_cnn()\n\ninput_face_left = Input(shape=input_shape, name='model_face_left')\ninput_face_right = Input(shape=input_shape, name='model_face_right')\n\nmodel_face_left = facial_feature_cnn(input_face_left)\nmodel_face_right = facial_feature_cnn(input_face_right)\n\nmerged_model = Lambda(lambda tensors: abs(tensors[0] - tensors[1]), name='absolute_difference')([model_face_left, model_face_right])\nmerged_model = GaussianNoise(0.5)(merged_model)\nmerged_model = Dense(64, activation='relu', name='d1')(merged_model)\nmerged_model = Dropout(0.1, name='drop1')(merged_model)\nmerged_model = Dense(1, activation='sigmoid', name='out')(merged_model)\n\nmodel = Model(inputs=[input_face_left, input_face_right], outputs=merged_model, name='lr_merger')\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(facial_feature_cnn.summary())\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test copy model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/aleju/face-comparer/blob/master/train.py#L252\nfrom keras.layers import Input, Dense, Activation, Dropout, Flatten, Dense, Add, Conv2D, MaxPooling2D, Lambda, BatchNormalization, GaussianNoise, LeakyReLU\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop\nfrom keras import backend as K\n\ninput_shape = (64, 64, 1)\ninit_dense = 'glorot_normal'\ninit_conv = 'orthogonal'\n\ndef make_facial_feature_cnn():\n    network_input = Input(shape=input_shape, )\n    face = Conv2D(32, kernel_size=(5,5), strides=(1,1), padding='same', name='conv1', kernel_initializer=init_conv)(network_input)\n    face = LeakyReLU(0.33)(face)\n    face = Conv2D(32, kernel_size=(5,5), strides=(1,1), padding='same', name='conv2', kernel_initializer=init_conv)(face)\n    face = LeakyReLU(0.33)(face)\n    face = Conv2D(64, kernel_size=(8,8), strides=(3,3), padding='same', name='conv3', kernel_initializer=init_conv)(face)\n    face = LeakyReLU(0.33)(face)\n    \n    face = Conv2D(64, kernel_size=(5,5), strides=(1,1), padding='same', name='conv4', kernel_initializer=init_conv)(face)\n    face = LeakyReLU(0.33)(face)\n    face = Conv2D(128, kernel_size=(9,9), strides=(4,4), padding='same', name='conv5', kernel_initializer=init_conv)(face)\n    face = LeakyReLU(0.33)(face)\n    face = Conv2D(128, kernel_size=(5,5), strides=(2,2), padding='same', name='conv6', kernel_initializer=init_conv)(face)\n    face = LeakyReLU(0.33)(face)\n    face= Dropout(0.25)(face)\n\n    face = Flatten()(face)\n    face = Dense(512, kernel_initializer=init_dense)(face)\n    face = BatchNormalization()(face)\n    face = Activation('tanh')(face)\n\n    return Model(network_input, face)\n\nfacial_feature_cnn = make_facial_feature_cnn()\n\ninput_face_left = Input(shape=input_shape, name='model_face_left')\ninput_face_right = Input(shape=input_shape, name='model_face_right')\n\nmodel_face_left = facial_feature_cnn(input_face_left)\nmodel_face_right = facial_feature_cnn(input_face_right)\n\nmerged_model = Lambda(lambda tensors: abs(tensors[0] - tensors[1]), name='absolute_difference')([model_face_left, model_face_right])\nmerged_model = GaussianNoise(0.5)(merged_model)\nmerged_model = Dropout(0.2)(merged_model)\nmerged_model = Dense(256, name='d1', init=init_dense)(merged_model)\nmerged_model = BatchNormalization()(merged_model)\nmerged_model = LeakyReLU(0.33)(merged_model)\nmerged_model = Dropout(0.5, name='drop1')(merged_model)\nmerged_model = Dense(1, activation='sigmoid', name='out')(merged_model)\n\nmodel = Model(inputs=[input_face_left, input_face_right], outputs=merged_model, name='lr_merger')\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(facial_feature_cnn.summary())\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nfrom keras.utils import plot_model\n\n!rm /kaggle/working/model.png\nplot_model(facial_feature_cnn, to_file='/kaggle/working/model_base.png', show_shapes=True)\n\nplot_model(model, to_file='/kaggle/working/model.png', show_shapes=True)\n#Image('/kaggle/working/model_base.png')\n#Image('/kaggle/working/model.png')\nNone","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomBatchIterator:\n    def __init__(self, datagen, batch_size):\n        self.datagen = datagen\n        self.batch_size = batch_size\n        \n    def make_batch(self):\n        images, labels = self.datagen.next_batch()\n\n        output_pairs = [np.zeros((self.batch_size, *images[0].shape)) for i in range(2)]\n        output_labels = np.zeros(self.batch_size)\n        \n        same_image_count = np.random.randint(1, self.batch_size)\n        output_labels[:same_image_count] = 1\n\n        for i in range(same_image_count):\n            label = labels[i]\n            random_image,_ = self.datagen.next(label) \n            output_pairs[0][i] = images[i]\n            output_pairs[1][i] = random_image\n\n        for i in range(same_image_count, self.batch_size):\n            label = labels[i]\n            while True:\n                random_label = random.choice(self.datagen.classes)\n                if random_label != label: break\n            random_image,_ = self.datagen.next(random_label)\n            output_pairs[0][i] = images[i]\n            output_pairs[1][i] = random_image\n\n        return output_pairs, output_labels\n    \n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self.make_batch()\n\ndef show_batch_pairs(image_batch_pairs, label_batch):\n    fig, ax = plt.subplots(len(image_batch_pairs[0]), 2, figsize=(20,20))\n    for r in range(len(image_batch_pairs[0])):\n        for c in [0,1]:\n            image = image_batch_pairs[c][r]\n            ax[r,c].imshow(image[:,:,0], cmap='gray')\n            ax[r,c].axis('off')\n            ax[r,c].set_title('y=%d' % label_batch[r])\n            \ntrain_it = CustomBatchIterator(datagen_train, batch_size)\ntest_it  = CustomBatchIterator(datagen_test, batch_size)\n#show_batch_pairs(*next(train_it))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(train_it, steps_per_epoch=10, epochs=1000, validation_data=test_it, validation_steps=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['val_accuracy'])\nplt.xlabel('epoch')\nplt.ylabel('val_acc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_image_pairs, test_image_labels = next(test_it)\ntest_image_pairs_left, test_image_pairs_right = test_image_pairs\npredictions = model.predict(test_image_pairs)\n\naccuracy = np.count_nonzero([1 if p >= 0.5 else 0 for p in predictions] == test_image_labels)/len(test_image_labels)\n\nstart = 10\nitems = 10\nfig, ax = plt.subplots(items, 2, figsize=(10, 10))\nfor r in range(start, start+items):\n    for c in range(0, 2):\n        image = test_image_pairs[c][r]\n        ax[r-start,c].imshow(image[:,:,0], cmap='gray')\n        ax[r-start,c].axis('off')\n    print('y=%.3f yHat=%.3f' % (test_image_labels[r], predictions[r]))\n\naccuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image, label = datagen_test.next()\n\ndef find_match(image):\n    output_pairs = [np.zeros((len(datagen_test.classes), *image.shape)) for i in range(2)]\n    output_pairs[0][:] = image\n\n    for i, class_name in enumerate(datagen_test.classes):\n        test_img, _ = datagen_test.next(class_name)\n        output_pairs[1][i,:] = test_img\n\n    predictions = model.predict(output_pairs)\n    classes = np.array(datagen_test.classes)\n    matched_class = classes[(predictions == predictions.max()).T[0]][0]\n    return matched_class, predictions.max()\n\nmatched_class, confidence = find_match(image)\nprint('Matched %d to %d: %.3f' % (label, matched_class, confidence))\n\nplt.imshow(image[:,:,0], cmap='gray')\nshow_class(str(matched_class), 'test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_trials = 100\nn_correct = 0\nfor i in tqdm(range(n_trials)):\n    image, label = datagen_test.next()\n    \n    matches = np.zeros(20, dtype=int)\n    for i in range(0, 20):\n        matches[i] = find_match(image)[0]\n    \n    matched_class = np.bincount(matches).argmax()\n    if matched_class == label:\n        n_correct += 1\nprint(\"Accuracy: %.3f\" % (n_correct/n_trials))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}