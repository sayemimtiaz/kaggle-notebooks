{"cells":[{"metadata":{},"cell_type":"markdown","source":"<!-- \n![title](source/title.png)\na normal html comment -->\n\n![title](https://raw.githubusercontent.com/emdemor/prediction-house-prices-on-Brazil/master/source/title.png)\n\n\n\n**Author**: Eduardo M. de Morais, MSc.\n\n**Date**: Jul 17 2020\n\n# Table of Contents\n1. [Introduction](#intro)\n2. [Methodology](#method)\n3. [Exploratory Analysis](#exp)\n4. [Building the Model](#model)\n5. [Realistic Model](#real-model)\n6. [Conclusion](#conc)\n\n<a id=\"intro\"></a>\n# 1.  Introduction\n\n\nThrough this notebook, I have attempted to predict rental prices from a dataset containing data from Brazil. Of course, this is and study case and some idealized procedure was assumed. The main objective here is to study and understand the data to use the knowledge to construct a model to predict the total rental price. The source dataset has the following columns:\n\n\n* **city** (*categorical*): City where the house or apartment is located. Here, we have two option of cities that the collector didn't specified. \n* **area** (*integer*): Area of constructed  property (The unity is not informed. I'm am assuming that is squared meters, the most usual in Brazil)\n* **rooms** (*integer*): The number of rooms in the property\n* **bathroom** (*integer*): The number of bathrooms in the property\n* **parking spaces** (*integer*): The number of parking spaces at the garage\n* **floor**: (*integer*): The number of the floor (if the property is an apartment)\n* **animal** (*boolean*): If is allowed to have animals at the house\n* **furniture**: (*boolean*): If is a furnished property\n* **hoa** (*float*): Homeowners association fee.\n* **rent amount** (*float*): Base value of rent\n* **property tax** (*float*): State property tax (ad valorem tax)\n* **fire insurance** (*float*): The value of fire insurance\n* **total** (*float*): The total money spent on rental activity\n\n<a id=\"method\"></a>\n# 2. Methodology\nIn general, when planning a place to live, we have to take into account all expenses and taxes to choose the best place. Because of this, the adopted target to this dataset will be the **total** column.\n\n\nIn real cases applications, the user has no information concerned to property cost, like  **hoa**, **rent amount**, **property tax**, **fire insurance** and **total**. In general, this variables are mutually correlated, firstly, because **total** is the sum the others and; secondly, because in real life, the taxes and fees are  proportional to property value and, somehow, to the rental price. An applicable model must be capable to predict the rental price based only on **city**, **area**, **rooms**, **bathroom**, **parking spaces**, **floor**, **animal** and **furniture**.   However, since this is a idealized study of case, which objective is only to practice the regression models, let's assume that the cost variables is accessible to user. So, our task here is to define the best model to the **total** rental price from the information of other columns.\n\nTo construct a system capable to make good recommendations, it is necessary to understand how the features influence the desired variable. To make this, I will split my work in three stages:\n\n**1. Exploring**: An exploratory analysis, to understand the structure of data, missing values, correlations and etc, to select the best set of features to be used at the analysis.\n\n\n**2. Modeling**: In this step, I try to find the best and simplest model for describing the data. I do this by testing the influence of features on total values. Since that the definition of a best model is something complicated and subjective, I will choose the model  which best efficiency in the follow approach:\n\n>A. Split dataset into test (33%) and training (67%)\n\n>B. Use training dataset to fit the parameters of model\n\n>C. Use the trained model to predict the **total** of test dataset\n\n>D. Use predicted an original **total** to evaluate the *mean average error* and *R2 score* metrics\n\nThe model with the lowest mean average error value will be considered the best model. In case of a tie, the one with the highest R2 will be chosen.\n\n**3. Optimizing**: With the chosen model, I want to give to user a system that returns the distribution of prices from a specified set of features. (*To be implemented in next versions*)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"exp\"></a>\n# 3. Exploratory Analysis\n\n### Table of Contents\n1. [Initializing](#init)\n2. [Useful Functions](#use-func)\n3. [Dataset](#dataset)\n4. [Pre-Processing](#pre-pro) \n5. [Correlation Analysis](#corr-an) \n6. [One Feature Linear Regression](#lin-reg) \n7. [Double Feature Linear Regression](#dbl-reg) \n\n<a id= \"init\"></a>\n## 3.1 Initializing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Matplotlib and seaborn for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Scipy for statistics\nfrom scipy import stats\n\n# os to manipulate files\nimport os\n\nfrom sklearn.metrics import mean_absolute_error,r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\n\ncolors = [ \"#3498db\", \"#e74c3c\", \"#2ecc71\",\"#9b59b6\", \"#34495e\", \"#95a5a6\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"use-func\"></a>\n## 3.2 Defining Useful Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_to_number(col,convert_type=int,changes = ['-']):\n    \n    # string will be considered as object\n    if col.dtype.name == 'object':\n        col_temp = col.copy()\n        \n        # Change any occurence in changes to ''\n        for change in changes:\n                col_temp = col_temp.str.replace(change,'')\n                \n        # Changes empty string elements for NaN\n        col_temp.loc[(col_temp == '')] = np.nan\n        \n        # Convert to number the not nan elements\n        col_temp[col_temp.notna()] = col_temp[col_temp.notna()].astype(convert_type)\n        \n        # Fill nan elements with the mean\n        col_temp = col_temp.fillna(int(col_temp.mean()))\n        \n        return col_temp\n    else:\n        return col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_predictions(X_new,y_new,descr = '',cols = ['area', 'hoa','rent amount','property tax']):\n    y_col = 'total'\n\n    #cols = ['area', 'hoa','rent amount','property tax','fire insurance']\n    \n    k = 0\n    for x_col in cols:\n        plt.close()\n        plt.figure(figsize=(8, 5))\n        plt.scatter(X_trn[x_col],y_trn,c='lightgray',label = 'Training Dataset',marker='o',zorder=1)\n        plt.scatter(X_new[x_col],y_new, label = 'Predictions on Test Dataset',marker='.', c=colors[k], lw = 0.5,zorder=2,alpha = 0.8)\n        #plt.scatter(X_tst[x_col],y_pr_tst, label = 'Predictions',marker='.', c='tab:blue', lw = 0.5,zorder=2)\n\n\n        plt.xlabel(x_col, size = 18)\n        plt.ylabel(y_col, size = 18); \n        plt.legend(prop={'size': 12});\n        plt.title(descr+y_col+' vs '+x_col, size = 20);\n        #plt.savefig('results/'+descr+y_col+' vs '+x_col+'.png')\n        plt.show()\n        k += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"dataset\"></a>\n## 3.3 The Dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First of all, lets import our dataset and make some initial exploratory data analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# Import Dataset\ndf1 = pd.read_csv(os.path.join(dirname,'houses_to_rent.csv')).drop('Unnamed: 0',axis=1)\n#df2 = pd.read_csv(os.path.join(dirname,'houses_to_rent_v2.csv'))\n#df2.columns = df1.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"pre-pro\"></a>\n## 3.4 Pre-Processing\n\nFrom the ```df1.info()``` method calling, we can see some problems. The **hoa**, **rent amount**, **property tax**, **fire insurance** and **total** columns, that are *float* columns are being interpreted by Pandas as string. That's because the monetary units was placed together the value. Besides this, some ```NaN``` values was indicated by the data collector with string 'Sem info' and 'Incluso'. We need to make a specified pre-processing here. For the *float* columns, I implemented a function called `convert_to_number()` in the [function section](#use-func).\n\nMy pre-processing method is:\n\n * Remove all occurences of 'R\\\\$' , -,'Sem info' and 'Incluso' from data\n\n * In the cases where the field is 'Sem info' or 'Incluso', replace by `numpy.nan`\n * In *float* columns, replace `NaN` values for the column `mean`\n * Create dummy variables for categorical features, dropping the last columns to prevent a *dummy variable trap*.\n * Removing outliers. ( I'm assuming outliers as the observations with *Z-score* greater than 4 )","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# elements to remove from the dataset\nremove = ['R','$',',','-','Sem info','Incluso']\n\n# columns of numerical data\ncols = ['hoa','rent amount','property tax','fire insurance','total','floor']\n\n# Making the substitutions\nfor col in cols:\n    df1[col]  = convert_to_number(df1[col],changes=remove)\n    \n# converting floor to int \ndf1['floor'] = df1['floor'].astype('int')\n\n# Getting dummies\ndf1[['animal','furniture']] = pd.get_dummies(df1[['animal','furniture']], prefix_sep='_', drop_first=True)\n\n# dealing with outliers\ncols = ['area','hoa','property tax','rent amount']\nfor col in cols:\n    df1 = df1[np.abs(stats.zscore(df1[col])) < 4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how the dataset is now:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"corr-an\"></a>\n## 3.5 Correlation Analysis\nNow, we want to understand the correlations between features of dataset. Let's plot get the ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = df1.corr()['total'].abs().sort_values(ascending=False).drop('total',axis=0).to_frame()\ncorrelations.plot(kind='bar');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#totalprice correlation matrix\nk = 10 #number of variables for heatmap\nplt.figure(figsize=(16,8))\ncorr = df1.corr()\n\nhm = sns.heatmap(corr, \n                 cbar=True, \n                 annot=True, \n                 square=True, fmt='.2f', \n                 annot_kws={'size': 10}, \n                 yticklabels=corr.columns.values,\n                 xticklabels=corr.columns.values,\n                 cmap=\"YlGnBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here that there is strong correlation between **rent amount**, **fire insurance** and **total**. As the **total** column is the sum of **hoa**, **rent amount**, **property tax** and **fire insurance**, its natural the appearing of this kind of correlations. This is a problem because, if we use some of this as a target, we have at least 1 spurious degree of freedom in our statistical model. So, as my approach is to predict the **total**, we have to drop or **fire insurance** or **rent amount**. Here, I'll choose to take off **fire insurance** since **rent amount** is a quantity of greater importance.\n\nThe first approach that we will take here is the linear fit of the two most correlated features with 'total' (except from 'fire insurance') to see, from the error analysis, how much they contribute to 'total'\n\n\n\n<a id=\"lin-reg\"></a>\n\n## 3.6 One Feature Linear Regression\n\nWith this section, i want to quantify the predictability of an one feature regression. First of all, lets consider only **rent amount**, that is the most correlated feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features and target\nx_col = 'rent amount'\ny_col = 'total'\n\nX = df1[[x_col]]\ny = df1[y_col]\n\n# splitting\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.333)\n\n# Create regression object\nMLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=1)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\n\n\n# Train the model using the training sets\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\ny_pr_trn = MLR.predict(X_trn_pl)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))\n\n# Plotting\nplt.figure(figsize=(8, 5))\nplt.scatter(X_tst,y_tst,c='lightgray',label = 'observations',alpha = 0.6,marker='.',zorder=1)\nplt.plot(X_tst,y_pr_tst, label = 'Predictions', c='tab:blue', lw = 3,zorder=2)\nplt.xlabel(x_col, size = 18)\nplt.ylabel(y_col, size = 18); \nplt.legend(prop={'size': 16});\nplt.title(y_col+' vs '+x_col, size = 20);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here that the R2 score is relatively good, but the error is to big: something between 14% and 15% of mean. That's not a bad thing because, despite the strong correlation between them, we have residual freedom to model the other features.\n\n<a id=\"dbl-reg\"></a>\n## 3.7 Double Feature Linear Regression\n\nThe two features most correlated  with **total** is **rent amount** and **fire insurance**. So, adopting an extremely innocent way of thinking, if we make a 2-features regression with this, we could argue that this will be the most reasonable 2-features model. That's is an error! When we think like this, we are not considering the effect of correlation.\n\n To test this, lets get the metrics of this regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Double-Feature Linear Regression - rent amount x fire insurance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features and target\nX = df1[['rent amount','fire insurance']]\ny = df1['total']\n\n# splitting\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.333)\n\n# Create regression object\nMLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=1)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\n\n# Train the model using the training sets\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\ny_pr_trn = MLR.predict(X_trn_pl)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, instead of **fire insurance**, lets get the third most correlated feature with **total**:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features and target\nX = df1[['rent amount','bathroom']]\ny = df1['total']\n\n# splitting\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.333)\n\n# Create regression object\nMLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=1)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\n\n# Train the model using the training sets\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\ny_pr_trn = MLR.predict(X_trn_pl)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So using **rent amount** and **fire insurance** is statistically equivalent to using  **rent amount** and **bathroom**.\nJust for fun, let's get the fourth most correlated feature, ie. **property tax**, to see what happens.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features and target\nX = df1[['rent amount','property tax']]\ny = df1['total']\n\n# splitting\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.333)\n\n# Create regression object\nMLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=1)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\n\n# Train the model using the training sets\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\ny_pr_trn = MLR.predict(X_trn_pl)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks how magical statistics is! Its better two use the first and fourth most correlated feature than first/fourth and first/third. The reason of this is because **fire insurance** and **bathroom** is more correlated with **rent amount** than **property tax** .\n\nWith this way of think, we can predict the **rent amount** and **hoa** will be a good predictor pair to **total** since both has significantly correlation with **total** but not with each other.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features and target\nX = df1[['rent amount','hoa']]\ny = df1['total']\n\n# splitting\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.333)\n\n# Create regression object\nMLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=1)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\n\n# Train the model using the training sets\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\ny_pr_trn = MLR.predict(X_trn_pl)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can improve this a little bit if we using quadratic regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features and target\nX = df1[['rent amount','hoa']]\ny = df1['total']\n\n# splitting\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.333)\n\n# Create regression object\nMLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=2)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\n\n# Train the model using the training sets\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\ny_pr_trn = MLR.predict(X_trn_pl)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"model\"></a>\n# 4. Building the Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After encoding the categorical data, we have now a dataset with numerical features and target. The best way to model data like this is with a kind of regression model. Here, let's try:\n\n1. [Linear Regression](#m-lin-reg)\n\n2. [Decision Tree](#m-dec-tree)\n\n3. [Random Forest](#m-rnd-frst)\n\n4. [Results](#m-res)\n\nFirstly, let's split the dataset into training and test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features and target\n#X = df1.drop('total',axis=1).copy()\nX = df1.drop(['total','fire insurance'],axis=1).copy()\ny = df1['total'].copy()\n\n# splitting\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.333)\n\n# dictionary\nmodel_mae = {}\nmodel_r2 = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"m-lin-reg\"></a>\n## 4.1. Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create regression object\nMLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=1)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\nmodel_mae['linear'] = mae\nmodel_r2['linear'] = r2\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting a reduced sample to test\nsize = len(X_tst)\nindexes = np.random.choice(len(X_tst), size, replace=False)\nX_new = X_tst.iloc[indexes]\ny_new = MLR.predict(poly.fit_transform(X_new))\n\nplot_predictions(X_new,y_new,descr = 'Linear Regression: ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2. Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\nd_tree = DecisionTreeRegressor()\nd_tree.fit(X_trn,y_trn)\n\ny_pr_tst = d_tree.predict(X_tst)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\n\nmodel_mae['tree'] = mae\nmodel_r2['tree'] = r2\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting a reduced sample to test\nsize = len(X_tst)\nindexes = np.random.choice(len(X_tst), size, replace=False)\nX_new = X_tst.iloc[indexes]\ny_new = d_tree.predict(X_new)\n\nplot_predictions(X_new,y_new,descr = 'Decision Tree: ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3. Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrnd_frst = RandomForestRegressor()\nrnd_frst.fit(X_trn,y_trn)\n\ny_pr_tst = rnd_frst.predict(X_tst)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\n\nmodel_mae['rnd_forest'] = mae\nmodel_r2['rnd_forest'] = r2\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Getting a reduced sample to test\nsize = len(X_tst)\nindexes = np.random.choice(len(X_tst), size, replace=False)\nX_new = X_tst.iloc[indexes]\ny_new = rnd_frst.predict(X_new)\n\nplot_predictions(X_new,y_new,descr = 'Random Forest: ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"m-res\"></a>\n## 4.4. Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Minor Error: ',min(model_mae, key=model_mae.get),', (',min(list(model_mae.values())),')')\nprint('Best R2    : ',max(model_r2, key=model_r2.get),', (',max(list(model_r2.values())),')')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, in most cases, the training procedure selects the **linear model** as the one that minimizes errors and maximizes R2 score.\n\n<a id=\"real-model\"></a>\n# 5. Realistic Model\n\nAs stated before, in a real case, the user wants to predict the cost variables according to the characteristics of the property. Let's study how to make this approach here.\n\nFirst of all, we have to drop all cost variables from dataset and split it into training and test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting features and target\n#X = df1.drop('total',axis=1).copy()\nX = df1.drop(['total','fire insurance','property tax','rent amount','hoa'],axis=1).copy()\ny = df1['total'].copy()\n\n# splitting\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.333)\n\n# dictionary\nmodel_mae = {}\nmodel_r2 = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's test some regression methods and collect its metrics ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create regression object\nMLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=1)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\nmodel_mae['linear'] = mae\nmodel_r2['linear'] = r2\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))\n\n\n# Getting a reduced sample to test\nsize = len(X_tst)\nindexes = np.random.choice(len(X_tst), size, replace=False)\nX_new = X_tst.iloc[indexes]\ny_new = MLR.predict(poly.fit_transform(X_new))\n\nplot_predictions(X_new,y_new,descr = 'REAL - Linear Regression: ',cols = ['area'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Quadratic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MLR = linear_model.LinearRegression()\n\npoly = PolynomialFeatures(degree=2)\nX_trn_pl = poly.fit_transform(X_trn)\nX_tst_pl = poly.fit_transform(X_tst)\nMLR.fit(X_trn_pl,y_trn)\n\ny_pr_tst = MLR.predict(X_tst_pl)\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\nmodel_mae['quadratic'] = mae\nmodel_r2['quadratic'] = r2\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))\n\n# Getting a reduced sample to test\nsize = len(X_tst)\nindexes = np.random.choice(len(X_tst), size, replace=False)\nX_new = X_tst.iloc[indexes]\ny_new = MLR.predict(poly.fit_transform(X_new))\n\nplot_predictions(X_new,y_new,descr = 'REAL - Quadratic Regression: ',cols = ['area'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\nd_tree = DecisionTreeRegressor()\nd_tree.fit(X_trn,y_trn)\n\ny_pr_tst = d_tree.predict(X_tst)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\n\nmodel_mae['tree'] = mae\nmodel_r2['tree'] = r2## Quadratic Regression\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))\n\n# Getting a reduced sample to test\nsize = len(X_tst)\nindexes = np.random.choice(len(X_tst), size, replace=False)\nX_new = X_tst.iloc[indexes]\ny_new = d_tree.predict(X_new)\n\nplot_predictions(X_new,y_new,descr = 'REAL - Decision Tree: ',cols = ['area'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrnd_frst = RandomForestRegressor()\nrnd_frst.fit(X_trn,y_trn)\n\ny_pr_tst = rnd_frst.predict(X_tst)\n\nmae = mean_absolute_error(y_tst,y_pr_tst)\nr2 = r2_score(y_tst,y_pr_tst)\n\nmodel_mae['rnd_forest'] = mae\nmodel_r2['rnd_forest'] = r2\n\nprint('MAE:{:7.2f},{:7.2f}% of mean'.format(mae,100*mae/y_pr_tst.mean()))\nprint('R2:{:6.3f}'.format(r2))\n\n# Getting a reduced sample to test\nsize = len(X_tst)\nindexes = np.random.choice(len(X_tst), size, replace=False)\nX_new = X_tst.iloc[indexes]\ny_new = rnd_frst.predict(X_new)\n\nplot_predictions(X_new,y_new,descr = 'REAL - Random Forest: ',cols = ['area'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Minor Error: ',min(model_mae, key=model_mae.get),', (',min(list(model_mae.values())),')')\nprint('Best R2    : ',max(model_r2, key=model_r2.get),', (',max(list(model_r2.values())),')')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"conc\"></a>\n# 6. Conclusion\n\nUsing the complete set of feature columns, we could fit a **Linear Model** which predicts the total rental value extremely well. The mean absolute error is something about **R\\\\$ 77,00** and the R2 **0.998**. However, this cannot be considered a real applicable model because, as we argued before, the cost variables used to define this model are correlated with the target. From a didactic perspective, it was possible to understand the influence of the number of features and its correlations on the quality of model and, in particular, it was possible to see that in certain situations, regression models can provide quite different results.\n\nFollowing the same steps, I tried to build a realistic model using only the property's features, like  **city**, **area**, **rooms**, **bathroom**, **parking spaces**, **floor**, **animal** and **furniture** (that are the  features accessible to the user), to predict the **total**. In this case, the predictability of the models significantly reduces. The best regression approach for this considerations is the **Random Forest**, with mean error of **R\\\\$ 1848,75** and R2 score of **0.65**. Although these metrics are quite bad compared to the ideal case, it is possible to find advantages in using this model. The mean of **total** columns is **R\\\\$ 5816,84** with and the standard deviation of **R\\\\$ 4643,00**. So, as our model is predicting data with mean absolute error significantly less than the standard deviation, we can conclude that there exists an influence of this feature to the total rental cost. For a most efficient approach, however, it is necessary to consider a greater number of  features.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}