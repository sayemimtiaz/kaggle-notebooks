{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Section 1: Main purpose of this notebook\n\n*I have seen many notebooks that imputed missing values using the information from the class labels for the classification task. This should be avoided, because it results in data leakage between train and test sets.*\n\n\n**Section 2:** I simply look at the dataset, and count the missing values.\n\n**Section 3 Part 1:** I create a binary classification task on an artificial dataset with two random features. Obviously, the accuracy should be around 50% since the random features are not informative about the class labels. When, furthermore, some values of a feature are set to `NaN` and imputed using the wrong way of imputation, the accuracies increased. This should not have happened. This demonstrates why that imputation method is wrong.\n\n**Section 3 Part 2:** Here, I do the same for the Water Potability task, and get around 71%.\n\n**Section 4:** I show how we can avoid data leakage between train and test sets. I do the model selection without using class labels of the test set.","metadata":{}},{"cell_type":"markdown","source":"# Section 2: Read the Water Potability data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-10T11:37:46.853169Z","iopub.execute_input":"2021-08-10T11:37:46.853597Z","iopub.status.idle":"2021-08-10T11:37:47.803272Z","shell.execute_reply.started":"2021-08-10T11:37:46.853497Z","shell.execute_reply":"2021-08-10T11:37:47.802299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/water-potability/water_potability.csv')\ndata.sample(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:37:50.226892Z","iopub.execute_input":"2021-08-10T11:37:50.227247Z","iopub.status.idle":"2021-08-10T11:37:50.29431Z","shell.execute_reply.started":"2021-08-10T11:37:50.227212Z","shell.execute_reply":"2021-08-10T11:37:50.293592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:37:51.032677Z","iopub.execute_input":"2021-08-10T11:37:51.033249Z","iopub.status.idle":"2021-08-10T11:37:51.045306Z","shell.execute_reply.started":"2021-08-10T11:37:51.033204Z","shell.execute_reply":"2021-08-10T11:37:51.043811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data has some missing values, and we would like to impute them using some strategies.","metadata":{}},{"cell_type":"markdown","source":"# Section 3: What you shouldn't do when imputing missing values","metadata":{}},{"cell_type":"markdown","source":"## Part 1: Artificial dataset example","metadata":{}},{"cell_type":"markdown","source":"Let me first create an artificial dataset. It has two features consisting of random integers between 0 and 4.\n\nWe expect that a machine learning model will have an accuracy about 50%.","metadata":{}},{"cell_type":"code","source":"random_column1 = np.random.randint(5, size=10000) #just 10000 random integers between 0 and 4\nrandom_column2 = np.random.randint(5, size=10000) #just 10000 random integers between 0 and 4\nlabels = np.random.randint(2, size=10000) #0 or 1 <- binary label\n\nartificial_data = pd.DataFrame()\nartificial_data['random_column1'], artificial_data['random_column2'] = random_column1, random_column2","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:38:04.910585Z","iopub.execute_input":"2021-08-10T11:38:04.911212Z","iopub.status.idle":"2021-08-10T11:38:04.925281Z","shell.execute_reply.started":"2021-08-10T11:38:04.911163Z","shell.execute_reply":"2021-08-10T11:38:04.924271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier()\ncv_result = cross_val_score(clf, artificial_data, labels)\nprint('Cross validation accuracy:', np.mean(cv_result))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:38:06.542868Z","iopub.execute_input":"2021-08-10T11:38:06.543226Z","iopub.status.idle":"2021-08-10T11:38:06.923852Z","shell.execute_reply.started":"2021-08-10T11:38:06.543196Z","shell.execute_reply":"2021-08-10T11:38:06.922721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's make some values of `random_column1` missing.","metadata":{}},{"cell_type":"code","source":"for i in range(len(artificial_data)):\n    if np.random.randint(4) == 0: #this has a probability 25% of being 0\n        artificial_data.iloc[i,0] = np.nan #so, approximately one quarter of the column1 is NaN now.","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:38:09.62484Z","iopub.execute_input":"2021-08-10T11:38:09.6252Z","iopub.status.idle":"2021-08-10T11:38:10.221865Z","shell.execute_reply.started":"2021-08-10T11:38:09.625168Z","shell.execute_reply":"2021-08-10T11:38:10.220941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# impute the missing values with means of labels (SHOULD NOT BE DONE)\nartificial_data['random_column1'] = artificial_data['random_column1'].fillna(artificial_data.groupby(labels)['random_column1'].transform('mean'))\n\nclf = DecisionTreeClassifier(random_state=0)\ncv_result = cross_val_score(clf, artificial_data, labels)\nprint('Cross validation accuracy:', np.mean(cv_result))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:38:10.837862Z","iopub.execute_input":"2021-08-10T11:38:10.838344Z","iopub.status.idle":"2021-08-10T11:38:10.88454Z","shell.execute_reply.started":"2021-08-10T11:38:10.838312Z","shell.execute_reply":"2021-08-10T11:38:10.883677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The accuracy increased to 62% !**\n\nThis is because we did something we shouldn't have done. We imputed missing values using the labels.","metadata":{}},{"cell_type":"markdown","source":"## Part 2: Water potability example\n\nIf we impute the missing values using the labels, let's see how much accuracy we get.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/water-potability/water_potability.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:38:31.668053Z","iopub.execute_input":"2021-08-10T11:38:31.668442Z","iopub.status.idle":"2021-08-10T11:38:31.687943Z","shell.execute_reply.started":"2021-08-10T11:38:31.668387Z","shell.execute_reply":"2021-08-10T11:38:31.687307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We impute using the labels - SHOULD NOT BE DONE\n\ndata['ph'] = data['ph'].fillna(data.groupby(['Potability'])['ph'].transform('mean'))\ndata['Sulfate']=data['Sulfate'].fillna(data.groupby(['Potability'])['Sulfate'].transform('mean'))\ndata['Trihalomethanes'] =data['Trihalomethanes'].fillna(data.groupby(['Potability'])['Trihalomethanes'].transform('mean'))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:38:32.4969Z","iopub.execute_input":"2021-08-10T11:38:32.497209Z","iopub.status.idle":"2021-08-10T11:38:32.507155Z","shell.execute_reply.started":"2021-08-10T11:38:32.497184Z","shell.execute_reply":"2021-08-10T11:38:32.506338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = data.drop('Potability', axis=1)\ny = data['Potability']","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:38:33.642611Z","iopub.execute_input":"2021-08-10T11:38:33.642956Z","iopub.status.idle":"2021-08-10T11:38:33.647726Z","shell.execute_reply.started":"2021-08-10T11:38:33.642927Z","shell.execute_reply":"2021-08-10T11:38:33.647095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = DecisionTreeClassifier(random_state=0)\ncv_result = cross_val_score(clf, x, y)\nprint(np.mean(cv_result))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:38:34.384848Z","iopub.execute_input":"2021-08-10T11:38:34.385378Z","iopub.status.idle":"2021-08-10T11:38:34.524221Z","shell.execute_reply.started":"2021-08-10T11:38:34.385328Z","shell.execute_reply":"2021-08-10T11:38:34.523163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We got around 71% accuracy. This will be an optimistic estimate for the decision tree classifier.\n\n**Note:** I am not saying that 71% accuracy is not achievable. This was just to state that imputing missing values with information using the labels (y values) is wrong, and should be avoided.","metadata":{}},{"cell_type":"markdown","source":"# Section 4: The correct way of imputation and analysis\n\nWe will\n\n- impute the missing values without using the y values (labels)\n- try different classifiers","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/water-potability/water_potability.csv')\n\nx = data.drop('Potability', axis=1)\ny = data['Potability']","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:38:44.629701Z","iopub.execute_input":"2021-08-10T11:38:44.630113Z","iopub.status.idle":"2021-08-10T11:38:44.650771Z","shell.execute_reply.started":"2021-08-10T11:38:44.630073Z","shell.execute_reply":"2021-08-10T11:38:44.649748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:38:46.283696Z","iopub.execute_input":"2021-08-10T11:38:46.284037Z","iopub.status.idle":"2021-08-10T11:38:46.290919Z","shell.execute_reply.started":"2021-08-10T11:38:46.284007Z","shell.execute_reply":"2021-08-10T11:38:46.289956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n#classifiers\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:38:47.671167Z","iopub.execute_input":"2021-08-10T11:38:47.671496Z","iopub.status.idle":"2021-08-10T11:38:47.706962Z","shell.execute_reply.started":"2021-08-10T11:38:47.671466Z","shell.execute_reply":"2021-08-10T11:38:47.706058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifiers = [DecisionTreeClassifier(random_state=0), RandomForestClassifier(random_state=0), \n               AdaBoostClassifier(random_state=0), SVC(random_state=0), \n               KNeighborsClassifier(), LogisticRegression(random_state=0)]\n\nclf_names = ['DT', 'RF', 'AB', 'SVC', 'KNN', 'LR']\n\nfor clfname, clf in zip(clf_names, classifiers):\n    pipeline = Pipeline([('scaler',StandardScaler()), ('imputer',SimpleImputer(strategy='mean')), (clfname,clf)])\n    cv_result = cross_val_score(pipeline, x_train, y_train)\n    print(clfname + ':', np.mean(cv_result))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:38:49.233917Z","iopub.execute_input":"2021-08-10T11:38:49.234282Z","iopub.status.idle":"2021-08-10T11:38:56.271585Z","shell.execute_reply.started":"2021-08-10T11:38:49.234246Z","shell.execute_reply":"2021-08-10T11:38:56.27035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like SVC is the best classifier among all. Let's tune the hyperparameters.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\npipeline = Pipeline([('scaler',StandardScaler()), ('imputer',SimpleImputer()), ('clf',SVC())])\nparameter_grid = {'imputer__strategy':['mean','median'],'clf__C':[.001,.01,.1,.2,.5,1.0,1.5,2.0,5.0], 'clf__kernel':['rbf','linear']}\nsearch = GridSearchCV(pipeline, parameter_grid)\nsearch.fit(x_train, y_train)\n\nprint('The best parameters are:', search.best_params_)\nprint('The best score cross validation score is:', search.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:45:05.871857Z","iopub.execute_input":"2021-08-10T11:45:05.87228Z","iopub.status.idle":"2021-08-10T11:45:35.95001Z","shell.execute_reply.started":"2021-08-10T11:45:05.872247Z","shell.execute_reply":"2021-08-10T11:45:35.948967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = search.best_estimator_\n\nclf.fit(x_train, y_train)\nprint('The test accuracy is', clf.score(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T11:46:08.330721Z","iopub.execute_input":"2021-08-10T11:46:08.331526Z","iopub.status.idle":"2021-08-10T11:46:08.668837Z","shell.execute_reply.started":"2021-08-10T11:46:08.331479Z","shell.execute_reply":"2021-08-10T11:46:08.667885Z"},"trusted":true},"execution_count":null,"outputs":[]}]}