{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro\n\nThe dataset contains 22,552 Airbnb listings in Berlin as of November 2018. For each listing we have the information about the property (e.g. property type, location, number of accommodated guests, cancellation policy, etc) and the host (e.g. whether the host is a super host, whether the identity of the host has been verified). \n\nThe goal of this project is to identify the factors that have the biggest impact on daily price of each listing. Below are some hypotheses that I have in mind before starting the analysis (although I actually have only used Aribnb once so far):\n* **Location**: the closer the property is to some top locations in Berlin (e.g. Berlin main train station, Reichtag, etc), the higher the price.\n* **Property type**: the price of an entire apartment should be higher than a shared apartment or a single bedroom.\n* **Host quality**: according to Airbnb page (https://www.airbnb.com/help/article/829/how-do-i-become-a-superhost), a host will become a superhost if he/she has both been an experience traveler and host and achived higher overall ratings. Properties that are offered by superhosts should have a higher price.\n* **Amenities**: if the property has offered some special amenities that others don't (e.g. TV, hangers, a laptop-friendly workplace), it should have a higher price.\n* **Policies**: properties with more favorable policies (e.g. more flexible cancellation policy) will haev a higher price.\n\nThe project can be devided into two parts:\n1. Explanatory data analysis: data cleansing, data visualisation, summary statistisc, feature engineering\n2. Feature importance using XGBoost (with Grid Search for model selection)\n\nThe explanatory analysis has confirmed most of the hypotheses above with statistical significance. In the end, two factors stand out as the most important features when predicting the daily price of Airbnb listings in Berlin:\n\nðŸ˜ Property types: the price of an entire apartment is higher than a shared apartment or a single bedroom.\n\nðŸ‘¨â€ðŸ‘©â€ðŸ‘§â€ðŸ‘¦ Accommodates: the more guests a property can host, the higher the price. This variable is a good proxy of the size of the property (which is not explicitly provided in the dataset)."},{"metadata":{},"cell_type":"markdown","source":"# Explanatory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Load the dataset\n\nFirst of all, let's load the dataset and have a first look into the shape and structure."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nlistings = pd.read_csv(\"/kaggle/input/berlin-airbnb-data/listings_summary.csv\")\nprint(listings.shape)\nlistings.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Describe the dataset\n\nNext, we examine the dataset with more details: column names, data types and missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.1 Check column names\nlistings.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.2 Check dataset info\nlistings.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.3 Examie missing values\nlistings_na = listings.isna().sum()\nlistings_na[listings_na.values > 0].sort_values(ascending=False) # Find out all variables that contain missing values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3: Pre-process column \"price\"\n\nSince \"price\" is our target variable, we need to make sure it is in good shape: no missing values, data type is correct, etc.\n\nFrom Step 2 we have seen that variable \"price\" does not have any missing values, therefore we can skip the examination of missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3.1 Describe column \"price\"\nlistings.describe(include=\"all\")[\"price\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 3.1 suggests that column \"price\" is of type object rather than int/float. Therefore, we need to convert this column into numeric value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3.2 Convert column \"price\" into a numeric variable\nlistings[\"price\"] = listings[\"price\"].apply(lambda x: x.replace(\"$\", \"\")) # Remove dollar sign\nlistings[\"price\"] = listings[\"price\"].apply(lambda x: x.replace(\",\", \"\")) # Remove thousand seperator\nlistings[\"price\"] = listings[\"price\"].astype(\"float\") # Cast the column into type float\nlistings.describe()[\"price\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The description of column \"price\" suggests that there might be outliers - 75% properties have a price that is lower than \\$ 70 yet the highest price is \\$ 90,000. We need to further investigate the outliers and remove them if necessary."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3.3 Check outliers\nimport numpy as np\nprint(\"99.5% properties have a price lower than {0: .2f}\".format(np.percentile(listings[\"price\"], 99.5)))\nlistings = listings[(listings.price <= np.percentile(listings[\"price\"], 99.5)) & (listings.price > 0)] # Exclude outliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since 99.5% entries have a price that is lower than 400, we choose to remove the rest as outliers. \n\nWe would like to create a new column \"price_range\" for visualisations later on. The choice of cutoff points (e.g. 20, 40, 60, etc) is inspired by the quartiles of column \"price\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3.4 Create column price range\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn\")\nprice_range = pd.cut(listings[\"price\"], \n                     bins=[0, 20, 40, 60, 80, 100, 120, 140, listings[\"price\"].max()], \n                     labels=[\"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-100\", \"100-120\", \"120-140\", \"140+\"])\nlistings[\"price_range\"] = price_range \nlistings[\"price_range\"].value_counts().sort_index().plot(kind=\"bar\")\nplt.title(\"Number of Listings in each Price Range\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4: Store variable names that might be interesting for exploration\n\nWith this step we will select the variables that could be helpful to explain the variation in property price.\n\nThe selection is based on the hypothesis that the following factors will affect the rent:\n* Host: identity verification, super host\n* Location: neighborhood, distance to top locations\n* Property conditions: amenities, property types, cleaninig fee, etc \n* Review scores\n* Property Size\n* Others: cancellation policy, instant bookable or not, etc"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Step 4: Split listing properties\nselected = []\nhost = ['host_is_superhost', 'host_neighbourhood', 'host_listings_count', 'host_total_listings_count', 'host_verifications', 'host_identity_verified']\nlocation = ['neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed']\ngeo = [\"latitude\", \"longitude\"]\ncondition = ['property_type', 'room_type', 'bed_type', 'amenities', 'cleaning_fee', 'minimum_nights']\nreview = ['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value']\nsize = ['space', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'square_feet']\nothers = ['instant_bookable', 'is_business_travel_ready', 'cancellation_policy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 5: Host statistics\n\nRegarding the host information we have two hypothesis:\n* If a property has a super host, its price should be higher.\n* If the host of a property has been verified, the property price should be higher."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Step 5: Host statistics\nimport matplotlib.pyplot as plt\n# 5.1 Descriptive statistics\nlistings.describe(include=\"all\")[host]\nfor col in host:\n    if listings[col].nunique() <= 10:\n        avg_price_host = listings.groupby(col).mean()[\"price\"]\n        avg_price_host.plot(kind=\"bar\")\n        plt.title(\"Avg. Price grouped by \"+col)\n        plt.show()\n    else:\n        continue","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since columns \"host_is_superhost\" and \"host_identity_verified\" are of data type Boolean, we would fill out missing values with False."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5.2 Fill out missing values\nlistings[\"host_is_superhost\"] = listings[\"host_is_superhost\"].replace(np.NAN, \"f\")\nlistings[\"host_identity_verified\"] = listings[\"host_identity_verified\"].replace(np.NAN, \"f\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5.3 Statistical test\nfrom scipy import stats\nfrom statsmodels.stats.multicomp import (pairwise_tukeyhsd, MultiComparison)\n# Define multicomp function\ndef multicomp(target_name, group_name, data):\n    if (np.nan in data[target_name]) | (np.nan in data[group_name]):\n        print(\"Please remove NaN in target variable or group variable!\")\n    elif (data[target_name].nunique() == 1) | (data[group_name].nunique() == 1):\n        print(\"There is only one unique value in target variable or group variable.\")\n    elif data[group_name].nunique() == 2:\n        mod = MultiComparison(data[target_name], data[group_name])\n        comp = mod.allpairtest(stats.ttest_ind)\n        print(comp[0])\n    else:\n        mod = MultiComparison(data[target_name], data[group_name])\n        print(mod.tukeyhsd().summary())\nmulticomp(\"price\", \"host_is_superhost\", listings)\nmulticomp(\"price\", \"host_identity_verified\", listings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The statistical tests show that the variable \"host_is_superhost\" and \"host_identity_verified\" has stat. sig. impact on price and thus should be included in the prediction model.\n\nWe also notice that column \"host_verifications\" stores the ways of identity verification that is provided by each host. It would be nice to know which are the most frequently used ways of identity verification."},{"metadata":{"trusted":true},"cell_type":"code","source":"selected.append(\"host_is_superhost\")\nselected.append(\"host_identity_verified\")\nselected","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5.3 Handle host verification\nlistings[\"host_ver_types\"] = listings[\"host_verifications\"].apply(lambda x: x[1:-1].replace(\"\\'\", \"\").split(\", \"))\nlistings[\"host_ver_type_counts\"] = listings[\"host_ver_types\"].apply(lambda x: len(x))\nlistings[\"host_ver_type_counts\"].hist()\nhost_ver_types = []\nfor i in listings[\"host_ver_types\"]:\n    host_ver_types += i\nhost_ver_types_freq = dict((x, host_ver_types.count(x)) for x in set(host_ver_types))\nhost_ver_types_freq = pd.DataFrame.from_dict(host_ver_types_freq, orient=\"index\")\nhost_ver_types_freq.reset_index(inplace=True)\nhost_ver_types_freq.columns = [\"Verification\", \"Frequency\"]\nhost_ver_types_freq = host_ver_types_freq.sort_values(by=\"Frequency\", ascending=True)\nhost_ver_types_freq.plot.barh(x=\"Verification\", y=\"Frequency\")\nplt.title(\"Most frequently used verification types\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 6: Geoplot\n\nThe dataset also includes the latitude and logitude of each property - this provides a perfect opportunity to do some geospatial plots!"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Step 6: Geoplot\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.offline import plot as plotoffline\nimport seaborn as sns\n# 6.1 Create dataset\ngeo = listings[['latitude', 'longitude', 'price', 'price_range']]\ngeo = geo.sort_values(\"price\", ascending=True) # This sorting is necessary for the color scale to work properly. \ngeo.describe()\n# 6.2 Simple scatter plot\nsns.scatterplot(x=\"longitude\", \n                y=\"latitude\", \n                hue=\"price\", \n                data=geo, \n                alpha=0.4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could also map the scatter plot to the actual Berlin map.\n\nFor this we would use the Mapbox API. Here you can find the instructions on how Mapbox works: https://plot.ly/python/scattermapbox/\n\nIn general, the price is higher in the north-eastern part of Berlin."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6.3 Map plot\npx.set_mapbox_access_token(\"XXX\") # Replace XXX with your Mapbox Token\nfig = px.scatter_mapbox(geo, \n                        lat=\"latitude\", \n                        lon=\"longitude\", \n                        color=\"price_range\",\n                        color_discrete_sequence=px.colors.sequential.Plasma,\n                        opacity=0.3, \n                        zoom=10)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To better evaluate the location of each listing, we would like to calcuate the distance between each listing and one of the Top5 locations in Berlin. Here we define the Top5 locations as:\n* Berlin main train station (hbf)\n* Berlin Tegel Airport (txl)\n* Brandenburg Tor (btor)\n* Museum Island (museum)\n* Reichstag (reichstag)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6.4 Calcuate the distance bwteen the listing and mianat tractions in Berlin\n# Formula to calculate distances\nfrom math import sin, cos, sqrt, atan2, radians\ndef distance(lat1, lat2, lon1, lon2):\n    R = 6373.0\n    rlat1 = radians(lat1)\n    rlat2 = radians(lat2)\n    rlon1 = radians(lon1)\n    rlon2 = radians(lon2)\n    rdlon = rlon2 - rlon1\n    rdlat = rlat2 - rlat1\n    a = sin(rdlat / 2)**2 + cos(rlat1) * cos(rlat2) * sin(rdlon / 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n    distance = R * c\n    return distance\n# Top locations in Berlin\ntoploc = {\"hbf\": [52.525293, 13.369359], \n          \"txl\": [52.558794, 13.288437], \n          \"btor\": [52.516497, 13.377683], \n          \"museum\": [52.517693, 13.402141], \n          \"reichstag\": [52.518770, 13.376166]}\ntoploc = pd.DataFrame.from_dict(toploc)\ntoploc_trans = toploc.transpose()\ntoploc_trans.columns = [\"latitude\", \"longitude\"]\nfig = px.scatter_mapbox(toploc_trans, \n                        lat=\"latitude\", \n                        lon=\"longitude\", \n                        zoom=10)\nfig.show()\n# Construct distance columns\ndist = []\nfor col in toploc.columns:\n    listings[\"dist_\"+col] = listings.apply(lambda x: distance(x.latitude, toploc[col][0], x.longitude, toploc[col][1]), axis=1)\n    dist.append(\"dist_\"+col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for distance in dist:\n    sns.scatterplot(x=distance, y=\"price\", data=listings, alpha=0.3)\n    plt.title(\"Correlation between price and \"+distance)\n    plt.show()\n    print(\"The correlation between price and \"+distance+ \" is {0[0]: .4f} with a p-value of {0[1]: .4f}.\".format(stats.pearsonr(listings[-listings[distance].isna()][distance], \n                                                                                            listings[-listings[distance].isna()][\"price\"])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: there is strong correlation between distances to Top5 locations and the listing price. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in dist:\n   listings[col+\"_close\"] = (listings[col] < listings[col].median())\n   print(listings.groupby(col+\"_close\").mean()[\"price\"])\nlistings[\"good_distance\"] = listings.apply(lambda x: any([x.dist_hbf_close, x.dist_txl_close, x.dist_museum_close, x.dist_reichstag_close]), axis=1)\nlistings.groupby(\"good_distance\").mean()[\"price\"].plot(kind=\"bar\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We found that listings with a good distance (i.e. identified as \"close\" to any of the Top5 locations) have a sig. higher price."},{"metadata":{"trusted":true},"cell_type":"code","source":"selected.append(\"good_distance\")\nselected","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 7: Neighbourhood statistics\n\nWe would also like to check the popularity and price level at each neighbourhood of Berlin."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Step 7: Neighbourhood statistics\n# 7.1 Top popular nerghbourhoods\nneighbourhood_group_pop = pd.DataFrame(listings[\"neighbourhood_group_cleansed\"].value_counts())\n# 7.2 Average price of each neighbourhood\nneighbourhood_group_price = listings.groupby(\"neighbourhood_group_cleansed\").mean()[\"price\"]\nneighbourhood_group_price = pd.DataFrame(neighbourhood_group_price)\n# 7.3 Create neighbourhood stats\nneighbourhood_stat = pd.merge(neighbourhood_group_pop, \n                              neighbourhood_group_price, \n                              how=\"inner\", left_index=True, right_index=True)\nneighbourhood_stat.reset_index(inplace=True)\nneighbourhood_stat.columns = [\"neighbourhood_group_cleansed\", \"count_properties\", \"avg_price\"]\nneighbourhood_stat = neighbourhood_stat.sort_values(by=\"count_properties\", ascending=False)\nneighbourhood_stat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 7.4 Plot\nfig = plt.figure(figsize=(5, 5))\nax = neighbourhood_stat.plot(x=\"neighbourhood_group_cleansed\", y=\"count_properties\", kind=\"bar\")\nneighbourhood_stat.plot(x=\"neighbourhood_group_cleansed\", y=\"avg_price\", secondary_y=True, color=\"red\", ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 8: Condition statistics\n\nWe would also like to check the impact of conditions."},{"metadata":{"trusted":true},"cell_type":"code","source":"listings[condition].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 8.1 Property type\nprop_type_avg_price = listings.groupby(\"property_type\").mean()[\"price\"]\nprop_type_count_listings = listings[\"property_type\"].value_counts()\nprop_type_stat = pd.merge(prop_type_count_listings, prop_type_avg_price, how=\"inner\", left_index=True, right_index=True)\nprop_type_stat.columns = [\"count_prop\", \"avg_price\"]\nprop_type_stat.sort_values(by=\"count_prop\", ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As 90% of listings are apartments, it does not help to explain the variation in price."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 8.2 Room type\nroom_type_avg_price = listings.groupby(\"room_type\").mean()[\"price\"]\nroom_type_count_listings = listings[\"room_type\"].value_counts()\nroom_type_stat = pd.merge(room_type_count_listings, room_type_avg_price, how=\"inner\", left_index=True, right_index=True)\nroom_type_stat.columns = [\"count_prop\", \"avg_price\"]\nroom_type_stat.sort_values(by=\"count_prop\", ascending=False).head(10)\nroom_type_avg_price.plot(kind=\"bar\")\nplt.title(\"Avg. Price per Room Type\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Entire apartment has a sig. higher price than a private room. Therefore we take it into our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"listings[\"is_entire_apt\"] = listings[\"room_type\"]==\"Entire home/apt\"\nselected.append(\"is_entire_apt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 8.3 Bed type\nlistings[\"bed_type\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"90%+ beds are real beds, so the bed type should have no sig. impact of price."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 8.4 Amendities\nlistings[\"amenities\"].head()\nlistings[\"amenities\"] = listings[\"amenities\"].apply(lambda x: x[1:-1].replace(\"\\'\", \"\").split(\",\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"listings[\"amenities\"].head()\namenity_types = []\nfor i in listings[\"amenities\"]:\n    amenity_types += i\namenity_types_freq = dict((x, amenity_types.count(x)) for x in set(amenity_types))\namenity_types_freq = pd.DataFrame.from_dict(amenity_types_freq, orient=\"index\")\namenity_types_freq.reset_index(inplace=True)\namenity_types_freq.columns = [\"Amenity\", \"Frequency\"]\namenity_types_freq = amenity_types_freq.sort_values(by=\"Frequency\", ascending=False)\namenity_types_freq.head(20).plot.barh(x=\"Amenity\", y=\"Frequency\")\nplt.title(\"Top20 most frequent amenity types\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"90%+ listings have the Top3 features: wifi, kitchen and heating. Therefore they won't have a huge impact on price. We choose to focuse on features that ~50% listings have. They are:\n* Hair dryer \n* Laptop friendly workspace\n* Hanger"},{"metadata":{"trusted":true},"cell_type":"code","source":"listings[\"with_hair_dryer\"] = listings[\"amenities\"].apply(lambda x: '\"Hair dryer\"' in x)\nlistings[\"lap_friendly\"] = listings[\"amenities\"].apply(lambda x: '\"Laptop friendly workspace\"' in x)\nlistings[\"with_hanger\"] = listings[\"amenities\"].apply(lambda x: \"Hangers\" in x)\nprint(multicomp(\"price\", \"with_hair_dryer\", listings))\nprint(multicomp(\"price\", \"lap_friendly\", listings))\nprint(multicomp(\"price\", \"with_hanger\", listings))\nfor i in [\"with_hair_dryer\", \"lap_friendly\", \"with_hanger\"]:\n    selected.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 8.5 Minimum nights\nlistings[\"minimum_nights\"].describe()\nlistings[\"min_nights_greater_than_two\"] = listings[\"minimum_nights\"] > 2\nmulticomp(\"price\", \"min_nights_greater_than_two\", data=listings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: properties that require at least three nights have sig. higher price. "},{"metadata":{"trusted":true},"cell_type":"code","source":"selected.append(\"min_nights_greater_than_two\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 8.6 Cleaning fee\n# Remove dollar sign\nlistings[\"cleaning_fee\"][-listings[\"cleaning_fee\"].isna()] = listings[\"cleaning_fee\"][-listings[\"cleaning_fee\"].isna()].apply(lambda x: x.replace(\"$\", \"\").replace(\",\", \"\"))\nlistings[\"cleaning_fee\"] = listings[\"cleaning_fee\"].astype(\"float\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"listings[\"cleaning_fee\"].isna().sum() # Check missing values\nlistings[\"cleaning_fee\"].describe()\nsns.scatterplot(x=\"cleaning_fee\", y=\"price\", data=listings, alpha=0.3)\nplt.title(\"Correlation bewteen Cleaning Fee and Price\")\nplt.show()\nprint(\"The correlation between cleaning fee and price is {0[0]: .4f} with a p-value of {0[1]: .4f}.\".format(stats.pearsonr(listings[-listings[\"cleaning_fee\"].isna()][\"cleaning_fee\"], \n                                                                                            listings[-listings[\"cleaning_fee\"].isna()][\"price\"])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: cleaning fee is highly correlated with price and thus should be selected into our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"selected.append(\"cleaning_fee\")\nselected","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 9: Review statistics\n\nOne hypothesis is that the higher the rating, the higher the price."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 9: Review statistics\n# 9.1 Examine the distribution of score ratings\nlistings[\"review_scores_rating\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 9.2 Scatter plot between review score and price\nimport seaborn as sns\nimport scipy.stats as stats\nsns.regplot(x=\"review_scores_rating\", y=\"price\", data=listings[listings[\"review_scores_rating\"]>=75])\nplt.title(\"Price vs Review Score Rating\")\nplt.show()\nprint(\"The correlation between review score and price is {0[0]: .4f} with a p-value of {0[1]: .4f}.\".format(stats.pearsonr(listings[-listings[\"review_scores_rating\"].isna()][\"review_scores_rating\"], \n                                                                                            listings[-listings[\"review_scores_rating\"].isna()][\"price\"])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no significant correlation between review score and price."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 9.3 Check the correlation between price and other scores\nfor col in review:\n    print((\"The pearson correlation coefficient between \" + col + \" and price is {0[0]: .4f}.\").format(stats.pearsonr(listings[-listings[col].isna()][col], \n                                                                                            listings[-listings[col].isna()][\"price\"])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no significant correlation between other review scores and price."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 10: Size\n# 10.1 Look at size-related variables\nlistings[size].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10.2 Check the correlation between number of accommodates and price\nlistings[\"accommodates\"].hist()\nlistings[\"accommodates\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"listings.groupby(\"accommodates\").mean()[\"price\"].plot(kind=\"bar\")\nplt.title(\"Avg. Price grouped by Number of Accommodates\")\nplt.show()\nprint(\"The pearson correlation coefficient between ther number of acoommodates and price is {0[0]: .4f} with a p-value of {0[1]: .4f}.\".format(stats.pearsonr(listings[\"accommodates\"], listings[\"price\"])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accommodates can be used as a proxy of space and has sig. correlation with price."},{"metadata":{"trusted":true},"cell_type":"code","source":"selected.append(\"accommodates\")\nselected","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10.3 Check the correlation bewteen accommodates and other size variables\nsize_variables = listings[size]\nsize_variables.drop([\"space\", \"square_feet\"], axis=1, inplace=True)\nsize_variables.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size_corr = size_variables.corr()\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(size_corr, cmap=colormap, annot=True, fmt=\".4f\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: the variable \"accommodates\" is highly correlated with \"beds\" and \"bedrooms\", so it is sufficient to include only accommodates into our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 11: Other conditions\nlistings[others].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 11.1 Instant bookable\nlistings[\"instant_bookable\"].value_counts()\nmulticomp(\"price\", \"instant_bookable\", listings)\nlistings.groupby(\"instant_bookable\").mean()[\"price\"].plot(kind=\"bar\")\nplt.title(\"Avg. Price split by Instant Bookable Policy\")\nplt.show()\nselected.append(\"instant_bookable\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 11.2 Ready for Business Travel\nlistings[\"is_business_travel_ready\"].value_counts()\nmulticomp(\"price\", \"is_business_travel_ready\", listings)\nlistings.groupby(\"is_business_travel_ready\").mean()[\"price\"].plot(kind=\"bar\")\nplt.title(\"Avg. Price split by Ready for Business Travel\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# 11.3 Cancellation plicy\nprint(listings[\"cancellation_policy\"].value_counts())\nmulticomp(\"price\", \"cancellation_policy\", listings)\nlistings.groupby(\"cancellation_policy\").mean()[\"price\"].plot(kind=\"bar\")\nplt.title(\"Avg. Price split by Cancellation Policy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"listings[\"cancellation_non_flexible\"] = listings[\"cancellation_policy\"]!=\"flexible\"\nlistings[\"cancellation_non_flexible\"].value_counts()\nmulticomp(\"price\", \"cancellation_non_flexible\", listings)\nselected.append(\"cancellation_non_flexible\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance"},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Feature Engineering\n\nFrom previous steps we have identified features that should be included in our model. We need to do some final preparation before putting them into regressor."},{"metadata":{"trusted":true},"cell_type":"code","source":"listings[selected].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.1 Convert string variables into categorical variables\nlistings[\"host_is_superhost\"] = listings[\"host_is_superhost\"]==\"t\"\nlistings[\"host_identity_verified\"] = listings[\"host_identity_verified\"]==\"t\"\nlistings[\"instant_bookable\"] = listings[\"instant_bookable\"]==\"t\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in listings[selected].select_dtypes(\"bool\").columns:\n    listings[col] = listings[col].astype(\"int\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"listings[selected].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.2 Standardisation\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nscaledFeatures = sc.fit_transform(listings[selected])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1.3 Load packages and create test set\nimport xgboost as xgb\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nX = scaledFeatures\ny = listings[\"price\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Model Selection\n\nWe choose XGBoost as our regressor for two reasons:\n1. We are trying to predict price which is a continuous variable. This means we have a prediction problem.\n2. We are interested in deriving featrue importance in the end and XGBoost fits as a perfect regressor to achieve that purpose."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.1 Initialize XGBoost classifier and find the best parameter sets with Grid Search CV\nxgb_clf = xgb.XGBRegressor()\nparameters = {'n_estimators': [120, 100, 140], 'max_depth':[3,5,7,9]}\ngrid_search = GridSearchCV(estimator=xgb_clf, param_grid=parameters, cv=5, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint(grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.2 Xgb with best parameters\nxgb_clf = xgb.XGBRegressor(n_estimators=100, max_depth=5)\nxgb_clf.fit(X_train, y_train)\ny_test_pred = xgb_clf.predict(X_test)\nprint(\"R^2 score is: {0: .4f}\".format(r2_score(y_test, y_test_pred)))\nprint(\"RMSE is: {0: .4f}\".format(np.sqrt(mean_squared_error(y_test, y_test_pred))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.3 Plot feature importance\nfeatureImport = pd.DataFrame(xgb_clf.feature_importances_, index=selected)\nfeatureImport.columns = [\"Importance\"]\nfeatureImport.sort_values([\"Importance\"], ascending=True).plot(kind=\"barh\")\nplt.title(\"XGBoost Relative Feature Importance\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Conclusion: the most important factors that decide Airbnb rent in Berlin area are the property type and the number of accommodated guests - the two together account for 70% of the explainig power of all variables included in the model. "},{"metadata":{},"cell_type":"markdown","source":"# Appendix\n\n* Scatterplots on Mapbox: https://plot.ly/python/scattermapbox/\n* Multicomparison on Python: http://www.statsmodels.org/devel/generated/statsmodels.sandbox.stats.multicomp.MultiComparison.html\n* XGBoost: https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":1}