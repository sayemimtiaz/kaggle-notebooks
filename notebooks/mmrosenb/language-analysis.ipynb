{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#imports\n\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport collections as co\nfrom io import StringIO\nimport matplotlib.pyplot as plt\nimport warnings\nfrom IPython.display import display, HTML, Markdown, display\n\n#constants\n%matplotlib inline\ndef printmd(string):\n    display(Markdown(string))\nalphaLev = .5"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#load in dataset\ncomplaintFrame = pd.read_csv(\"../input/consumer_complaints.csv\")"},{"cell_type":"markdown","metadata":{},"source":"# CFPB Consumer Complaints: Language Analysis\n\nIn this notebook, I will perform EDA and language analysis on the text-sensitive data found in the [CFPB Consumer Complaints](https://www.kaggle.com/cfpb/us-consumer-finance-complaints) dataset. You can find my analysis on the non-text-sensitve EDA within [this script](https://www.kaggle.com/mmrosenb/d/cfpb/us-consumer-finance-complaints/eda-on-consumer-complaints). As we saw in some summary statistics within that script, about $432499$ observations do not have text-sensitive data, which makes this section a general down-sizing our sample. That being said, this section potentially carries the most important aspects of the consumer complaint.\n\nWe will start by pre-processing our text data. Some of the code below is adapted from [Mike Chirico's EDA](https://www.kaggle.com/mchirico/d/cfpb/us-consumer-finance-complaints/analyzing-text-in-consumer-complaints)."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#consider only narrative observations\ncomplaintNarrativeFrame = complaintFrame[complaintFrame[\"consumer_complaint_narrative\"].notnull()]\n# build a fast way to get strings\n# adapted from \n# https://www.kaggle.com/mchirico/d/cfpb/us-consumer-finance-complaints/analyzing-text-in-consumer-complaints\ns = StringIO()\ncomplaintNarrativeFrame[\"consumer_complaint_narrative\"].apply(lambda x: s.write(x))\nk=s.getvalue()\ns.close()\nk=k.lower()\nk=k.split()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Next only want valid strings\nwords = co.Counter(nltk.corpus.words.words())\nstopWords =co.Counter( nltk.corpus.stopwords.words() )\nk=[i for i in k if i in words and i not in stopWords]\nc = co.Counter(k)\nprintmd(\"We see that we $\" + str(len(k)) + \"$ legal word tokens in our corpus. There are $\" + str(\n        len(list(c.most_common())))\n       + \"$ legal non-stopword types in our corpus.\")"},{"cell_type":"markdown","metadata":{},"source":"As discussed on [Mike Chirico's EDA](https://www.kaggle.com/mchirico/d/cfpb/us-consumer-finance-complaints/analyzing-text-in-consumer-complaints), `k` represents the array of all legal words with stopwords removed for the sentences concatenated, and `c` represents  a per-word counter over the legal words within our dataset. Let us take a look at the rank-frequency graph of our vocabulary, the $15$ most common words, and the $15$ least common words."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"wordFrequencyFrame = pd.DataFrame(c.most_common(len(c)),columns = [\"Word\",\"Frequency\"])\n#plot frequency on rank\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18, 7)\n#freq-rank\nax1.plot(wordFrequencyFrame.index,wordFrequencyFrame[\"Frequency\"])\nax1.set_title(\"Frequency on Rank of Vocabulary\")\nax1.set_xlabel(\"Rank\")\nax1.set_ylabel(\"Frequency\")\n#freq-logRank\nax2.plot(np.log(wordFrequencyFrame.index + 1),np.log(wordFrequencyFrame[\"Frequency\"]))\nax2.set_title(\"Log-Frequency on Log-Rank of Vocabulary\")\nax2.set_xlabel(\"Log Rank\")\nax2.set_ylabel(\"Frequency\")\nplt.show()\nprintmd(\"_Figure 1: Frequency-Rank Graphs of Our Vocabulary._\")\n#get 15 most common\ntop15FrequencyFrame = wordFrequencyFrame.iloc[0:15,:]\ndisplay(top15FrequencyFrame)\nprintmd(\"_Table 1: The $15$ most frequent words with their frequencies_\")\n#get 15 least common\nbottom15FrequencyFrame = wordFrequencyFrame.iloc[(wordFrequencyFrame.shape[0]-15):wordFrequencyFrame.shape[0],:]\ndisplay(bottom15FrequencyFrame)\nprintmd(\"_Table 2: The $15$ least frequent words with their frequencies_\")"},{"cell_type":"markdown","metadata":{},"source":"We see by the log-frequency on log-rank graph (Figure 1, right) that fitting a [Zipf Distribution](https://en.wikipedia.org/wiki/Zipf%27s_law) to this graph may potentially over-prediction the probability of less frequent words occuring, which suggests that our vocabulary is much more right-skewed than in a more ideal vocabulary. We see that the 15 most common words portray words that are very financially relevant, such as credit, account, loan, and bank.\n\nLet us now view the token-type graph to study richness of the vocabulary in the corpus."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#get token-type list\ntypeSet = set([]) #we will add to this over time\ntypeTokenList = [] #we will add tuples to this\nfor i in range(len(k)):\n    givenToken = k[i]\n    if (givenToken not in typeSet): #we should get a new type count\n        typeSet.add(givenToken)\n    #then add information to type-token list\n    typeTokenList.append((i+1,len(typeSet)))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#then plot\ntypeTokenFrame = pd.DataFrame(typeTokenList,columns = [\"Token Count\",\"Type Count\"])\nplt.plot(typeTokenFrame[\"Token Count\"],typeTokenFrame[\"Type Count\"])\nplt.xlabel(\"Token Count\")\nplt.ylabel(\"Type Count\")\nplt.title(\"Token Count on Type Count\")\nplt.show()\nprintmd(\"_Figure 2: Type-Token Graph for full vocabulary._\")"},{"cell_type":"markdown","metadata":{},"source":"We see that the growth of our vocabulary begins to slow after around $1000000$ tokens in our corpus, which is about $27\\%$ of the way through our corpus. To me, this suggests that the vocabulary is not extremely diverse, although it is difficult to compare without studying the relationship with other corpora.\n\nIt would be very interesting to see if the richness of vocabulary changes based on the product being addressed. Let us take a look at the distribution of products over observations with complaint narratives and the type-token graphs for each product."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"productCountFrame = complaintNarrativeFrame.groupby(\"product\")[\"consumer_complaint_narrative\"].count()\n#from pylab import *\n#val = 3+10*rand(5)    # the bar lengths\npos = np.arange(productCountFrame.shape[0])+.5    # the bar centers on the y axis\n\nplt.barh(pos,productCountFrame, align='center')\nplt.yticks(pos,productCountFrame.index)\nplt.xlabel('Count')\nplt.ylabel(\"Product Type\")\nplt.title('Distribution of Product Type')\nplt.grid(True)\nplt.show()\nprintmd(\"_Figure 3: Distribution of product types._\")\nprintmd(\"The number of narratives of the product type 'Other financial service' is $\" + str(\n        productCountFrame[\"Other financial service\"]) + \"$.\")"},{"cell_type":"markdown","metadata":{},"source":"We see that our distribution seems very uneven, as we have a large amount of Mortgage, Debt Collection, and credit reporting narratives, but relatively few observations in money transfer, other financial services, and payday loans. This may suggest that it would be difficult to predict some of these smaller groups if we are interested in a predictive model on this issue."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#declare functions before making type-token procedures\ndef makeTypeTokenFrame(tokenList):\n    #helper that makes our type-token frame for a given token list\n    typeSet = set([]) #we will add to this over time\n    typeTokenList = [] #we will add tuples to this\n    for i in range(len(tokenList)):\n        givenToken = tokenList[i]\n        if (givenToken not in typeSet): #we should get a new type count\n            typeSet.add(givenToken)\n        #then add information to type-token list\n        typeTokenList.append((i+1,len(typeSet)))\n    return pd.DataFrame(typeTokenList,columns = [\"Token Count\",\"Type Count\"])\n\ndef makeTokenList(consumerComplaintFrame):\n    #helper that makes token list from the given complaint frame\n    s = StringIO()\n    consumerComplaintFrame[\"consumer_complaint_narrative\"].apply(lambda x: s.write(x))\n    k = s.getvalue() #gets string of unprocessed words\n    s.close()\n    #get actual unprocessed words\n    #k = k.lower()\n    k = k.split()\n    k = [i for i in k if i in words and i not in stopWords] #only consider legal words\n    return k\n\ndef getTokenTypeFrameForProduct(consumerComplaintFrame,productName):\n    #helper that gets our token-type frame for narratives of a given product name\n    #get observations with this product name\n    givenProductComplaintFrame = consumerComplaintFrame[consumerComplaintFrame[\"product\"] == productName]\n    #then get token list\n    tokenList = makeTokenList(givenProductComplaintFrame)\n    #then make type-token frame\n    return makeTypeTokenFrame(tokenList)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#run through our observations\ntypeTokenFrameDict = {} #we will adds to this\nfor productName in productCountFrame.index:\n    typeTokenFrameDict[productName] = getTokenTypeFrameForProduct(complaintNarrativeFrame,productName)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"cmap = plt.get_cmap('Dark2')\ncolorList = [cmap(i) for i in np.linspace(0, 1, len(typeTokenFrameDict))]\nfor i in range(len(typeTokenFrameDict)):\n    productName = list(typeTokenFrameDict)[i]\n    givenProductTokenTypeFrame = typeTokenFrameDict[productName]\n    plt.plot(givenProductTokenTypeFrame[\"Token Count\"],\n             givenProductTokenTypeFrame[\"Type Count\"],label = productName,\n            c = colorList[i])\nplt.legend(bbox_to_anchor = (1.6,1))\nplt.xlabel(\"Token Count\")\nplt.ylabel(\"Type Count\")\nplt.title(\"Token-Type Graph\\nBy Product Name\")\nplt.show()\nprintmd(\"_Figure 4: Token-Type Graph By Product Name._\")"},{"cell_type":"markdown","metadata":{},"source":"This graph makes one obvious thing apparent: that there are many more mortgage complaint observations than other categories. While it is difficult to define which of these products have the richest vocabulary due to the difference in line lengths, it is very obvious that debt collection and credit reporting seem to have less rich vocabularies than the other products. This may be essential for distinguishing the groups, although it is currently difficult to tell why this is. I am open to discussion on possible hypotheses for this.\n\nWhat may also be useful is to consider if we could predict whether a customer would eventually dispute the final resolution of a claim based on the language in the complaint itself. While it is likely this will also be a function of the company's decision of the company's response to the consumer, it may be interesting to study if vagueness or complexity of a dispute would factor into whether a consumer disputes a resolution."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def getTokenTypeFrameForDispute(consumerComplaintFrame,disputeLev):\n    #helper that gets our token-type frame for narratives of a dispute level\n    #get observations with this product name\n    givenDisputeComplaintFrame = consumerComplaintFrame[consumerComplaintFrame[\"consumer_disputed?\"] == disputeLev]\n    #then get token list\n    tokenList = makeTokenList(givenDisputeComplaintFrame)\n    #then make type-token frame\n    return makeTypeTokenFrame(tokenList)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"consumerDisputeDict = {} #we will add to this\nfor disputeLev in complaintNarrativeFrame[\"consumer_disputed?\"].unique():\n    consumerDisputeDict[disputeLev] = getTokenTypeFrameForDispute(complaintNarrativeFrame,disputeLev)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"for disputeLev in consumerDisputeDict:\n    DisputeTokenTypeFrame = consumerDisputeDict[disputeLev]\n    plt.plot(DisputeTokenTypeFrame[\"Token Count\"],\n             DisputeTokenTypeFrame[\"Type Count\"],label = disputeLev)\nplt.legend(bbox_to_anchor = (1.3,1))\nplt.xlabel(\"Token Count\")\nplt.ylabel(\"Type Count\")\nplt.title(\"Token-Type Graph\\nBy Whether Consumer Disputed\")\nplt.show()\nprintmd(\"_Figure 5: Token-Type Graph By whether the consumer disputed._\")"},{"cell_type":"markdown","metadata":{},"source":"While it looks as though the \"Yes\" observations have a slightly richer vocabulary than the \"No\" observations, seems to be a relatively small difference.\n\nNonetheless, I think this would be one of the interesting prediction problems to see whether or not the language of a given consumer complaint is as much of a reason for a dispute as the company response.\n\n## Predicting Consumer Disputes\n\nWe will initially model our language using bag-of-words with TF-IDF encodings. This is for the sake of simplicity: if there may be another possible language model that may better represent the language-generation process for prediction sake, I may re-model the language."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n#make mappable for vocabulary\ncounterList = c.most_common()\nvocabDict = {} #we will add to this\nfor i in range(len(counterList)):\n    vocabWord = counterList[i][0]\n    vocabDict[vocabWord] = i\n#make array of tf-idf counts\nvectorizer = TfidfVectorizer(min_df=1,stop_words = stopWords,vocabulary = vocabDict)\nunigramArray = vectorizer.fit_transform(complaintNarrativeFrame[\"consumer_complaint_narrative\"])"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#generate our language matrix\nlanguageFrame = pd.DataFrame(unigramArray.toarray(),columns = vectorizer.get_feature_names())\nprintmd(\"The number of features extracted is $\" + str(languageFrame.shape[1]) + \"$.\")"},{"cell_type":"markdown","metadata":{},"source":"Given that this is an extremely high-dimensional model, it would be useful to consider a form of dimensionality reduction on this likely sparse vocabulary. We will consider a form of principal component analysis for this purpose."},{"cell_type":"markdown","metadata":{},"source":"TODO:\n\n* Fix Color Scheme For Figure 4\n\n* Fix $x$-ticks on Figure 2\n\n* Fix iPython Display Objects to adjust for Kaggle Notebook structure\n\n* Run PCA"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}