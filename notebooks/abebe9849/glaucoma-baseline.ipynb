{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install geffnet\n!pip install timm\n!pip install ttach","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"class CFG:\n    debug=False\n    n_classes = 2\n    lr=1e-4\n    batch_size=16\n    epochs=10# you can train more epochs\n    seed=777\n    n_fold=4\n    warmup=-1\n    device=1\n    amp = True\n    amp_inf = False\n    smooth = False\n    smooth_alpha = 0.1\n    efnet_num = 10##:0,1,2\n    drop_rate = 0.25\n    crop = True##bool\n    psuedo_label = False\n    pseudo_predict = \"2020-11-06_14:50:43.385109_predict.csv\"\n    TTA=True\n    Attention=True\n    white = False#ごましおになる\n    model_name = \"tf_efficientnet_b0_ns\"\n    zoom = True\nSIZE = 512","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport glob\n\nopen_nega = glob.glob(\"../input/glaucomadataset/Non Glaucoma/*png\")\nopen_pos = glob.glob(\"../input/glaucomadataset/Glaucoma/*tif\")\n\n\n\nopen_p = pd.DataFrame()\nopen_p[\"file\"] = open_pos\nopen_p[\"label\"] = 1\n\nopen_n = pd.DataFrame()\nopen_n[\"file\"] = open_nega\nopen_n[\"label\"] = 1\n\nopen_ = pd.concat([open_n,open_p])\nprint(open_.head())\n\nopen_[\"from_china\"] = 0\n\n\n\nchina = pd.read_csv(\"../input/panda-efnetb2-180-weight/china_gla.csv\")\nchina[\"file\"] = [\"../input/ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K/Training Images/{}\".format(china[\"filename\"].values[i]) for i in range(len(china))]\nchina[\"label\"] = china[\"Gla\"]\nchina_ = china.drop([\"Unnamed: 0\",\"Patient Age\",\"ID\",\"Patient Sex\"],axis=1)\nchina_1 = china_.head(300)\nchina_0 = china_.tail(300)\ncat_df = pd.concat([china_1,china_0])\ncat_df = cat_df.reset_index(drop=True)\n\ncat_df[\"from_china\"] = 1\n\ndf = pd.concat([cat_df,open_])\ndf.to_csv(\"concat_df.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import geffnet\nimport sys\nimport gc\nimport os\nimport random\nimport time\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\n\n#from  torch.cuda.amp import autocast, GradScaler \nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport pydicom\nimport sklearn.metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom functools import partial\nfrom tqdm import tqdm\nimport ttach as tta##TTA\nimport timm\n\nimport matplotlib as mpl\nmpl.use ( 'Agg') # must be written both in import intermediate\nimport matplotlib.pyplot as plt\nimport sklearn.metrics as metric\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.models as models\nfrom albumentations import Compose, Normalize, HorizontalFlip, VerticalFlip,RandomGamma, RandomRotate90,GaussNoise,Cutout\nfrom albumentations.pytorch import ToTensorV2\n\n\n# Utils\n# ====================================================","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(torch.cuda.current_device())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class efenet_Model_attention(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        if CFG.efnet_num ==0:\n            m = geffnet.efficientnet_b0(pretrained=True, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==1:\n            m = geffnet.efficientnet_b1(pretrained=True, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==2:\n            m = geffnet.efficientnet_b2(pretrained=True, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==10:\n            m = timm.create_model(CFG.model_name, pretrained=True, num_classes=CFG.n_classes)\n        self.enc = nn.Sequential(*list(m.children())[:-3]) \n        nc = list(m.children())[-1].in_features\n        self.atten = nn.Sequential(\n                nn.Conv2d(nc, 1, 1),\n                nn.Sigmoid())\n        self.head = nn.Sequential(nn.AdaptiveAvgPool2d(1),nn.Flatten(),nn.Linear(nc,512),\n                            nn.ReLU(),nn.BatchNorm1d(512), nn.Dropout(0.5),nn.Linear(512,2))\n    def forward(self, x):\n        x = self.enc(x)#ベースのモデルの流れに同じ\n        atten = self.atten(x)\n        #print(x.size(),atten.size())\n        y = x*atten\n        x = self.head(y)\n        return x,atten\n\nclass baseline_model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        if CFG.efnet_num ==0:\n            self.model = geffnet.efficientnet_b0(pretrained=True, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==1:\n            self.model = geffnet.efficientnet_b1(pretrained=True, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==2:\n            self.model = geffnet.efficientnet_b2(pretrained=True, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==10:\n            self.model = timm.create_model(CFG.model_name, pretrained=False, num_classes=CFG.n_classes)\n        elif CFG.efnet_num<3:\n            self.model.classifier = nn.Linear(self.model.classifier.in_features, 2)\n\n    def forward(self, x):\n        \n        if CFG.efnet_num ==10:\n            x = self.model(x)\n        else:\n            x = self.model(x)#ベースのモデルの流れに同じ\n        return x\n\nclass baseline_Model_attention(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        if CFG.efnet_num ==0:\n            m = geffnet.efficientnet_b0(pretrained=False, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==1:\n            m = geffnet.efficientnet_b1(pretrained=False, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==2:\n            m = geffnet.efficientnet_b2(pretrained=False, drop_rate=CFG.drop_rate)\n        elif CFG.efnet_num ==10:\n            m = timm.create_model(CFG.model_name, pretrained=True, num_classes=CFG.n_classes)\n        self.enc = nn.Sequential(*list(m.children())[:-3]) \n        nc = list(m.children())[-1].in_features\n        self.atten = nn.Sequential(\n                nn.Conv2d(nc, 1, 1),\n                nn.Sigmoid())\n        self.head = nn.Sequential(nn.AdaptiveAvgPool2d(1),nn.Flatten(),nn.Linear(nc,512),\n                            nn.ReLU(),nn.BatchNorm1d(512), nn.Dropout(0.5),nn.Linear(512,2))\n    def forward(self, x):\n        x = self.enc(x)#ベースのモデルの流れに同じ\n        atten = self.atten(x)\n        #print(x.size(),atten.size())\n        y = x*atten\n        x = self.head(y)\n        return x,atten\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\ndef get_pad_width(im, new_shape, is_rgb=True):\n    pad_diff = new_shape - im.shape[0], new_shape - im.shape[1]\n    t, b = math.floor(pad_diff[0]/2), math.ceil(pad_diff[0]/2)\n    l, r = math.floor(pad_diff[1]/2), math.ceil(pad_diff[1]/2)\n    if is_rgb:\n        pad_width = ((t,b), (l,r), (0, 0))\n    else:\n        pad_width = ((t,b), (l,r))\n    return pad_width\n\ndef crop_object(img, thresh=10, maxval=200, square=False):\n    \"\"\"\n    Source: https://stackoverflow.com/questions/49577973/how-to-crop-the-biggest-object-in-image-with-python-opencv\n    \"\"\"\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# convert to grayscale\n    #plt.imshow(gray,cmap=\"gray\")\n    #plt.show()#普通に白黒のがみえる\n    # threshold to get just the signature (INVERTED)\n    retval, thresh_gray = cv2.threshold(gray, thresh=thresh, maxval=maxval, type=cv2.THRESH_BINARY)\n    #plt.imshow(thresh_gray,cmap=\"gray\")\n    #plt.show()\n    contours, hierarchy = cv2.findContours(thresh_gray,cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n    #https://qiita.com/anyamaru/items/fd3d894966a98098376c\n    # Find object with the biggest bounding box\n    mx = (0,0,0,0)      # biggest bounding box so far\n    mx_area = 0\n    for cont in contours:\n        x,y,w,h = cv2.boundingRect(cont)\n        area = w*h\n        if area > mx_area:\n            mx = x,y,w,h\n            mx_area = area\n    x,y,w,h = mx#(0,0,0,0)なのはcontoursに何も入ってないから\n    crop = img[y:y+h, x:x+w]\n    if square:\n        pad_width = get_pad_width(crop, max(crop.shape))\n        crop = np.pad(crop, pad_width=pad_width, mode='constant', constant_values=255)\n    if CFG.white:\n        black = [0, 0, 0]\n        white = [255, 255, 255]\n        crop[np.where((crop == black).all(axis=2))] = white\n    if CFG.zoom:\n        h = crop.shape[0]\n        w = crop.shape[1]\n        h_ = int(h*0.15)\n        w_ = int(w*0.15)\n        crop = crop[h_:h-h_,w_:w-w_]\n    return crop\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##train_test_split\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(cat_df, test_size=0.3,stratify = cat_df[\"label\"], random_state=2020)\ntrain = train.reset_index(drop=True)\ntest = test.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##train_valid_split\nif CFG.debug:\n    folds = train.sample(n=200, random_state=CFG.seed).reset_index(drop=True).copy()\nelse:\n    folds = train.copy()\ntrain_labels = folds[\"label\"].values\nkf = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\nfor fold, (train_index, val_index) in enumerate(kf.split(folds.values, train_labels)):\n    print(\"num_train,val\",len(train_index),len(val_index),len(val_index)+len(train_index))\n    folds.loc[val_index, 'fold'] = int(fold)\n\nfolds['fold'] = folds['fold'].astype(int)\nfolds.to_csv('folds.csv', index=None)\nfolds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, df,transform1=None, transform2=None):\n        self.df = df\n        self.transform = transform1\n        self.transform_ = transform2\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_path = self.df['file'].values[idx]\n        image = cv2.imread(file_path)\n        if CFG.crop:\n            image = crop_object(image)\n        try:\n            image = cv2.resize(image,(SIZE,SIZE))\n        except Exception as e:\n            print(file_path)\n        label_ = self.df[\"label\"].values[idx]\n        if self.transform:\n            image = self.transform(image=image)['image']\n        if self.transform_:\n            image = self.transform_(image=image)['image']\n\n        \n            \n        label = torch.tensor(label_)\n        #print(label_,type(label_),label,label.size())\n        \n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics as metric\n\ndef auc(y,y_hat):\n    return metric.roc_auc_score(y,y_hat)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_transforms1(*, data):\n\n    #train,valid以外だったら怒る\n    \n    if data == 'train':\n        return Compose([\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            #GaussNoise(p=0.5),\n            #RandomRotate90(p=0.5),\n            #RandomGamma(p=0.5),\n            #RandomAugMix(severity=3, width=3, alpha=1., p=0.5),\n            #GaussianBlur(p=0.5),\n            #GridMask(num_grid=3, p=0.3),\n            #Cutout(p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n        ])\n    \n    elif data == 'valid':\n        return Compose([\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            )\n        ])\n\ndef to_tensor(*args):\n\n        return Compose([\n            ToTensorV2()\n        ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"glaucomadataset由来のデータは学習済みモデルに通したベクトルを次元圧縮すると割とはっきり疾患か否かが別れていることがわかる。"},{"metadata":{},"cell_type":"markdown","source":"OScularのデータセット（以下中国産のデータセットと呼ぶ）は分離が難しそう。"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip\n!pip install -U torch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"@contextmanager\ndef timer(name):\n    t0 = time.time()\n    LOGGER.info(f'[{name}] start')\n    yield\n    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')\n\n    \ndef init_logger(log_file='train.log'):\n    from logging import getLogger, DEBUG, FileHandler,  Formatter,  StreamHandler\n    \n    log_format = '%(asctime)s %(levelname)s %(message)s'\n    \n    stream_handler = StreamHandler()\n    stream_handler.setLevel(DEBUG)\n    stream_handler.setFormatter(Formatter(log_format))\n    \n    file_handler = FileHandler(log_file)\n    file_handler.setFormatter(Formatter(log_format))\n    \n    logger = getLogger('RSNA2020')\n    logger.setLevel(DEBUG)\n    logger.addHandler(stream_handler)\n    logger.addHandler(file_handler)\n    \n    return logger\nimport datetime\ndt_now = datetime.datetime.now()\ndt_now_ = str(dt_now).replace(\" \",\"_\")\nprint(\"実験開始\",dt_now_)\nLOG_FILE = 'train{}.log'.format(dt_now_)\nLOGGER = init_logger(LOG_FILE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def onehot_encoding(label, n_classes):\n    \"\"\"\n  　torch,tensor(k_0,k_1,k_2,)k_iは0~n_classes-1のどれか→one_hot\n  　\"\"\"\n    return torch.eye(n_classes)[label.type(torch.long)]\ndef label_smoothing(label,epsilon,n_classes):\n    onehot = onehot_encoding(label, n_classes).float()\n    label = onehot * (1 - epsilon) + torch.ones_like(onehot)* epsilon/n_classes\n    return label\n\ndef cross_entropy_loss(input, target, reduction):\n    logp = nn.functional.log_softmax(input, dim=1)\n    loss = torch.sum(-logp * target, dim=1)\n    if reduction == 'none':\n        return loss\n    elif reduction == 'mean':\n        return loss.mean()\n    elif reduction == 'sum':\n        return loss.sum()\n    else:\n        raise ValueError(\n            '`reduction` must be one of \\'none\\', \\'mean\\', or \\'sum\\'.')\n\n\ndef train_fn(fold):\n    print(f\"### fold: {fold} ###\")\n\n        \n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n    train_dataset = TrainDataset(folds.loc[trn_idx].reset_index(drop=True), \n                                 transform1=get_transforms1(data='train'),transform2=to_tensor())#\n    valid_dataset = TrainDataset(folds.loc[val_idx].reset_index(drop=True), \n                                 transform1=get_transforms1(data='valid'),transform2=to_tensor())#\n\n    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=4)\n    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=4)\n    \n    if CFG.Attention:\n        model = efenet_Model_attention()\n    else:\n        model = efnet_model()\n    model.to(device)\n    \n    optimizer = Adam(model.parameters(), lr=CFG.lr, amsgrad=False)\n    #scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True, eps=1e-6)\n    #scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=0.001)\n    \n    criterion = nn.CrossEntropyLoss()#weight = class_weight\n    softmax = nn.Softmax(dim = 1)\n    scaler = torch.cuda.amp.GradScaler()\n    #criterion = FocalLoss_BCE()#doesn't work\n    best_score = 0\n    best_loss = np.inf\n    best_preds = None\n    \n    \n    for epoch in range(CFG.epochs):\n        \n        start_time = time.time()\n\n        model.train()\n\n        if CFG.warmup>epoch:\n            print(\"freeze\")\n            for param in model.model.parameters():\n                param.requires_grad = False\n            for param in model.model.classifier.parameters():\n                param.requires_grad = True\n        \n        if CFG.warmup<=epoch:\n            print(\"unfreeze\")\n            for param in model.parameters():\n                param.requires_grad = True\n        avg_loss = 0.\n\n        tk0 = tqdm(enumerate(train_loader), total=len(train_loader))\n\n        for i, (images, labels) in tk0:\n            optimizer.zero_grad()\n\n            images = images.to(device)\n            smooth_labels = label_smoothing(labels,CFG.smooth_alpha,2).to(device)\n            #print(\"smoooth\",smooth_labels.size())#torch.Size([64, 3, 512, 512, 3])\n            labels = labels.to(device)\n            if CFG.Attention:\n                y_preds,attention = model(images.float())\n            else:\n                y_preds = model(images.float())\n            if CFG.smooth:\n                loss = cross_entropy_loss(y_preds, smooth_labels,\"mean\")\n            else:\n                loss = criterion(y_preds, labels.long())\n            loss.backward()\n            optimizer.step()\n                #optimizer.zero_grad()\n\n            avg_loss += loss.item() / len(train_loader)\n        model.eval()\n        avg_val_loss = 0.\n        preds = []\n        valid_labels = []\n        masks = []\n        tk1 = tqdm(enumerate(valid_loader), total=len(valid_loader))\n\n        for i, (images, labels) in tk1:\n\n            images = images.to(device)\n            smooth_labels = label_smoothing(labels,CFG.smooth_alpha,2).to(device)\n            labels = labels.to(device)\n\n            with torch.no_grad():\n                if CFG.Attention:\n                    y_preds,attention = model(images.float())\n                    masks.append(attention.to(\"cpu\").detach().numpy())\n                else:\n                    y_preds = model(images.float())\n                if CFG.smooth:\n                    loss = cross_entropy_loss(y_preds, smooth_labels,\"mean\")\n                else:\n                    loss = criterion(y_preds,labels.long())\n\n\n\n            valid_labels.append(labels.to('cpu').numpy())\n            softmax = nn.Softmax(dim = 1)\n            y_preds = softmax(y_preds)\n            #if i ==0:\n                #print(y_preds.shape,y_preds.dtype,y_preds)#torch.Size([16, 5]) torch.float32\n\n            preds.append(y_preds.to('cpu').numpy())\n            avg_val_loss += loss.item() / len(valid_loader)\n            \n        #scheduler.step(avg_val_loss)\n            \n        preds = np.concatenate(preds)\n        if CFG.Attention:\n            masks = np.concatenate(masks)\n        #print(preds.shape)\n        valid_labels = np.concatenate(valid_labels)\n        #valid_labels = np.identity(5)[valid_labels]\n\n        print(preds.shape,valid_labels.shape)\n\n        score = auc(valid_labels,preds[:,1])\n\n        elapsed = time.time() - start_time\n        LOGGER.debug(f'  Epoch {epoch+1} - avg_train_loss: {avg_loss:.6f}  avg_val_loss: {avg_val_loss:.6f}  time: {elapsed:.0f}s')\n        \n        if score>best_score:#loglossのスコアが良かったら予測値を更新...best_epochをきめるため\n            best_score = score\n            best_preds = preds\n            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Score: {best_score:.4f}')\n            torch.save(model.state_dict(), f'fold{fold}_{dt_now_}_baseline.pth')#各epochのモデルを保存。。。best_epoch終了時のモデルを推論に使用する？\n    \n    return best_preds, valid_labels,masks\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npreds = []\nvalid_labels = []\nmodels =[]\nfor fold in range(CFG.n_fold):\n    _preds, _valid_labels,_model = train_fn(fold)\n    preds.append(_preds)\n    valid_labels.append(_valid_labels)\n    models.append(_model)\n    \n##\npreds_ = np.concatenate(preds)\nvalid_labels_ = np.concatenate(valid_labels)\n\nscore = auc(valid_labels_,preds_)\nimport datetime\n\ndt_now = datetime.datetime.now()\nprint(\"現在時刻\",dt_now)\nprint(\"=====AUC(CV)======\",score)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, df, transform1=None, transform2=None):\n        self.df = df\n        self.transform = transform1\n        self.transform_ = transform2\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_path = self.df['file'].values[idx]\n        image = cv2.imread(file_path)\n        if CFG.crop:\n            image = crop_object(image)\n        image = cv2.resize(image,(SIZE,SIZE))\n        label = self.df[\"label\"].values[idx]\n        if self.transform:\n            image = self.transform(image=image)['image']\n        if self.transform_:\n            image = self.transform_(image=image)['image']\n\n        \n        return image\n\n\ndef inference(model, test_loader, device):\n\n    transforms = tta.Compose([\n        tta.HorizontalFlip(),\n        tta.VerticalFlip(),\n        tta.Rotate90(angles=[0, 180]),\n        tta.Multiply(factors=[0.9, 1, 1.1]),        \n    ])\n    model.to(device) \n    if CFG.TTA and CFG.Attention:\n        model = model\n    elif CFG.TTA:\n        model = tta.ClassificationTTAWrapper(model, transforms)\n    \n    model.to(device) \n    model.eval()\n    probs = []\n    masks = []\n\n    for i, images in tqdm(enumerate(test_loader), total=len(test_loader)):\n            \n        images = images.to(device)\n        with torch.no_grad():\n            if CFG.Attention and CFG.Attention:\n                attention_ = []\n                for transformer in transforms:\n                    augmented_image = transformer.augment_image(images.float())\n                    y_preds,attention = model(augmented_image)\n                    attention = transformer.deaugment_mask(attention)\n                    attention_.append(attention)\n                attention = torch.stack(attention_)\n                attention = torch.mean(attention,axis=0)\n            elif CFG.Attention:\n                y_preds,attention = model(image.float())\n\n            else:\n                y_preds = model(images)\n            softmax = nn.Softmax(dim = 1)\n            y_preds = softmax(y_preds)\n            \n            \n        probs.append(y_preds.to('cpu').numpy())\n        if CFG.Attention:\n            masks.append(attention.to(\"cpu\").detach().numpy())\n\n    probs = np.concatenate(probs)\n    if CFG.Attention:\n        masks = np.concatenate(masks)\n        print(masks.shape)\n\n\n    \n    return probs,masks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit():\n        print('run inference')\n        test_dataset = TestDataset(test, transform1=get_transforms1(data='valid'),transform2=to_tensor())\n        test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)\n        probs = []\n        masks = []\n        for fold in range(CFG.n_fold):\n            weights_path = f\"../input/panda-efnetb2-180-weight/fold{fold}_2020-11-24_022939.936386_baseline.pth\"\n            if CFG.Attention:\n                model = baseline_Model_attention()\n            else:\n                model = baseline_model()\n            state_dict = torch.load(weights_path,map_location=device)\n            model.load_state_dict(state_dict)\n            _probs,_mask = inference(model, test_loader, device)\n            probs.append(_probs)\n            masks.append(_mask)\n\n        probs = np.mean(probs, axis=0)\n        if CFG.Attention:\n            masks = np.mean(masks, axis=0)\n\n        return probs,masks\n\npredict,masks= submit()\nif CFG.Attention:\n    np.save(\"attention_masks.npy\",masks)\nprint(test.head())\nprint(test[\"label\"].values.shape,predict.shape)\nscore = auc(test[\"label\"].values,predict[:,1])\nprint(\"=====AUC(inner_test)======\",score)\ntest[\"predict\"] = predict[:,1]\ntest.to_csv(\"predict.csv\",index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可視化"},{"metadata":{"trusted":true},"cell_type":"code","source":"masks = np.load(\"attention_masks.npy\")\n%matplotlib  inline\nfor idx in tqdm(range(len(test))):\n    file_path = test['file'].values[idx]\n    label  = test['label'].values[idx]\n    pred = test['predict'].values[idx]\n    mask = masks[idx,:,:,:].transpose((1,2,0))[:,:,0]\n    image = cv2.imread(file_path)\n    image = crop_object(image)\n    image = cv2.resize(image,(SIZE,SIZE))\n    plt.title(f\"{file_path}__label{label}_pred{pred}\")\n    plt.imshow(image)\n    plt.show()\n    plt.imshow(mask,cmap='jet')\n    plt.show()\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}