{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"083a1d7f-4221-5409-6f74-f23fe1e37999"},"source":"<h2>Introduction:</h2>\nThis is my First kernel, I have attempted to understand which features contribute to the Price of the houses.\n<br> A shoutout to SRK and Anisotropic from whom iv learned a lot about data visualisation</br>"},{"cell_type":"markdown","metadata":{"_cell_guid":"0e2c67b6-c28f-f899-7713-4c528e07702d"},"source":"<h2>Lets import the libraries we need for now<h2>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6c786d0-1aef-d094-54d0-444ee41b8031"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls"},{"cell_type":"markdown","metadata":{"_cell_guid":"baab0e51-e8f3-9205-5c2d-95073658f40b"},"source":"\n<h2>now we import the dataset</h2>\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16c82a74-5e6e-8c9e-ebb1-df2ea8b03949"},"outputs":[],"source":"data = pd.read_csv('../input/kc_house_data.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1bba3da-ba0e-83f4-0e5d-79ccc1e07fcf"},"outputs":[],"source":"# Lets check it out \ndata.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"18f65a0f-e14f-adf2-21a3-411e03e762f3"},"source":"<h2>now lets check out how many NaN values are there</h2>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0eded96-861f-6790-ca99-597c6af0c551"},"outputs":[],"source":"data.isnull().sum()"},{"cell_type":"markdown","metadata":{"_cell_guid":"19bad7a9-dad8-0c74-29f1-91d561a277b7"},"source":"wow! so we just dont need to bother about using Imputer and handeling the NaN values\n<br>Now lets check out how the data actually is</br>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"050aada4-cbdc-38b4-acf9-3b8b488fce46"},"outputs":[],"source":"print(data.info())\nprint(\"**\"*40)\nprint(data.describe())"},{"cell_type":"markdown","metadata":{"_cell_guid":"db28a559-4940-08cf-235b-71c26d846d60"},"source":"Oops, we forgot to convert the date to datetime, lets get that done first"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"856c20cc-b72f-a90f-74ba-3bf7aec3cf8b"},"outputs":[],"source":"data['date'] = pd.to_datetime(data['date'])\n# while im at it, let me create a year and month column too\ndata['year'], data['month'] = data['date'].dt.year, data['date'].dt.month"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa25e866-ad81-c89f-1cfd-4387e16e3ad4"},"outputs":[],"source":"# as we have everything from the date column, lets simply remove it \ndel data['date']"},{"cell_type":"markdown","metadata":{"_cell_guid":"7184003c-d6b0-069c-1ba0-4e79119a8851"},"source":"<h2>We have finished the preliminary data cleaning, now lets visualize and check out for some correlation that we can use</h2>"},{"cell_type":"markdown","metadata":{"_cell_guid":"535f2f44-cab2-fbaa-1c11-19cf6ff56d81"},"source":"the dataset includes latitude and longitude for each entry, lets plot it out and see if specific areas sold more houses or less"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7e0983e-fd9a-dcd4-1653-ba4b3de5baf5"},"outputs":[],"source":"plt.figure(figsize=(12,12))\nsns.jointplot( 'long','lat',data = data, size=9 , kind = \"hex\")\nplt.xlabel('Longitude', fontsize=10)\nplt.ylabel('Latitude', fontsize=10)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"cfa0e4ff-62d5-b362-a85e-243377d0d325"},"source":"as we guessed, there are some areas where many houses were sold\n<br>\n<h2>lets try out the pearson correlation</h2></br>\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ff07a38-10a8-58e1-6fa9-f92b5216c603"},"outputs":[],"source":"dataa = [\n    go.Heatmap(\n        z= data.corr().values,\n        x= data.columns.values,\n        y= data.columns.values,\n        colorscale='Viridis',\n        text = True ,\n        opacity = 1.0\n        \n    )\n]\n\nlayout = go.Layout(\n    title='Pearson Correlation',\n    xaxis = dict(ticks='', nticks=30),\n    yaxis = dict(ticks='' ),\n    width = 800, height = 600,\n    \n)\n\nfig = go.Figure(data=dataa, layout=layout)\npy.iplot(fig, filename='Housedatacorr')"},{"cell_type":"markdown","metadata":{"_cell_guid":"3851f4c2-6463-7e34-eac6-d426e238bece"},"source":"In the price col we can see some rows are so very close to zero, but lets not remove them from the data set as of yet, it may be useful for some ensemble process\n<br>\n<h2>Now lets try out some trees and ensemble methods for a better understanding of the feature importances</h2></br>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f6b823f1-0c2e-7027-0415-fb9dfbee9cd9"},"outputs":[],"source":"# the models we will run\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\n\n# some metrics to help us out\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import mean_squared_error as mse"},{"cell_type":"markdown","metadata":{"_cell_guid":"0bb771c3-1b67-8105-69b5-25a36c2c5c7c"},"source":"lets remove the price col from data as we will need it now "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6405ae67-8d5c-3839-b73c-f79e9142270a"},"outputs":[],"source":"target = data['price']\n# we dont need the price column in data anymore\ndel data['price']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b83ebe4d-23e4-01f0-7718-b17989637f4b"},"outputs":[],"source":"X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0515ff4d-ccbb-27bb-18be-2147c3678457"},"source":"Now im going to find feature_importances using various ensemble methods "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0cfd33d5-2bff-6502-ec48-c35640b726a9"},"outputs":[],"source":"dr = DecisionTreeRegressor()\ndr.fit(X_train,y_train)\ndrimp = dr.feature_importances_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc49b9df-ff5b-d756-54e9-fd2f6b11658a"},"outputs":[],"source":"rfr = RandomForestRegressor(n_estimators=100)\nrfr.fit(X_train,y_train)\nrfrimp = rfr.feature_importances_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b81f6480-15d0-f516-5f63-8ad83b034cc2"},"outputs":[],"source":"gbr =  GradientBoostingRegressor(n_estimators=100)\ngbr.fit(X_train,y_train)\ngbrimp = gbr.feature_importances_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d128528f-ed52-0580-ad97-783e8287d995"},"outputs":[],"source":"abr =  AdaBoostRegressor(n_estimators=100)\nabr.fit(X_train,y_train)\nabrimp = abr.feature_importances_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a8f6a6b-d845-6677-036d-b748134d0017"},"outputs":[],"source":"etr =  ExtraTreesRegressor(n_estimators=100)\netr.fit(X_train,y_train)\netrimp = etr.feature_importances_"},{"cell_type":"markdown","metadata":{"_cell_guid":"879f5629-527a-bc3f-b953-5aa387881b17"},"source":"lets create a data frame that has all these values \n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5474081-3864-f641-dd43-9586c213df9d"},"outputs":[],"source":"d = {'Decision Tree':drimp, 'Random Forest':rfrimp, 'Gradient Boost':gbrimp,'Ada boost':abrimp, 'Extra Tree':etrimp}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a36f963c-e581-2aa9-df0b-1c0ad9457766"},"outputs":[],"source":"features = pd.DataFrame(data = d)\n# lets check out features\nfeatures.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a0ce0cb-b81a-5184-676d-bd8c443fcd74"},"source":"One good way to check how important a feature is will be to calculate the mean from each method "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f16e43b9-0363-a6ea-6632-54af24835c5d"},"outputs":[],"source":"features['mean'] = features.mean(axis= 1) \n# we forgot to add the names of the features\nfeatures['names'] = data.columns.values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"69d7a0dd-a6eb-f054-4dd2-edfe045e2654"},"outputs":[],"source":"#lets check it out now \nfeatures.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"81d36412-c5ef-4b8d-90e9-a4bcb902cb09"},"source":"<h2>Now i'll plot a barplot to illustrate how the mean of each feature has fared</h2>\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c2e5c8d-3912-0e3d-b016-18e14a566dc1"},"outputs":[],"source":"y = features['mean'].values\nx = features['names'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = features['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Mean Feature Importance',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'Feature Importance for Housing Price',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='barplothouse')"},{"cell_type":"markdown","metadata":{"_cell_guid":"e66c3591-aaa8-934c-4c13-e73606c525eb"},"source":"<h1>Conclusion</h1>\n<br>We can see that there are two very prominent features i.e. sqft_living and grade, which according to all the models are very useful to predict the price of the house.</br>\n<br>A close third is the Latitude, which one can consider as the area where the house is</br>\n<br> We were also expecting the bathrooms feature and the sqft_above feature to be high ranking in the barplot, as it was considered imp according to Pearson corr. </br>\n<br>But maybe the ensemble's know better or it has simply overfitted the data</br>\n<br> I think there might be a few more features that can be extracted from the dataset which might give more insight , in accordance to our intuitive and qualitative thinking</br>\n<br>Hope you enjoyed it! please give me some feedback below and upvote if you liked it! </br>"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}