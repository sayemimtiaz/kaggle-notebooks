{"cells":[{"metadata":{"_uuid":"5d6ba4cbea1a979cdfda17b17e4abf216ba9f435","_cell_guid":"9c8d6158-debf-425d-b7e7-2ebdfce2e406"},"cell_type":"markdown","source":"**Predict Diabetes From Medical Records**"},{"metadata":{"collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"},"cell_type":"markdown","source":"Diabetes mellitus (DM), commonly referred to as diabetes, is a group of metabolic disorders in which there are high blood sugar levels over a prolonged period.  Type 1 diabetes results from the pancreas's failure to produce enough insulin.  Type 2 diabetes begins with insulin resistance, a condition in which cells fail to respond to insulin properly.  As of 2015, an estimated 415 million people had diabetes worldwide, with type 2 diabetes making up about 90% of the cases. This represents 8.3% of the adult population.    Source: https://en.wikipedia.org/wiki/Diabetes_mellitus\n\nThe [Pima Indians Diabetes Database](https://www.kaggle.com/uciml/pima-indians-diabetes-database) can be used to train machine learning models to predict if a given patient has diabetes.  This dataset contains measurements relating to Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, and Age.  \n"},{"metadata":{"_uuid":"46aaadc5a4b6db28876d62cc5131e11ac8022b80","_cell_guid":"6950f5fb-7d29-404a-9de1-cfe88e801008"},"cell_type":"markdown","source":"*Step 1: Import Python Packages*"},{"metadata":{"collapsed":true,"_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"37867649-a1fc-44f7-acd7-9794152a9c04","_uuid":"8a89dacf039a9f136a8d06799ffe5068d1a60e39","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas import read_csv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport graphviz \nfrom sklearn import model_selection\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, learning_curve, StratifiedKFold, train_test_split\nfrom sklearn.metrics import confusion_matrix, make_scorer, accuracy_score \nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neural_network import MLPClassifier as MLPC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"2eea7459357647b00d7c58930db769ec3c843873","_cell_guid":"4f6fb117-9aa2-4182-b1c8-529c3bfdf3c9"},"cell_type":"markdown","source":"*Step 2: Define Helper Functions*"},{"metadata":{"collapsed":true,"_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"f42887f6-2a04-4eb4-a912-764a05776d64","_uuid":"89fb0cd2287578fec1c291e1d610d0a9116663b7","trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Plots a learning curve. http://scikit-learn.org/stable/modules/learning_curve.html\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    return plt\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ndef compareABunchOfDifferentModelsAccuracy(a, b, c, d):\n    \"\"\"\n    compare performance of classifiers on X_train, X_test, Y_train, Y_test\n    http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n    http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score\n    \"\"\"    \n    print('\\nCompare Multiple Classifiers: \\n')\n    print('K-Fold Cross-Validation Accuracy: \\n')\n    names = []\n    models = []\n    resultsAccuracy = []\n    models.append(('LR', LogisticRegression()))\n    models.append(('RF', RandomForestClassifier()))\n    models.append(('KNN', KNeighborsClassifier()))\n    models.append(('SVM', SVC()))\n    models.append(('LSVM', LinearSVC()))\n    models.append(('GNB', GaussianNB()))\n    models.append(('DTC', DecisionTreeClassifier()))\n    models.append(('GBC', GradientBoostingClassifier()))\n    for name, model in models:\n        model.fit(a, b)\n        kfold = model_selection.KFold(n_splits=10, random_state=7)\n        accuracy_results = model_selection.cross_val_score(model, a,b, cv=kfold, scoring='accuracy')\n        resultsAccuracy.append(accuracy_results)\n        names.append(name)\n        accuracyMessage = \"%s: %f (%f)\" % (name, accuracy_results.mean(), accuracy_results.std())\n        print(accuracyMessage) \n    # Boxplot\n    fig = plt.figure()\n    fig.suptitle('Algorithm Comparison: Accuracy')\n    ax = fig.add_subplot(111)\n    plt.boxplot(resultsAccuracy)\n    ax.set_xticklabels(names)\n    ax.set_ylabel('Cross-Validation: Accuracy Score')\n    plt.show()    \n      \ndef defineModels():\n    print('\\nLR = LogisticRegression')\n    print('RF = RandomForestClassifier')\n    print('KNN = KNeighborsClassifier')\n    print('SVM = Support Vector Machine SVC')\n    print('LSVM = LinearSVC')\n    print('GNB = GaussianNB')\n    print('DTC = DecisionTreeClassifier')\n    print('GBC = GradientBoostingClassifier \\n\\n')\n\nnames = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\", \"MLPClassifier\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\"]    \n\nclassifiers = [\n    KNeighborsClassifier(),\n    SVC(kernel=\"linear\"),\n    SVC(kernel=\"rbf\"),\n    GaussianProcessClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    MLPClassifier(),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()\n]\n\ndict_characters = {0: 'Healthy', 1: 'Diabetes'}","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"81f983b2cd7c874506495c507542a155f331fac2","_cell_guid":"5d9edef2-3ae8-4894-b3c5-027f7b4a653b"},"cell_type":"markdown","source":"*Step 3: Inspect and Clean Data*"},{"metadata":{"_uuid":"8c19e0bf65102ac0c578cdac61380897ec2bb92e","_cell_guid":"d1764e55-aae5-4959-9c67-5f3ea5203188"},"cell_type":"markdown","source":"When starting a new data analysis project it is important to inspect, understand, and clean the data.  When inspecting the first ten rows of data one thing that jumps out right away is that there are measurements of zero for both insulin levels and skin thickness.  "},{"metadata":{"_uuid":"924edc193c853395713bcfdade0a26f36633bf03","_cell_guid":"65475636-e376-491c-9ff5-f558598afd37","trusted":true},"cell_type":"code","source":"dataset = read_csv('../input/diabetes.csv')\ndataset.head(10)","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"3c1e827efa192861b80ea25355106d6600ef9273","_cell_guid":"67fd6314-6176-4449-9839-b209ab0bd373"},"cell_type":"markdown","source":"It would be a serious medical problem if a patient had an insulin level and skin thickness measurement of zero.  As such, we can conclude that this dataset uses the number zero to represent missing or null data.  Here we can see that as many as half of the rows contain columns with missing data."},{"metadata":{"_uuid":"d87a65f1087b89f570fc14ffc96115577e6824bf","_cell_guid":"033d06fa-6a63-4c88-8745-6ab0eb9dc80a","trusted":true},"cell_type":"code","source":"def plotHistogram(values,label,feature,title):\n    sns.set_style(\"whitegrid\")\n    plotOne = sns.FacetGrid(values, hue=label,aspect=2)\n    plotOne.map(sns.distplot,feature,kde=False)\n    plotOne.set(xlim=(0, values[feature].max()))\n    plotOne.add_legend()\n    plotOne.set_axis_labels(feature, 'Proportion')\n    plotOne.fig.suptitle(title)\n    plt.show()\nplotHistogram(dataset,\"Outcome\",'Insulin','Insulin vs Diagnosis (Blue = Healthy; Orange = Diabetes)')\nplotHistogram(dataset,\"Outcome\",'SkinThickness','SkinThickness vs Diagnosis (Blue = Healthy; Orange = Diabetes)')","execution_count":4,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"0031b33c44f66dfc7437586c0b7a3e466979c42f","_cell_guid":"6c9eff67-66d0-46c4-80ea-3e2451471683","trusted":true},"cell_type":"code","source":"dataset2 = dataset.iloc[:, :-1]\nprint(\"# of Rows, # of Columns: \",dataset2.shape)\nprint(\"\\nColumn Name           # of Null Values\\n\")\nprint((dataset2[:] == 0).sum())","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"6c8a90ee5f18fd5e214eec426ae38802c66503e1","_cell_guid":"4751b521-e977-46d6-948c-de0bf90e6082","trusted":true},"cell_type":"code","source":"print(\"# of Rows, # of Columns: \",dataset2.shape)\nprint(\"\\nColumn Name              % Null Values\\n\")\nprint(((dataset2[:] == 0).sum())/768*100)","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"ae01a03b7471670b369afb918679cf932ea9e6d9","_cell_guid":"694d1b57-4b0c-45b3-a266-2c672895445c"},"cell_type":"markdown","source":"Approximately 50% of the patients did not have their insulin levels measured.  This causes me to be concerned that maybe the doctors only measured insulin levels in unhealthy looking patients -- or maybe they only measured insulin levels after having first made a preliminary diagnosis.  If that were true then this would be a form of [data leakage](https://www.kaggle.com/dansbecker/data-leakage), and it would mean that our model would not generalize well to data collected from doctors who measure insulin levels for every patient.  In order to test this hypothesis I will check whether or not the Insulin and SkinThickness features are correlated with a diagnostic outcome (healthy/diabetic).  What we find is that these features are not highly correlated with any given outcome -- and as such we can rule out our concern of data leakage.\n"},{"metadata":{"_uuid":"c2039ea71625f74a46452421ecd2ada3618465fa","_cell_guid":"d9459be8-3ac9-477c-846b-471ce09cda23","trusted":true},"cell_type":"code","source":"g = sns.heatmap(dataset.corr(),cmap=\"BrBG\",annot=False)","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"39901d449b5617ce676357f85c85a051b2b0fe6b","_cell_guid":"c8d41550-c10e-4b7d-b5fd-7baa1c4a8560","trusted":true},"cell_type":"code","source":"dataset.corr()","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"bde3a1afd9a3a74efe90bb3b121b7cd5ac8c70e8","_cell_guid":"63397347-a145-4fc0-b8a9-2cb06837f448"},"cell_type":"markdown","source":"The Insulin and SkinThickness measurements are not highly correlated with any given outcome -- and as such we can rule out our concern of data leakage.  The zero values in these categories are still erroneous, however, and therefore should not be included in our model.  It is best to replace these values with some distribution of values near to the median measurement.  Also note that it is best to impute these values *after* the [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function in order to prevent yet another form of data leakage (i.e. the testing data should not be used when calculating the median value to use during imputation).  The following histogram illustrates that the null values have indeed been replaced with median values."},{"metadata":{"_uuid":"305e3128b63c6ec838c9c804ffc7e5bf7ef54b92","_cell_guid":"8aac39aa-0047-4963-ad7a-f1b98e92f4f5","trusted":true},"cell_type":"code","source":"data = read_csv('../input/diabetes.csv')\nX = data.iloc[:, :-1]\ny = data.iloc[:, -1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nimputer = Imputer(missing_values=0,strategy='median')\nX_train2 = imputer.fit_transform(X_train)\nX_test2 = imputer.transform(X_test)\nX_train3 = pd.DataFrame(X_train2)\nplotHistogram(X_train3,None,4,'Insulin vs Diagnosis (Blue = Healthy; Orange = Diabetes)')\nplotHistogram(X_train3,None,3,'SkinThickness vs Diagnosis (Blue = Healthy; Orange = Diabetes)')","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"2f9b3686bc6a2cfc70379cb76872454f7d871f0b","_cell_guid":"0a5497a2-c66d-4e8a-936b-317e5bb3ee69","trusted":true},"cell_type":"code","source":"labels = {0:'Pregnancies',1:'Glucose',2:'BloodPressure',3:'SkinThickness',4:'Insulin',5:'BMI',6:'DiabetesPedigreeFunction',7:'Age'}\nprint(labels)\nprint(\"\\nColumn #, # of Zero Values\\n\")\nprint((X_train3[:] == 0).sum())\n# data[:] = data[:].replace(0, np.NaN)\n# print(\"\\nColumn #, # of Null Values\\n\")\n# print(np.isnan(X_train3).sum())","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"1047f9eab8fd3ba251509aeb7147d678fb8a58fd","_cell_guid":"ffa86ef6-7a50-4110-aea0-198d208134c5"},"cell_type":"markdown","source":"*Step 4: Evaluate Classification Models*"},{"metadata":{"_uuid":"289dab0f5b01ad7713d1571b9998d6a216caaed2","_cell_guid":"1743b613-a0b7-4371-af2d-ba22ee28f839"},"cell_type":"markdown","source":"Because we have replaced all of the erroneous, missing, and null values with median values we are now ready to train and evaluate our models for predicting diabetes."},{"metadata":{"_uuid":"e4fc9b2ffc3999bf85badc844ec7d1bda9df07ba","_cell_guid":"f42c3b2f-7926-4d4d-bb14-cfd2088cc29c","trusted":true},"cell_type":"code","source":"compareABunchOfDifferentModelsAccuracy(X_train2, y_train, X_test2, y_test)\ndefineModels()\n# iterate over classifiers; adapted from https://www.kaggle.com/hugues/basic-ml-best-of-10-classifiers\nresults = {}\nfor name, clf in zip(names, classifiers):\n    scores = cross_val_score(clf, X_train2, y_train, cv=5)\n    results[name] = scores\nfor name, scores in results.items():\n    print(\"%20s | Accuracy: %0.2f%% (+/- %0.2f%%)\" % (name, 100*scores.mean(), 100*scores.std() * 2))","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"da888d8ba44fd872cbd6365a34fc15b9f962a72a","_cell_guid":"fb320556-96f2-4ee3-b8c5-5d7c2e4dea2a"},"cell_type":"markdown","source":"*Step 5: Examine Decision Tree Model in More Detail*"},{"metadata":{"_uuid":"33b00d6abeafbb0dae97a359ad690f4e42da9ede","_cell_guid":"139c8318-7534-429a-95b8-93cfa50889fe"},"cell_type":"markdown","source":"Next let's explore the decision tree model in more detail."},{"metadata":{"_kg_hide-input":true,"_uuid":"b61c2106275d1d58033b65a1a0d8c6067dbf7b03","_cell_guid":"8a7f778a-1191-48e9-818b-d6914ab1dfd9","trusted":true},"cell_type":"code","source":"def runDecisionTree(a, b, c, d):\n    model = DecisionTreeClassifier()\n    accuracy_scorer = make_scorer(accuracy_score)\n    model.fit(a, b)\n    kfold = model_selection.KFold(n_splits=10, random_state=7)\n    accuracy = model_selection.cross_val_score(model, a, b, cv=kfold, scoring='accuracy')\n    mean = accuracy.mean() \n    stdev = accuracy.std()\n    prediction = model.predict(c)\n    cnf_matrix = confusion_matrix(d, prediction)\n    #plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,title='Normalized confusion matrix')\n    plot_learning_curve(model, 'Learning Curve For DecisionTreeClassifier', a, b, (0.60,1.1), 10)\n    #learning_curve(model, 'Learning Curve For DecisionTreeClassifier', a, b, (0.60,1.1), 10)\n    plt.show()\n    plot_confusion_matrix(cnf_matrix, classes=dict_characters,title='Confusion matrix')\n    plt.show()\n    print('DecisionTreeClassifier - Training set accuracy: %s (%s)' % (mean, stdev))\n    return\nrunDecisionTree(X_train2, y_train, X_test2, y_test)\nfeature_names1 = X.columns.values\ndef plot_decision_tree1(a,b):\n    dot_data = tree.export_graphviz(a, out_file=None, \n                             feature_names=b,  \n                             class_names=['Healthy','Diabetes'],  \n                             filled=False, rounded=True,  \n                             special_characters=False)  \n    graph = graphviz.Source(dot_data)  \n    return graph \nclf1 = tree.DecisionTreeClassifier(max_depth=3,min_samples_leaf=12)\nclf1.fit(X_train2, y_train)\nplot_decision_tree1(clf1,feature_names1)","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"bb7735c74d944489e2ad2bff3c2b24e7e33ee555","_cell_guid":"66b90423-8b54-4c88-93ea-2f56ab4ccb68"},"cell_type":"markdown","source":"*Step 6: Evaluate Feature Importances*"},{"metadata":{"_uuid":"be75ff5780480f1615c8db3fc0a4bfb9b7a28fa0","_cell_guid":"04491720-b03a-45c4-b5f7-075079a76564"},"cell_type":"markdown","source":"Many [kernel authors](https://www.kaggle.com/uciml/pima-indians-diabetes-database/kernels) have neglected to deal with the null values and missing data discussed in this notebook.  This mistake did not actually have much of an impact on the performance of most of their models, however, because, as it happens, the Insulin and SkinThickness measurements are actually very poor predictors and are assigned low feature importances as compared to features such as blood glucose levels and body mass index."},{"metadata":{"_kg_hide-input":true,"_uuid":"a4fafaf035a47504df06e17a6b0ba539957d8cc2","_cell_guid":"73925719-607e-4041-9012-4890c5e28867","trusted":true},"cell_type":"code","source":"feature_names = X.columns.values\nclf1 = tree.DecisionTreeClassifier(max_depth=3,min_samples_leaf=12)\nclf1.fit(X_train2, y_train)\nprint('Accuracy of DecisionTreeClassifier: {:.2f}'.format(clf1.score(X_test2, y_test)))\ncolumns = X.columns\ncoefficients = clf1.feature_importances_.reshape(X.columns.shape[0], 1)\nabsCoefficients = abs(coefficients)\nfullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\nprint('DecisionTreeClassifier - Feature Importance:')\nprint('\\n',fullList,'\\n')\n\nfeature_names = X.columns.values\nclf2 = RandomForestClassifier(max_depth=3,min_samples_leaf=12)\nclf2.fit(X_train2, y_train)\nprint('Accuracy of RandomForestClassifier: {:.2f}'.format(clf2.score(X_test2, y_test)))\ncolumns = X.columns\ncoefficients = clf2.feature_importances_.reshape(X.columns.shape[0], 1)\nabsCoefficients = abs(coefficients)\nfullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\nprint('RandomForestClassifier - Feature Importance:')\nprint('\\n',fullList,'\\n')\n\nclf3 = XGBClassifier()\nclf3.fit(X_train2, y_train)\nprint('Accuracy of XGBClassifier: {:.2f}'.format(clf3.score(X_test2, y_test)))\ncolumns = X.columns\ncoefficients = clf3.feature_importances_.reshape(X.columns.shape[0], 1)\nabsCoefficients = abs(coefficients)\nfullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\nprint('XGBClassifier - Feature Importance:')\nprint('\\n',fullList,'\\n')\n","execution_count":13,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"fecb03d811c12a928a4ed9199a4b1532834f9a28","_cell_guid":"deb59ccf-c7cd-4a0d-a37e-33ef28987f88"},"cell_type":"markdown","source":"In the end we were able to predict diabetes from medical records with an accuracy of approximately 82%.  This was done by using tree-based classifiers that focus on important features such as blood glucose levels and body mass index.  In fact, we only lose 5% accuracy by dropping all of the data except for blood glucose levels and body mass index (see below)."},{"metadata":{"_kg_hide-input":true,"_uuid":"420c5fd425ff2de2bde384df9f5874b1f1ed025b","_cell_guid":"257d3eca-95c0-4fdf-90d2-589c1f34985c","trusted":true},"cell_type":"code","source":"data = read_csv('../input/diabetes.csv')\ndata2 = data.drop(['Pregnancies','BloodPressure','DiabetesPedigreeFunction', 'Age','SkinThickness','Insulin'], axis=1)\nX2 = data2.iloc[:, :-1]\ny2 = data2.iloc[:, -1]\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X2, y2, test_size=0.2, random_state=1)\nimputer = Imputer(missing_values=0,strategy='median')\nX_train3 = imputer.fit_transform(X_train3)\nX_test3 = imputer.transform(X_test3)\nclf4 = XGBClassifier()\nclf4.fit(X_train3, y_train3)\nprint('Accuracy of XGBClassifier in Reduced Feature Space: {:.2f}'.format(clf4.score(X_test3, y_test3)))\ncolumns = X2.columns\ncoefficients = clf4.feature_importances_.reshape(X2.columns.shape[0], 1)\nabsCoefficients = abs(coefficients)\nfullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\nprint('\\nXGBClassifier - Feature Importance:')\nprint('\\n',fullList,'\\n')\n\nclf3 = XGBClassifier()\nclf3.fit(X_train2, y_train)\nprint('\\n\\nAccuracy of XGBClassifier in Full Feature Space: {:.2f}'.format(clf3.score(X_test2, y_test)))\ncolumns = X.columns\ncoefficients = clf3.feature_importances_.reshape(X.columns.shape[0], 1)\nabsCoefficients = abs(coefficients)\nfullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\nprint('XGBClassifier - Feature Importance:')\nprint('\\n',fullList,'\\n')\n","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"2b2051010b7b7fa0607a954a5c036ffd0202536c","_cell_guid":"85815bfb-2bcc-4a54-9bf4-60498f8060d1"},"cell_type":"markdown","source":"*Step 7: Summarize Results*"},{"metadata":{"_uuid":"a39de037bd996485391a5a6faee0cf25619cf6cd","_cell_guid":"64798bc2-59be-4ee2-995b-4a70c3c8f1e2"},"cell_type":"markdown","source":"In this notebook we predicted diabetes from medical records with an accuracy of approximately 82%  -- and we also discussed topics such as missing data, data imputation, and feature importances."}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}