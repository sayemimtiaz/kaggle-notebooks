{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <p style=\"background-color:  #ff8080; font-family: Helvetica, fantasy; line-height: 1.3; font-size: 26px; letter-spacing: 3px; text-align: center; color: #ffffff\">Heart Attack Prediction using Extreme Gradient Boosting (XGBoost)</p>\n\n![](https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/pink-porcelain-anatomical-heart-royalty-free-image-1597338342.jpg)","metadata":{}},{"cell_type":"markdown","source":"<p style=\"background-color:  #ff8080; font-family: Helvetica, fantasy; line-height: 1.3; font-size: 26px; letter-spacing: 3px; text-align: center; color: #ffffff\">Dataset description</p>\n\n- age : Age of the patient\n- Sex : 1 = male; 0 = female\n- exng: exercise induced angina (1 = yes; 0 = no)\n- oldpeak: ST depression induced by exercise relative to rest\n- slp: the slope of the peak exercise ST segment (2 = upsloping; 1 = flat; 0 = downsloping)\n- thall: 2 = normal; 1 = fixed defect; 3 = reversable defect.\n- caa: number of major vessels (0-3)\n- cp : Chest Pain type chest pain type\n - Value 0: typical angina\n - Value 1: atypical angina\n - Value 2: non-anginal pain\n - Value 3: asymptomatic\n- trtbps : resting blood pressure (in mm Hg)\n- chol : cholestoral in mg/dl fetched via BMI sensor\n- fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n- restecg : resting electrocardiographic results\n - Value 0: normal\n - Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n- thalachh : maximum heart rate achieved\n- output: target : 0 = less chance of heart attack 1 = more chance of heart attack","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport random\n\nrandom.seed(224)\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/heart-attack-analysis-prediction-dataset/heart.csv')\nprint(df.shape)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Renaming columns\ndf.columns = ['Age', 'Sex', 'ChestPainType', 'RestingBloodPressure', 'Cholesterol', 'FastingBloodSugar', 'RestingECG', 'MaxHeartRate',\n       'ExerciseInducedAngina', 'PreviousPeak', 'Slope', 'MajorBloodVessels', 'ThalRate', 'ProbHA']\n\ncategoricals = ['Sex', 'ChestPainType', 'FastingBloodSugar', 'RestingECG', 'ExerciseInducedAngina', 'Slope', 'ThalRate', 'ProbHA']\nnumericals = [i for i in df.columns if i not in categoricals]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Investigate categorical features","metadata":{}},{"cell_type":"code","source":"for col in df[categoricals]:\n    print(f'We have {len(df[col].unique())} unique values in --{col}-- column: {df[col].unique()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count plots for categorical features\nx=0\nfig=plt.figure(figsize=(15,10),constrained_layout =True)\nplt.subplots_adjust(wspace = 0.5)\nplt.suptitle(\"Count of the Categorical Variables\",y=0.95, family='Sherif', size=18, weight='bold')\nfor i in df[categoricals]:\n    ax = plt.subplot(241+x)\n    ax = sns.countplot(data=df, x=i, color = 'salmon')\n    plt.grid(axis='y')\n    x+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Investigate numerical features","metadata":{}},{"cell_type":"code","source":"df[numericals].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = df.corr()\n\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)]=True\nwith sns.axes_style('white'):\n    fig, ax = plt.subplots(figsize=(18,10))\n    sns.heatmap(corr,  mask=mask, cmap='YlGnBu', annot=True, center=0, vmin=-1, vmax=0.8,\n                square=True, cbar_kws={'shrink':.5, 'orientation': 'vertical'}, linewidth=.02)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=0\nfig=plt.figure(figsize=(15,10),constrained_layout =True)\nplt.subplots_adjust(wspace = 0.5)\nplt.suptitle(\"Distribution of numerical variables\",y=0.95, family='Sherif', size=18, weight='bold')\nfor i in df[numericals]:\n    ax = plt.subplot(231+x)\n    ax = sns.boxplot(data=df, y=i, color = 'salmon')\n    x+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Outlier: Cholesterol > 500; MaxHeartRate < 80\n","metadata":{}},{"cell_type":"code","source":"df.drop(df[df['Cholesterol'] > 500].index, inplace = True)\ndf.drop(df[df['MaxHeartRate'] < 80].index, inplace = True)\ndf.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"print(df['ProbHA'].value_counts())\n\npie, ax = plt.subplots(figsize=[15,10])\nlabels = ['More chance to HA', 'Less chance to HA']\ncolors = ['#ff8533', '#7070db']\nplt.pie(x = df['ProbHA'].value_counts(), autopct='%.2f%%', explode=[0.02]*2, labels=labels, pctdistance=0.5, textprops={'fontsize': 14}, colors = colors)\nplt.title('Distributin of target variable in %')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=0\nfig=plt.figure(figsize=(15,10),constrained_layout =True)\nplt.subplots_adjust(wspace = 0.5)\nplt.suptitle(\"Count of the categorical variables by target variable\",y=0.95, family='Sherif', size=18, weight='bold')\nfor i in df[categoricals]:\n    ax = plt.subplot(241+x)\n    ax = sns.countplot(data=df, x=i, hue='ProbHA', palette = colors)\n    ax.legend_.remove()\n    plt.grid(axis='y')\n    x+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Insights:\n - The number of males that are more likely to have a HA from the number of total males is higher than females.\n - The individuals who present a typical angina chest type are more likely to have a HA.\n - The individuals with normal (0) resting electrocardiographic results (Resting ECG) appear to be more likely to suffer a HA.\n - If angina is exercise induced, is more likely to suffer a HA.\n - If the slope of the peak exercise ST segment is flat, is more likely to suffer a HA.\n - If the thal rate is reversable defect, is more likely to suffer a HA. ","metadata":{}},{"cell_type":"code","source":"x=0\nfig=plt.figure(figsize=(15,10),constrained_layout =True)\nplt.subplots_adjust(wspace = 0.5)\nplt.suptitle(\"Distribution of numerical variables by target variable\",y=0.95, family='Sherif', size=18, weight='bold')\nfor i in df[numericals]:\n    ax = plt.subplot(231+x)\n    ax = sns.histplot(data=df, x=i, hue='ProbHA', palette=colors, element='poly')\n    ax.legend_.remove()\n    x+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=0\nfig=plt.figure(figsize=(15,10),constrained_layout =True)\nplt.subplots_adjust(wspace = 0.5)\nplt.suptitle(\"Relationships between age and numerical features by target variable\",y=0.95, family='Sherif', size=18, weight='bold')\nfor i in df[numericals[1:]]:\n    ax = plt.subplot(231+x)\n    ax = sns.scatterplot(data=df, x='Age', y=i, hue='ProbHA', palette=colors)\n    ax.legend_.remove()\n    x+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"# Split into features & target; train & test\n# Normalize features\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Normalizer\ny = df['ProbHA']\nX = df.drop('ProbHA', axis = 1)\n\nnormalize = Normalizer()\nX = normalize.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123, shuffle = True, stratify = y)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\nmodels = [('DT', DecisionTreeClassifier()),\n          ('LR', LogisticRegression()), \n          ('SGDC', SGDClassifier()), \n          ('SVC', SVC())]\n\n# Baseline models trainining and evaluation\nfor name, model in models:\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    acc = accuracy_score(y_test, preds)\n    print(f'The accuracy of {name} is {acc:.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensembling with XGBoost (Extreme Gradient Boosting)","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\n# Basline XGBClassifier\nxgb_cl = xgb.XGBClassifier()\nxgb_cl.fit(X_train, y_train)\npreds = xgb_cl.predict(X_test)\nscore = accuracy_score(y_test, preds)\nprint(f'The accuracy of XGBClassifier is {score:.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters tuning for XGBoost","metadata":{}},{"cell_type":"code","source":"# Grid search\nfrom sklearn.model_selection import GridSearchCV\n\nparams_grid = {'learning_rate':[0.01, 0.1, 0.5, 0.9],\n              'n_estimators':[100,200,300],\n              'subsample':[0.3, 0.5, 0.9],\n               'max_depth':[2,3,4],\n               'colsample_bytree':[0.3,0.5,0.7,1]}\ngrid = GridSearchCV(estimator=xgb_cl, param_grid=params_grid, scoring='accuracy', cv = 10)\n\ngrid.fit(X_train, y_train)\nprint(f'Best params found for XGBoost are: {grid.best_params_}')\nprint(f'Best accuracy obtained by the best params: {grid.best_score_}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = grid.best_estimator_.predict(X_test)\nprint(accuracy_score(y_test, preds))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix, roc_curve, auc\n# Confusion matrix\nconfusion_matrix(y_test, preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(grid.best_estimator_, X_test, y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Out of 61 samples, the XGBoost misscassified 6","metadata":{}},{"cell_type":"markdown","source":"## AUC evaluation of XGBoost","metadata":{}},{"cell_type":"code","source":"probs = grid.best_estimator_.predict_proba(X_test)\npred = probs[:,1]\nfpr, tpr, threshold = roc_curve(y_test, pred)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(12,8))\nplt.title('ROC')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0,1], [0,1], 'r--')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}