{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Round 2 risk factors - automated article summary tables\n\n![](https://sportslogohistory.com/wp-content/uploads/2018/09/georgia_tech_yellow_jackets_1991-pres-1.png)\n\n**GOAL:** Create automated article summary tables that address risk factor studies related to COVID-19\n\n**SCENARIO:** When a new virus is discovered and causes a pandemic, it is important for scientists to get specific information coming from all scientific literature that may help them combat the pandemic.  One of the primary areas that scientists need to understand is risk factors from comorbidities like heart disease, diabetes etc. and whether a person suffering from one or more of these conditions is at a higher risk of a severe case or worse, becoming a fatality.\n\n**PROBLEM:** The challenege, however, is that the number of scientific papers created is large and the papers are published very rapidly, making it nearly impossible for scientists to digest and understand important data in this sea of data.  When the COVID-19 Challenge started in March 2020, there were about 50,000 documents in the competition corpus.  As of this writing (June 12, 2020), there are about 140,000 documents in the corpus, almost triple in a three month period. Just keeping up with the volume of papers released is a daunting task, not to mention the near impossible task of searching through them trying to isolate what comorbidities are risk factors that present an increased risk of a severe case or fatality and recording the data in tables for review.\n\n**SOLUTION:** Create an unsupervised scientific literature understanding system that can analyze a very large corpus - made up of tens or hundreds of thousands of scientific papers and return specific text excerpts containing statistical information to identify comorbiditiy risk factors, allowing a single researcher or small team to extract risk factor data in minutes not days or weeks.\n\n\n**PROCESS:**\n- Load the CORD19 Corpus into a dataframe\n- Keep only those studies related to COVID-19 @ 35,000\n- For each risk factor focus the dataframe for that topic\n- Search each row of the dataframe for text that possibly contains desired ratios\n- Locate excerpts in the text which possibly contain desired ratios\n- If confirmed a ratio is located, extract the ratio, low and high bounds, p value and determine if statistically significant, adjusted and if it pertins to severe or fatal\n- Add the study to the DF with the proper columns filled based on above\n- Extract the study type and sample size (use Bert for sample size)\n- Save the DF to CSV file in the proper format","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport functools\nimport re\n### BERT QA\nimport torch\n!pip install -q transformers --upgrade\nfrom transformers import *\nmodelqa = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nprint ('packages loaded')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# load the full text articles from the CORD19 dataset but keep only those documents related to COVID-19","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# keep only documents with covid -cov-2 and cov2\ndef search_focus(df):\n    dfa = df[df['abstract'].str.contains('covid')]\n    dfb = df[df['abstract'].str.contains('-cov-2')]\n    dfc = df[df['abstract'].str.contains('cov2')]\n    dfd = df[df['abstract'].str.contains('ncov')]\n    frames=[dfa,dfb,dfc,dfd]\n    df = pd.concat(frames)\n    df=df.drop_duplicates(subset='title', keep=\"first\")\n    return df\n\n# load the meta data from the CSV file\n#usecols=['title','journal','abstract','authors','doi','publish_time','sha','full_text_file']\ndf=pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv')\nprint ('ALL CORD19 articles',df.shape)\n#fill na fields\ndf=df.fillna('no data provided')\n#drop duplicate titles\ndf = df.drop_duplicates(subset='title', keep=\"first\")\n#keep only 2020 dated papers\ndf=df[df['publish_time'].str.contains('2020')]\n# convert abstracts to lowercase\ndf[\"abstract\"] = df[\"abstract\"].str.lower()+df[\"title\"].str.lower()\n#show 5 lines of the new dataframe\ndf=search_focus(df)\nprint ('Keep only COVID-19 related articles',df.shape)\n\nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nimport math\n\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_tables(ref_entries):\n    extract=''\n    for x in ref_entries.values():\n        if 'html' in x:\n            start = '<html>'\n            end = '</html>'\n            x=str(x).lower()\n            dat=(x.split(start))[1].split(end)[0]\n            extract=extract+' '+dat\n    \n    return extract\n\ndf['tables']='N/A'\nfor index, row in df.iterrows():\n    #print (row['pdf_json_files'])\n    if 'no data provided' not in row['pdf_json_files'] and os.path.exists('/kaggle/input/CORD-19-research-challenge/'+row['pdf_json_files'])==True:\n        with open('/kaggle/input/CORD-19-research-challenge/'+row['pdf_json_files']) as json_file:\n            #print ('in loop')\n            data = json.load(json_file)\n            body=format_body(data['body_text'])\n            #ref_entries=format_tables(data['ref_entries'])\n            #print (body)\n            body=body.replace(\"\\n\", \" \")\n            text=row['abstract']+' '+body.lower()\n            df.loc[index, 'abstract'] = text\n            #df.loc[index, 'tables'] = ref_entries\n\ndf=df.drop(['pdf_json_files'], axis=1)\ndf=df.drop(['sha'], axis=1)\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.core.display import display, HTML\nimport string\n\ndef extract_ratios(text,word):\n    extract=''\n    if word in text:\n        res = [i.start() for i in re.finditer(word, text)]\n    for result in res:\n        extracted=text[result:result+75]\n        extracted2=text[result-200:result+75]\n        level=''\n        if 'sever' in extracted2:\n            level=level+' severe'\n        if 'fatal' in extracted2:\n            level=level+' fatal'\n        if 'death' in extracted2:\n            level=level+' fatal'\n        if 'mortality' in extracted2:\n            level=level+' fatal'\n        if 'hospital' in extracted2:\n            level=level+' severe'\n        if 'intensive' in extracted2:\n            level=level+' severe'\n        #print (extracted)\n        #if '95' in extracted or 'odds ratio' in extracted or 'p>' in extracted or '=' in extracted or 'p<' in extracted or '])' in extracted or '(rr' in extracted:\n        if 'odds ratio' in extracted or '])' in extracted or '(rr' in extracted or '(ar' in extracted or '(hr' in extracted or '(or' in extracted or '(aor' in extracted or '(ahr' in extracted:\n            if '95%' in extracted:\n                extract=extract+' '+extracted+' '+level\n    #print (extract)\n    return extract\n\ndef get_ratio(text):\n    char1 = '('\n    char2 = '95%'\n    ratio=text[text.find(char1)+1 : text.find(char2)]\n    ratio=ratio.replace('â','')\n    return ratio\n\n# get the upper and lower bounds from the extracted data\ndef get_bounds(text):\n    raw=''\n    char1 = 'ci'\n    char2 = ')'\n    data=text[text.find(char1)+1 : text.find(char2)]\n    \n    if '-' in data:\n        raw=data.split('-')\n        low=raw[0][-5:]\n        hi=raw[1][:5]\n    \n    if 'to' in data and raw=='':\n        raw=data.split('to')\n        low=raw[0][-5:]\n        hi=raw[1][:5]\n        \n    if ',' in data and raw=='':\n        raw=data.split(',')\n        low=raw[0][-5:]\n        hi=raw[1][:5]\n    \n    if raw=='':\n        return '-','-'\n    low=low.replace('·','.')\n    low = re.sub(\"[^0-9.]+\", \"\", low)\n        \n    return low,hi\n\n# get the p value fomr the extracted text\ndef get_pvalue(text):\n    raw=''\n    pvalue=''\n    char1 = 'ci'\n    char2 = ')'\n    data=text[text.find(char1)+1 : text.find(char2)]\n    \n    if 'p=' in data:\n        raw=data.split('p=')\n        pvalue='p='+raw[1][:7]\n        \n    if 'p =' in data:\n        raw=data.split('p =')\n        pvalue='p='+raw[1][:7]\n    \n    if 'p>' in data and raw=='':\n        raw=data.split('p>')\n        pvalue='p>'+raw[1][:7]\n        \n    if 'p<' in data and raw=='':\n        raw=data.split('p<')\n        pvalue='p<'+raw[1][:7]\n    \n    if pvalue=='':\n        return '-'\n    pvalue=pvalue.replace('â','')\n    return pvalue\n\n# extract study design\ndef extract_design(text):\n    words=['retrospective','prospective cohort','retrospective cohort', 'systematic review',' meta ',' search ','case control','case series,','time series','cross-sectional','observational cohort', 'retrospective clinical','virological analysis','prevalence study','literature','two-center']\n    study_types=['retrospective','prospective cohort','retrospective cohort','systematic review','meta-analysis','literature search','case control','case series','time series analysis','cross sectional','observational cohort study', 'retrospective clinical studies','virological analysis','prevalence study','literature search','two-center']\n    extract=''\n    res=''\n    for word in words:\n        if word in text:\n            res = [i.start() for i in re.finditer(word, text)]\n        for result in res:\n            extracted=text[result-30:result+30]\n            extract=extract+' '+extracted\n    i=0\n    study=''\n    for word in words:\n        if word in extract:\n            study=study_types[i]\n        #print (extract)\n        i=i+1\n    return study\n\n# BERT pretrained question answering module\ndef answer_question(question,text,model,tokenizer):\n    input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\n    input_ids = tokenizer.encode(input_text)\n    token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    #print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n    answer=(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n    # show qeustion and text\n    #tokenizer.decode(input_ids)\n    answer=answer.replace(' ##','')\n    #print (answer)\n    return answer\n\ndef process_text(df1,focus):\n    significant=''\n    #df_results = pd.DataFrame(columns=['date','study','link','extracted','ratio','lower bound','upper bound','significant','p-value'])\n    df_results = pd.DataFrame(columns=['Date', 'Study', 'Study Link', 'Journal', 'Study Type', 'Severity of Disease', 'Severity lower bound', 'Severity upper bound', 'Severity p-value', 'Severe significance', 'Severe adjusted', 'Hand-calculated Severe', 'Fatality', 'Fatality lower bound', 'Fatality upper bound', 'Fatality p-value', 'Fatality significance', 'Fatality adjusted', 'Hand-calculated Fatality', 'Multivariate adjustment', 'Sample size', 'Study population', 'Critical Only', 'Discharged vs. Death', 'Added on', 'DOI', 'CORD_UID'])\n    for index, row in df1.iterrows():\n        study_type=''\n        study_type=extract_design(row['abstract'])\n        extracted=extract_ratios(row['abstract'],focus)\n        if extracted!='':\n            ratio=get_ratio(extracted)\n            lower_bound,upper_bound=get_bounds(extracted)\n            if lower_bound!='-' and lower_bound!='':\n                if float(lower_bound)>1:\n                    significant='yes'\n                else:\n                    significant='no'\n            else:\n                significant='-'\n            pvalue=get_pvalue(extracted)\n            \n            if 'aor' in extracted or 'arr' in extracted or 'ahr' in extracted or 'arr' in extracted or 'adjusted' in extracted:\n                adjusted='yes'\n            else: adjusted='no'\n            \n            if 'fatal' in extracted and 'severe' not in extracted:\n                severe='-'\n                slb='-'\n                sub='-'\n                spv='-'\n                ss='-'\n                sa='-'\n                fatal=ratio\n                flb=lower_bound\n                fub=upper_bound\n                fpv=pvalue\n                fs=significant\n                fa=adjusted\n            else:\n                fatal='-'\n                flb='-'\n                fub='-'\n                fpv='-'\n                fs='-'\n                fa='-'\n                severe=ratio\n                slb=lower_bound\n                sub=upper_bound\n                spv=pvalue\n                ss=significant\n                sa=adjusted\n            \n            ### get sample size\n            sample_q='how many patients cases studies were included collected or enrolled'\n            sample=row['abstract'][0:1000]\n            sample_size=answer_question(sample_q,sample,modelqa,tokenizer)\n            if '[SEP]' in sample_size or '[CLS]' in sample_size:\n                sample_size='-'\n            sample_size=sample_size.replace(' , ',',')\n                \n            link=row['doi']\n            linka='https://doi.org/'+link\n            #to_append = [row['publish_time'],row['title'],linka,extracted,ratio,lower_bound,upper_bound,significant,pvalue]\n            to_append = [row['publish_time'], row['title'], linka, row['journal'], study_type, severe, slb, sub, spv, ss, sa, '-', fatal, flb, fub, fpv, fs, fa, '-', '-', sample_size, '-', '-', '-', '-', row['doi'], row['cord_uid']]\n            df_length = len(df_results)\n            df_results.loc[df_length] = to_append\n    return df_results\n\nfocuses=['hypertension','diabetes','male','gender','heart disease', 'copd','smok',' age ','cerebrovascular','cardiovascular disease','cancer','kidney disease','respiratory disease','drinking','obes','liver disease']\n\nfor focus in focuses:\n    df1 = df[df['abstract'].str.contains(focus)]\n    df_results=process_text(df1,focus)\n    df_results=df_results.sort_values(by=['Date'], ascending=False)\n    df_table_show=HTML(df_results.to_html(escape=False,index=False))\n    display(HTML('<h1> Risk Factor '+focus+'</h1>'))\n    display(df_table_show)\n    file=focus+'.csv'\n    df_results.to_csv(file,index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}