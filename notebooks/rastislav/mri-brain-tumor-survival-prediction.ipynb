{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup env","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport glob\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# neural imaging\nimport nilearn as nl\nimport nibabel as nib\nimport nilearn.plotting as nlplt\n!pip install git+https://github.com/miykael/gif_your_nifti # nifti to gif \nimport gif_your_nifti.core as gif2nif\n\n\n# ml libs\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import CSVLogger\nfrom keras.utils.np_utils import to_categorical   \nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n\n\n# Make numpy printouts easier to read.\nnp.set_printoptions(precision=3, suppress=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define constants**","metadata":{}},{"cell_type":"code","source":"# DEFINE seg-areas  \nSEGMENT_CLASSES = {\n    0 : 'NOT tumor',\n    1 : 'NECROTIC/CORE', \n    2 : 'EDEMA',\n    3 : 'ENHANCING' # original 4 -> converted into 3 later\n}\n\n# days start interval\nSURVIVAL_CATEGORIES= {\n    'SHORT' : 0 , # 0-300\n    'MEDIUM' : 300,  # 300-450\n    'LONG' : 450, # 450 and more\n}\n\n# there are 155 slices per volume\n# to start at 5 and use 145 slices means we will skip the first 5 and last 5 \nVOLUME_SLICES = 100 \nVOLUME_START_AT = 22 # first slice of volume that we will include\nIMG_SIZE=128\nTRAIN_DATASET_PATH='../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Split Dataset into train/test/validation**<br>\n0.65/0.20/0.15<br>","metadata":{}},{"cell_type":"code","source":"# lists of directories with studies\ntrain_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n# file BraTS20_Training_355 has ill formatted name for for seg.nii file\ntrain_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')\n\n\ndef pathListIntoIds(dirList):\n    x = []\n    for i in range(0,len(dirList)):\n        x.append(dirList[i][dirList[i].rfind('/')+1:])\n    return x\n\ntrain_and_test_ids = pathListIntoIds(train_and_val_directories); ","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check if the background of images contains only zero values**<br>\nVisualize zero values in image (black values ==> background) , converted into another color so its easier to see ","metadata":{}},{"cell_type":"code","source":"my_loc_slice=73\nimage_volume=nib.load(TRAIN_DATASET_PATH+'BraTS20_Training_001/BraTS20_Training_001_t1.nii').get_fdata()\nmy_img=image_volume[:,:,my_loc_slice]\nmy_converted_img = my_img.copy()\nmy_converted_img[my_converted_img == 0] = 666\n\n\nf, axarr = plt.subplots(1,2, figsize = (10, 5))\naxarr[0].imshow(my_img)\naxarr[1].imshow(my_converted_img)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Count number of pixels for each segment for each slice in volume","metadata":{}},{"cell_type":"code","source":"def maskSizeForSlice(path,i_slice):\n    totals = dict([(1, 0), (2, 0), (3, 0)])\n    image_volume=nib.load(path).get_fdata()\n    # flatten 3D image into 1D array and convert mask 4 to 2\n    arr=image_volume[:,:,i_slice].flatten()\n    arr[arr == 4] = 3\n\n    unique, counts = np.unique(arr, return_counts=True)\n    unique = unique.astype(int)\n    values_dict=dict(zip(unique, counts))\n    for k in range(1,4):\n        totals[k] += values_dict.get(k,0)\n    return totals","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_loc_slice=73\nmy_loc_class=1\nseg_sum=maskSizeForSlice(TRAIN_DATASET_PATH+'BraTS20_Training_001/BraTS20_Training_001_seg.nii',my_loc_slice)\n\n\nimage_volume=nib.load(TRAIN_DATASET_PATH+'BraTS20_Training_001/BraTS20_Training_001_seg.nii').get_fdata()\nimage_loc=image_volume[:,:,my_loc_slice]\nimage_loc[image_loc != my_loc_class] = 0\n\n# plot segment only for class 'my_loc_class'\nplt.imshow(image_loc)\n\nimage_loc=image_loc.flatten()\ncount = np.count_nonzero(image_loc == my_loc_class)\nprint(f'count class {my_loc_class}: {count}')\nprint(seg_sum)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Survival prediction","metadata":{}},{"cell_type":"markdown","source":"**Lets see what is the age distrubution in our dataset and their survival days**<br>\nskip not GTR values..","metadata":{}},{"cell_type":"code","source":"import csv\n\ncsv_path = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/survival_info.csv'\n\nage_dict = {}\ndays_dict = {}\n\n\nwith open(csv_path, mode='r') as csv_file:\n    csv_reader = csv.reader(csv_file,delimiter = ',')\n  #  row_count = sum(1 for row in csv_reader)\n #   print(f'total rows: {row_count} .')\n    at_line = 0\n    category_short = 0\n    category_medium = 0\n    category_long = 0\n    max_days = 0\n    for row in csv_reader:\n        if at_line == 0:\n            print(f'Column names are {\", \".join(row)}')\n            at_line += 1\n        else:\n            if (row[3] != \"GTR\"):\n                continue\n            print(row)\n            key = row[0]\n            age = row[1]\n            days = row[2]\n            if (not days.isnumeric()):\n                continue\n            age_dict[key] = float(age)\n            days_dict[key] = int(days)\n            max_days = max(max_days,int(days))\n            if int(days) < 250:\n                category_short += 1\n            elif (int(days) >= 250 and int(days) <= 450):\n                category_medium += 1\n            else:\n                category_long += 1\n            at_line+=1\n\n    print(f'Processed {at_line} lines.')\n    print(category_short,category_medium,category_long)\n    print(max_days)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nfrom itertools import cycle\n#age_dict, days_dict \n\n# round values in dictionary\nage_dict_rounded = {key : round(age_dict[key], 0) for key in age_dict}\n# survival days are very distinct values => move the values in ranges per 20\ndays_dict_rounded = {key : round(days_dict[key]/20)*20 for key in days_dict}\n\n# count same numbers => create statistics how many times is there person with same age\nage_dict_rounded_counted = Counter(age_dict_rounded.values())\ndays_dict_rounded_counted = Counter(days_dict_rounded.values())\n\ncycol = cycle('bgrcmk')\ncolors = list()\nfor i in range(len(age_dict_rounded_counted)):\n    colors.append(next(cycol))\n    \ncycol = cycle('bgrcmk')    \ncolorsDays = list()\nfor i in range(len(days_dict_rounded_counted)):\n    colorsDays.append(next(cycol))\n\nplt.figure(figsize=(16, 6), dpi=80)\nplt.xlabel('Number of people with (rounded) age')\nplt.ylabel('Age (rounded)')\nplt.title(\"(Rounded) age distrubution in dataset\")\nplt.bar(list(age_dict_rounded_counted.keys()), age_dict_rounded_counted.values(), color=colors)\nplt.show()\n\nplt.figure(figsize=(14, 6), dpi=80)\nplt.xlabel('Days survived')\nplt.ylabel('Number of people (rounded to 20)')\nplt.title(\"Survival days distribution\")\nplt.bar(list(days_dict_rounded_counted.keys()), days_dict_rounded_counted.values(),width=15, color=colorsDays)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Computing segment sizes\nFind number of pixels for each class in volume, no need to compute as ration to image size, since all images are of same size 240x240","metadata":{}},{"cell_type":"code","source":"# get number of pixels for each segment as dictionary\n# original images contain segment values (0,1,2,4) => 4 is our 3 ... :)\ndef getMaskSizesForVolume(image_volume):\n    totals = dict([(1, 0), (2, 0), (3, 0)])\n    for i in range(VOLUME_SLICES):\n        # flatten 2D image into 1D array and convert mask 4 to 2\n        arr=image_volume[:,:,i+VOLUME_START_AT].flatten()\n        arr[arr == 4] = 3\n        \n        unique, counts = np.unique(arr, return_counts=True)\n        unique = unique.astype(int)\n        values_dict=dict(zip(unique, counts))\n        for k in range(1,4):\n            totals[k] += values_dict.get(k,0)\n    return totals","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compute brain volume size** => ignore background","metadata":{}},{"cell_type":"code","source":"# returns count of non zero elements in whole 3D volume\ndef getBrainSizeForVolume(image_volume):\n    total = 0\n    for i in range(VOLUME_SLICES):\n        arr=image_volume[:,:,i+VOLUME_START_AT].flatten()\n        image_count=np.count_nonzero(arr)\n        total=total+image_count\n    return total\n\nexample_volume=nib.load(TRAIN_DATASET_PATH+'BraTS20_Training_001/BraTS20_Training_001_t1.nii').get_fdata()\n\nf, axarr = plt.subplots(1,2, figsize = (8, 4))\naxarr[0].imshow(example_volume[:,:,VOLUME_START_AT])\naxarr[1].imshow(example_volume[:,:,VOLUME_START_AT+30])\n\nprint(f'total count: {getBrainSizeForVolume(example_volume)}')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create only age: category data\n\n# id: age, categories\ndef getListAgeDays(id_list):\n    x_val = []\n    y_val = []\n    for i in id_list:\n        if (i not in age_dict):\n            continue\n        masks = getMaskSizesForVolume(nib.load(TRAIN_DATASET_PATH + f'BraTS20_Training_{i[-3:]}/BraTS20_Training_{i[-3:]}_seg.nii').get_fdata())\n        brain_vol = getBrainSizeForVolume(nib.load(TRAIN_DATASET_PATH + f'BraTS20_Training_{i[-3:]}/BraTS20_Training_{i[-3:]}_t1.nii').get_fdata())\n        masks[1] = masks[1]/brain_vol\n        masks[2] = masks[2]/brain_vol\n        masks[3] = masks[3]/brain_vol\n        merged=[age_dict[i],masks[1],masks[2],masks[3]] ## add segments\n        x_val.append(merged) \n        if (days_dict[i] < 250):\n            y_val.append([1,0,0])\n        elif (days_dict[i] >= 250 and days_dict[i] < 450):\n            y_val.append([0,1,0])\n        else:\n            y_val.append([0,0,1])\n            \n    return np.array(x_val), np.array(y_val)\n\nX_all, y_all = getListAgeDays(train_and_test_ids)\n\nprint(f'X_test: {X_all.shape}')\ndf = pd.DataFrame(np.concatenate((X_all, y_all), axis=1) , columns = [\"age\",f\"{SEGMENT_CLASSES[1]}\",f\"{SEGMENT_CLASSES[2]}\",f\"{SEGMENT_CLASSES[3]}\",\"short\",\"medium\",\"long\"])\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normalize the data**\nperforming min-max scaling into range [0, 1]","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()\nv = X_all\nv_scaled = scaler.fit_transform(v)\nX_all = v_scaled\n\ndf = pd.DataFrame(X_all, columns = [\"age normalised\",f\"{SEGMENT_CLASSES[1]}\",f\"{SEGMENT_CLASSES[2]}\",f\"{SEGMENT_CLASSES[3]}\"])\ndisplay(df)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**View data distributions**","metadata":{}},{"cell_type":"code","source":"sns.set()\n\ndf = pd.DataFrame(X_all, columns = [\"age\", SEGMENT_CLASSES[1],SEGMENT_CLASSES[2],SEGMENT_CLASSES[3]])\nsns.pairplot(df, diag_kind='kde')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train test split**","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_all,y_all,test_size = 0.2, random_state = 42, shuffle = True)\n\n\nprint(\"x_train shape:\",X_train.shape)\nprint(\"x_test shape:\", X_train.shape)\nprint(\"y_train shape:\",y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random forest ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\nrfc = RandomForestClassifier(n_estimators=3, random_state=0)\n\n# fit the model to the training set\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_test)\n\nprint('Model accuracy score with 3 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n\n\n\naccuracies = cross_val_score(rfc, X_train, y_train, cv=3)\n# rfc.fit(X_train,y_train)\n\nprint(\"Cross validation: Train Score:\",np.mean(accuracies))\nprint(\"Cross validation: Test Score:\",rfc.score(X_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualize the most important features**<br>\nWe can see that the most important feature is age","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(X_train, columns = [\"age\",f\"{SEGMENT_CLASSES[1]}\",f\"{SEGMENT_CLASSES[2]}\",f\"{SEGMENT_CLASSES[3]}\"])\n\nfeature_scores = pd.Series(rfc.feature_importances_, index=df.columns).sort_values(ascending=False)\nfeature_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x=feature_scores, y=feature_scores.index)\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion matrix**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n\nsns.set(font_scale=1.2) \nsns.heatmap(cm, annot=True, annot_kws={\"size\": 12}) \n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Gridsearch**<br>\nTo find best score, I will try different value of:\n1. n_estimators -> in range (1,100) with step 1\n2. criterion parameters -> gini and entropy","metadata":{}},{"cell_type":"code","source":"grid = {\n    'n_estimators':np.arange(1,100,1),\n    'criterion':['gini','entropy']\n    }\n\nrfc_ = RandomForestClassifier(random_state = 42)\nrf_grid = GridSearchCV(rfc_, grid, cv=5)\nrf_grid.fit(X_train,y_train)\n\nprint(\"Hyperparameters:\",rf_grid.best_params_)\nprint(\"Train Score:\", rf_grid.best_score_)\nprint(\"Test Score:\",rf_grid.score(X_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\n#convert one hot into multilabel\ny_train_multi=np.argmax(y_train, axis=1)\ny_test_multi =np.argmax(y_test, axis=1)\n\nsvc = SVC(random_state = 42, C=10, degree=3, gamma=1, kernel='poly')\nsvc.fit(X_train,y_train_multi)\naccuracies = cross_val_score(svc, X_train, y_train_multi)\n\n\ny_pred = svc.predict(X_test)\n\nprint('Model accuracy score : {0:0.4f}'. format(accuracy_score(y_test_multi, y_pred)))\n\n\nprint(\"Cross validation: Train Score:\",np.mean(accuracies))\nprint(\"Cross validation: Test Score:\",svc.score(X_test,y_test_multi))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert to one hot\ny_pred=y_pred.astype(int)\nn_values = np.max(y_pred) + 1\ny_pred_hot=np.eye(n_values)[y_pred]\n\ncm = confusion_matrix(y_test.argmax(axis=1), y_pred_hot.argmax(axis=1))\n\nsns.set(font_scale=1.2) \nsns.heatmap(cm, annot=True, annot_kws={\"size\": 12}) \n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test_multi, y_pred))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GridSearch**<br>\nTo find best score, I will try different value of:\n1. C -> Regularization parameter\n2. kernel -> Specifies the kernel type to be used in the algorithm\n3. degree -> Degree of the polynomial kernel function\n4. gamma -> Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.","metadata":{}},{"cell_type":"code","source":"grid = {\n    'C':[0.01,0.1,1,10,15,20],\n    'kernel' : [\"linear\",\"poly\",\"rbf\",\"sigmoid\"],\n    'degree' : [1,3,5,7],\n    'gamma' : [0.01,1]\n}\n\nsvm  = SVC();\nsvm_grid = GridSearchCV(svm, grid, cv = 5)\nsvm_grid.fit(X_train,y_train_multi)\nprint(\"Best Parameters:\",svm_grid.best_params_)\nprint(\"Train Score:\",svm_grid.best_score_)\nprint(\"Test Score:\",svm_grid.score(X_test,y_test_multi))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn  = KNeighborsClassifier(n_neighbors=38, p=2, weights='distance')\nknn.fit(X_train,y_train_multi)\naccuracies = cross_val_score(knn, X_train, y_train_multi)\n\ny_pred = knn.predict(X_test)\n\nprint('Model accuracy score : {0:0.4f}'. format(accuracy_score(y_test_multi, y_pred)))\n\n\nprint(\"Cross validation: Train Score:\",np.mean(accuracies))\nprint(\"Cross validation: Test Score:\",knn.score(X_test,y_test_multi))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert to one hot\ny_pred=y_pred.astype(int)\nn_values = np.max(y_pred) + 1\ny_pred_hot=np.eye(n_values)[y_pred]\n\ncm = confusion_matrix(y_test.argmax(axis=1), y_pred_hot.argmax(axis=1))\n\nsns.set(font_scale=1.2) \nsns.heatmap(cm, annot=True, annot_kws={\"size\": 12}) \n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test_multi, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GridSearch**<br>\nTo find best score, I will try different value of:\n1. n_neighbors -> Number of neighbors to use by default for kneighbors queries\n2. p -> Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance\n3. weights -> weight function used in prediction\n","metadata":{}},{"cell_type":"code","source":"grid = {\n    'n_neighbors':np.arange(1,75),\n    'p':np.arange(1,5),\n    'weights':['uniform','distance']\n    }\n\nknn = KNeighborsClassifier()\nknn_grid = GridSearchCV(knn,grid,cv=5)\nknn_grid.fit(X_train,y_train_multi)\n\nprint(\"Hyperparameters:\",knn_grid.best_params_)\nprint(\"Train Score:\",knn_grid.best_score_)\nprint(\"Test Score:\",knn_grid.score(X_test,y_test_multi))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MLP Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n\n\nclf = MLPClassifier(\n    hidden_layer_sizes=11,\n    max_iter=150,\n    alpha=1e-05,\n    solver='lbfgs',\n    verbose=10,\n    random_state=6,\n    tol=0.000000001\n)\n\nclf.fit(X_train, y_train_multi)\naccuracies = cross_val_score(clf, X_train, y_train_multi)\ny_pred = clf.predict(X_test)\n\nprint('Model accuracy score : {0:0.4f}'. format(accuracy_score(y_test_multi, y_pred)))\n\n\nprint(\"Cross validation: Train Score:\",np.mean(accuracies))\nprint(\"Cross validation: Test Score:\",knn.score(X_test,y_test_multi))","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert to one hot\ny_pred=y_pred.astype(int)\nn_values = np.max(y_pred) + 1\ny_pred_hot=np.eye(n_values)[y_pred]\n\ncm = confusion_matrix(y_test.argmax(axis=1), y_pred_hot.argmax(axis=1))\n\nsns.set(font_scale=1.2) \nsns.heatmap(cm, annot=True, annot_kws={\"size\": 12}) \n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GridSearch**<br>\nTo find best score, I will try different value of:\n1. solver -> ‘lbfgs’ is an optimizer in the family of quasi-Newton methods. (we will use 'lbfgs', because we have very small dataset)\n2. max_iter -> Maximum number of iterations.\n3. alpha -> L2 penalty (regularization term) parameter.\n4. hidden_layer_sizes -> The ith element represents the number of neurons in the ith hidden layer\n5. random state -> Determines random number generation for weights and bias initialization, \n                  -> train-test split if early stopping is used, and batch sampling when solver=’sgd’ or ‘adam","metadata":{}},{"cell_type":"code","source":"grid = {\n    'solver': ['lbfgs'],\n    'max_iter': [25,50,100,150,200,300,500,1000 ],\n    'alpha': 10.0 ** -np.arange(1, 10),\n    'hidden_layer_sizes':np.arange(10, 15),\n    'random_state':[0,1,2,3,4,5,6,7,8,9]\n}\nclf_grid = GridSearchCV(MLPClassifier(), grid, n_jobs=-1)\n\nclf_grid.fit(X_train, y_train_multi)\n\nprint(\"Hyperparameters:\",clf_grid.best_params_)\nprint(\"Train Score:\",clf_grid.best_score_)\nprint(\"Test Score:\",clf_grid.score(X_test, y_test_multi))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion<br>\n\nTODO :)\n","metadata":{}}]}