{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Unsupervised Learning Applications","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Customer Segmentation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Customer segmentation is simply a way of arranging your customers into smaller groups/segment according to certain characteristics/attribute. In oorder to target specific, relevant marketing messages at each group.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The Original data set is from the Kaggle 'Online Retail Data Set II' provided by UCL.\n\nThis Online Retail II data set contains all the transactions occurring for a UK-based and registered, non-store online retail between 01/12/2009 and 09/12/2011.The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers.\n\nThis Data has been transformed in previous Kernel (https://www.kaggle.com/odezi45/customer-segmentation-using-rfm-analysis), so I will just quickly run through it here.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nfrom datetime import *\nimport matplotlib\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nimport matplotlib as mpl\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns; sns.set()\nfrom sklearn.cluster import MeanShift , estimate_bandwidth\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Data and Perform all Pre-Processing Task","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In my previous kernel, I already process the data and conducted RFM Analysis. In this post, I will applying various clustering method from scikit-learn on the already processed data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv('/kaggle/input/online-retail-ii-uci/online_retail_II.csv')\n\n# convert InvoiceDate column to the right format\nsales['InvoiceDate'] = pd.to_datetime(sales['InvoiceDate'])\n\n# From the descriptive statistics, we can see some negative values in Quantity and Price, so Lets have a look\nnegprice = sales[sales['Price'] < 0]\nnegquantity = sales[sales['Quantity'] < 0]\n\n## take out negative price from the sales data\nsales = sales[sales['Price']>= 0]\n\n## we have also identify some descriptions that doesnt look like sales\nsales2 = sales[sales['Description'].isin(['?',\n'?????',\n'back charges',\n'bad quality',\n'Came as green?',\n'Came as green?',\n'cant find',\n'cant find',\n'check',\n'checked',\n'checked',\n'code mix up 72597',\n'code mix up 72597',\n'coding mix up',\n'crushed',\n'crushed',\n'damaged',\n'damaged/dirty',\n'damaged?',\n'damages',\n'damages etc',\n'damages, lost bits etc',\n'damages?',\n'damges',\n'Damp and rusty',\n'dirty',\n'dirty, torn, thrown away.',\n'display',\n'entry error',\n'faulty',\n'for show',\n'given away',\n'gone',\n'Gone',\n'incorrect credit',\n'lost',\n'lost in space',\n'lost?',\n'missing',\n'Missing',\n'missing (wrongly coded?)',\n'missing?',\n'missings',\n'reverse mistake',\n'Rusty ',\n'Rusty connections',\n'show',\n'show display',\n'smashed',\n'sold in wrong qnty',\n'This is a test product.',\n'used for show display',\n'wet',\n'wet & rotting',\n'wet and rotting',\n'wet cartons',\n'wet ctn',\n'wet damages',\n'Wet, rusty-thrown away',\n'wet/smashed/unsellable',\n'wrong code',\n'wrong ctn size',\n'Zebra invcing error'])]\n\n## so lets take those spurious sales out\nsales = sales[~sales.apply(tuple,1).isin(sales2.apply(tuple,1))]\n\n## About 20% of the dataset has missing customer ID and 0.4% of the dataset has no description\n# SO I willa ssume that the missing customer id are 9999 and the description is 'Unlnown'\n\nsales[['Customer ID']] =sales[['Customer ID']].fillna(99999)\nsales[['Description']] =sales[['Description']].fillna('Unknown')\n\n# lets also take out all negative quantity as, they are either returns or errors in the data.\nsales = sales[sales['Quantity'] > 0]\nsales = sales[sales['Customer ID'] != 99999]\n\n## Now Lets find the first and second time a customer ordered by aggregating the values\nsales_ = sales.groupby('Invoice').agg(\n    Customer =('Customer ID', 'first'),\n    InvoiceDate2=('InvoiceDate', 'min'))\nsales_.reset_index(inplace = True)\nsales_['daterank'] = sales_.groupby('Customer')['InvoiceDate2'].rank(method=\"first\", ascending=True)\n\n# find customers second purchase and name dataframe sales_\nsales_ = sales_[sales_['daterank']== 2]\nsales_.drop(['Invoice', 'daterank'], axis=1, inplace=True)\nsales_.columns = ['Customer ID', 'InvoiceDate2']\n\n# Lets Aggregate the data to find certain customer metrics \nsales['amount'] = sales['Price'] * sales['Quantity']\nsalesgroup = sales.groupby('Customer ID').agg(\n    Country=('Country', 'first'),\n    sum_price=('Price', 'sum'),\n    sum_quantity=('Quantity', 'sum'),\n    max_date=('InvoiceDate', 'max'),\n    min_date=('InvoiceDate', 'min'),\n    count_order=('Invoice', 'nunique'),\n    avgitemprice=('Price', 'mean'),\n    monetary =('amount', 'sum'),\n    count_product=('Invoice', 'count'))\n\nsalesgroup.reset_index(inplace = True)\n\n\n#Find the max date of this study\nmaxdate = sales['InvoiceDate'].max()\n\n#Calculate AOV. Item per basket\nsalesgroup['avgordervalue'] = salesgroup['monetary']/salesgroup['count_order']\nsalesgroup['itemsperbasket'] = salesgroup['sum_quantity']/salesgroup['count_order']\n\n# join the data with the dataframe containing customer id with 2nd visits\nsalesgroup = pd.merge(salesgroup, sales_ , how='left', on=['Customer ID'])\n\n# find difference between first purchase and 2nd purchase \nsalesgroup['daysreturn']  = salesgroup['InvoiceDate2']- salesgroup['min_date']\nsalesgroup['daysreturn'] = salesgroup['daysreturn']/np.timedelta64(1,'D')\nsalesgroup['daysmaxmin']  = salesgroup['max_date']- salesgroup['min_date']\nsalesgroup['daysmaxmin'] = (salesgroup['daysmaxmin']/np.timedelta64(1,'D'))+1\n\n#calculate Frequency and Recency\nsalesgroup['frequency'] = np.where(salesgroup['count_order'] >1,salesgroup['count_order']/salesgroup['daysmaxmin'],0)\nsalesgroup['recency']  = maxdate- salesgroup['max_date']\nsalesgroup['recency'] = salesgroup['recency']/np.timedelta64(1,'D')\n\n# Now we have the values for Recency, Frequency and Monetary parameters. Each customer will get a note between 1 and 4 for each parameter.\n#By Applying quantile method we group each quantile into 25% of the population. \n\n#so letsdefine the quantile and save it ina dictionary\nquintiles2 = salesgroup[['recency', 'frequency', 'monetary']].quantile([.2, .4, 0.6, .8]).to_dict()\n\ndef r_score2(y):\n    if y <= quintiles2['recency'][.2]:\n        return 5\n    elif y <= quintiles2['recency'][.4]:\n        return 4\n    elif y <= quintiles2['recency'][.6]:\n        return 3\n    elif y <= quintiles2['recency'][.8]:\n        return 2\n    else:\n        return 1\n    \ndef fm_score2(y, k):\n    if y <= quintiles2[k][.2]:\n        return 1\n    elif y <= quintiles2[k][.4]:\n        return 2\n    elif y <= quintiles2[k][.6]:\n        return 3\n    elif y <= quintiles2[k][.8]:\n        return 4\n    else:\n        return 5    \n\n#lets get the RFM values by calling the function above\n\nsalesgroup['R2'] = salesgroup['recency'].apply(lambda y: r_score2(y))\nsalesgroup['F2'] = salesgroup['frequency'].apply(lambda y: fm_score2(y, 'frequency'))\nsalesgroup['M2'] = salesgroup['monetary'].apply(lambda y: fm_score2(y, 'monetary'))\n\nsalesgroup['RFM Score2'] = salesgroup['R2'].map(str) + salesgroup['F2'].map(str) + salesgroup['M2'].map(str)\nsalesgroup['RFM Score2'] = salesgroup['RFM Score2'].astype(int)\n\n\ndef mapl(salesgroup, r_rule, fm_rule, label, colname='new_label'):\n    salesgroup.loc[(salesgroup['R2'].between(r_rule[0], r_rule[1]))\n            & (salesgroup['F2'].between(fm_rule[0], fm_rule[1])), colname] = label\n    return salesgroup\n\nsalesgroup['new_label'] = ''\n\nsalesgroup = mapl(salesgroup, (4,5), (4,5), 'Champions')\nsalesgroup = mapl(salesgroup, (2,5), (3,5), 'Loyal customers')\nsalesgroup = mapl(salesgroup, (3,5), (1,3), 'Potential loyalist')\nsalesgroup = mapl(salesgroup, (4,5), (0,1), 'New customers')\nsalesgroup = mapl(salesgroup, (3,4), (0,1), 'Promising')\nsalesgroup = mapl(salesgroup, (2,3), (2,3), 'Needing attention')\nsalesgroup = mapl(salesgroup, (2,3), (0,2), 'About to sleep')\nsalesgroup = mapl(salesgroup, (0,2), (2,5), 'At risk')\nsalesgroup = mapl(salesgroup, (0,1), (4,5), 'Cant loose them')\nsalesgroup = mapl(salesgroup, (1,2), (1,2), 'Hibernating')\nsalesgroup = mapl(salesgroup, (0,2), (0,2), 'Lost')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_cleansed = salesgroup[['Customer ID', 'Country', 'sum_price', 'sum_quantity', 'monetary',\n        'frequency', 'recency','R2', 'F2', 'M2','RFM Score2', 'new_label']]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have a preped dataset, lets start applying some clustering methodology","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot styling\nplt.rcParams['figure.figsize'] = (16, 9)\nmpl.style.use('ggplot') # for ggplot-like style\n# let us Visualize the existing data\nplot_frequency=sns.distplot(sales_cleansed['F2'], color=\"#e74c3c\")\nplot_monetary=sns.distplot(sales_cleansed['M2'], color=\"#3498db\")\nplt.xlabel('Customer Attributes')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare data frame with columns for clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_cl = sales_cleansed[[ 'monetary','frequency', 'recency']] #3 column attributes\nsales_cl2 = sales_cleansed[[ 'monetary','frequency']] #2 column attributes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaled data to standardize","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a scaler object\nscaler = StandardScaler()\n# Fit the inputs (calculate the mean and standard deviation feature-wise)\nscaler.fit(sales_cl)\nsales_scaled = scaler.transform(sales_cl)\n\nscaler.fit(sales_cl2)\nsales_scaled2 = scaler.transform(sales_cl2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-means model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"K-means works by grouping the points together in such a way that the distance between all the points and the midpoint of the cluster they belong to is minimized.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let visualise a 3-D of the monetary, frequency & recency of each customer\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Set up a figure twice as tall as it is wide\nfig = plt.figure(figsize=(18, 9))\nfig.suptitle('Relationship btw Customer Attributes')\n\n# First subplot\nd2 = fig.add_subplot(1, 2, 1)\nd2.scatter(sales_scaled2[:, 0], sales_scaled2[:, 1])\nd2.set_xlabel('monetary')\nd2.set_ylabel('frequency')\nd2.set_title('Customer in 2-D')\n\n# Second subplot\nd3 = fig.add_subplot(1, 2, 2, projection='3d')\nd3.scatter(sales_scaled[:, 0], sales_scaled[:, 1]) #, sales_scaled[:, 2]) #, sales_scaled[:, 2])\nd3.set_xlabel('monetary')\nd3.set_ylabel('frequency')\nd3.set_zlabel('recency')\nd3.set_title('Customer in 3-D')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initializing K-Means with 9 clusters\nkmeans = KMeans(n_clusters=9)\n# Fitting with inputs\nkmean3D = kmeans.fit(sales_scaled)\n# Predicting the clusters\nlabels3D = kmean3D.predict(sales_scaled)\n# Getting the cluster centers\nC3D = kmean3D.cluster_centers_\n##################################################################################################\nkmean2D = kmeans.fit(sales_scaled2)\n# Predicting the clusters\nlabels2D = kmean2D.predict(sales_scaled2)\n# Getting the cluster centers\nC2D = kmean2D.cluster_centers_\n\n\n# visualise the 6 cluster k_mean\nfig = plt.figure(figsize=(18, 9))\nfig.suptitle('3D and 2D Kmean Plot for 9 Clusters')\n\n# First subplot\nd2 = fig.add_subplot(1, 2, 1)\nd2.scatter(sales_scaled2[:, 0], sales_scaled2[:, 1], c=labels2D, s=10, cmap='Paired')\nd2.scatter(C2D[:, 0], C2D[:, 1], marker='*', c='red', s=30)\nd2.set_xlabel('monetary')\nd2.set_ylabel('frequency')\nd2.set_title('K-Mean Customer in 2-D')\n\n# Second subplot\nd3 = fig.add_subplot(1, 2, 2, projection='3d')\nd3.scatter(sales_scaled[:, 0], sales_scaled[:, 1], sales_scaled[:, 2], c=labels3D, s=10, cmap='Paired')\nd3.scatter(C3D[:, 0], C3D[:, 1], C3D[:, 2], marker='*', c='red', s=500)\nd3.set_xlabel('monetary')\nd3.set_ylabel('frequency')\nd3.set_zlabel('recency')\nd3.set_title('K-Mean Customer in 3-D')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parameter Tuning for K-Mean to find Optimum Clusters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# finding the best number of clusters\n# WCSS measures sum of distances of observations from their cluster centroids which is given by the below formula.\n# by plotting the Within Cluster Sum Of Squares (WCSS) against the the number of clusters (K Value) \n# then we can identify the optimal number of clusters value. \nplt.figure(figsize=(12, 8))\n\nwcss = []\nfor i in range(1, 15):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 412)\n    kmeans.fit(sales_scaled)\n    wcss.append(kmeans.inertia_)\nwcss2 = []\nfor i in range(1, 15):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 412)\n    kmeans.fit(sales_scaled2)\n    wcss2.append(kmeans.inertia_)\n    \n    \nfig, axs = plt.subplots(1, 2, figsize=(18, 9), sharey=True)\nfig.suptitle('The Elbow Method')\naxs[0].plot(range(1, 15), wcss)\naxs[0].set_title('3D')\naxs[0].set_xlabel('Number of clusters')\naxs[0].set_ylabel('WCSS')\n\naxs[1].plot(range(1, 15), wcss2)\naxs[1].set_title('2D')\naxs[1].set_xlabel('Number of clusters')\naxs[1].set_ylabel('WCSS')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Updating K-Means with optimum cluster derived through the elbow method","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting K-Means to the dataset using 4 clusters\nkmeans = KMeans(n_clusters = 4, init = 'k-means++', random_state = 42)\ny_kmeans = kmeans.fit_predict(sales_scaled)\n#beginning of  the cluster numbering with 1 instead of 0\ny_kmeans1=y_kmeans\nlabels3D=y_kmeans+1\n# New Dataframe called cluster\ncluster3D = pd.DataFrame(labels3D)\n# centroid \nC3D = kmeans.cluster_centers_\n# Adding cluster to the Dataset1\n#sales_cleansed['cluster'] = cluster\n#################################################################################################\n\n# Fitting K-Means to the dataset using 4 clusters\nkmeans2D = KMeans(n_clusters = 4, init = 'k-means++', random_state = 42)\ny_kmeans2D = kmeans2D.fit_predict(sales_scaled2)\n#beginning of  the cluster numbering with 1 instead of 0\ny_kmeans12D=y_kmeans2D\nlabels2D=y_kmeans2D+1\n# New Dataframe called cluster\ncluster2D = pd.DataFrame(labels2D)\n# centroid \nC2D = kmeans2D.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the output of 4 clusters in 3-Dimension\nfig = plt.figure(figsize=(18, 9))\nfig.suptitle('3D and 2D Kmean Plot for 4 Clusters')\n\n# First subplot\nd2 = fig.add_subplot(1, 2, 1)\nd2.scatter(sales_scaled2[:, 0], sales_scaled2[:, 1], c=labels2D, s=10, cmap='Paired')\nd2.scatter(C2D[:, 0], C2D[:, 1], marker='*', c='red', s=100)\nd2.set_xlabel('monetary')\nd2.set_ylabel('frequency')\nd2.set_title('K-Mean Customer in 2-D')\n\n# Second subplot\nd3 = fig.add_subplot(1, 2, 2, projection='3d')\nd3.scatter(sales_scaled[:, 0], sales_scaled[:, 1], sales_scaled[:, 2], c=labels3D, s=10, cmap='Paired')\nd3.scatter(C3D[:, 0], C3D[:, 1], C3D[:, 2], marker='*', c='red', s=100)\nd3.set_xlabel('monetary')\nd3.set_ylabel('frequency')\nd3.set_zlabel('recency')\nd3.set_title('K-Mean Customer in 3-D')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mean shift clustering algorithm\n\n## What is Meanshift?\n\nMeanshift is a clustering algorithm that assigns the datapoints to the clusters iteratively by shifting points towards the mode. The mode can be understood as the highest density of datapoints (in the region, in the context of the Meanshift).\nplease refer to http://www.chioka.in/meanshift-algorithm-for-the-rest-of-us-python/ for more info.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# #############################################################################\n# Compute clustering with MeanShift default parameter\n\n# The following bandwidth can be automatically detected using\nbandwidth = estimate_bandwidth(sales_scaled, quantile=0.2, n_samples=5870)\n\nmsc = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nmsc.fit(sales_scaled)\nlabels3D = msc.labels_\nC3D = msc.cluster_centers_\nlabels_unique = np.unique(labels3D)\nn_clusters_ = len(labels_unique)\n\nprint(\"number of estimated meanshift clusters 3D : %d\" % n_clusters_)\n\n# #############################################################################\n\n# The following bandwidth can be automatically detected using\nbandwidth = estimate_bandwidth(sales_scaled2, quantile=0.2, n_samples=5870)\n\nmsc = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nmsc.fit(sales_scaled2)\nlabels2D = msc.labels_\nC2D = msc.cluster_centers_\nlabels_unique2 = np.unique(labels2D)\nn_clusters_2 = len(labels_unique2)\n\nprint(\"number of estimated meanshift clusters 2D : %d\" % n_clusters_2)\n\n# #############################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the output \nfig = plt.figure(figsize=(18, 9))\nfig.suptitle('3D and 2D Meanshift Plots with lots of clusters')\n\n# First subplot\nd2 = fig.add_subplot(1, 2, 1)\nd2.scatter(sales_scaled2[:, 0], sales_scaled2[:, 1], c=labels2D, s=10, cmap='Paired')\nd2.scatter(C2D[:, 0], C2D[:, 1], marker='*', c='red', s=100)\nd2.set_xlabel('monetary')\nd2.set_ylabel('frequency')\nd2.set_title('Meanshift Customer in 2-D')\n\n# Second subplot\nd3 = fig.add_subplot(1, 2, 2, projection='3d')\nd3.scatter(sales_scaled[:, 0], sales_scaled[:, 1], sales_scaled[:, 2], c=labels3D, s=10, cmap='Paired')\nd3.scatter(C3D[:, 0], C3D[:, 1], C3D[:, 2], marker='*', c='red', s=100)\nd3.set_xlabel('monetary')\nd3.set_ylabel('frequency')\nd3.set_zlabel('recency')\nd3.set_title('Meanshift Customer in 3-D')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"43 clusters is too many clusters and impossible to interprete so lets restrict the number of cluster","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# #############################################################################\n# Compute clustering with MeanShift\n\n# The following bandwidth can be automatically detected using\nbandwidth = estimate_bandwidth(sales_scaled, quantile=0.9, n_samples=5870)\n\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(sales_scaled)\nlabels3D = ms.labels_\nC3D = ms.cluster_centers_\nlabels_unique = np.unique(labels3D)\nn_clusters_ = len(labels_unique)\n\nprint(\"number of estimated clusters 3D : %d\" % n_clusters_)\n\n# #############################################################################\n\nbandwidth = estimate_bandwidth(sales_scaled2, quantile=0.5, n_samples=5870)\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(sales_scaled2)\nlabels2D = ms.labels_\nC2D = ms.cluster_centers_\nlabels_unique2 = np.unique(labels2D)\nn_clusters_2 = len(labels_unique2)\n\nprint(\"number of estimated clusters 2D : %d\" % n_clusters_2)\n\n# #############################################################################\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the output \nfig = plt.figure(figsize=(18, 9))\nfig.suptitle('3D and 2D Meanshift Plots with fewer clusters')\n\n# First subplot\nd2 = fig.add_subplot(1, 2, 1)\nd2.scatter(sales_scaled2[:, 0], sales_scaled2[:, 1], c=labels2D, s=10, cmap='Paired')\nd2.scatter(C2D[:, 0], C2D[:, 1], marker='*', c='red', s=150)\nd2.set_xlabel('monetary')\nd2.set_ylabel('frequency')\nd2.set_title('Meanshift Customer in 2-D')\n\n# Second subplot\nd3 = fig.add_subplot(1, 2, 2, projection='3d')\nd3.scatter(sales_scaled[:, 0], sales_scaled[:, 1], sales_scaled[:, 2], c=labels3D, s=10, cmap='Paired')\nd3.scatter(C3D[:, 0], C3D[:, 1], C3D[:, 2], marker='*', c='red', s=150)\nd3.set_xlabel('monetary')\nd3.set_ylabel('frequency')\nd3.set_zlabel('recency')\nd3.set_title('Meanshift Customer in 3-D')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gaussian Mixture Models\n\nA Gaussian mixture model (GMM) attempts to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset. In the simplest case, GMMs can be used for finding clusters in the same manner as k-means:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the basic GMM model using the previous number of cluster which is 9\nfrom sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components=9).fit(sales_scaled)\nlabels3D = gmm.predict(sales_scaled)\n#######################################################################\n\ngmm = GaussianMixture(n_components=9).fit(sales_scaled2)\nlabels2D = gmm.predict(sales_scaled2)\n\n#######################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the output \nfig = plt.figure(figsize=(18, 9))\nfig.suptitle('3D and 2D Gaussian Mixture Plots with 9 clusters')\n\n# First subplot\nd2 = fig.add_subplot(1, 2, 1)\nd2.scatter(sales_scaled2[:, 0], sales_scaled2[:, 1], c=labels2D, s=10, cmap='Paired')\nd2.set_xlabel('monetary')\nd2.set_ylabel('frequency')\nd2.set_title('Gaussian Mixture Customer in 2-D')\n\n# Second subplot\nd3 = fig.add_subplot(1, 2, 2, projection='3d')\nd3.scatter(sales_scaled[:, 0], sales_scaled[:, 1], sales_scaled[:, 2], c=labels3D, s=10, cmap='Paired')\nd3.set_xlabel('monetary')\nd3.set_ylabel('frequency')\nd3.set_zlabel('recency')\nd3.set_title('Gaussian Mixture Customer in 3-D')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Determining the number of components using AIC and BIC\n\nWe can correct over-fitting by adjusting the model likelihoods using some analytic criterion such as the Akaike information criterion (AIC) or the Bayesian information criterion (BIC)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_components = np.arange(1, 50)\nmodels3D = [GaussianMixture(n, covariance_type='full', random_state=0).fit(sales_scaled)\n          for n in n_components]\nmodels2D = [GaussianMixture(n, covariance_type='full', random_state=0).fit(sales_scaled)\n          for n in n_components]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the output \nfig = plt.figure(figsize=(18, 9))\nfig.suptitle('3D and 2D Gaussian Mixture Plots with fewer clusters')\n\nd3 = fig.add_subplot(1, 2, 1)\nd3.plot(n_components, [m.bic(sales_scaled) for m in models3D], label='BIC')\nd3.plot(n_components, [m.aic(sales_scaled) for m in models3D], label='AIC')\nd3.legend(loc='best')\nd3.set_xlabel('n_components')\nd3.set_title('Gaussian Mixture Customer in 3-D');\n\nd2 = fig.add_subplot(1, 2, 2)\nd2.plot(n_components, [m.bic(sales_scaled) for m in models2D], label='BIC')\nd2.plot(n_components, [m.aic(sales_scaled) for m in models2D], label='AIC')\nd2.legend(loc='best')\nd2.set_xlabel('n_components')\nd2.set_title('Gaussian Mixture Customer in 2-D');\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimal number of clusters is the value that minimizes the AIC or BIC, depending on which approximation we wish to use. Both the AIC and BIC shows that the optimum value is between 40- 50 components which is way too much and difficult to interpret as seen in the Meanshift clustering model so I will probably take the previous deep in the line graph which is between 10 and 20 components.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets plot the 40 component GMM\ngmm = GaussianMixture(n_components=40).fit(sales_scaled)\nlabels3D = gmm.predict(sales_scaled)\n########################################################################################\n\n# lets plot the 40 component GMM\ngmm = GaussianMixture(n_components=40).fit(sales_scaled2)\nlabels2D = gmm.predict(sales_scaled2)\n########################################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the output \nfig = plt.figure(figsize=(18, 9))\nfig.suptitle('3D and 2D Gaussian Mixture Plots with 40 clusters')\n\n# First subplot\nd2 = fig.add_subplot(1, 2, 1)\nd2.scatter(sales_scaled2[:, 0], sales_scaled2[:, 1], c=labels2D, s=10, cmap='Paired')\nd2.set_xlabel('monetary')\nd2.set_ylabel('frequency')\nd2.set_title('Gaussian Mixture Customer in 2-D')\n\n# Second subplot\nd3 = fig.add_subplot(1, 2, 2, projection='3d')\nd3.scatter(sales_scaled[:, 0], sales_scaled[:, 1], sales_scaled[:, 2], c=labels3D, s=10, cmap='Paired')\nd3.set_xlabel('monetary')\nd3.set_ylabel('frequency')\nd3.set_zlabel('recency')\nd3.set_title('Gaussian Mixture Customer in 3-D')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DBSCAN Clustering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import DBSCAN\nfrom sklearn import metrics\n# Generate sample data\n\n# Compute DBSCAN\ndb = DBSCAN(eps=0.4, min_samples=3).fit(sales_scaled)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels3D = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels3D)) - (1 if -1 in labels3D else 0)\nn_noise_ = list(labels3D).count(-1)\n\nprint('3D Estimated number of clusters: %d' % n_clusters_)\nprint('3D Estimated number of noise points: %d' % n_noise_)\nprint(\"3D Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(sales_scaled, labels3D))\n############################################################################################\ndb = DBSCAN(eps=0.4, min_samples=3).fit(sales_scaled2)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels2D = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels2D)) - (1 if -1 in labels2D else 0)\nn_noise_ = list(labels2D).count(-1)\n\nprint('2D Estimated number of clusters: %d' % n_clusters_)\nprint('2D Estimated number of noise points: %d' % n_noise_)\nprint(\"2D Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(sales_scaled2, labels2D))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the output \nfig = plt.figure(figsize=(18, 9))\nfig.suptitle('3D and 2D DBSCAN Plots with 40 clusters')\n\n# First subplot\nd2 = fig.add_subplot(1, 2, 1)\nd2.scatter(sales_scaled2[:, 0], sales_scaled2[:, 1], c=labels2D, s=10, cmap='Paired')\nd2.set_xlabel('monetary')\nd2.set_ylabel('frequency')\n#d2.legend(loc='best')\nd2.set_title('DBSCAN Customer in 2-D')\n\n# Second subplot\nd3 = fig.add_subplot(1, 2, 2, projection='3d')\nd3.scatter(sales_scaled[:, 0], sales_scaled[:, 1], sales_scaled[:, 2], c=labels3D, s=10, cmap='Paired')\nd3.set_xlabel('monetary')\nd3.set_ylabel('frequency')\nd3.set_zlabel('recency')\n\nd3.set_title('DBSCAN Customer in 3-D')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From the previous RFM analysis(https://www.kaggle.com/odezi45/customer-segmentation-using-rfm-analysis), we can deduce that classifying customers into 9 clusters is more viable and easily explained, The various clustering method plots also show the somewhat distinct clusters.\nIt is now up to Marketting Managers, Customer Insight teams to determine what sort of communication or promotion strategy to employ in otther to convert customers from one segment to the other or maybe push more customer to a new segment preferably towards the top right corner of plots.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"for more information on the advantages and disadvantages of the clustering techniques please refer to https://www.analytixlabs.co.in/blog/types-of-clustering-algorithms/","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}