{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#importing basic libraries\nimport numpy as np\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf=pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf.info()\n#no missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#equal division of positive and negative sentiment\ndf['sentiment'].value_counts().plot(kind='pie',autopct='%.1f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TEXT CLEANING AND TRAINING STEP BY STEP:\n\n1)Removal of HTML contents like \"< br>\".\n\n2)Removal of punctutions, special characters like '\\'.\n\n3)Removal of stopwords like is, the which do not offer much insight.\n\n4)Stemming/Lemmatization to bring back multiple forms of same word to their common root like 'coming', 'comes' into 'come'.\n\n5)Vectorization - Encode the numeric values once you have cleaned it.\n\n6)Fit the data to the ML model.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#1. Removing all html tags\n\nfrom bs4 import BeautifulSoup\ndef html_remover(text):\n    soup=BeautifulSoup(text,'html.parser')\n    a=soup.get_text()\n    return a\ndf['review']=df['review'].apply(html_remover)\ndf['review'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#2. Removal of punctuations and special characters\nimport re\ndef sp_char_remover(review):\n    review = re.sub('\\[[^]]*\\]', ' ', review)\n    review = re.sub('[^a-zA-Z]', ' ', review)\n    return review\ndf['review']=df['review'].apply(sp_char_remover)\ndf['review'][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Converting To lower\ndef lower(text):\n    return text.lower()\ndf['review']=df['review'].apply(lower)\ndf['review'][2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#3. Removal of stopwords\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\ndef stopword_remover(text):\n    x=[]\n    text=text.split()    #splitting into individual words\n    for i in text:\n        if i not in stopwords.words('english'):\n            x.append(i)\n    return x\n\ndf['review']=df['review'].apply(stopword_remover)\ndf['review'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#4. Lemmatizing the stopwords and then joining it back\nfrom nltk.stem import WordNetLemmatizer\nlem=WordNetLemmatizer()\n\ndef temp(text):\n    text=\" \".join(text)\n    return text\n\ndef lemma_join(text):\n    text=[lem.lemmatize(word) for word  in text]\n    text=temp(text)\n    return text\n\ndf['review']=df['review'].apply(lemma_join)        \ndf['review'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Separation into training and testing\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test, train_data_label, test_data_label = train_test_split(df['review'], df['sentiment'], test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Changing Labels to 1 and 0 for the ease of understanding where 1 is positive review and 0 is negative review.\ntrain_data_label=(train_data_label.replace({'positive':1,'negative':0}))\ntest_data_label=(test_data_label.replace({'positive':1,'negative':0}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating cleaned corpus from the cleaned df['review'] dataset for the purpose of training\ncorpus_train = []\ncorpus_test  = []\n\nfor i in df_train.index:\n    temp=df_train[i]\n    corpus_train.append(temp)\n\nfor j in df_test.index:\n    temp1=df_test[j]\n    corpus_test.append(temp1)\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dummy corpus to perform Vectorization\ncorpus_train2=corpus_train\ncorpus_test2=corpus_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#5. Count Vectorization (Bag of words model)\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer()\ncv_train=cv.fit_transform(corpus_train2)\ncv_test=cv.transform(corpus_test2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#6. Using a Support vector classifier for training our model\nfrom sklearn.svm import LinearSVC\nlin_svc=LinearSVC(C=0.5,random_state=42,max_iter=10000)\nlin_svc.fit(cv_train,train_data_label)\n\ny_pred=lin_svc.predict(cv_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\n\nprint(classification_report(test_data_label,y_pred))\nprint(\"ACCURACY SCORE IS: \",accuracy_score(test_data_label,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**Conclusion**\n\nAfter Following the steps to pre-process the reviews and train our Classifier we find a 86% accuracy score for our model. I.e, given a new review with a 86% accuracy it can distingusih between Positive reviews and Negative reviews.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}