{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nIn this project I will be exploring machine learning models through an investigation relating to heart failures. The aim of this investigation is to use a machine learning model to determine the relationship between a persons gender and the different factors of a heart failure and the purpose of this investigation to evaluate a certain machine learning models effectiveness in making predictions.","metadata":{}},{"cell_type":"markdown","source":"#### Hypotheses:\n\nI predict that the best model to use for making predictions for my dataset will be a decision tree classifier as im predicting a boolean feature meaning it wouldnt work with a regressor or a random forest\n\nDue to me not having knowledge on the different factors of a heart failure, I dont know which features will have the strongest affect on my predictions although I predict that all the features will have an equal affect.\n\nI predict that my machine learning model will be accurate in making predictions as it has a wide range of features relating to the people health and I predict that a persons gender will be easily through the patterns in their health data\n\n","metadata":{}},{"cell_type":"markdown","source":"# Setting up\nThe code below contain all the libraries I need to import to set up of machine learning model. The key features of each library are shown below. ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualisation\nfrom sklearn.tree import DecisionTreeClassifier ,plot_tree # The decision tree classifier model and a tool for visualising it\nfrom sklearn.model_selection import train_test_split # To split our data into validation data and testing data\n\nprint(\"All flies under the input direcotry:\")\n\n# This code will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:48:53.77777Z","iopub.execute_input":"2021-06-29T06:48:53.778097Z","iopub.status.idle":"2021-06-29T06:48:53.788998Z","shell.execute_reply.started":"2021-06-29T06:48:53.778069Z","shell.execute_reply":"2021-06-29T06:48:53.78809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gather and explore the data\nNow that we have set up our code, it is time to import the data into our project. Our data files are available in the read-only \"../input/\" directory in the Pandas DataFrame.\n\nI chose to use this data because it had a versatile features that could be used to be predicted and make predictions, it had a large amount of features, was a mix between boolean and numerical data and due to personal interest. Due to the data being a mix between boolean and numerical, it is suitable for a decision tree classifier and I chose to predict the person's gender as this was suitable for our model becuase it is a boolean response and because it was the one of the only features that didn't direclty relate to the person's heart failuer. \n\nMost of the features that I will be using to predict the person's age are relating to the person's health, specifically things to do with their heart failure. The goal is to have the two genders having different patterns for each health feature so that the model has accurate predictions.","metadata":{}},{"cell_type":"code","source":"Predict_gender_heart_failure = '../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv'\n# Create a new Pandas DataFrame with our training data\nPredict_gender_heart_failure_data = pd.read_csv(Predict_gender_heart_failure)\n# Reads our data\n\nPredict_gender_heart_failure_data.describe(include='all')\n# Gives a description of the our data including things like average, count and min/max","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:49:11.703276Z","iopub.execute_input":"2021-06-29T06:49:11.703834Z","iopub.status.idle":"2021-06-29T06:49:11.758092Z","shell.execute_reply.started":"2021-06-29T06:49:11.7038Z","shell.execute_reply":"2021-06-29T06:49:11.757329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n\n* From this description of our data I can see that we have data for 299 different people and that we have no missing values for each feauture\n* I can see that most features have a small standard deviation except 'creatinie_phosphokinase' and 'platelets'\n* We can see that we have a large range of ages for the people in our data and that the average age is 60\n\n# Prepare the data\nBefore I start training our model we need to prepare our data. Since my data has no missing values or outliers, I dont have to add any code before working wiht the data.\n\nFirst I made a list of all the feature we want to include in making our predicitions\n","metadata":{}},{"cell_type":"code","source":"selected_columns = ['diabetes', 'high_blood_pressure', 'sex', 'anaemia', 'serum_creatinine', 'serum_sodium', 'creatinine_phosphokinase', 'platelets', 'ejection_fraction']\n\n# Creates th new data set containing only the features we want\nprepared_data = Predict_gender_heart_failure_data[selected_columns]\n\n# Drop rows (axis=0) that contain missing values\nprepared_data = prepared_data.dropna(axis=0)\n\n# Describes the data again so we can make sure the 'count' values are all the same and that there are no other problems\nprepared_data.describe(include='all')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:49:55.881736Z","iopub.execute_input":"2021-06-29T06:49:55.88224Z","iopub.status.idle":"2021-06-29T06:49:55.928981Z","shell.execute_reply.started":"2021-06-29T06:49:55.882198Z","shell.execute_reply":"2021-06-29T06:49:55.928077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Seperating Data into Target and Testing:\n\nThen after this I seperated the target feature (sex) from the rest of the features. I did this by assigning the sex feature to the value 'y' and the value 'X' to the rest of the features. This was done because we need there two values seperate so that we can train our models\n\n\n","metadata":{}},{"cell_type":"code","source":"# Separate out the prediction target\ny = prepared_data.sex\n\n# Drop the target column (axis=1) from the original dataframe and use the rest as our feature data\nX = prepared_data.drop('sex', axis=1)\n\n# Take a look at a small portion of the data again to make sure the 'y' value is gone\nX.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:42:42.29875Z","iopub.execute_input":"2021-06-29T06:42:42.299258Z","iopub.status.idle":"2021-06-29T06:42:42.314617Z","shell.execute_reply.started":"2021-06-29T06:42:42.299224Z","shell.execute_reply":"2021-06-29T06:42:42.313604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split Data Into Training and Validation Data:\n\nThe last step in preparing our data is spliting the target and testing data into two halves; training and validation. We do this because to evaluate the accuaracy of our model we need to have another set of data that the model has not seen before. In the process, both the data from the 'y' and 'X' value is spilt into two halves in a random order and assigned to the values: train_X, val_X, train_y and val_y. the train_X and train_y are used for training the model and after the training the val_X is inputted into the model and then the val_y and the predictions are compared to see if the model is accurate.\n\nFor our investigation, our 300 rows of data has been split up into 224 rows of training data and 75 rows of validation data because I wanted a large amount of data for training to make sure I wasnt underfitting my model but I also made sure to have a decent amount of validation data so that I can make a good comparison between the validation data and the models predictions and to make sure i wasnt over fitting my model. ","metadata":{}},{"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:42:47.530295Z","iopub.execute_input":"2021-06-29T06:42:47.530659Z","iopub.status.idle":"2021-06-29T06:42:47.537482Z","shell.execute_reply.started":"2021-06-29T06:42:47.53063Z","shell.execute_reply":"2021-06-29T06:42:47.536103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, due to all our categorical data already being in the binary form (a form that a decision tree classifier can work with as it uses numbers), we don't have to encode any categories.","metadata":{}},{"cell_type":"markdown","source":"# Choosing and Training the Model\n\nThe model I have chosen too investiate for his project is the decision tree classifier. This is a type of decision tree used for classification tasks that have a binary output. In our case we have made the \"0\" value equal to the category \"male\" and the \"1\" value equal to the category \"women\". These trees are built in a process know as binary recursive partitioning which is the iterative process of splitting data on each branch to form groups of similar data. \n\nDecision trees consist of different types of nodes: \n* The root node - the first node on the decision tree\n* The branch node - the other nodes on thar tree the spilt the data\n* The leaf noeds - the nodes that predicts the outcome\n\nTo figure out which categories are used on each nodes, where different nodes are and the optimal splits, the decision trees uses gini impurity. These calculations are all done by the decision tree classifier SOFTWARE that we imported.\n\nThe steps needed to input and train my decision tree classifier will be in the comments of the code below ","metadata":{}},{"cell_type":"code","source":"# For our initial model I will limit the max_depts to 3 nodes so it is simply displayed for me make observations\nsex_predictor = DecisionTreeClassifier(max_depth=3)\n\n# This line of code fits the model to our data\n# I am only using the training versions of 'X' and 'y' so that the models predictions can be compare to the validation data later one\nsex_predictor.fit(train_X, train_y)\n\n# Once I have fitted my data I can display my decision tree using this code\nplt.figure(figsize = (20,10))\nplot_tree(sex_predictor,\n          feature_names=train_X.columns,\n          class_names=['male', 'female'], # <-- The models two outputs \n          filled=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:50:37.814912Z","iopub.execute_input":"2021-06-29T06:50:37.815278Z","iopub.status.idle":"2021-06-29T06:50:38.656757Z","shell.execute_reply.started":"2021-06-29T06:50:37.815249Z","shell.execute_reply":"2021-06-29T06:50:38.655788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations:\n\n- I can see the different gini impurity values for each node\n- I can see that there are more 'male' leaf nodes then 'female'\n- I can see the different sample sizes of each node\n- I can see whihc feature they have chosen to use at each node","metadata":{}},{"cell_type":"markdown","source":"# Evaluation and Hyperparameter Tuning\n\nNow that I have trained my model and displayed it, I can evaluated the models accuracy by comparing its predictions with the validation data","metadata":{}},{"cell_type":"code","source":"# Make prediction with the model using the validation X data\npred = sex_predictor.predict(val_X)\n\nd = []\nds = pd.DataFrame(data=d)\n\n# Added the val_y and the val_X predictions to the data to compared the results\nds['Males'] = val_y\nds['predicted'] = pred\n\nds.head(6) ","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:28.804286Z","iopub.execute_input":"2021-06-29T06:44:28.804647Z","iopub.status.idle":"2021-06-29T06:44:28.820602Z","shell.execute_reply.started":"2021-06-29T06:44:28.804618Z","shell.execute_reply":"2021-06-29T06:44:28.819551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nI can see that the results for this decision tree when using val_X don't match up with the val_y. This shows us that the model with a max depth of 3 is fairly innaccurate. This because a model with a max depth of 3 isnt detailed enough meaning it is underfitting the data.\n\nA way to make a model more accurate after it has been trained is by modifying its hyper parameters. Since I used all the suitable features I could, the only other way for me to change my models hyper parameters would be to change its tree depth. Changing a models tree depth changes its accuracy has it changes how detailed the model is meaning it changes how well the data can fit the model.\n\nTo evaluate my models accuracy I will use this code which finds out the amount of errors my model has. From here we can begin tuning our model with other tree depths its optimal depth. ","metadata":{}},{"cell_type":"code","source":"males_list = ds['Males'].values.tolist() # Turns the males column results into a list\nprint(males_list)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:51:42.283948Z","iopub.execute_input":"2021-06-29T06:51:42.284362Z","iopub.status.idle":"2021-06-29T06:51:42.289468Z","shell.execute_reply.started":"2021-06-29T06:51:42.28432Z","shell.execute_reply":"2021-06-29T06:51:42.288344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for loop for calculating the amount of errors\n\nerror_count = 0\n\nfor a in males_list: \n    if males_list[a] == pred[a]: \n        error_count = error_count + 1\n    else: \n        error_count = error_count + 0\n\nprint(\"The amount of errors in the predictions:\", error_count)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:51:44.025077Z","iopub.execute_input":"2021-06-29T06:51:44.025463Z","iopub.status.idle":"2021-06-29T06:51:44.032233Z","shell.execute_reply.started":"2021-06-29T06:51:44.025427Z","shell.execute_reply":"2021-06-29T06:51:44.031146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 3 depth model was proven to be inaccurate as 26 of its 75 predictions were wrong. I will now experiment with other tree depths to try obtains a better accuracy","metadata":{}},{"cell_type":"code","source":"# Making the other decision trees\n\nsex_predictor_2 = DecisionTreeClassifier(max_depth=5)\nsex_predictor_3 = DecisionTreeClassifier(max_depth=10)\nsex_predictor_4 = DecisionTreeClassifier(max_depth=25)\nsex_predictor_5 = DecisionTreeClassifier(max_depth=50)\nsex_predictor_6 = DecisionTreeClassifier(max_depth=100)\n\n# Training the other decision trees\n\nsex_predictor_2.fit(train_X, train_y)\nsex_predictor_3.fit(train_X, train_y)\nsex_predictor_4.fit(train_X, train_y)\nsex_predictor_5.fit(train_X, train_y)\nsex_predictor_6.fit(train_X, train_y)\n\n# Making predictions using the other decision trees\n\npred2 = sex_predictor_2.predict(val_X)\npred3 = sex_predictor_3.predict(val_X)\npred4 = sex_predictor_4.predict(val_X)\npred5 = sex_predictor_5.predict(val_X)\npred6 = sex_predictor_6.predict(val_X)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:51:47.420923Z","iopub.execute_input":"2021-06-29T06:51:47.421347Z","iopub.status.idle":"2021-06-29T06:51:47.450688Z","shell.execute_reply.started":"2021-06-29T06:51:47.42131Z","shell.execute_reply":"2021-06-29T06:51:47.449992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Caluclating the amount of errors for all the new predictions\n\nerror_count2 = 0\n\nfor a in pred2: \n    if males_list[a] == pred2[a]: \n        error_count2 = error_count2 + 1\n    else: \n        error_count2 = error_count2 + 0\n\nprint(\"The amount of errors in the predictions of a tree with a max depth of 5:\", error_count2)\n\nerror_count3 = 0\n\nfor a in pred3: \n    if males_list[a] == pred3[a]: \n        error_count3 = error_count3 + 1\n    else: \n        error_count3 = error_count3 + 0\n\nprint(\"The amount of errors in the predictions of a tree with a max depth of 10:\", error_count3)\n\nerror_count4 = 0\n\nfor a in pred4: \n    if males_list[a] == pred4[a]: \n        error_count4 = error_count4 + 1\n    else: \n        error_count4 = error_count4 + 0\n\nprint(\"The amount of errors in the predictions of a tree with a max depth of 25:\", error_count4)\n\nerror_count5 = 0\n\nfor a in pred5: \n    if males_list[a] == pred5[a]: \n        error_count5 = error_count5 + 1\n    else: \n        error_count5 = error_count5 + 0\n\nprint(\"The amount of errors in the predictions of a tree with a max depth of 50:\", error_count5)\n\nerror_count6 = 0\n\nfor a in pred6: \n    if males_list[a] == pred6[a]: \n        error_count6 = error_count6 + 1\n    else: \n        error_count6 = error_count6 + 0\n\nprint(\"The amount of errors in the predictions of a tree with a max depth of 100:\", error_count6)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:52:12.247534Z","iopub.execute_input":"2021-06-29T06:52:12.24789Z","iopub.status.idle":"2021-06-29T06:52:12.260592Z","shell.execute_reply.started":"2021-06-29T06:52:12.247859Z","shell.execute_reply":"2021-06-29T06:52:12.25953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The outputs above clearly show that 5 is the optimal tree depth for the model as it had the lowest error count. This means anything above 5 would have been overfitting the model","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nIn conclusion, the decision tree classifier was proven to be a fairly accurate model after we tuned it. To improve my predictions, I could have tried experimenting with other hyperparameters or I could have tried using another suitable model. Also, it was shown that the gender of a person has a substantial affect on their heart failure factors as the accuracy of my predictions shows that there was a clear relationship between the factors.","metadata":{}}]}