{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Problem Definition\n\nThe Pima Indians Diabetes Dataset involves predicting the onset of diabetes within 5 years in Pima Indians given medical details.\n\nIt is a binary (2-class) classification problem. The number of observations for each class is not balanced. There are 768 observations with 8 input variables and 1 output variable. Missing values are believed to be encoded with zero values. The variable names are as follows:\n- Number of times pregnant.\n- Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n- Diastolic blood pressure (mm Hg).\n- Triceps skinfold thickness (mm).\n- 2-Hour serum insulin (mu U/ml).\n- Body mass index (weight in kg/(height in m)^2).\n- Diabetes pedigree function.\n- Age (years).\n- Class variable (0 or 1).\n\n<u>Goal</u>: Predict the onset of diabetes within 5 years in Pima Indians given medical details.\n\nWe are going to cover the following steps:\n2. Load our data\n3. Understand our data with descriptive statistics\n4. Understand our data with visualization\n5. Prepare our data\n6. Feature Selection\n7. Evaluate the Performance of Algorithms with Resampling\n8. Algorithm Performance Metrics\n9. Spot-Check Algorithms\n10. Compare Algorithms\n11. Automate Workflows with Pipelines\n12. Improve Performance with Ensembles\n13. Improve Performance with Algorithm Tuning\n14. References and Credits\n\n# 2. Load data\n\nLet's start off by loading the libraries required for this project.\n\n## 2.1 Import libraries\n\nFirst, let's import all of the modules, functions and objects we are going to use in this project.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load libraries\nimport seaborn as sns\nimport numpy\nfrom numpy import arange\nfrom numpy import set_printoptions\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import Binarizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import RFE\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Load data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load dataset\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfilename = '/kaggle/input/pima-indians-diabetes-database/diabetes.csv'\ndata = read_csv(filename)\n\n# df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\ndata.rename(columns={'Pregnancies': 'preg', 'Glucose': 'plas', 'BloodPressure':'pres', 'SkinThickness':'skin', 'Insulin':'test', 'BMI':'mass', 'DiabetesPedigreeFunction':'pedi', 'Age':'age', 'Outcome':'class'}, \n            inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Understand our data with descriptive statistics\n\nWe are going to cover the following steps:\n1. Take a peek at our raw data.\n2. Review the dimensions of our dataset.\n3. Review the data types of attributes in our data.\n4. Summarize the distribution of instances across classes in our dataset.\n5. Summarize our data using descriptive statistics.\n6. Understand the relationships in our data using correlations.\n7. Review the skew of the distributions of each attribute.\n\n## 3.1 Peek at our data\n\nLet's review the first five rows of the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"peek = data.head(5)\nprint(peek)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Dimensions of our data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shape = data.shape\nprint(shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that the dataset has 768 rows and 9 columns.\n\n## 3.3 Data Type For Each Attribute","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"types = data.dtypes\nprint(types)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that most of the attributes are integers and that mass and pedi are floating point types.\n\n## 3.4 Descriptive Statistics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statistical Summary\nset_option('display.width', 100)\nset_option('precision', 3)\ndescription = data.describe()\nprint(description)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There are no missing/NA values, hence we do not need to handle missing values (i.e data imputation is not required)\n\n## 3.5 Class Distribution (Classification Only)\nOn classification problems we need to know how balanced the class values are. Highly imbalanced problems (a lot more observations for one class than another) are common and may need special handling in the data preparation stage of our project.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class Distribution\nclass_counts = data.groupby('class').size()\nprint(class_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that there are nearly double the number of observations with class 0 (no onset of diabetes) than there are with class 1 (onset of diabetes).\n\n## 3.6 Correlations Between Attributes\n\nCorrelation refers to the relationship between two variables and how they may or may not change together. The most common method for calculating correlation is Pearson's Correlation Coefficient, that assumes a normal distribution of the attributes involved. A correlation of -1 or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no correlation at all. Some machine learning algorithms like linear and logistic regression can suffer poor performance if there are highly correlated attributes in our dataset. As such, it is a good idea to review all of the pairwise correlations of the attributes in our dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairwise Pearson correlations\ncorrelations = data.corr(method='pearson')\nprint(correlations)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- preg and age are positively correlated (i.e. > 0.5)\n\n## 3.7 Skew of Univariate Distributions\nSkew refers to a distribution that is assumed Gaussian (normal or bell curve) that is shifted or squashed in one direction or another. Many machine learning algorithms assume a Gaussian distribution. Knowing that an attribute has a skew may allow us to perform data preparation to correct the skew and later improve the accuracy of our models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Skew for each attribute\nskew = data.skew()\nprint(skew)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The skew result show a positive (right) or negative (left) skew. Values closer to zero show less skew.\n- preg, test, pedi, age, class are positively skewed (i.e. > 0.5)\n- pres is negatively skewed (i.e. < -0.5)\n- plas, skin, mass can be classified as not skewed (i.e. they are between +0.5 and -0.5)\n\n- Questions\n    - What should we do with the columns which are skewed? \n    - How should we transform them so that their skewed nature does not have an adverse effect on our prediction?\n    - Which transformation should we apply for left-skewed columns and which transformation should we apply for right-skewed data?\n- Possible Answers: The following section has been taken from https://rcompanion.org/handbook/I_12.html\n    - For right-skewed data—tail is on the right, positive skew—, common transformations include square root, cube root, and log.\n    - For left-skewed data—tail is on the left, negative skew—, common transformations include square root (constant – x), cube root (constant – x), and log (constant – x).\n    - Because log (0) is undefined—as is the log of any negative number—, when using a log transformation, a constant should be added to all values to make them all positive before transformation.  It is also sometimes helpful to add a constant when using other transformations.\n    - Another approach is to use a general power transformation, such as Tukey’s Ladder of Powers or a Box–Cox transformation.  These determine a lambda value, which is used as the power coefficient to transform values.  X.new = X ^ lambda for Tukey, and X.new = (X ^ lambda – 1) / lambda for Box–Cox.\n    - The function transformTukey in the rcompanion package finds the lambda which makes a single vector of values—that is, one variable—as normally distributed as possible with a simple power transformation. \n    - The Box–Cox procedure is included in the MASS package with the function boxcox.  It uses a log-likelihood procedure to find the lambda to use to transform the dependent variable for a linear model (such as an ANOVA or linear regression).  It can also be used on a single vector.\n\n# 4. Understand our data with visualization\n\nWe are going to cover the following visualizations:\n1. Univariate Plots (Histograms, Density Plots, Box and Whisker Plots)\n2. Multivariate Plots (Correlation Matrix Plot, Scatter Plot Matrix)\n\n## 4.1.1 Univariate Plots (Histograms)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Histograms\ndata.hist()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- age, pedi, preg, skin, test: Exponential-like distribution\n- class: Bimodal distribution\n- mass, plas, pres: Gaussian-like distribution\n\n\n## 4.1.2 Univariate Plots (Density Plots)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Univariate Density Plots\ndata.plot(kind='density', subplots=True, layout=(3,3), sharex=False)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see the distribution for each attribute is clearer than the histograms.\n\n## 4.1.3 Univariate Plots (Box and Whisker Plots)\n\nAnother useful way to review the distribution of each attribute is to use Box and Whisker Plots or boxplots for short. Boxplots summarize the distribution of each attribute, drawing a line for the median (middle value) and a box around the 25th and 75th percentiles (the middle 50% of the data). The whiskers give an idea of the spread of the data and dots outside of the whiskers show candidate outlier values (values that are 1.5 times greater than the size of spread of the middle 50% of the data).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box and Whisker Plots\ndata.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that the spread of attributes is quite different. Some like age, test and skin appear quite skewed towards smaller values.\n\n## 4.2.1 Multivariate Plots (Correlation Matrix Plot)\nCorrelation gives an indication of how related the changes are between two variables. If two variables change in the same direction they are positively correlated. If they change in opposite directions together (one goes up, one goes down), then they are negatively correlated. We can calculate the correlation between each pair of attributes. This is called a correlation matrix. We can then plot the correlation matrix and get an idea of which variables have a high correlation with each other. This is useful to know, because some machine learning algorithms like linear and logistic regression can have poor performance if there are highly correlated input variables in our data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correction Matrix Plot\ncorrelations = data.corr()\n# plot correlation matrix\nfig = pyplot.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = numpy.arange(0,9,1)\nax.set_xticklabels(data.columns)\nax.set_yticklabels(data.columns)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that the matrix is symmetrical, i.e. the bottom left of the matrix is the same as the top right. \n- This is useful as we can see two different views on the same data in one plot. \n- We can also see that <u>each variable is perfectly positively correlated with each other</u> in the diagonal line from top left to bottom right.\n- There are <u>no negative correlations</u>\n\n## 4.2.2 Multivariate Plots (Scatter Plot Matrix)\n\nA scatter plot shows the relationship between two variables as dots in two dimensions, one axis for each attribute. We can create a scatter plot for each pair of attributes in our data. Drawing all these scatter plots together is called a scatter plot matrix. Scatter plots are useful for spotting structured relationships between variables, like whether we could summarize the relationship between two variables with a line. Attributes with structured relationships may also be correlated and good candidates for removal from our dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatterplot Matrix\ng = sns.PairGrid(data, diag_sharey=False)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot, colors=\"C0\")\ng.map_diag(sns.kdeplot, lw=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Like the Correlation Matrix Plot, the scatter plot matrix is symmetrical. This is useful to look at the pairwise relationships from different perspectives.\n\n# 5. Prepare data\n\nMany machine learning algorithms make assumptions about our data. It is often a very good idea to prepare our data in such way to best expose the structure of the problem to the machine learning algorithms that we intend to use.\n\nWe are going to cover the following steps:\n1. Rescale data.\n2. Standardize data.\n3. Normalize data.\n\n## Need For Data Pre-processing\n\nDifferent algorithms make different assumptions about our data and may require different transforms. Further, when we follow all of the rules and prepare our data, sometimes algorithms can deliver better results without pre-processing.\n\n## 5.1 Rescale Data\nWhen our data is comprised of attributes with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale. Often this is referred to as normalization and attributes are often rescaled into the range between 0 and 1. This is useful for optimization algorithms used in the core of machine learning algorithms like gradient descent. It is also useful for algorithms that weight inputs like regression and neural networks and algorithms that use distance measures like k-Nearest Neighbors.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rescale data (between 0 and 1)\narray = data.values\n# separate array into input and output components\nX = array[:,0:8]\nY = array[:,8]\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX = scaler.fit_transform(X)\n# summarize transformed data\nset_printoptions(precision=3)\nprint(rescaledX[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- After rescaling we can see that all of the values are in the range between 0 and 1.\n\n## 5.2 Standardize Data\n\nStandardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1. It is most suitable for techniques that assume a Gaussian distribution in the input variables and work better with rescaled data, such as linear regression, logistic regression and linear discriminate analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardize data (0 mean, 1 stdev)\nscaler = StandardScaler().fit(X)\nrescaledX = scaler.transform(X)\n# summarize transformed data\nset_printoptions(precision=3)\nprint(rescaledX[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The values for each attribute now have a mean value of 0 and a standard deviation of 1.\n\n## 5.3 Normalize Data\nNormalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called a unit norm or a vector with the length of 1 in linear algebra). This pre-processing method can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using algorithms that weight input values such as neural networks and algorithms that use distance measures such as k-Nearest Neighbors.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize data (length of 1)\nscaler = Normalizer().fit(X)\nnormalizedX = scaler.transform(X)\n# summarize transformed data\nset_printoptions(precision=3)\nprint(normalizedX[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The rows are normalized to length 1.\n\n# 6. Feature Selection\nFeature selection is a process where we automatically select those features in our data that contribute most to the prediction variable or output in which we are interested. Having irrelevant features in our data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression. Three benefits of performing feature selection before modeling our data are:\n- Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n- Improves Accuracy: Less misleading data means modeling accuracy improves.\n- Reduces Training Time: Less data means that algorithms train faster.\n\n## 6.1 Univariate Feature Selection\nStatistical tests can be used to select those features that have the strongest relationship with the output variable. Below we use the chi-squared (chi2) statistical test for non-negative features to select 4 of the best features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\n# feature extraction\ntest = SelectKBest(score_func=chi2, k=4)\nfit = test.fit(X, Y)\n# summarize scores\nset_printoptions(precision=3)\nprint(fit.scores_)\nfeatures = fit.transform(X)\n# summarize selected features\nprint(features[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- we can see the scores for each attribute and the 4 attributes chosen (those with the highest scores): plas, test, mass and age.\n\n## 6.2 Recursive Feature Elimination\nThe Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute. Below, we use RFE with the logistic regression algorithm to select the top 3 features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Extraction with RFE\n# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model, 3)\nfit = rfe.fit(X, Y)\nprint((\"Num Features: %d\") % fit.n_features_)\nprint((\"Selected Features: %s\") % fit.support_)\nprint((\"Feature Ranking: %s\") % fit.ranking_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that RFE chose the top 3 features as preg, mass and pedi. These are marked True in the support array and marked with a choice 1 in the ranking array.\n\n## 6.3 Principal Component Analysis\nPrincipal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form. Generally this is called a data reduction technique. A property of PCA is that we can choose the number of dimensions or principal components in the transformed result. Below, we use PCA and select 3 principal components.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Extraction with PCA\n# feature extraction\npca = PCA(n_components=3)\nfit = pca.fit(X)\n# summarize components\nprint((\"Explained Variance: %s\") % fit.explained_variance_ratio_)\nprint(fit.components_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.4 Feature Importance\nBagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features. Below, we construct a ExtraTreesClassifier classifier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Importance with Extra Trees Classifier\n# feature extraction\nmodel = ExtraTreesClassifier()\nmodel.fit(X, Y)\nprint(model.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We are given an importance score for each attribute where the larger the score, the more important the attribute. \n- The scores suggest at the importance of plas, age and mass.\n\n# 7. Evaluate the Performance of Algorithms with Resampling\n\nWe are  going to try the following four methods:\n- Train and Test Sets.\n- k-fold Cross Validation.\n- Leave One Out Cross Validation.\n- Repeated Random Test-Train Splits.\n\n## 7.1 Split into Train and Test Sets\n\nThe simplest method that we can use to evaluate the performance of a machine learning algorithm is to use different training and testing datasets. We can take our original dataset and split it into two parts. Train the algorithm on the first part, make predictions on the second part and evaluate the predictions against the expected results. The size of the split can depend on the size and specifics of our dataset, although it is common to use 67% of the data for training and the remaining 33% for testing.\n\nThis algorithm evaluation technique is very fast. It is ideal for large datasets (millions of records) where there is strong evidence that both splits of the data are representative of the underlying problem. Because of the speed, it is useful to use this approach when the algorithm we are investigating is slow to train. A downside of this technique is that it can have a high variance. This means that differences in the training and test dataset can result in meaningful differences in the estimate of accuracy. Below, we split our data into 67%/33% splits for training and test and evaluate the accuracy of a Logistic Regression model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate using a train and a test set\ntest_size = 0.33\nseed = 7\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\nresult = model.score(X_test, Y_test)\nprint((\"Accuracy: %.3f%%\") % (result*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the estimated accuracy for the model was approximately 78.7%. In addition to specifying the size of the split, we also specify the random seed. Because the split of the data is random, we want to ensure that the results are reproducible. By specifying the random seed we ensure that we get the same random numbers each time we run the code and in turn the same split of data. If we want to compare this result to the estimated accuracy of another machine learning algorithm or the same algorithm with a different configuration. To ensure the comparison was apples-for-apples, we must ensure that they are trained and tested on exactly the same data.\n\n## 7.2 K-fold Cross Validation\nCross validation is an approach that we can use to estimate the performance of a machine learning algorithm with less variance than a single train-test set split. It works by splitting the dataset into k-parts (e.g. k = 5 or k = 10). Each split of the data is called a fold. The algorithm is trained on k - 1 folds with one held back and tested on the held back fold. This is repeated so that each fold of the dataset is given a chance to be the held back test set. After running cross validation we end up with k different performance scores that we can summarize using a mean and a standard deviation.\n\nThe result is a more reliable estimate of the performance of the algorithm on new data. It is more accurate because the algorithm is trained and evaluated multiple times on different data. The choice of k must allow the size of each test partition to be large enough to be a reasonable sample of the problem, whilst allowing enough repetitions of the train-test evaluation of the algorithm to provide a fair estimate of the algorithms performance on unseen data. For modest sized datasets in the thousands or tens of thousands of records, k values of 3, 5 and 10 are common. Below, we use 10-fold cross validation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate using Cross Validation\nnum_folds = 10\nseed = 7\nkfold = KFold(n_splits=num_folds, random_state=seed)\nmodel = LogisticRegression()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint((\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- we report both the mean and the standard deviation of the performance measure. \n- When summarizing performance measures, it is a good practice to summarize the distribution of the measures, in this case assuming a Gaussian distribution of performance (a very reasonable assumption) and recording the mean and standard deviation.\n\n## 7.3 Leave One Out Cross Validation\nWe can configure cross validation so that the size of the fold is 1 (k is set to the number of observations in our dataset). This variation of cross validation is called leave-one-out cross validation. The result is a large number of performance measures that can be summarized in an effort to give a more reasonable estimate of the accuracy of our model on unseen data.\nA downside is that it can be a computationally more expensive procedure than k-fold cross validation. Below, we use leave-one-out cross validation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate using Leave One Out Cross Validation\nnum_folds = 10\nloocv = LeaveOneOut()\nmodel = LogisticRegression()\nresults = cross_val_score(model, X, Y, cv=loocv)\nprint((\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see in the standard deviation that the score has more variance than the k-fold cross validation results described above.\n\n## 7.4 Repeated Random Test-Train Splits\n\nAnother variation on k-fold cross validation is to create a random split of the data like the train/test split described above, but repeat the process of splitting and evaluation of the algorithm multiple times, like cross validation. This has the speed of using a train/test split and the reduction in variance in the estimated performance of k-fold cross validation. We can also repeat the process many more times as needed to improve the accuracy. A down side is that repetitions may include much of the same data in the train or the test split from run to run, introducing redundancy into the evaluation. Below, we split the data into a 67%/33% train/test split and repeat the process 10 times.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate using Shuffle Split Cross Validation\nn_splits = 10\ntest_size = 0.33\nseed = 7\nkfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)\nmodel = LogisticRegression()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint((\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What Techniques to Use When\n- Generally k-fold cross validation is the gold standard for evaluating the performance of a machine learning algorithm on unseen data with k set to 3, 5, or 10.\n- Using a train/test split is good for speed when using a slow algorithm and produces performance estimates with lower bias when using large datasets.\n- Techniques like leave-one-out cross validation and repeated random splits can be useful intermediates when trying to balance variance in the estimated performance, model training speed and dataset size.\n\n# 8. Algorithm Performance Metrics\n\n## Classification Metrics\nClassification problems are perhaps the most common type of machine learning problems and as such there are a myriad of metrics that can be used to evaluate predictions for these problems.\nBelow, we will demonstrate how to use the following metrics:\n- Classification Accuracy.\n- Logarithmic Loss.\n- Area Under ROC Curve.\n- Confusion Matrix.\n- Classification Report.\n\n## 8.1 Classification Accuracy\nClassification accuracy is the number of correct predictions made as a ratio of all predictions made. This is the most common evaluation metric for classification problems, it is also the most misused. It is really only suitable when there are an equal number of observations in each class (which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation Classification Accuracy\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LogisticRegression()\nscoring = 'accuracy'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint((\"Accuracy: %.3f (%.3f)\") % (results.mean(), results.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that the ratio is reported. This can be converted into a percentage by multiplying the value by 100, giving an accuracy score of approximately 77% accurate.\n\n## 8.2 Logarithmic Loss\nLogarithmic loss (or logloss) is a performance metric for evaluating the predictions of probabilities of membership to a given class. The scalar probability between 0 and 1 can be seen as a measure of confidence for a prediction by an algorithm. Predictions that are correct or incorrect are rewarded or punished proportionally to the confidence of the prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation Classification LogLoss\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LogisticRegression()\nscoring = 'neg_log_loss'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint((\"Logloss: %.3f (%.3f)\") % (results.mean(), results.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Smaller logloss is better with 0 representing a perfect logloss.\n\n## 8.3 Area Under ROC Curve\nArea under ROC Curve (or AUC for short) is a performance metric for binary classification problems. The AUC represents a model's ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model that is as good as random. ROC can be broken down into sensitivity and specificity. A binary classification problem is really a trade-off between sensitivity and specificity.\n- Sensitivity is the true positive rate also called the recall. It is the number of instances from the positive (first) class that actually predicted correctly.\n- Specificity is also called the true negative rate. It is the number of instances from the negative (second) class that were actually predicted correctly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation Classification ROC AUC\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LogisticRegression()\nscoring = 'roc_auc'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint((\"AUC: %.3f (%.3f)\") % (results.mean(), results.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see the AUC is relatively close to 1 and greater than 0.5, suggesting some skill in the predictions\n\n## 8.4 Confusion Matrix\nThe confusion matrix is a handy presentation of the accuracy of a model with two or more classes. The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells of the table are the number of predictions made by a machine learning algorithm. For example, a machine learning algorithm can predict 0 or 1 and each prediction may actually have been a 0 or 1. Predictions for 0 that were actually 0 appear in the cell for prediction = 0 and actual = 0, whereas predictions for 0 that were actually 1 appear in the cell for prediction = 0 and actual = 1. And so on.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation Classification Confusion Matrix\ntest_size = 0.33\nseed = 7\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\npredicted = model.predict(X_test)\nmatrix = confusion_matrix(Y_test, predicted)\nprint(matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Although the array is printed without headings, we can see that the majority of the predictions fall on the diagonal line of the matrix (which are correct predictions).\n\n## 8.5 Classi\fcation Report\nThe scikit-learn library provides a convenience report when working on classification problems to give us a quick idea of the accuracy of a model using a number of measures. The classification report() function displays the precision, recall, F1-score and support for each class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation Classification Report\ntest_size = 0.33\nseed = 7\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\npredicted = model.predict(X_test)\nreport = classification_report(Y_test, predicted)\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see good prediction and recall for the algorithm.\n\n# 9. Spot-Check Algorithms\n\nLet's take a look at six classification algorithms that we can spot-check on our data. \n- Linear Machine Learning Algorithms (Logistic Regression, Linear Discriminant Analysis)\n- Non-linear Machine Learning Algorithms (k-Nearest Neighbors, Naive Bayes, Classification and Regression Trees, Support Vector Machines)\n\n## 9.1 Logistic Regression\n\nLogistic regression assumes a Gaussian distribution for the numeric input variables and can model binary classification problems.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression Classification\nnum_folds = 10\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LogisticRegression()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9.2 Linear Discriminant Analysis\n\nLinear Discriminant Analysis or LDA is a statistical technique for binary and multiclass classification. It too assumes a Gaussian distribution for the numerical input variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# LDA Classification\nnum_folds = 10\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LinearDiscriminantAnalysis()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9.3 k-Nearest Neighbors\nThe k-Nearest Neighbors algorithm (or KNN) uses a distance metric to find the k most similar instances in the training data for a new instance and takes the mean outcome of the neighbors as the prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN Classification\nnum_folds = 10\nkfold = KFold(n_splits=10, random_state=7)\nmodel = KNeighborsClassifier()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9.4 Naive Bayes\nNaive Bayes calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are estimated for new data and multiplied together, assuming that they are all independent (a simple or naive assumption). When working with real-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for input variables using the Gaussian Probability Density Function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gaussian Naive Bayes Classification\nkfold = KFold(n_splits=10, random_state=7)\nmodel = GaussianNB()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9.5 Classification and Regression Trees\nClassification and Regression Trees (CART or just decision trees) construct a binary tree from the training data. Split points are chosen greedily by evaluating each attribute and each value of each attribute in the training data in order to minimize a cost function (like the Gini index).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# CART Classification\nkfold = KFold(n_splits=10, random_state=7)\nmodel = DecisionTreeClassifier()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9.6 Support Vector Machines\nSupport Vector Machines (or SVM) seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and influence where the line is placed. SVM has been extended to support multiple classes. Of particular importance is the use of different kernel functions via the kernel parameter. A powerful Radial Basis Function is used by default.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM Classification\nkfold = KFold(n_splits=10, random_state=7)\nmodel = SVC()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10. Compare Algorithms\n\n## Compare Machine Learning Algorithms Consistently\nThe key to a fair comparison of machine learning algorithms is ensuring that each algorithm is evaluated in the same way on the same data. We can achieve this by forcing each algorithm to be evaluated on a consistent test harness.\n\nThe 10-fold cross validation procedure is used to evaluate each algorithm, importantly configured with the same random seed to ensure that the same splits to the training data are performed and that each algorithm is evaluated in precisely the same way. Each algorithm is given a short name, useful for summarizing results afterward.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Algorithms\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The output above provides a box and whisker plot showing the spread of the accuracy scores across each cross validation fold for each algorithm.\n- From these results, it would suggest that both logistic regression and linear discriminant analysis are perhaps worthy of further study on this problem.\n\nLet's rescale the data and check whether or not the accuracy improves.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rescale data (between 0 and 1)\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX = scaler.fit_transform(X)\n\n# Compare Algorithms\n# prepare models\nmodels = []\nmodels.append(('ScaledLR', LogisticRegression()))\nmodels.append(('ScaledLDA', LinearDiscriminantAnalysis()))\nmodels.append(('ScaledKNN', KNeighborsClassifier()))\nmodels.append(('ScaledCART', DecisionTreeClassifier()))\nmodels.append(('ScaledNB', GaussianNB()))\nmodels.append(('ScaledSVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv_results = cross_val_score(model, rescaledX, Y, cv=kfold, scoring=scoring) # note that we have replaced X with rescaledX\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Mean Estimated Accuracy on raw training data (X_train, Y_train)\n    - LR: 0.773\n    - LDA: 0.773\n    - KNN: 0.727\n    - CART: 0.693\n    - NB: 0.755\n    - SVM: 0.760\n- Mean Estimated Accuracy on re-scaled training data (X_train, Y_train)\n    - ScaledLR: 0.768\n    - ScaledLDA: 0.773\n    - ScaledKNN: 0.745\n    - ScaledCART: 0.700\n    - ScaledNB: 0.755\n    - ScaledSVM: 0.771\n- Observations:\n    - the accuracy of our best performing model (i.e. LR in this case) has decreased, why?\n    - accuracy of LDA (linear) and NB (non-linear) remained the same\n    - the accuracy of non-linear models (KNN, CART and SVM) has become better, why?\n    \nLet's standardize the data and check whether or not the accuracy improves.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardize data (0 mean, 1 stdev)\nscaler = StandardScaler().fit(X)\nrescaledX = scaler.transform(X)\n\n# Compare Algorithms\n# prepare models\nmodels = []\nmodels.append(('StandardizedLR', LogisticRegression()))\nmodels.append(('StandardizedLDA', LinearDiscriminantAnalysis()))\nmodels.append(('StandardizedKNN', KNeighborsClassifier()))\nmodels.append(('StandardizedCART', DecisionTreeClassifier()))\nmodels.append(('StandardizedNB', GaussianNB()))\nmodels.append(('StandardizedSVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv_results = cross_val_score(model, rescaledX, Y, cv=kfold, scoring=scoring) # note that we have replaced X with rescaledX\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Mean Estimated Accuracy on raw training data (X_train, Y_train)\n    - LR: 0.773\n    - LDA: 0.773\n    - KNN: 0.727\n    - CART: 0.693\n    - NB: 0.755\n    - SVM: 0.760\n- Mean Estimated Accuracy on standardized training data (X_train, Y_train)\n    - StandardizedLR: 0.780\n    - StandardizedLDA: 0.773\n    - StandardizedKNN: 0.742\n    - StandardizedCART: 0.694\n    - StandardizedNB: 0.755\n    - StandardizedSVM: 0.766\n- Observations:\n    - the accuracy of LR, KNN and SVM has become better\n    - the accuracy of LDA, CART and NB has remained same\n\nLet's normalize the data and check whether or not the accuracy improves.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize data (length of 1)\nscaler = Normalizer().fit(X)\nnormalizedX = scaler.transform(X)\n\n# Compare Algorithms\n# prepare models\nmodels = []\nmodels.append(('NormalizedLR', LogisticRegression()))\nmodels.append(('NormalizedLDA', LinearDiscriminantAnalysis()))\nmodels.append(('NormalizedKNN', KNeighborsClassifier()))\nmodels.append(('NormalizedCART', DecisionTreeClassifier()))\nmodels.append(('NormalizedNB', GaussianNB()))\nmodels.append(('NormalizedSVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv_results = cross_val_score(model, normalizedX, Y, cv=kfold, scoring=scoring) # note that we have replaced X with normalizedX\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Mean Estimated Accuracy on raw training data (X_train, Y_train)\n    - LR: 0.773\n    - LDA: 0.773\n    - KNN: 0.727\n    - CART: 0.693\n    - NB: 0.755\n    - SVM: 0.760\n- Mean Estimated Accuracy on normalized training data (X_train, Y_train)\n    - NormalizedLR: 0.650\n    - NormalizedLDA: 0.672\n    - NormalizedKNN: 0.689\n    - NormalizedCART: 0.628\n    - NormalizedNB: 0.646\n    - NormalizedSVM: 0.654\n- Observations:\n    - the accuracy of all of our models has decreased, why?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 11. Automate Workflows with Pipelines\n\n## 11.1 Automating Machine Learning Workflows \nThere are standard workflows in applied machine learning. Standard because they overcome common problems like data leakage in our test harness. Python scikit-learn provides a Pipeline utility to help automate machine learning workflows. Pipelines work by allowing for a linear sequence of data transforms to be chained together culminating in a modeling process that can be evaluated. The goal is to ensure that all of the steps in the pipeline are constrained to the data available for the evaluation, such as the training dataset or each fold of the cross validation procedure.\n\n## 11.2 Data Preparation and Modeling Pipeline\nAn easy trap to fall into in applied machine learning is leaking data from our training dataset to our test dataset. To avoid this trap we need a robust test harness with strong separation of training and testing. This includes data preparation. Data preparation is one easy way to leak knowledge of the whole training dataset to the algorithm. For example, preparing our data using normalization or standardization on the entire training dataset before learning would not be a valid test because the training dataset would have been influenced by the scale of the data in the test set.\n\nPipelines help us prevent data leakage in our test harness by ensuring that data preparation like standardization is constrained to each fold of our cross validation procedure. Below, we show this important data preparation and model evaluation workflow on our dataset. The pipeline is defined with two steps:\n1. Standardize the data.\n2. Learn a Linear Discriminant Analysis model.\n\nThe pipeline is then evaluated using 10-fold cross validation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a pipeline that standardizes the data then creates a model\n# create pipeline\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('lda', LinearDiscriminantAnalysis()))\nmodel = Pipeline(estimators)\n# evaluate pipeline\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 11.3 Feature Extraction and Modeling Pipeline\nFeature extraction is another procedure that is susceptible to data leakage. Like data preparation, feature extraction procedures must be restricted to the data in our training dataset. The pipeline provides a handy tool called the FeatureUnion which allows the results of multiple feature selection and extraction procedures to be combined into a larger dataset on which a model can be trained. Importantly, all the feature extraction and the feature union occurs within each fold of the cross validation procedure. Below, we demonstrate the pipeline defined with four steps:\n1. Feature Extraction with Principal Component Analysis (3 features).\n2. Feature Extraction with Statistical Selection (6 features).\n3. Feature Union.\n4. Learn a Logistic Regression Model.\n\nThe pipeline is then evaluated using 10-fold cross validation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a pipeline that extracts features from the data then creates a model\n# create feature union\nfrom sklearn.pipeline import FeatureUnion\nfeatures = []\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=6)))\nfeature_union = FeatureUnion(features)\n# create pipeline\nestimators = []\nestimators.append(('feature_union', feature_union))\nestimators.append(('logistic', LogisticRegression()))\nmodel = Pipeline(estimators)\n# evaluate pipeline\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can notice how the FeatureUnion is it's own Pipeline that in turn is a single step in the final Pipeline used to feed Logistic Regression. \n- In this manner, we can embed pipelines within pipelines.\n\n# 12. Improve Performance with Ensembles\n\nEnsembles can give us a boost in the accuracy on our dataset. We will step through Boosting, Bagging and Majority Voting and demonstrate how we can continue to ratchet up the accuracy of the models on our own datasets.\n\n## 12.1 Combine Models Into Ensemble Predictions\nThe three most popular methods for combining the predictions from different models are:\n- Bagging. Building multiple models (typically of the same type) from different subsamples of the training dataset.\n- Boosting. Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the sequence of models.\n- Voting. Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions.\n\n## 12.2 Bagging Algorithms\nBootstrap Aggregation (or Bagging) involves taking multiple samples from our training dataset (with replacement) and training a model for each sample. The final output prediction is averaged across the predictions of all of the sub-models. We are going to cover the following three bagging models:\n- Bagged Decision Trees.\n- Random Forest.\n- Extra Trees.\n\n### 12.2.1 Bagged Decision Trees\nBagging performs best with algorithms that have high variance. A popular example are decision trees, often constructed without pruning. Below, we use the BaggingClassifier with the Classification and Regression Trees algorithm (DecisionTreeClassifier). A total of 100 trees are created.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bagged Decision Trees for Classification\nfrom sklearn.ensemble import BaggingClassifier\nseed = 7\nkfold = KFold(n_splits=10, random_state=seed)\ncart = DecisionTreeClassifier()\nnum_trees = 100\nmodel = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 12.2.2 Random Forest\nRandom Forests is an extension of bagged decision trees. Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of each tree, only a random subset of features are considered for each split. We can construct a Random Forest model for classification using the RandomForestClassifier class. Below, we demonstrate using Random Forest for classification with 100 trees and split points chosen from a random selection of 3 features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nnum_trees = 100\nmax_features = 3\nkfold = KFold(n_splits=10, random_state=7)\nmodel = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 12.2.3 Extra Trees\nExtra Trees are another modification of bagging where random trees are constructed from samples of the training dataset. We can construct an Extra Trees model for classification using the ExtraTreesClassifier class. Below, we demonstrate of extra trees with the number of trees set to 100 and splits chosen from 7 random features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extra Trees Classification\nfrom sklearn.ensemble import ExtraTreesClassifier\nnum_trees = 100\nmax_features = 7\nkfold = KFold(n_splits=10, random_state=7)\nmodel = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 12.3 Boosting Algorithms\nBoosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes of the models before them in the sequence. Once created, the models make predictions which may be weighted by their demonstrated accuracy and the results are combined to create a final output prediction. The two most common boosting ensemble machine learning algorithms are:\n- AdaBoost.\n- Stochastic Gradient Boosting.\n\n### 12.3.1 AdaBoost\nAdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or dificult they are to classify, allowing the algorithm to pay or less attention to them in the construction of subsequent models. We can construct an AdaBoost model for classification using the AdaBoostClassifier class4. Below, we demonstrate the construction of 30 decision trees in sequence using the AdaBoost algorithm.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# AdaBoost Classification\nfrom sklearn.ensemble import AdaBoostClassifier\nnum_trees = 30\nseed=7\nkfold = KFold(n_splits=10, random_state=seed)\nmodel = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 12.3.2 Stochastic Gradient Boosting\nStochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps one of the best techniques available for improving performance via ensembles. We can construct a Gradient Boosting model for classification using the GradientBoostingClassifier class5. Below, we demonstrate Stochastic Gradient Boosting for classification with 100 trees.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stochastic Gradient Boosting Classification\nfrom sklearn.ensemble import GradientBoostingClassifier\nseed = 7\nnum_trees = 100\nkfold = KFold(n_splits=10, random_state=seed)\nmodel = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 12.4 Voting Ensemble\nVoting is one of the simplest ways of combining the predictions from multiple machine learning algorithms. It works by first creating two or more standalone models from our training dataset. A Voting Classifier can then be used to wrap our models and average the predictions of the sub-models when asked to make predictions for new data. The predictions of the sub-models can be weighted, but specifying the weights for classifiers manually or even heuristically is difficult. More advanced methods can learn how to best weight the predictions from sub-models, but this is called stacking (stacked aggregation) and is currently not provided in scikit-learn. \n\nWe can create a voting ensemble model for classification using the VotingClassifier class. In the code below, we combine the predictions of logistic regression, classification and regression trees and support vector machines together for a classification problem.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Voting Ensemble for Classification\nfrom sklearn.ensemble import VotingClassifier\nkfold = KFold(n_splits=10, random_state=7)\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('cart', model2))\nmodel3 = SVC()\nestimators.append(('svm', model3))\n# create the ensemble model\nensemble = VotingClassifier(estimators)\nresults = cross_val_score(ensemble, X, Y, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 13. Improve Performance with Algorithm\nTuning Machine learning models are parameterized so that their behavior can be tuned for a given problem. Models can have many parameters and finding the best combination of parameters can be treated as a search problem.\n\n## 13.1 Machine Learning Algorithm Parameters\nAlgorithm tuning is a final step in the process of applied machine learning before finalizing our model. It is sometimes called hyperparameter optimization where the algorithm parameters are referred to as hyperparameters, whereas the coefficients found by the machine learning algorithm itself are referred to as parameters. Optimization suggests the search-nature of the problem. Phrased as a search problem, we can use different search strategies to find a good and robust parameter or set of parameters for an algorithm on a given problem. Python scikit-learn provides two simple methods for algorithm parameter tuning:\n- Grid Search Parameter Tuning.\n- Random Search Parameter Tuning.\n\n## 13.2 Grid Search Parameter Tuning\nGrid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid. We can perform a grid search using the GridSearchCV class. Below, we evaluate different alpha values for the Ridge Regression algorithm. This is a one-dimensional grid search.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid Search for Algorithm Tuning\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nalphas = numpy.array([1,0.1,0.01,0.001,0.0001,0])\nparam_grid = dict(alpha=alphas)\nmodel = Ridge()\ngrid = GridSearchCV(estimator=model, param_grid=param_grid)\ngrid.fit(X, Y)\nprint(grid.best_score_)\nprint(grid.best_estimator_.alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The above code lists out the optimal score achieved and the set of parameters in the grid that achieved that score. \n- In this case the alpha value of 1.0.\n\n## 13.3 Random Search Parameter Tuning\nRandom search is an approach to parameter tuning that will sample algorithm parameters from a random distribution (i.e. uniform) for a fixed number of iterations. A model is constructed and evaluated for each combination of parameters chosen. We can perform a random search for algorithm parameters using the RandomizedSearchCV class. Below, we evaluate different random alpha values between 0 and 1 for the Ridge Regression algorithm. A total of 100 iterations are performed with uniformly random alpha values selected in the range between 0 and 1 (the range that alpha values can take).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Randomized for Algorithm Tuning\nfrom scipy.stats import uniform\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import RandomizedSearchCV\nparam_grid = {'alpha': uniform()}\nmodel = Ridge()\nrsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, random_state=7)\nrsearch.fit(X, Y)\nprint(rsearch.best_score_)\nprint(rsearch.best_estimator_.alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The above code produces results much like those in grid search. An optimal alpha value near 1.0 is discovered.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 14. References and Credits\n- Thank you to Jason Brownlee https://machinelearningmastery.com/\n- Used the following link to get column names in the correlation matrix https://www.geeksforgeeks.org/how-to-get-column-names-in-pandas-dataframe/\n- For visualization https://seaborn.pydata.org/examples/scatterplot_matrix.html and https://seaborn.pydata.org/examples/pair_grid_with_kde.html\n- For transformations on skewed data https://rcompanion.org/handbook/I_12.html\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}