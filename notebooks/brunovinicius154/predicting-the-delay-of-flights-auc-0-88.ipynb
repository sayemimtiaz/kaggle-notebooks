{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Libs\nimport os\nimport numpy as np # Linear Algebra\nimport pandas as pd # Data Manipulation\npd.set_option('MAX_ROWS', None) # Setting pandas to display a N number of columns\nfrom collections import Counter # Data Manipulation\nimport seaborn as sns # Data Viz\nimport matplotlib.pyplot as plt # Data Viz\nfrom sklearn import tree # Modelling a tree\nfrom sklearn.impute import SimpleImputer # Perform Imputation\nfrom imblearn.over_sampling import SMOTE # Perform oversampling\nfrom sklearn.preprocessing import OneHotEncoder # Perform OneHotEnconding\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score,cross_val_predict # Cross Validation\nfrom sklearn.linear_model import LogisticRegression # Modelling\nfrom sklearn.metrics import classification_report, roc_auc_score,precision_score,recall_score # Evaluating the Model\n\n\n#warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Collecting the data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Collecting data\ndf_2019 = pd.read_csv('/kaggle/input/flight-delay-prediction/Jan_2019_ontime.csv')\ndf_2020 = pd.read_csv('/kaggle/input/flight-delay-prediction/Jan_2020_ontime.csv')\ndf_2019.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Problem definition.\n\nPredict whether a particular flight will be delayed or not. The data refer to flights from January-19 and January-20, so we can use the data to predict flight delays in January for the next period (year-2020).\n\n - Binary classification problem.\n - 21 variables per dataset.\n - Dataset with flights from Jan-19 and Jan-20.\n - Variable response is 'ARR_DEL15'\n\nVariable dictionary:\n\n    'DAY_OF_MONTH': Day of the month.\n    'DAY_OF_WEEK': Day of the week.\n    'OP_UNIQUE_CARRIER': Unique transport code.\n    'OP_CARRIER_AIRLINE_ID': Unique aviation operator code.\n    'OP_CARRIER': IATA code of the operator.\n    'TAIL_NUM': Tail number.\n    'OP_CARRIER_FL_NUM': Flight number.\n    'ORIGIN_AIRPORT_ID': Origin airport ID.\n    'ORIGIN_AIRPORT_SEQ_ID': Origin airport ID - SEQ.\n    'ORIGIN': Airport of Origin.\n    'DEST_AIRPORT_ID': ID of the destination airport.\n    'DEST_AIRPORT_SEQ_ID': Destination airport ID - SEQ.\n    'DEST': Destination airport.\n    'DEP_TIME': Flight departure time.\n    'DEP_DEL15': Departure delay indicator\n    'DEP_TIME_BLK': block of time (hour) where the match has been postponed.\n    'ARR_TIME': Flight arrival time.\n    'ARR_DEL15': Arrival delay indicator.\n    'CANCELLED': Flight cancellation indicator.\n    'DIVERTED': Indicator if the flight has been diverted.\n    'DISTANCE': Distance between airports.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Unifying the bases.\n\nWe will unify the bases of 2019 and 2020 to analyze the data as a whole.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating year indicator.\ndf_2019['year'] = 2019\ndf_2020['year'] = 2020\n\n#Checking if the bases have the same columns\nprint(set(df_2020.columns) == set(df_2019.columns))\n\n#Generating the unique base\ndataset = pd.concat([df_2019,df_2020])\nprint(dataset.shape)\nprint('\\n')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initial data selection.\nWe will select the variables that we will work on to discover patterns in the data.\n  \nWe will remove all identifiers with the exception 'OP_CARRIER_FL_NUM', which we will transform into an index of our database. The main reason for remove identifiers is that they are irrelevant for analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = dataset.drop(['OP_UNIQUE_CARRIER','OP_CARRIER_AIRLINE_ID','OP_CARRIER','TAIL_NUM', 'ORIGIN_AIRPORT_ID','ORIGIN_AIRPORT_SEQ_ID','DEST_AIRPORT_ID','DEST_AIRPORT_SEQ_ID','Unnamed: 21'], axis=1)\ndata = data.set_index('OP_CARRIER_FL_NUM')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Cleaning the dataset / Discretization\n\nMissing values:\n\nRegarding the missing values, considering that they make up 2.5% less of the data, we will adopt the strategy of eliminating them by the line from our database.\n\nData Type:\n\nWe will transform the types of the variables 'DISTANCE', 'ARR_TIME', 'DEP_TIME', 'CANCELED', 'DIVERTED', 'DEP_DEL15', 'ARR_DEL15' to categorical dtype, as they are categorical variables.\n\nDiscretization:\n\nWe will create distance ranges (categories) for the 'DISTANCE' variable.\nThe advantage is the improvement in the understanding of the knowledge discovered, reduction of the processing time when training some algorithm, and reduction of the search space.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataframe summary\npd.DataFrame({'unicos':data.nunique(),\n              'missing': data.isna().sum()/data.count(),\n              'tipo':data.dtypes})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing values\ndata.dropna(inplace=True)\n\n#Transformation of data types\ncolunas = ['DAY_OF_WEEK','DAY_OF_MONTH','DEP_DEL15','ARR_DEL15','CANCELLED','DIVERTED']\nfor col in colunas:\n  data[col] = data[col].astype('category') \n\n#Discretization\ndata['DISTANCE_cat'] = pd.qcut(data['DISTANCE'], q=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataframe summary after pre-processing\npd.DataFrame({'unicos':data.nunique(),\n              'missing': data.isna().mean()*100,\n              'tipo':data.dtypes})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Analysis\n\nQuestions we want to answer from the data!\n  - The concentration of delay and non-delay both on departure and on arrival?\n  - The proportion of delayed flights that were diverted?\n  - Are delays due to day_of_week and day_of_month?\n  - The concentration of delay's by 'DEP_TIME_BLK'?\n  - Which airport in Origin stands out in delays?\n  - Which airport in Destination stands out in delays?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The concentration of delay and timely arrivals both on departure and on arrival?\nf, (ax,ax1) = plt.subplots(1,2, figsize=(12,6))\ndep = sns.countplot(data['DEP_DEL15'], ax=ax)\ndep.set_title('Depatures')\ndep.set_xlabel('Labels')\ndep.set_ylabel('Freq')\n\narr = sns.countplot(data['ARR_DEL15'], ax=ax1)\narr.set_title('Arrivals')\narr.set_xlabel('Labels')\narr.set_ylabel('Freq')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graphs above, we can see a greater concentration of flights with timely departures and arrivals.\n\nAnother insight that we can observe is that the proportions are very similar in the two variables, that is, it is very likely that the departures or not in delay are very important for predictive modeling about delayed arrivals.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Percentage of delayed flights that are canceled or diverted?\nvoos_atrasados = data.loc[data['ARR_DEL15'] == 1,['DIVERTED']]\n\n\nf, ax= plt.subplots(figsize=(12,6))\n\n#Desvios\ndesv = sns.countplot(voos_atrasados['DIVERTED'], ax=ax)\ndesv.set_title('Diverted Flights')\ndesv.set_xlabel('Labels')\ndesv.set_ylabel('Freq')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see any flight with delay was diverted.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delays due to day_of_week and day_of_month?\n\nweek = data[['DAY_OF_WEEK','ARR_DEL15']].groupby('DAY_OF_WEEK').sum().sort_values(by='ARR_DEL15',ascending=False)\nweek['PERCENTUAL'] = week['ARR_DEL15']/(week['ARR_DEL15'].sum())*100\nmonth = data[['DAY_OF_MONTH','ARR_DEL15']].groupby('DAY_OF_MONTH').sum().sort_values(by='ARR_DEL15',ascending=False)\nmonth['PERCENTUAL'] = month['ARR_DEL15']/(month['ARR_DEL15'].sum())*100\n\nprint('>> Delayed flights by weekday<<')\nprint(week)\nprint('\\n')\nprint('>> Delayed flights by monthday <<')\nprint(month)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Day of week 4 (Wednesday) has the highest incidence of delays.\n\nRegarding the days of the month, although more distributed, the 24th and 2nd are the ones that stand out.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concentration of delays due to 'DEP_TIME_BLK'?\ntime_blk = data[['DEP_TIME_BLK','ARR_DEL15']].groupby('DEP_TIME_BLK').sum().sort_values(by='ARR_DEL15',ascending=False)\ntime_blk['PERCENTUAL'] = time_blk['ARR_DEL15']/(time_blk['ARR_DEL15'].sum())*100\ntime_blk","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most delays occur between 4:00 pm and 7:00 pm, in the late afternoon.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Which 'Origin' airport stands out in delay?\norigin_later = data[['ORIGIN','DEP_DEL15']].groupby('ORIGIN').sum().sort_values(by='DEP_DEL15',ascending=False)\norigin_later['PERCENTUAL'] = origin_later['DEP_DEL15']/(origin_later['DEP_DEL15'].sum())*100\norigin_later.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We note that ORD (Chicago O'Hare International Airport) and DFW (Dallas / Ft Worth, TX, USA - Dallas Ft Worth International) airports are the ones with the most delays.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Which airport of Destination stands out in delays?\ndest_later = data[['DEST','ARR_DEL15']].groupby('DEST').sum().sort_values(by='ARR_DEL15',ascending=False)\ndest_later['PERCENTUAL'] = dest_later['ARR_DEL15']/(dest_later['ARR_DEL15'].sum())*100\ndest_later.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, the same airports with the longest delays at origin are also the ones with the highest delays at destination airports. In the modeling stage, we can perform the OneHotEncoder and maintain only the 3 to 5 largest airports to avoid high dimensionality.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Creating Variables\n\nThrough the analysis of the first graph of the exploratory analysis, we had the insight that the delays in the departure of the flights (DEP_DEL15) can help us to model the delays in the arrival (ARR_DEL15) of the flights. That way we can create related variables as below.\n\n-We can create ARR_TIME_BLOCK.\n\n-The number of delays within a DEP_TIME_BLK.\n\n-The number of delays DEP_DEL15 per ORIGIN.\n\n-The number of delays ARR_DEL15 per DEST.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper function to create ARR_TIME_BLOCK\ndef arr_time(x):\n\n  if x >= 600 and x <= 659:\n    return '0600-0659'\n  elif x>=1400 and x<=1459:\n    return '1400-1459'\n  elif x>=1200 and x<=1259:\n    return '1200-1259'\n  elif x>=1500 and x<=1559:\n    return '1500-1559'\n  elif x>=1900 and x<=1959:\n    return '1900-1959'\n  elif x>=900 and x<=959:\n    return '0900-0959'\n  elif x>=1000 and x<=1059:\n    return  '1000-1059'\n  elif x>=2000 and x<=2059:\n    return '2000-2059'\n  elif x>=1300 and x<=1359:\n    return '1300-1359'\n  elif x>=1100 and x<=1159:\n    return '1100-1159'\n  elif x>=800 and x<=859:\n    return '0800-0859'\n  elif x>=2200 and x<=2259:\n    return '2200-2259'\n  elif x>=1600 and x<=1659:\n    return '1600-1659'\n  elif x>=1700 and x<=1759:\n    return '1700-1759'\n  elif x>=2100 and x<=2159:\n    return '2100-2159'\n  elif x>=700 and x<=759:\n    return '0700-0759'\n  elif x>=1800 and x<=1859:\n    return '1800-1859'\n  elif x>=1 and x<=559:\n    return '0001-0559'\n  elif x>=2300 and x<=2400:\n    return '2300-2400'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can create ARR_TIME_BLOCK.\ndata['ARR_TIME'] = data['ARR_TIME'].astype('int')\ndata['ARR_TIME_BLOCK'] = data['ARR_TIME'].apply(lambda x :arr_time(x))\ndata.reset_index(inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Amount of delays within a DEP_TIME_BLK.\ncount_time_blk = data[['DEP_TIME_BLK','ARR_DEL15']].groupby('DEP_TIME_BLK').sum().sort_values(by='ARR_DEL15',ascending=False)\ncount_time_blk.reset_index(inplace=True)\ncount_time_blk.head()\ndata1 = data.merge(count_time_blk, left_on='DEP_TIME_BLK', right_on='DEP_TIME_BLK') \ndata1.rename({'ARR_DEL15_y':'quant_dep_time_blk','ARR_DEL15_x':'ARR_DEL15' }, inplace=True, axis=1)\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of delays DEP_DEL15 per ORIGIN.\ncount_later_origin = data[['ORIGIN','DEP_DEL15']].groupby('ORIGIN').sum().sort_values(by='DEP_DEL15',ascending=False)\ncount_later_origin.reset_index(inplace=True)\ncount_later_origin.head()\ndata2 = data1.merge(count_later_origin, left_on='ORIGIN', right_on='ORIGIN')\ndata2.rename({'DEP_DEL15_y':'count_later_origin','DEP_DEL15_x':'DEP_DEL15' }, inplace=True, axis=1)\ndata2.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of delays ARR_DEL15 per DEST.\ncount_later_dest = data[['DEST','ARR_DEL15']].groupby('DEST').sum().sort_values(by='ARR_DEL15',ascending=False)\ncount_later_dest.reset_index(inplace=True)\ncount_later_dest.head()\ndata3 = data2.merge(count_later_dest, left_on='DEST', right_on='DEST')\ndata3.rename({'ARR_DEL15_y':'count_later_dest','ARR_DEL15_x':'ARR_DEL15' },inplace=True, axis=1)\ndata3.head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Final Model\nFor training the final model, we will train the model on the 2019 data and use the 2020 data for validation. Such an approach makes sense since we have data for January of each year and we can predict what will happen in January of the following year.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Preparation\nbase_final = data3.copy()\nbase_final.drop(['DEP_TIME','ARR_TIME','OP_CARRIER_FL_NUM'], inplace=True, axis=1)\nbase_final.set_index('year',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate target, numeric and categorical variables 'ORIGIN', 'DEST'\n\ntarget_final = base_final[['ARR_DEL15']]\n\ncat_vars_final = base_final.select_dtypes(['object','category'])\ncat_vars_final = cat_vars_final.loc[:, ['DAY_OF_MONTH', 'DAY_OF_WEEK','DEP_DEL15','DEP_TIME_BLK','CANCELLED',\n                            'DIVERTED','DISTANCE_cat','ARR_TIME_BLOCK']]\n\n#One Hot Encoder\n\nenc = OneHotEncoder().fit(cat_vars_final)\n\ncat_vars_ohe_final = enc.transform(cat_vars_final).toarray()\ncat_vars_ohe_final = pd.DataFrame(cat_vars_ohe_final, index= cat_vars_final.index, \n                      columns=enc.get_feature_names(cat_vars_final.columns.tolist()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logisitc Regression Model\n\n\n#Dividing into training and test data: 2019 - training, 2020 - testing\ntarget_2019_final = target_final[target_final.index == 2019]\ntarget_2020_final = target_final[target_final.index == 2020]\n\ncat_vars_ohe_2019_final = cat_vars_ohe_final[cat_vars_ohe_final.index == 2019]\ncat_vars_ohe_2020_final = cat_vars_ohe_final[cat_vars_ohe_final.index == 2020]\n\n\n#Instantizing Model\nlr_model_final = LogisticRegression(C=1.0,n_jobs=-1,verbose=1, random_state=154)\n\n#training\nlr_model_final.fit(cat_vars_ohe_2019_final, target_2019_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation of the Final Model\nThe average AUC in training data was 0.89, we can see that dividing the data by time (2019.2020) generated a good increase in our control metric.\n\nFor the test data, the AUC fell slightly 0.88 which is very good, it indicates that we are not suffering from overfitting. However, if we analyze the recall we observe the value of 0.73, that is, of all 'delay' events we are correctly classifying 73% of our target category.\n\nIf it is very important to have a better performance in classifying our target category (for example if we were talking about classifying diseases), we could evaluate the threshold and favor the recall over precision, in this way, we would start to classify many flights that do not would delay, however, we would get right most of the delays.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Validação Cruzada -Treino\ncv = StratifiedKFold(n_splits=3, shuffle=True)\nresult = cross_val_score(lr_model_final,cat_vars_ohe_2019_final,target_2019_final, cv=cv, scoring='roc_auc', n_jobs=-1)\nprint(f'A média: {np.mean(result)}')\nprint(f'Limite Inferior: {np.mean(result)-2*np.std(result)}')\nprint(f'Limite Superior: {np.mean(result)+2*np.std(result)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test Data\n\n# Predict\npred = lr_model_final.predict(cat_vars_ohe_2020_final)\npred_prob = lr_model_final.predict_proba(cat_vars_ohe_2020_final)\n\n# print classification report\nprint(\"Relatório de Classificação:\\n\", \n       classification_report(target_2020_final, pred, digits=4))\n\n# print the area under the curve\nprint(f'AUC: {roc_auc_score(target_2020_final,pred_prob[:,1])}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ROC Curve\nfrom yellowbrick.classifier import ROCAUC\nvisualizer = ROCAUC(lr_model_final, classes=[\"nao_atraso\", \"atraso\"])\n\nvisualizer.fit(cat_vars_ohe_2019_final, target_2019_final)         \nvisualizer.score(cat_vars_ohe_2020_final, target_2020_final)                                   \nvisualizer.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ROC curve is a measure of performance for classification problems at different thresholds.\n\nThrough the ROC curves above, we can see that our model has, in general, a true positive rate for class 0.0 higher than for our target category. However, with low thresholds, we observed a high TPR for our target class with a low FPR, that is, at a low threshold our model would be able to distinguish the positive class with greater success.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from yellowbrick.classifier import precision_recall_curve\nviz = precision_recall_curve(lr_model_final, cat_vars_ohe_2019_final, target_2019_final, cat_vars_ohe_2020_final, target_2020_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot above shows us the trade-off between precision and recall. If we seek a greater recall, to favor our positive class, we will sacrifice the precision of the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Manipulating the threshold\nMany classifiers use a decision_function to generate a positive class score or the predict_proba function to compute the probability of the positive class. If the score or probability is higher than the threshold then the positive class is selected, otherwise, the negative class is selected.\n\nHere in our case we manipulate the threshold, use the value of -3, compared to the score generated by the decision_function (distance to a 'hyperplane' of equal probabilities for the classes) and we obtained a recall of 0.94, that is, we would hit 94% of our positive class, however, at the cost of having a precision of only 18%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scores_final = lr_model_final.decision_function(cat_vars_ohe_2020_final)\ny_pred_recall = (y_scores_final > -3)\n\nprint(f'New precision: {precision_score(target_2020_final,y_pred_recall)}')\nprint(f'New recall: {recall_score(target_2020_final,y_pred_recall)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nWe conclude that the variable 'DEP_DEL15' is the most relevant for understanding flights that arrive late to their destination. Acting on the causes of flight departure delays would already prevent any delays. Modeling only with categorical variables we can obtain an AUC of 0.88 on test data, additionally, we observed that we could achieve 94% accuracy on the positive class by manipulating the threshold of our model, at the cost of classifying many non-delays as delays.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}