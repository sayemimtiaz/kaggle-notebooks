{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About:\nFirst thing i want to mention is that the notebook is not complete yet! most part not gonna changed but improvement is under the construction! Anyway i create the notebook base on kaggle micro courses from python till feature engineering, reading some blog posts from toward data science(all are mentioned at reference links), kaggle(mentioned too) and roaming in scikit learn APIs and LightGB documents. so if you want to get familiar with EDA, first part is for you (mostly data analysis works covering but not complete yet!). if you want to go further part 2 is covered machine learning task with building 3 hot regression models, and a light gradient boosting regressor with simple coding in beginner level, and even if you are ok with feature engineering part 3 is the thing for you!(some part are lack of suitable description that in next weeks they'll be complete.)\nhope you find it valuable."},{"metadata":{},"cell_type":"markdown","source":"# Introduction:\nWe are going through an inevitable concept in every data science project that is \"Exploaratory Data Analysis\" aka \"EDA\".\nI'm not going to extend it so much as i prepare some useful references at the end of this notebook, but just in case to get familiar with the following cells, you should know EDA is an approach or philosophy for data analysis that employs a variety of techniques (mostly graphical) to make the part of your analysis work more reasonable and prepare insights for machine learning part(for more detail check The first reference link at the end of The notebook)\nIn the middle of our work, we will goinig to do machine learning task with 4 model in beginner level.\nAt last ending up our work with using of feature engineering concepts for improving and boosting our model."},{"metadata":{},"cell_type":"markdown","source":"# 1.EDA"},{"metadata":{},"cell_type":"markdown","source":"# 1.1.Import Required Modules"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder \nimport sklearn.metrics\nfrom sklearn.metrics import mean_absolute_error\nimport category_encoders as ce\nfrom sklearn.decomposition import TruncatedSVD\nimport itertools\nfrom sklearn.feature_selection import SelectKBest, f_regression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.2.Loading data to df (DataFrame!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"nyc_filepath = '/kaggle/input/nyc-property-sales/'\nos.listdir(nyc_filepath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(nyc_filepath + 'nyc-rolling-sales.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.3.Checking the types of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"types = list()\nfor idx, col in enumerate(df.columns):\n    dtype = str(df[col].dtype)\n    types.append([col, dtype, idx])\n    \nprint('[COLUMN NAME, '+'COLUMN DATA TYPE, '+'COLUMN POSITION IN DATA FRAME]') \ntypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.4.Dropping unnecessary columns!( if they're already exist!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First thing that seems utterly inutile! is 'Unnamed: 0' column\ndf.drop(['Unnamed: 0'], axis=1, inplace=True)\n\n# Second one was 'EASE-MENT', cause it is full empty column!\ndf.drop(['EASE-MENT'], axis=1, inplace=True)\n\n# Third one for this notebook is 'SALE TIME'. it's truely an important Feature if we would to trace sale dates!\ndf.drop(['SALE DATE'], axis=1, inplace=True)\n\n# Fourth one is 'APARTMENT NUMBER'. not going to use of it!\ndf.drop(['APARTMENT NUMBER'], axis=1, inplace=True)\n\n# Fifth one is 'BUILDING CLASS CATEGORY' becuase we have a better of it named 'BUILDING CLASS AT PRESENT'!\ndf.drop(['BUILDING CLASS CATEGORY'], axis=1, inplace=True)\n\n# Sisxth one is 'BUILDING CLASS AT PRESENT' as same reason as above one, but i'm not sure yet in next version it could be not drop!\ndf.drop(['BUILDING CLASS AT PRESENT'], axis=1, inplace=True)\n\n# Seventh one is 'TAX CLASS AT PRESENT', not sure again it could be a positive result in our model in next version i decide about it!\ndf.drop(['TAX CLASS AT PRESENT'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we can see the modified data frame after 7 columns dropped!"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.5.Renaming columns and index if we have better names for them!"},{"metadata":{},"cell_type":"markdown","source":"in following cells i'm going to fix index of data frame with help of 'ADDRESS' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.ADDRESS.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making index for our df with one of it's column called 'ADDRESS'! \n# in below code we added new column named 'Property' at the end of our data frame\ndf['Property'] = [idx for idx in range(df.ADDRESS.count())]\n\n# here we fixed the index issue!\ndf.index = df['Property']\n\n# aftre that we just drop the added column from end of our data frame!\ndf = df.drop(['Property'], axis=1)\n\nprint('Now it seems we have a neat dataframe so far!')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.6.Trying to find duplicate rows and drop them! (if they're already exist!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Investigating for Duplicated rows!\nprint(f\"Original dataframe shape: {df.shape}\")\n\nduplicate_rows = df[df.duplicated()]\nprint(f\"Duplicated rows: {duplicate_rows.shape}\")\n\n# Dropping the duplicates \ndf = df.drop_duplicates()\nprint('Modified dataframe shape: {}'.format(df.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.7.Detecting missing values!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Detecting_missing_vals(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum() / data.isnull().count() * 100)\n    tpt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    \n    types=list()\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n        \n    tpt['Types'] = types\n    \n    return(np.transpose(tpt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Detecting_missing_vals(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems likely strange feeling which as a 'Data Scientist' you getting a data set without any missing values!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# By the way this is another tool that pandas gives it to you for checking missing values!\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.8.Detecting and Hunting Outliers with plotting tools and statistical concepts respectively!"},{"metadata":{},"cell_type":"markdown","source":"In statistics, an outlier is an observation point that is distant from other observations.\nThe above definition suggests that outlier is something which is separate or different from the crowd.(more detail in reference link)"},{"metadata":{},"cell_type":"markdown","source":"In descriptive statistics, a box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points.\nAbove definition suggests, that if there is an outlier it will plotted as point in boxplot but other population will be grouped together and display as boxes. Let’s try and see it ourselves."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=df['BLOCK'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"you can see those points beyond the about 13,800... they're outliers or noise, in the next weeks i'm going to extend them much more!"},{"metadata":{},"cell_type":"markdown","source":"from now on we track the 'SALE PRICE' column in three following cells and try to do some changes!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['SALE PRICE']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here i'm replacing '-' with 0, as i had no idea about those values i decided to replace them with just bunch of zeros!\nbut we could fill those values with the mean of the SALE PRICE column, In next version of this notebook i'm going to make the right decision for them! don't worry"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['SALE PRICE'].replace(to_replace='-', value=0, regex=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here i change the type of data in 'SALE PRICE' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['SALE PRICE'] = df['SALE PRICE'].astype('float64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding Outliers with 'Interquartile Range' score or IQR score! (midspread, middle 50% and H_spread are the other names!)\nSimply it differences lower 25% from upper 75% and returns the 50%(between upper and lower quartiles) or in other words central value's dispersion of the features.\nIt measures like the Standard deviation and Variance but in a more robust way!"},{"metadata":{"trusted":true},"cell_type":"code","source":"Q1 = df.quantile(0.25) # first quartile\nQ3 = df.quantile(0.75) # third quartile\nIQR = Q3 - Q1\nprint(\"We can see central value of our columns:\\n\")\nprint(IQR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Original df rows\ndf.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modified df rows\ndf = df[ ~( (df < (Q1 - 1.5 * IQR) ) | (df > (Q3 + 1.5 * IQR) ) ).any(axis=1)]\ndf.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Because of IQR outlier detection technique we gave a messy index for rows!\ndf.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reindex data set as well as shown in below!\ndf.index = [idx for idx in range(df.ADDRESS.count())]\ndf.index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.9.Plotting and Explorating the data"},{"metadata":{},"cell_type":"markdown","source":"we are depicting some concepts and information about data with lovely plots!"},{"metadata":{},"cell_type":"markdown","source":"in below we're plotting the number of properties that are belonging to each five boroughs"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['BOROUGH'] ,  palette='Set3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"listing five new york boroughs for iterating in following after that"},{"metadata":{"trusted":true},"cell_type":"code","source":"Five_boroughs = [\"Manhattan\", \"Bronx\", \"Brooklyn\", \"Queens\", \"Staten Island\"]\n\nfor idx, bor in zip(df['BOROUGH'].unique(), Five_boroughs):\n    print('There are #{} Properties belong to {} Borough.'.format( df[ df['BOROUGH'] == idx].shape[0], bor ),'\\n' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I just went a step further for percentage of properties belong to each borough!\nfor idx, bor in zip(df['BOROUGH'].unique(), Five_boroughs):\n    print(\"{:.2f}% of Properties belong to {} Borough.\"\\\n          .format( 100 * df[ df['BOROUGH'] == idx ].shape[0] / df.shape[0] , bor), '\\n'  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Baking a \"Pie\" Based on Preceding Cells !"},{"metadata":{"trusted":true},"cell_type":"code","source":"Boroughs = ['Manhattan', 'Bronx', 'Brooklyn', 'Queens', 'Staten Island']\n\nProperties_Per_Each_Borough = [10751, 5196, 16201, 21252, 7407] # we already had them!\n\ncolors = ['#BE61CA', '#7CDDDD', 'yellowgreen', 'orange', 'purple'] # based on https://www.nycgo.com/neighborhoods-boroughs/about-nyc-five-boroughs/\n\nexplode = (0.1, 0, 0.1, 0.1, 0)\n\nplt.pie(Properties_Per_Each_Borough, explode= explode, labels = Boroughs, colors=colors,\\\n        startangle=90, autopct= '%1.1f%%', shadow=True)\n    \nprint(\"\\nPlotting Pie Chart:\")\nplt.title('Proportioning of Properties Per each Borough')\n\nplt.axis('equal')\n\nplt.legend(['Manhat', 'Bronx', 'Brooklyn', 'Queens', 'Staten Island'],\n           loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From now on within two or three cells in following trying to get some question's answers!\nprices = 0\nfor price in df['SALE PRICE']:\n    prices += price\n    \nAVG_PRICE = prices / len(df['SALE PRICE'])\nprint('Average price of Properties Sale Prices is: {:.2f}$'.format(AVG_PRICE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are {:.2f}% of Properties with more than half of the Average Price!\"\\\n      .format(100 * df[ df['SALE PRICE'] > 478475.24 ].shape[0] / df.shape[0] )  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we could get average(mean) price and other stats features with pandas built_in methods as shown in below!"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_price = df['SALE PRICE'].max()\nmin_price = df['SALE PRICE'].min()\ncount = df['SALE PRICE'].count()\nmean_price = df['SALE PRICE'].mean()\nprint('max_price: {}$\\nmin_price: {}$\\ncount: {}\\nmean_price: {:.2f}$'.format(max_price, min_price, count, mean_price))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing for settle in \"Rise of Modelling Age\" with some basic information gathering about our numerical and categorical data!"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = [col for col in df.columns if df[col].dtype == 'object']\nnumerical_cols = [col for col in df.columns if df[col].dtype in ['int64', 'float64']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I suprised as much as you when saw 'LAND SQUARE FEET' and 'GROSS SQUARE FEET' in categorical features!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['LAND SQUARE FEET']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Familiar pattern used for getting rid of '-' values!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# this part could slightly changed in next version of the notebook\ndf['LAND SQUARE FEET'].replace(to_replace='-', value=0, regex=True, inplace=True)\ndf['LAND SQUARE FEET'] = df['LAND SQUARE FEET'].astype('int64')\ndf['LAND SQUARE FEET']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['GROSS SQUARE FEET'].replace(to_replace='-', value=0, regex=True, inplace=True)\ndf['GROSS SQUARE FEET'] = df['GROSS SQUARE FEET'].astype('int64')\ndf['GROSS SQUARE FEET']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.Machine learning task (Regression models in beginner level!)"},{"metadata":{},"cell_type":"markdown","source":"Model Processing Without Cross Validation ! (shouldn't account it's result as a considerable one!) "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['SALE PRICE'], axis=1)\ny = df['SALE PRICE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size = 0.8, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_full.shape, X_valid_full.shape, y_train.shape, y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I just redefined them with more compacted names!"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = [col for col in X_train_full.columns if X_train_full[col].dtype == 'object']\nnum_cols = [col for col in X_train_full.columns if X_train_full[col].dtype in ['int64', 'float64']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_full['NEIGHBORHOOD'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_full['ADDRESS'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_full['BUILDING CLASS AT TIME OF SALE'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Choosing those columns with cardinality less than 250 ( cardinality refers to the number of unique values for each categorical column )"},{"metadata":{"trusted":true},"cell_type":"code","source":"low_car_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 250  and\n                X_train_full[cname].dtype == 'object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = num_cols + low_car_cols # get numerical and categorical columns together\nX_train = X_train_full[cols].copy() # cloning \nX_valid = X_valid_full[cols].copy() # cloning","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"label encoder in one of the basic categorical encoder that maps a unique number to each categorical data for a column"},{"metadata":{"trusted":true},"cell_type":"code","source":"lb_enc = LabelEncoder()\n\nlabel_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()\n\nfor col in low_car_cols:\n    label_X_train[col] = lb_enc.fit_transform(X_train[col])\n    label_X_valid[col] = lb_enc.transform(X_valid[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In three following cells i'm using tree based regressor models!"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nRFR = RandomForestRegressor(n_estimators=170, random_state=0, n_jobs=4)\nRFR_Trained = RFR.fit(label_X_train, y_train)\npreds = RFR_Trained.predict(label_X_valid)\nprint('RFR Mean Absolute Error: {:.2f}'.format(mean_absolute_error(y_valid, preds)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean Absolute Error (MAE): measures the average magnitude of the errors in a set of predictions, so here in above cell our random forest regressor can tell the price of a property by around 306k usd error from it's real value!"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nGBR = GradientBoostingRegressor(n_estimators=600, learning_rate=0.055, random_state=0)\nGBR_Trained = GBR.fit(label_X_train, y_train)\npreds = GBR.predict(label_X_valid)\nprint('GBR Mean Absolute Error: {:.2f}'.format(mean_absolute_error(preds, y_valid)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remind that there is a trade off between n_estimators and learning_rate according to scikit learn documents!"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nXGBR = XGBRegressor(n_estimators=700, n_jobs=6)\n\nXGBR_Trained = XGBR.fit(label_X_train, y_train,\n                        early_stopping_rounds=20,\n                        eval_set=[(label_X_valid, y_valid)],\n                        verbose=False)\n\npredictions = XGBR.predict(label_X_valid)\nprint(\"XGBR Mean Absolute Error: {:.2f}\".format((mean_absolute_error(predictions, y_valid))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model Processing With Cross Validation! (More Real and Calculable Results!)"},{"metadata":{},"cell_type":"markdown","source":"From now on we are trying to give this chance to our model to facing with and learning from all of the data set each time as a\nsubset part of the whole with cross validation concept, so we can trust more to the result of this kind of model processing\nthan we just met before!"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cross_valid = X.copy()\ny_cross_valid = y.copy()\n\nenc = LabelEncoder()\nX_cross_valid[low_car_cols] = X_cross_valid[low_car_cols].apply(enc.fit_transform)\nX_cross_valid = X_cross_valid[num_cols].join(X_cross_valid[low_car_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"our model is going to take five experiments based on 'cv' parameter we set to 5 in below! each time one part is gonna be a validation set and all other gonna be train sets... and each time we have a result from our model.. at the end we can see the average of those five experiments!"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nRF_scores = -1 * cross_val_score(RFR_Trained, X_cross_valid, y_cross_valid,\n                                 cv=5,\n                                 scoring='neg_mean_absolute_error')\n\nprint(\"Random Forest Regressor cross validation MAE scores:\\n\",RF_scores)\nprint(f\"Average Random Forest MAE score (across experiments):\\n{RF_scores.mean():.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance of our model get worse than before it's considerable but this way we can more trust to our model's performance!"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nGB_scores = -1 * cross_val_score(GBR_Trained, X_cross_valid, y_cross_valid,\n                                 cv=5,\n                                 scoring='neg_mean_absolute_error')\n\nprint(\"Gradient Boosting cross validation MAE scores:\\n\",GB_scores)\nprint(f\"Average Gradient Boosting MAE score (across experiments):\\n{GB_scores.mean():.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nXGB_scores = -1 * cross_val_score(XGBR_Trained, X_cross_valid, y_cross_valid,\n                                  cv=5,\n                                  scoring='neg_mean_absolute_error')\n\nprint(\"Extended Gradient Boosting cross validation MAE scores:\\n\",XGB_scores)\nprint(f\"Average XGB MAE score (across experiments):\\n{XGB_scores.mean():.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LightGB models are fascinating! So Lets Do it together!"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = cat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_encoder = LabelEncoder()\nbase_encoded = df[cat_features].apply(base_encoder.fit_transform)\nbase_encoded.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features = ['BOROUGH', 'BLOCK', 'LOT', 'ZIP CODE','RESIDENTIAL UNITS', 'COMMERCIAL UNITS', 'TOTAL UNITS',\\\n                'LAND SQUARE FEET', 'GROSS SQUARE FEET', 'YEAR BUILT', 'TAX CLASS AT TIME OF SALE', 'SALE PRICE']\ndf1 = df.copy()\ndf1 = df1[num_features].join(base_encoded) # here we have a data frame with numerical and encoded categorical features together","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'm going to use of slicing for splitting data with 80, 10 and 10 percent for train, valid and test data sets respectively!\n\nvalid_fraction = 0.1\nvalid_size = int(len(df1) * valid_fraction)\n\ntrain = df1[:-2 * valid_size] # 80% of data!\n\nvalid = df1[-2 * valid_size:-valid_size] # 10% of what is remained!\n\ntest = df1[-valid_size:] # last 10% of that!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Your Promissing Land! in Modelling\nFor more information about LightGBs check this out! (https://lightgbm.readthedocs.io/en/latest/Parameters.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfeature_cols = train.columns.drop('SALE PRICE')\n\ndtrain = lgb.Dataset(train[feature_cols], label=train['SALE PRICE'])\ndvalid = lgb.Dataset(valid[feature_cols], label=valid['SALE PRICE'])\n\n\nparams = {\n        'boosting_type': 'gbdt',\n        'objective': 'mae',\n        'metric': 'rmse',\n        'max_depth':7,\n        'learning_rate': 0.2555,\n        'num_leaves': 31,\n        'verbose': 1, \n        }\n\nn_estimators = 100\n\nbst = lgb.train(params, dtrain, n_estimators, valid_sets=[dvalid],\\\n                early_stopping_rounds=15, verbose_eval=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_pred = bst.predict(test[feature_cols])\ntest_score = mean_absolute_error(test['SALE PRICE'], test_pred)\n\nprint(f\"Test MAE: {test_score:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is amazing that we get a considerable! improving as the error for our Model! first model get around 306k usd error for a property sale price but this one is more better than that!"},{"metadata":{},"cell_type":"markdown","source":"# 3.Feature Enginnering:"},{"metadata":{},"cell_type":"markdown","source":"# 3.1.Baseline Model\n"},{"metadata":{},"cell_type":"markdown","source":"The combination of borough, block, and lot forms a unique key for property in New York City. Commonly called a \"BBL\"."},{"metadata":{"trusted":true},"cell_type":"code","source":" df = df.assign(BBL = df['BOROUGH'] + df['BLOCK'] + df['LOT'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = cat_cols\ncat_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_features = df.columns.drop(cat_features)\nnumeric_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = LabelEncoder()\nencoded = df[cat_features].apply(encoder.fit_transform)\nencoded.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df[numeric_features].join(encoded)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_fraction = 0.1\nvalid_size = int(len(data) * valid_fraction)\n\ntrain = data[:-2 * valid_size]\n\nvalid = data[-2 * valid_size:-valid_size]\n\ntest = data[-valid_size:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"in cell below we are trace back the target distribution in train, valid and test sets respectively!"},{"metadata":{"trusted":true},"cell_type":"code","source":"for each in [train, valid, test]:\n    print(f\"SALE PRICE mean dispersion = {each['SALE PRICE'].mean():.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = train.columns.drop('SALE PRICE')\n\ndtrain = lgb.Dataset(train[feature_cols], label=train['SALE PRICE'])\ndvalid = lgb.Dataset(valid[feature_cols], label=valid['SALE PRICE'])\n\nparams = {\n        'boosting_type': 'gbdt',\n        'objective': 'mae',\n        'metric': 'rmse', \n        'learning_rate': 0.2555,\n        'num_leaves': 31,\n        'max_depth':7,\n        'verbose': 0, \n        }\n    \nn_estimators = 100\n\nbst = lgb.train(params, dtrain, n_estimators, valid_sets=[dvalid],\\\n                early_stopping_rounds=15, verbose_eval=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypred = bst.predict(test[feature_cols])\nscore = mean_absolute_error(test['SALE PRICE'], ypred)\n\nprint(f\"Test MAE: {score:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.2.Categorical Encodings:\nLabel Encoding:"},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_data = data[numeric_features].join(encoded)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we are just rewriting some of our previous work but more concrete!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data_splits(dataframe, valid_fraction=0.1):\n    valid_fraction = 0.1\n    valid_size = int(len(dataframe) * valid_fraction)\n\n    train = dataframe[ :-2 * valid_size]\n    # valid size == test size, last two sections of the data\n    valid = dataframe[-2 * valid_size : -valid_size]\n    test = dataframe[-valid_size:]\n    \n    return train, valid, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(train, valid):\n    \n    feature_cols = train.columns.drop('SALE PRICE')\n\n    dtrain = lgb.Dataset(train[feature_cols], label=train['SALE PRICE'])\n    dvalid = lgb.Dataset(valid[feature_cols], label=valid['SALE PRICE'])\n\n    params = {\n            'boosting_type': 'gbdt',\n            'objective': 'mae',\n            'metric': 'rmse',\n            'max_depth': 7,\n            'num_leaves':31,\n            'learning_rate': 0.2555,\n            'verbose': 0, \n            }\n\n    n_estimators = 100\n\n    bst = lgb.train(params, dtrain, n_estimators, valid_sets=[dvalid],\\\n                    early_stopping_rounds=15, verbose_eval=False)\n    \n    valid_pred = bst.predict(valid[feature_cols])\n    valid_score = mean_absolute_error(valid['SALE PRICE'], valid_pred)\n    \n    print(f\"Validation MAE score: {valid_score:.2f}\")\n    return bst","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We focused on Model validation stage Error!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training a model on the baseline data\ntrain, valid, _ = get_data_splits(baseline_data)\nbst = train_model(train, valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Count Encoding:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count Encoding\ncat_features = ['NEIGHBORHOOD', 'ADDRESS', 'BUILDING CLASS AT TIME OF SALE']\n\ncount_enc = ce.CountEncoder()\n\ncount_encoded = count_enc.fit_transform(df[cat_features])\n\ndata = baseline_data.join(count_encoded.add_suffix(\"_count\"))\n\n# Training a model on the baseline data\ntrain, valid, test = get_data_splits(data)\nModel_CouEnc = train_model(train, valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we got worse result!"},{"metadata":{},"cell_type":"markdown","source":"Target Encoding (Regression Task!):"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target Encoding\ncat_features = ['NEIGHBORHOOD', 'ADDRESS', 'BUILDING CLASS AT TIME OF SALE']\n\n# Create the encoder itself\ntarget_enc = ce.TargetEncoder(cols=cat_features)\n\ntrain, valid, _ = get_data_splits(data)\n\n# Fit the encoder using the categorical features and target, so we avoid data(target) leakage!\ntarget_enc.fit(train[cat_features], train['SALE PRICE'])\n\n# Transform the features, rename the columns with _target suffix, and join to dataframe\ntrain = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\nvalid = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))\n\nModel_TarEnc = train_model(train, valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"worse result than two preceding cells!!"},{"metadata":{},"cell_type":"markdown","source":"CatBoost Encoding (another kind of Target Encoding!) :"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CatBoost Encoding\ncat_features = ['NEIGHBORHOOD', 'ADDRESS', 'BUILDING CLASS AT TIME OF SALE']\n\ntarget_enc = ce.CatBoostEncoder(cols=cat_features)\n\ntrain, valid, _ = get_data_splits(data)\n\n# Fit the encoder using the categorical features and target, so we avoid data(target) leakage!\ntarget_enc.fit(train[cat_features], train['SALE PRICE'])\n\n# Transform the features, rename the columns with _target suffix, and join to dataframe\ntrain = train.join(target_enc.transform(train[cat_features]).add_suffix('_cb'))\nvalid = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_cb'))\n\nMode_CatBoost = train_model(train, valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"i don't know how should i explain these crazy results, but if you had one just let me know! (by the way i'm not professional!)"},{"metadata":{},"cell_type":"markdown","source":"you're going to see a spectacular encoder! just watch this beast and it's performance!"},{"metadata":{},"cell_type":"markdown","source":"Reference Link! https://www.kaggle.com/matleonard/encoding-categorical-features-with-svd"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Singular Value Decomposition (SVD)\nsvd = TruncatedSVD(n_components=3)\n\ntrain, valid, _ = get_data_splits(baseline_data)\n\n# Create a sparse matrix with cooccurence counts\npair_counts = train.groupby(['NEIGHBORHOOD', 'BUILDING CLASS AT TIME OF SALE'])['SALE PRICE'].count()\npair_counts.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a data frame with .unstack method\npair_matrix = pair_counts.unstack(fill_value=0)\npair_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd_encoding = pd.DataFrame(svd.fit_transform(pair_matrix))\nsvd_encoding.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded = svd_encoding.reindex(baseline_data['NEIGHBORHOOD']).set_index(baseline_data.index)\nencoded.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join encoded feature to the dataframe, with info in the column names\ndata_svd = baseline_data.join(encoded.add_prefix(\"NEIGHBORHOOD_BUILDING CLASS_SVD_\"))\ndata_svd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, valid, _ = get_data_splits(data_svd)\nSVD_model = train_model(train, valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it was better than preceding nightmares!"},{"metadata":{},"cell_type":"markdown","source":"# 3.3.Feature Generation:"},{"metadata":{},"cell_type":"markdown","source":"Interactions : as kaggle instructor mentioned it is just combination of numerical features together as new feature feeding for our model "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Interactions\ninteractions = pd.DataFrame(index = df.index)\n\nfor col1, col2 in itertools.combinations(cat_features, 2):\n    new_col_name = '_'.join([col1, col2])\n    # Convert to strings and combine\n    new_values = df[col1].map(str) + \"_\" + df[col2].map(str)\n    label_enc = LabelEncoder()\n    interactions[new_col_name] = label_enc.fit_transform(new_values)\n    \nbaseline_data = baseline_data.join(interactions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, valid, _ = get_data_splits(baseline_data)\nFeatGen_model = train_model(train, valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we still can hold out our hope"},{"metadata":{},"cell_type":"markdown","source":"# 3.4.Feature Selection:"},{"metadata":{},"cell_type":"markdown","source":"Univariate Feature Selection : The simplest and fastest methods are based on univariate statistical tests. For each feature, measure how strongly the target depends on the feature using a statistical test like  χ2 (chi_square)  or ANOVA."},{"metadata":{},"cell_type":"markdown","source":"With SelectKBest, we define the number of features to keep, based on the score from the scoring function. Using .fit_transform(features, target) we get back an array with only the selected features."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = baseline_data.columns.drop('SALE PRICE')\n\n# Keep Top 5 features\nselector = SelectKBest(f_regression, k=5)\nX_new = selector.fit_transform(baseline_data[feature_cols], baseline_data['SALE PRICE'])\nX_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# due to leakage issue we do it again but this time using of train set after data Splitting!\nfeature_cols = baseline_data.columns.drop('SALE PRICE')\ntrain, valid, _ = get_data_splits(baseline_data)\n\n# Keep 5 features\nselector = SelectKBest(f_regression, k=5)\n\nX_new = selector.fit_transform(train[feature_cols], train['SALE PRICE'])\nX_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can use .inverse_transform to get back an array with the shape of the original data.\nso now we can trace back of all rejected columns by their main charactrestic which is 0 values!"},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=train.index, \n                                 columns=feature_cols)\nselected_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A DataFrame with the same index and columns as the training set,\nbut all the dropped columns are filled with zeros,so\nnon_zero variance is our margin! for detecting Top 5 best Features from the others!"},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_columns = selected_features.columns[selected_features.var() != 0]\n\n# Get the valid dataset with the selected features.\nvalid[selected_columns].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm going to use of data_svd(best encoded data among others we saw!) to splitting and then take those best features that just selected based\non univariate stat test with hope of better result!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, valid, _ = get_data_splits(data_svd)\n\ntrain = train[selected_columns].join(df['SALE PRICE'])\nvalid = valid[selected_columns].join(df['SALE PRICE'])\n\nmodel_B5F = train_model(train, valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"L1 regularization Feature Selection (for Regression Task!) :"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\n\ntrain, valid, _ = get_data_splits(baseline_data)\n\nX, y = train[train.columns.drop(\"SALE PRICE\")], train['SALE PRICE']\n\n# I didn't pass any parameter, so it is as naive as it can be!\nlasso = Lasso().fit(X, y)\nmodel = SelectFromModel(lasso, prefit=True)\n\nX_new = model.transform(X)\nX_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get back the kept features as a DataFrame with dropped columns as all 0s\nselected_features = pd.DataFrame(model.inverse_transform(X_new), \n                                 index=X.index,\n                                 columns=X.columns)\n\n# Dropped columns have values of all 0s, keep other columns \nselected_columns = selected_features.columns[selected_features.var() != 0]\nselected_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reference Links: "},{"metadata":{},"cell_type":"markdown","source":"# EDA explanation and example\n# https://www.itl.nist.gov/div898/handbook/eda/section1/eda11.htm\n# https://towardsdatascience.com/exploratory-data-analysis-in-python-c9a77dfa39ce\n# https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba\n# https://www.kaggle.com/gpreda/santander-eda-and-prediction\n# Categorical Encoding Techniques_ SVD\n# https://www.kaggle.com/matleonard/encoding-categorical-features-with-svd"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}