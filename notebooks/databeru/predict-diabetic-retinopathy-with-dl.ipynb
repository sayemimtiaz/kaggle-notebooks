{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predict Diabetic Retinopathy\n## \\# Class activation heatmap for image classification\n## \\# Grad-CAM class activation visualization\n\n![eyes](https://i.imgur.com/hC8FYup.png)","metadata":{}},{"cell_type":"markdown","source":"# Table of contents\n\n[<h3>1. Load and transform the dataset</h3>](#1)\n\n[<h3>2. Load the Images with a generator</h3>](#2)\n\n[<h3>3. Test 27 canned architectures with pre-trained weights</h3>](#3)\n\n[<h3>4. Train the model MobileNetV2</h3>](#4)\n\n[<h3>5. Visualize the result</h3>](#5)\n\n[<h3>6. Class activation heatmap for image classification</h3>](#6)\n\n[<h3>7. Using a two-class model (DR and No_DR)</h3>](#7)\n\n## About the Data\nThe images consist of gaussian filtered retina scan images to detect diabetic retinopathy. The original dataset is available at APTOS 2019 Blindness Detection. These images are resized into 224x224 pixels so that they can be readily used with many pre-trained deep learning models.\n\nAll of the images are already saved into their respective folders according to the severity/stage of diabetic retinopathy using the train.csv file provided. You will find five directories with the respective images:\n\n- 0 - No_DR\n- 1 - Mild\n- 2 - Moderate\n- 3 - Severe\n- 4 - Proliferate_DR\n\nThe dataset contains an export.pkl file which is a ResNet34 model trained on the dataset for 20 epochs using the FastAI library.\n\n## Acknowledgements\n[APTOS 2019 Blindness Detection](https://www.kaggle.com/c/aptos2019-blindness-detection/overview)","metadata":{}},{"cell_type":"markdown","source":"# 1. Load and transform the dataset<a class=\"anchor\" id=\"1\"></a>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display, Markdown\nimport matplotlib.cm as cm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport tensorflow as tf\nfrom time import perf_counter\nimport seaborn as sns\n\ndef printmd(string):\n    # Print with Markdowns    \n    display(Markdown(string))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:08:43.523675Z","iopub.execute_input":"2021-08-27T05:08:43.523975Z","iopub.status.idle":"2021-08-27T05:08:49.21617Z","shell.execute_reply.started":"2021-08-27T05:08:43.5239Z","shell.execute_reply":"2021-08-27T05:08:49.215313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_dir = Path('../input/diabetic-retinopathy-224x224-gaussian-filtered/gaussian_filtered_images/gaussian_filtered_images')\n\n# Get filepaths and labels\nfilepaths = list(image_dir.glob(r'**/*.png'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:08:49.219302Z","iopub.execute_input":"2021-08-27T05:08:49.219555Z","iopub.status.idle":"2021-08-27T05:08:52.026614Z","shell.execute_reply.started":"2021-08-27T05:08:49.219531Z","shell.execute_reply":"2021-08-27T05:08:52.025556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\n# Concatenate filepaths and labels\nimage_df = pd.concat([filepaths, labels], axis=1)\n\n# Shuffle the DataFrame and reset index\nimage_df = image_df.sample(frac=1).reset_index(drop = True)\n\n# Show the result\nimage_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:08:52.030364Z","iopub.execute_input":"2021-08-27T05:08:52.030677Z","iopub.status.idle":"2021-08-27T05:08:52.062175Z","shell.execute_reply.started":"2021-08-27T05:08:52.030649Z","shell.execute_reply":"2021-08-27T05:08:52.061071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"# Display some pictures of the dataset with their labels\nfig, axes = plt.subplots(nrows=3, ncols=4, figsize=(10, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(image_df.Filepath[i]))\n    ax.set_title(image_df.Label[i])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:08:52.063731Z","iopub.execute_input":"2021-08-27T05:08:52.064079Z","iopub.status.idle":"2021-08-27T05:08:53.103833Z","shell.execute_reply.started":"2021-08-27T05:08:52.064045Z","shell.execute_reply":"2021-08-27T05:08:53.096692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the number of pictures of each category\nvc = image_df['Label'].value_counts()\nplt.figure(figsize=(9,5))\nsns.barplot(x = vc.index, y = vc, palette = \"rocket\")\nplt.title(\"Number of pictures of each category\", fontsize = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:08:53.106994Z","iopub.execute_input":"2021-08-27T05:08:53.107322Z","iopub.status.idle":"2021-08-27T05:08:53.265882Z","shell.execute_reply.started":"2021-08-27T05:08:53.10729Z","shell.execute_reply":"2021-08-27T05:08:53.26498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Load the Images with a generator<a class=\"anchor\" id=\"2\"></a>","metadata":{}},{"cell_type":"code","source":"def create_gen():\n    # Load the Images with a generator and Data Augmentation\n    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n        validation_split=0.1\n    )\n\n    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n    )\n\n    train_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='training',\n        rotation_range=30, # Uncomment to use data augmentation\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    val_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='validation',\n        rotation_range=30, # Uncomment to use data augmentation\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    test_images = test_generator.flow_from_dataframe(\n        dataframe=test_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=False\n    )\n    \n    return train_generator,test_generator,train_images,val_images,test_images","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:08:53.269044Z","iopub.execute_input":"2021-08-27T05:08:53.269404Z","iopub.status.idle":"2021-08-27T05:08:53.27968Z","shell.execute_reply.started":"2021-08-27T05:08:53.269367Z","shell.execute_reply":"2021-08-27T05:08:53.278804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Test 27 canned architectures with pre-trained weights<a class=\"anchor\" id=\"3\"></a>\n\nMore info about the architectures under: [Module: tf.keras.applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications?hl=enhttps://www.tensorflow.org/api_docs/python/tf/keras/applications?hl=en)","metadata":{}},{"cell_type":"code","source":"def get_model(model):\n# Load the pretained model\n    kwargs =    {'input_shape':(224, 224, 3),\n                'include_top':False,\n                'weights':'imagenet',\n                'pooling':'avg'}\n    \n    pretrained_model = model(**kwargs)\n    pretrained_model.trainable = False\n    \n    inputs = pretrained_model.input\n\n    x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n\n    outputs = tf.keras.layers.Dense(5, activation='softmax')(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:08:53.281127Z","iopub.execute_input":"2021-08-27T05:08:53.281634Z","iopub.status.idle":"2021-08-27T05:08:53.293289Z","shell.execute_reply.started":"2021-08-27T05:08:53.281599Z","shell.execute_reply":"2021-08-27T05:08:53.292424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate in train and test data\ntrain_df, test_df = train_test_split(image_df, train_size=0.9, shuffle=True, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:08:53.29484Z","iopub.execute_input":"2021-08-27T05:08:53.295275Z","iopub.status.idle":"2021-08-27T05:08:53.307106Z","shell.execute_reply.started":"2021-08-27T05:08:53.295239Z","shell.execute_reply":"2021-08-27T05:08:53.306295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dictionary with the models\nmodels = {\n    \"DenseNet121\": {\"model\":tf.keras.applications.DenseNet121, \"perf\":0},\n    \"MobileNetV2\": {\"model\":tf.keras.applications.MobileNetV2, \"perf\":0},\n    \"DenseNet169\": {\"model\":tf.keras.applications.DenseNet169, \"perf\":0},\n    \"DenseNet201\": {\"model\":tf.keras.applications.DenseNet201, \"perf\":0},\n    \"EfficientNetB0\": {\"model\":tf.keras.applications.EfficientNetB0, \"perf\":0},\n    \"EfficientNetB1\": {\"model\":tf.keras.applications.EfficientNetB1, \"perf\":0},\n    \"EfficientNetB2\": {\"model\":tf.keras.applications.EfficientNetB2, \"perf\":0},\n    \"EfficientNetB3\": {\"model\":tf.keras.applications.EfficientNetB3, \"perf\":0},\n    \"EfficientNetB4\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"EfficientNetB5\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"EfficientNetB6\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"EfficientNetB7\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"InceptionResNetV2\": {\"model\":tf.keras.applications.InceptionResNetV2, \"perf\":0},\n    \"InceptionV3\": {\"model\":tf.keras.applications.InceptionV3, \"perf\":0},\n    \"MobileNet\": {\"model\":tf.keras.applications.MobileNet, \"perf\":0},\n    \"MobileNetV2\": {\"model\":tf.keras.applications.MobileNetV2, \"perf\":0},\n    \"MobileNetV3Large\": {\"model\":tf.keras.applications.MobileNetV3Large, \"perf\":0},\n    \"MobileNetV3Small\": {\"model\":tf.keras.applications.MobileNetV3Small, \"perf\":0},\n    \"NASNetMobile\": {\"model\":tf.keras.applications.NASNetMobile, \"perf\":0},\n    \"ResNet101\": {\"model\":tf.keras.applications.ResNet101, \"perf\":0},\n    \"ResNet101V2\": {\"model\":tf.keras.applications.ResNet101V2, \"perf\":0},\n    \"ResNet152\": {\"model\":tf.keras.applications.ResNet152, \"perf\":0},\n    \"ResNet152V2\": {\"model\":tf.keras.applications.ResNet152V2, \"perf\":0},\n    \"ResNet50\": {\"model\":tf.keras.applications.ResNet50, \"perf\":0},\n    \"ResNet50V2\": {\"model\":tf.keras.applications.ResNet50V2, \"perf\":0},\n    \"VGG16\": {\"model\":tf.keras.applications.VGG16, \"perf\":0},\n    \"VGG19\": {\"model\":tf.keras.applications.VGG19, \"perf\":0},\n    \"Xception\": {\"model\":tf.keras.applications.Xception, \"perf\":0}\n}\n\n# Create the generators\ntrain_generator,test_generator,train_images,val_images,test_images=create_gen()\nprint('\\n')\n\n# Fit the models\nfor name, model in models.items():\n    \n    # Get the model\n    m = get_model(model['model'])\n    models[name]['model'] = m\n    \n    start = perf_counter()\n    \n    # Fit the model\n    history = m.fit(train_images,validation_data=val_images,epochs=1,verbose=1)\n    \n    # Sav the duration, the train_accuracy and the val_accuracy\n    duration = perf_counter() - start\n    duration = round(duration,2)\n    models[name]['perf'] = duration\n    print(f\"{name:20} trained in {duration} sec\")\n    \n    val_acc = history.history['val_accuracy']\n    models[name]['val_acc'] = [round(v,4) for v in val_acc]\n    \n    train_acc = history.history['accuracy']\n    models[name]['train_accuracy'] = [round(v,4) for v in train_acc]","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-27T05:08:53.309015Z","iopub.execute_input":"2021-08-27T05:08:53.309305Z","iopub.status.idle":"2021-08-27T05:21:18.788087Z","shell.execute_reply.started":"2021-08-27T05:08:53.309281Z","shell.execute_reply":"2021-08-27T05:21:18.786959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame with the results\nmodels_result = []\n\nfor name, v in models.items():\n    models_result.append([ name, \n                          models[name]['train_accuracy'][-1],\n                          models[name]['val_acc'][-1], \n                          models[name]['perf']])\n    \ndf_results = pd.DataFrame(models_result, \n                          columns = ['model','train_accuracy','val_accuracy','Training time (sec)'])\ndf_results.sort_values(by='val_accuracy', ascending=False, inplace=True)\ndf_results.reset_index(inplace=True,drop=True)\ndf_results","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:21:18.78956Z","iopub.execute_input":"2021-08-27T05:21:18.790218Z","iopub.status.idle":"2021-08-27T05:21:18.82012Z","shell.execute_reply.started":"2021-08-27T05:21:18.790176Z","shell.execute_reply":"2021-08-27T05:21:18.819098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,5))\nsns.barplot(x = 'model', y = 'train_accuracy', data = df_results)\nplt.title('Accuracy on the Training Set (after 1 epoch)', fontsize = 15)\nplt.ylim(0,1)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:21:18.821747Z","iopub.execute_input":"2021-08-27T05:21:18.822176Z","iopub.status.idle":"2021-08-27T05:21:19.209522Z","shell.execute_reply.started":"2021-08-27T05:21:18.822136Z","shell.execute_reply":"2021-08-27T05:21:19.20852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,5))\nsns.barplot(x = 'model', y = 'val_accuracy', data = df_results)\nplt.title('Accuracy on the Validation Set (after 1 epoch)', fontsize = 15)\nplt.ylim(0,1)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:21:19.211249Z","iopub.execute_input":"2021-08-27T05:21:19.211605Z","iopub.status.idle":"2021-08-27T05:21:19.558054Z","shell.execute_reply.started":"2021-08-27T05:21:19.211558Z","shell.execute_reply":"2021-08-27T05:21:19.557179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,5))\nsns.barplot(x = 'model', y = 'Training time (sec)', data = df_results)\nplt.title('Training time for each model in sec', fontsize = 15)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:21:19.559959Z","iopub.execute_input":"2021-08-27T05:21:19.560323Z","iopub.status.idle":"2021-08-27T05:21:19.917562Z","shell.execute_reply.started":"2021-08-27T05:21:19.560287Z","shell.execute_reply":"2021-08-27T05:21:19.916598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Train the model MobileNetV2<a class=\"anchor\" id=\"4\"></a>","metadata":{}},{"cell_type":"code","source":"# Load the pretained model\npretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\n\npretrained_model.trainable = False","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:21:19.919133Z","iopub.execute_input":"2021-08-27T05:21:19.919671Z","iopub.status.idle":"2021-08-27T05:21:20.878588Z","shell.execute_reply.started":"2021-08-27T05:21:19.919632Z","shell.execute_reply":"2021-08-27T05:21:20.87777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(5, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    batch_size = 32,\n    epochs=10,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=2,\n            restore_best_weights=True\n        )\n    ]\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-27T05:21:20.881541Z","iopub.execute_input":"2021-08-27T05:21:20.882353Z","iopub.status.idle":"2021-08-27T05:23:31.909453Z","shell.execute_reply.started":"2021-08-27T05:21:20.882313Z","shell.execute_reply":"2021-08-27T05:23:31.908596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\nplt.title(\"Accuracy\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:23:31.911255Z","iopub.execute_input":"2021-08-27T05:23:31.911633Z","iopub.status.idle":"2021-08-27T05:23:32.075229Z","shell.execute_reply.started":"2021-08-27T05:23:31.911596Z","shell.execute_reply":"2021-08-27T05:23:32.074262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history)[['loss','val_loss']].plot()\nplt.title(\"Loss\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:23:32.076851Z","iopub.execute_input":"2021-08-27T05:23:32.07721Z","iopub.status.idle":"2021-08-27T05:23:32.216343Z","shell.execute_reply.started":"2021-08-27T05:23:32.07717Z","shell.execute_reply":"2021-08-27T05:23:32.215312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Visualize the result<a class=\"anchor\" id=\"5\"></a>","metadata":{}},{"cell_type":"code","source":"results = model.evaluate(test_images, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:23:32.217927Z","iopub.execute_input":"2021-08-27T05:23:32.218267Z","iopub.status.idle":"2021-08-27T05:23:36.040311Z","shell.execute_reply.started":"2021-08-27T05:23:32.218231Z","shell.execute_reply":"2021-08-27T05:23:36.039516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"printmd(\" ## Test Loss: {:.5f}\".format(results[0]))\nprintmd(\"## Accuracy on the test set: {:.2f}%\".format(results[1] * 100))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:23:36.041729Z","iopub.execute_input":"2021-08-27T05:23:36.042062Z","iopub.status.idle":"2021-08-27T05:23:36.055486Z","shell.execute_reply.started":"2021-08-27T05:23:36.042027Z","shell.execute_reply":"2021-08-27T05:23:36.054621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the label of the test_images\npred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\n# Display the result\nprint(f'The first 5 predictions: {pred[:5]}')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:23:36.05715Z","iopub.execute_input":"2021-08-27T05:23:36.058013Z","iopub.status.idle":"2021-08-27T05:23:38.127346Z","shell.execute_reply.started":"2021-08-27T05:23:36.057969Z","shell.execute_reply":"2021-08-27T05:23:38.126332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ny_test = list(test_df.Label)\nprint(classification_report(y_test, pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:23:38.128681Z","iopub.execute_input":"2021-08-27T05:23:38.129228Z","iopub.status.idle":"2021-08-27T05:23:38.149783Z","shell.execute_reply.started":"2021-08-27T05:23:38.129186Z","shell.execute_reply":"2021-08-27T05:23:38.148865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncf_matrix = confusion_matrix(y_test, pred, normalize='true')\nplt.figure(figsize = (10,6))\nsns.heatmap(cf_matrix, annot=True, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(y_test)))\nplt.title('Normalized Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:23:38.150998Z","iopub.execute_input":"2021-08-27T05:23:38.15152Z","iopub.status.idle":"2021-08-27T05:23:38.396878Z","shell.execute_reply.started":"2021-08-27T05:23:38.151471Z","shell.execute_reply":"2021-08-27T05:23:38.395997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display some pictures of the dataset with their labels and the predictions\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:23:38.398477Z","iopub.execute_input":"2021-08-27T05:23:38.398835Z","iopub.status.idle":"2021-08-27T05:23:39.616473Z","shell.execute_reply.started":"2021-08-27T05:23:38.398797Z","shell.execute_reply":"2021-08-27T05:23:39.615174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Class activation heatmap for image classification<a class=\"anchor\" id=\"6\"></a>\n## Grad-CAM class activation visualization\n*Code adapted from keras.io*","metadata":{}},{"cell_type":"code","source":"def get_img_array(img_path, size):\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n    array = tf.keras.preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size \"size\"\n    array = np.expand_dims(array, axis=0)\n    return array\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load the original image\n    img = tf.keras.preprocessing.image.load_img(img_path)\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    superimposed_img.save(cam_path)\n\n    # Display Grad CAM\n#     display(Image(cam_path))\n    \n    return cam_path\n    \npreprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\ndecode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n\nlast_conv_layer_name = \"Conv_1\"\nimg_size = (224,224)\n\n# Remove last layer's softmax\nmodel.layers[-1].activation = None","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:23:39.617953Z","iopub.execute_input":"2021-08-27T05:23:39.618428Z","iopub.status.idle":"2021-08-27T05:23:39.645071Z","shell.execute_reply.started":"2021-08-27T05:23:39.618386Z","shell.execute_reply":"2021-08-27T05:23:39.644291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the part of the pictures used by the neural network to classify the pictures\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 10),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    img_path = test_df.Filepath.iloc[i]\n    img_array = preprocess_input(get_img_array(img_path, size=img_size))\n    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n    cam_path = save_and_display_gradcam(img_path, heatmap)\n    ax.imshow(plt.imread(cam_path))\n    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:23:39.646659Z","iopub.execute_input":"2021-08-27T05:23:39.647376Z","iopub.status.idle":"2021-08-27T05:23:42.619329Z","shell.execute_reply.started":"2021-08-27T05:23:39.647233Z","shell.execute_reply":"2021-08-27T05:23:42.618251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Using a two-class model (DR and No_DR)<a class=\"anchor\" id=\"7\"></a>\n\nAs we have seen before, the prediction are very accurate to predict if someone has Diabetic Retinopathy or not. Nevertheless, it is not good at predicting the intensity of Diabetic Retinopathy when it is present. Maybe because it is subjective to the doctor to rate the intensity degree and different doctors don't have the same way to evaluate them. Maybe there are other factors taken in consideration to evaluate the intensity, which are independent from the pictures. As this point, without knowing more about the data, we can only speculate.\n\nIn this chapter, we'll reduce the label to a two-class model, because we can imagine that the most important part of this analysis is to find out if someone has Diabetic Retinopathy or not.","metadata":{}},{"cell_type":"code","source":"# Map the labels to have only \"No_DR\" and \"DR\"\nimage_df_red = image_df.copy()\nimage_df_red['Label'] = image_df_red['Label'].apply(lambda x: x if x == 'No_DR' else 'DR')\nimage_df_red.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:23:42.620691Z","iopub.execute_input":"2021-08-27T05:23:42.621199Z","iopub.status.idle":"2021-08-27T05:23:42.635664Z","shell.execute_reply.started":"2021-08-27T05:23:42.621156Z","shell.execute_reply":"2021-08-27T05:23:42.634624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the number of pictures of each category\nvc = image_df_red['Label'].value_counts()\nplt.figure(figsize=(9,5))\nsns.barplot(x = vc.index, y = vc, palette = \"rocket\")\nplt.title(\"Number of pictures of each category\", fontsize = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:23:42.637068Z","iopub.execute_input":"2021-08-27T05:23:42.637734Z","iopub.status.idle":"2021-08-27T05:23:42.817226Z","shell.execute_reply.started":"2021-08-27T05:23:42.637694Z","shell.execute_reply":"2021-08-27T05:23:42.816425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate in train and test data\ntrain_df, test_df = train_test_split(image_df_red, train_size=0.9, shuffle=True, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:23:42.818388Z","iopub.execute_input":"2021-08-27T05:23:42.818971Z","iopub.status.idle":"2021-08-27T05:23:42.82566Z","shell.execute_reply.started":"2021-08-27T05:23:42.818929Z","shell.execute_reply":"2021-08-27T05:23:42.824804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the generators\ntrain_generator,test_generator,train_images,val_images,test_images=create_gen()\n\n# Load the pretained model\npretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\n\npretrained_model.trainable = False\n\n\ninputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(2, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    batch_size = 32,\n    epochs=10,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-27T05:23:42.827056Z","iopub.execute_input":"2021-08-27T05:23:42.827615Z","iopub.status.idle":"2021-08-27T05:25:03.2826Z","shell.execute_reply.started":"2021-08-27T05:23:42.827562Z","shell.execute_reply":"2021-08-27T05:25:03.281846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\nplt.title(\"Accuracy\")\nplt.show()\n\npd.DataFrame(history.history)[['loss','val_loss']].plot()\nplt.title(\"Loss\")\nplt.show()\n\nresults = model.evaluate(test_images, verbose=0)\n\nprintmd(\" ## Test Loss: {:.5f}\".format(results[0]))\nprintmd(\"## Accuracy on the test set: {:.2f}%\".format(results[1] * 100))\nprint('\\n')\n\n# Predict the label of the test_images\npred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\n# Display the result\nprint(f'The first 5 predictions: {pred[:5]}')\n\nfrom sklearn.metrics import classification_report\ny_test = list(test_df.Label)\nprint(classification_report(y_test, pred))\n\ncf_matrix = confusion_matrix(y_test, pred, normalize='true')\nplt.figure(figsize = (10,6))\nsns.heatmap(cf_matrix, annot=True, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(y_test)))\nplt.title('Normalized Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T05:25:03.290299Z","iopub.execute_input":"2021-08-27T05:25:03.29062Z","iopub.status.idle":"2021-08-27T05:25:07.298554Z","shell.execute_reply.started":"2021-08-27T05:25:03.29057Z","shell.execute_reply":"2021-08-27T05:25:07.297644Z"},"trusted":true},"execution_count":null,"outputs":[]}]}