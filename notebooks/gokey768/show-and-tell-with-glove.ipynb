{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Imports\nThe following packages are imported:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import *\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nimport IPython\n\nfrom PIL import Image\nfrom PIL import ImageColor\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nfrom PIL import ImageOps\n\n\nimport tempfile\nfrom six.moves.urllib.request import urlopen\nfrom six import BytesIO\n\nimport os\nimport pathlib\n\n# For measuring the inference time.\nimport time\n\n# Check available GPU devices.\nprint(\"The following GPU devices are available: %s\" % tf.test.gpu_device_name())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data\n- data_dir is the path to the images and the `results.csv`\n- image_dir is the path exculsively to the images\n- csv_file is the path to the `results.csv` file"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '../input/flickr-image-dataset/flickr30k_images'\nimage_dir = f'{data_dir}/flickr30k_images'\ncsv_file = f'{data_dir}/results.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we read the csv file as a dataframe and make some observations from it.\nFor a quick EDA we are going to \n- check the shape of the dataframe\n- check the names of the columns\n- find out the unique image names there are"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(csv_file, delimiter='|')\n\nprint(f'[INFO] The shape of dataframe: {df.shape}')\nprint(f'[INFO] The columns in the dataframe: {df.columns}')\nprint(f'[INFO] Unique image names: {len(pd.unique(df[\"image_name\"]))}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick observation here is to see that the dataframe has `158915` elements but only `31783` image names. This means that there is a duplicacy involved. On further inspection we will see that each image has 5 unique captions attached to it ($31783\\times 5=158915$)\n\nWhile looking into the dataframe I found out that `19999` had some messed up entries. This has led me to manually change the entries in that row."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = ['image_name', 'comment_number', 'comment']\ndel df['comment_number']\n\n# Under scrutiny I had found that 19999 had a messed up entry\ndf['comment'][19999] = ' A dog runs across the grass .'\n\n# Image names now correspond to the absolute position\ndf['image_name'] = image_dir+'/'+df['image_name']\n\n# <start> comment <end>\ndf['comment'] = \"<start> \"+df['comment']+\" <end>\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffle the dataframe\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SIZE = len(df)\n\ntrain_size = int(0.7* SIZE) \nval_size = int(0.1* SIZE)\ntest_size = int(0.2* SIZE)\n\ntrain_size, val_size, test_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Enter different indices.\nindex = 10000\n\nimage_name = df['image_name'][index]\ncomment = df['comment'][index]\n\nprint(comment)\n\nIPython.display.Image(filename=image_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Handling\n- Defined the size of the vocab which is `5000`.\n- Initialized the Tokenizer class.\n    - Standardized (all to lower case)\n    - Filters the punctuations\n    - Splits the text\n    - Creates the vocabulary (`<start>, <end> and <unk>` is defined)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choose the top 5000 words from the vocabulary\ntop_k = 5000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                  oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we fit the `tokenizer` object on the captions. This helps in the updation of the vocab that the `tokenizer` object might have.\n\nIn the first iteration the vocabulary does not start from `0`. Both the dictionaries have 1 as the key or value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# build the vocabulary\ntokenizer.fit_on_texts(df['comment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is a sanity check function\ndef check_vocab(word):\n    i = tokenizer.word_index[word]\n    print(f\"The index of the word: {i}\")\n    print(f\"Index {i} is word {tokenizer.index_word[i]}\")\n    \ncheck_vocab(\"<end>\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are padding the sentences so that each of the sentences are of the same length."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip -q glove.6B.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = 100\n\npath_to_glove_file = os.path.join(\n    os.path.expanduser(\"~\"), \"/kaggle/working/glove.6B.100d.txt\"\n)\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))\n\n\nword_index = tokenizer.word_index\nembedding_matrix = np.zeros((len(word_index), EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the tokenized vectors\ntrain_seqs = tokenizer.texts_to_sequences(df['comment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cap_vector.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cap = cap_vector[:train_size] \nval_cap = cap_vector[train_size:train_size+val_size]\ntest_cap = cap_vector[train_size+val_size:]\n\ntrain_cap.shape, val_cap.shape, test_cap.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cap_train_ds = tf.data.Dataset.from_tensor_slices(train_cap)\ncap_val_ds = tf.data.Dataset.from_tensor_slices(val_cap)\ncap_test_ds = tf.data.Dataset.from_tensor_slices(test_cap)\n\ncap_train_ds, cap_val_ds, cap_test_ds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Handling\n- Load the image\n- decode jpeg\n- resize\n- standardize"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_img(image_path):\n    # Decode file as strings\n    img = tf.io.read_file(image_path)\n    # Decode image, (0 to 1)\n    img = tf.image.decode_jpeg(img)\n    # Resize the image to specific height and width\n    img = tf.image.resize(img, (299, 299))\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_name = df['image_name'].values\n\ntrain_img = img_name[:train_size] \nval_img = img_name[train_size:train_size+val_size]\ntest_img = img_name[train_size+val_size:]\n\ntrain_img.shape, val_img.shape, test_img.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_train_ds = tf.data.Dataset.from_tensor_slices(train_img).map(load_img)\nimg_val_ds = tf.data.Dataset.from_tensor_slices(val_img).map(load_img)\nimg_test_ds = tf.data.Dataset.from_tensor_slices(test_img).map(load_img)\n\nimg_train_ds, img_val_ds, img_test_ds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Joint Dataset\nWe have the individual datasets with us. We need to zip the img and the cap dataset now."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = tf.data.Dataset.zip((img_train_ds, cap_train_ds))\nval_ds = tf.data.Dataset.zip((img_val_ds, cap_val_ds))\ntest_ds = tf.data.Dataset.zip((img_test_ds, cap_test_ds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cache, prefecth and batch the dataset\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 256\n\ntrain_ds = train_ds.shuffle(42).batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.shuffle(42).batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.shuffle(42).batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sanity check for the division of datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"for img, cap in test_ds.take(1):\n    print(img.shape)\n    print(cap.shape)\n    plt.imshow(img[0])\n    for c in cap[0]:\n        print(tokenizer.index_word[c.numpy()],end=' ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model\n### Show (Encoder)\n- InceptionV3: This will act like the feature extractor\n- Use an FC layer to extract the features of the image\n- The features will be used as the initial hidden state for the RNN\n\n### Tell (Decoder)\n- The initial hidden state is used\n- The text is embedded\n- Usage of an LSTM to produce softmax on the vocab\n- Loss with captions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some global variables\nEMBEDDIN_DIM = 100\nVOCAB_SIZE = 5000\nUNITS_RNN = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNN_Encoder(tf.keras.Model):\n    \n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        self.embedding_dim = embedding_dim\n        \n    def build(self, input_shape):\n        self.inception = tf.keras.applications.InceptionV3(include_top=False,\n                                                           weights='imagenet')\n        self.inception.trainable=False\n        self.gap = GlobalAveragePooling2D()\n        self.fc = Dense(units=self.embedding_dim,\n                        activation='sigmoid')\n        \n    def call(self, x):\n        x = self.inception(x)\n        x = self.gap(x)\n        x = self.fc(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the CNN\nencoder = CNN_Encoder(EMBEDDIN_DIM)\nfor image, caption in train_ds.take(1):\n    print(encoder(image).shape)\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RNN_Decoder(tf.keras.Model):\n    def __init__(self, embedding_dim, units, vocab_size):\n        super(RNN_Decoder, self).__init__()\n        self.units = units\n        self.embedding_dim = embedding_dim\n        self.vocab_size = vocab_size\n        self.embedding = Embedding(len(word_index),\n                                   EMBEDDING_DIM,\n                                   weights=[embedding_matrix],\n                                   input_length=80,\n                                   trainable=True)\n    \n    def build(self, input_shape):\n        self.gru = GRU(units=self.units,\n                       return_sequences=True,\n                       return_state=True)\n        self.fc1 = Dense(self.units)\n        self.fc2 = Dense(self.vocab_size)\n\n    def call(self, x, initial_zero=False):\n        # x, (batch, 512)\n        # hidden, (batch, 256)\n        if initial_zero:\n            initial_state = decoder.reset_state(batch_size=x.shape[0])\n            output, state = self.gru(inputs=x,\n                                     initial_state=initial_state)\n        else:\n            output, state = self.gru(inputs=x)\n        # output, (batch, 256)\n        x = self.fc1(output)\n        x = self.fc2(x)\n        \n        return x, state\n    \n    def embed(self, x):\n        return self.embedding(x)\n    \n    def reset_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the RNN\ndecoder = RNN_Decoder(embedding_dim=EMBEDDIN_DIM,\n                      units=UNITS_RNN,\n                      vocab_size=VOCAB_SIZE)\nfor image, caption in train_ds.take(1):\n    features = tf.expand_dims(encoder(image),1) # (batch, 1, 128)\n    em_words = decoder.embed(caption)\n    x = tf.concat([features,em_words],axis=1)\n    print(x.shape)\n    predictions, state = decoder(x, True)\n    print(predictions.shape)\n    print(state.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = CNN_Encoder(EMBEDDIN_DIM)\ndecoder = RNN_Decoder(embedding_dim=EMBEDDIN_DIM,\n                      units=UNITS_RNN,\n                      vocab_size=VOCAB_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use `Adam` as the optimizer.\n\nThe loss is `SparseCategoricalCrossentropy`, because here it would be inefficient to use one-hot-encoders are the ground truth. We will also use mask to help mask the `<pad>` so that we do not let the sequence model learn to overfit on the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(img_tensor, target):\n    # img_tensor (batch, 224,224,3)\n    # target     (batch, 80)\n    loss = 0\n    with tf.GradientTape() as tape:\n        features = tf.expand_dims(encoder(img_tensor),1) # (batch, 1, 128)\n        em_words = decoder.embed(target)\n        x = tf.concat([features,em_words],axis=1)\n        predictions, _ = decoder(x, True)\n\n        loss = loss_function(target[:,1:], predictions[:,1:-1,:])\n\n    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n    gradients = tape.gradient(loss, trainable_variables)\n\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_plot = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 50\n\nfor epoch in range(EPOCHS):\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(train_ds.take(20)):\n        loss = train_step(img_tensor, target)\n        total_loss += loss\n        if batch % 10 == 0:\n            print (f'Epoch {epoch} Batch {batch} Loss {loss.numpy():.4f}')\n    # storing the epoch end loss value to plot later\n    loss_plot.append(loss)\n    print(f'Loss: {total_loss/20:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(loss_plot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"img, cap = next(iter(test_ds.take(1)))\n\nimg[0].shape, cap[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = tf.expand_dims(img[0],0)\ncap = tf.expand_dims(cap[0],0)\n\nimg.shape, cap.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = tf.expand_dims(encoder(img),1) # (1, 1, 128)\n\nfeature.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For the image\nprediction, _ = decoder(feature, True)\nprint(prediction.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word = tf.reshape(tokenizer.word_index['<start>'], shape=(1,1))\nem_words = decoder.embed(word)\nprint(em_words.shape)\n\nprediction, _ = decoder(em_words)\nidx = tf.random.categorical(tf.squeeze(prediction,1), 1)[0][0].numpy()\nword = tokenizer.index_word[idx]\nprint(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nwhile word != '<end>':\n    print(word, end=\" \")\n    if count > 20:\n        break\n    word_int = tf.reshape(tokenizer.word_index[word], shape=(1,1))  \n    em_words = decoder.embed(word_int)\n    prediction, _ = decoder(em_words)\n    idx = tf.random.categorical(tf.squeeze(prediction,1), 1)[0][0].numpy()\n    word = tokenizer.index_word[idx]\n    count += 1\n\nplt.imshow(image[0])\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}