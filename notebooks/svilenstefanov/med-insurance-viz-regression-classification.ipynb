{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This is my first data science project. Some of the approaches might not be the best ones, but I am still learning and always open to any feedback.\n\nWe will first start with analysis of the features in the dataset and the relationship between them, to get better idea of which features should and which shouldn't be taken into account to determine the dependent variable. We will also run statistical tests to compare different sets of data.\nThen, we will split the data into training set and test set, will apply machine learning models on the training data and will try to:\n* predict the insurance charges on the test data;\n* identify clusters;\n* predict the region of the customer (this non-binary variable was chosen just for training purposes)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# allow multi-outputs\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input = pd.read_csv(\"../input/insurance/insurance.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data analysis\n\n### Let's peek into the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input.head(5)\ninput.info()\ninput.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 7 columns and 1338 observations. Three of the columns are categorical - sex, smoker and region, + children column which takes a value between 0 and 5.\n\nOne of our first tasks would be to observe the distribution of each numeric variable.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Distributions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in input.loc[:,['age','bmi','children','charges']].columns:\n    sns.distplot(a=input[col]);\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution looks normal for 'BMI', but non-normal for 'age', 'children' and 'charges'.\n\nThis means that **for 'age' and 'charges' we should look at the median instead of the mean values**. Their mean values would be highly affected by the number of outliers and will significantly vary from their median. This is well illustrated by the example below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input.groupby('children').charges.agg(['median','mean']).plot(kind='bar', title='Charges by number of children - mean vs median');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Note: 'Children' is a numeric variable in the source data, but we will treat it as a categorical one, due to it being discrete rather than continuous (it is countable, accepting only integer values between 0 and 5).*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## How the charges relate to the other categorical variables?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.title('Charges by smoker status')\nsns.violinplot(data=input, x='smoker', y='charges');\nplt.show()\n\nplt.figure(figsize=(8,5))\nsns.scatterplot(x=input['bmi'], y=input['charges'], hue=input['smoker']);\nplt.title('Charges by BMI');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's not a surprise that smokers pay more than non-smokers. \n##### However, the charges go up for smokers with the increase in their BMI. Interestingly, for non-smokers such tendency is not observed.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Region-wise analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nplt.title('Charges by region')\nsns.swarmplot(x=input['region'], y=input['charges']);\nplt.show();\n\nplt.figure(figsize=(8,8))\nplt.title('Charges by region')\nsns.boxplot(data=input, x='region', y='charges');\nsns.swarmplot(data=input, x='region', y='charges', size=2, color=\".3\")\nplt.show();\n\ninput.groupby('region').charges.agg(['mean','median']).sort_values(by='region', ascending=False).plot(kind=\"bar\", title='Mean and median charges by region');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the charts above it seems that in Southeast region the charges are a bit higher than in the other regions... But wait, this is only if we talk about the mean. If we have a look at the median values, the picture is different - the charges are higher in the Northeast. Two conclusions:\n* The charges are 'normally' highest in the Northeast region\n* If there are specific, outlying cases, the charges in the Southeast tend to go up much higher than in the other regions\n\nThis could be a result of specific demographic characteristics in these regions. For example, let's observe the smoker status by region.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt_smoker_byRegion = input.groupby(['region', 'smoker']).agg({'smoker':'count'})\ncnt_byRegion = input.groupby('region').agg({'smoker':'count'})\ncnt_smoker_byRegion.div(cnt_byRegion, level='region')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### The proportion of smokers by region is highest (25%) for Southeast. Smokers are significantly less proportionally in the other regions, where they are between 17.9% and 20.7%. \nThis is a good explanation why the charges in Southeast are high. However, this doesn't explain anything about the Northeast. As we said, charges are non-normally distributed, so we should be more interested in median than in mean.\n\nWe can additionally explore the relation between region and BMI:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.boxplot(data=input, x='region', y='bmi', hue='smoker');\nplt.title(\"BMI by region and smoker status\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The boxplot for BMI gives one more finding:\n##### the Southeast region has the highest average BMI for both smokers and non-smokers amongst all regions! It still doesn't say anything about the Northeast, though!\n\nThis gives us no other chance but to see how this competition would look like if we exclude the outliers from the picture.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=input.loc[(((input.region=='southeast') & (input.charges<42000)) | ((input.region=='northeast') & (input.charges<35000)))],\n            x='smoker', y='charges', hue='region');\nplt.show();\n\nsns.boxplot(data=input.loc[(((input.region=='southeast') & (input.charges<42000)) | ((input.region=='northeast') & (input.charges<35000)))],\n            x='sex', y='charges', hue='region');\nplt.show();\n\nsns.boxplot(data=input.loc[(((input.region=='southeast') & (input.charges<42000)) | ((input.region=='northeast') & (input.charges<35000)))],\n            x='children', y='charges', hue='region')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input.loc[(((input.region=='southeast') & (input.charges<42000)) | ((input.region=='northeast') & (input.charges<35000)))].groupby(['region','children']).charges.median().plot(kind=\"bar\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's run the Kruskal-Wallis test to compare the median charges by region.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import kruskal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sw = input.loc[input.region=='southwest','charges']\nse = input.loc[input.region=='southeast','charges']\nne = input.loc[input.region=='northeast','charges']\nnw = input.loc[input.region=='northwest','charges']\n\nkruskal(sw, se, ne, nw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although we can see some differences, it seems that they are not statistically significant, because the P-value is greater than 0.05.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fg = sns.FacetGrid(data=input, row='region', col='children');\nfg.map(plt.scatter, 'bmi', 'charges');\nfg.add_legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No other findings here. We can humbly leave this with the conclusion that:\n##### Ð¢he charges are just naturally high in Northeast, although for some specific cases (e.g. smoking, high BMI, many children) the charges tend to go higher in Southeast. However, according to Kruskal-Wallis test, these differences are not statistically significant.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Gender (in)equality?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We'll try below to analyze the charges by gender. We'll also have a look at the gender characteristics - to make sure that the charges have not been affected by the gender alone!\n\nWe'll first notice that there is some difference in charges, which is very slightly higher for females than for males (just to remind - we are working with median):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input.groupby('sex').charges.agg(['mean','median'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nplt.title('Charges by gender')\nsns.boxplot(data=input, x='sex', y='charges');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the average charges are very similar for both genders, it's mainly affected by outliers. It seems that there are more outliers for females, which have dragged their average charges up. We will try below to re-create the boxplot, but by excluding the outliers (females' charges >30k and males' charges > 40k):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nplt.title('Charges by gender')\nsns.boxplot(data=input.loc[(((input.sex == 'female') & (input.charges < 30000)) | ((input.sex == 'male') & (input.charges < 40000))),:], x='sex', y='charges');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input.loc[(((input.sex == 'female') & (input.charges < 30000)) | ((input.sex == 'male') & (input.charges < 40000))),:].groupby('sex').charges.agg(['mean','median'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both the boxplot and the mean/median table show that when we exclude the special outlying cases, male customers have been charged significantly more than females.\nHowever, does this have anything to do with smoking and BMI? Shouldn't we have a look at these two features and how they are distributed gender-wise?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=input, x='smoker', y='charges', hue='sex')\nplt.title(\"Charges by gender and smoker status\")\nplt.show();\n\ninput.groupby(['sex','smoker']).charges.median().sort_values().plot(title='Median charges by gender and smoker status');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, that's an interesting finding:\n* On the one hand, female non-smokers have been charged about 9% more (on median) than male non-smokers\n* On the other hand, female smokers have been charged about 20% less (on median) than male smokers\n\nHow is that possible? Well, let's see how the BMI will fit into the picture.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input.groupby(['sex', 'smoker']).bmi.mean().sort_values().plot(title='Average BMI by gender and smoker status');\n\nsns.lmplot(data=input, x='bmi', y='charges', hue='sex');\nplt.title('Regression line for BMI/Charges, gender-wise')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Here's the explanation: Female smokers have been charged significantly less than male smokers. This might be due to the fact that female smokers have significantly lower BMI than male smokers.\n\nThe steeper line for males on the second chart shows how much quicker their charges go up with the increase of BMI, compared to females.\n\nThe last part here will be to run a Mann-Whitney test to compare the median charges for males and females.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import mannwhitneyu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mannwhitneyu(input.loc[input.sex=='female','charges'].values,input.loc[input.sex=='male','charges'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The P-Value is greater than 0.05, meaning that there is no significant difference between males' and females' charges. Hence, there is no gender inequality.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Does the number of children matter?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nplt.title('Charges by number of children')\nsns.boxplot(data=input, x='children', y='charges');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average charges increase with the increase of the number of children, but decrease for 5 children. However, for people having no children the costs are high - they are comparable to the costs for people having 4 children.\n\nWe'll now run a Kruskal-Wallis test to compare the median charges.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the charges by number of children\nchildren = []\nfor i in range(0,6):\n    children.append(input.loc[input.children==i,'charges'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kruskal(children[0], children[1], children[2], children[3], children[4], children[5])\nkruskal(children[0], children[4])\nkruskal(children[2], children[5])\nkruskal(children[1], children[5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Kruskal-Wallis test for charges by number of children confirms that the children do matter - there is significant difference in the median charges. It also confirms that the charges for 'no children' are similar to those for 4 children; charges for 2 children are similar to 5 children.\n\nHowever, from the boxplots above we can see that there are so many outliers for 0-3 children, therefore any conclusions here might be inappropriate.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = input.iloc[:,0:6]\ny = input.iloc[:,6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Encode categorical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LabelEncoder_X1 = LabelEncoder()\nLabelEncoder_X4 = LabelEncoder()\nLabelEncoder_X5 = LabelEncoder()\nX.iloc[:,1] = LabelEncoder_X1.fit_transform(X.iloc[:,1])\nX.iloc[:,4] = LabelEncoder_X4.fit_transform(X.iloc[:,4])\nX.iloc[:,5] = LabelEncoder_X5.fit_transform(X.iloc[:,5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nct = ColumnTransformer([('one_hot_encoder', OneHotEncoder(categories='auto'), [1,4,5])], remainder='passthrough')\nX = ct.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Escape the dummy variable trap","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X[:,[1,2,4,5,6,8,9,10]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Split into test and training set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train regression models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Multiple Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nregressor1 = LinearRegression()\nregressor1.fit(X_train, y_train)\n\ny_pred1 = regressor1.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Measure the result","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAE1 = mean_absolute_error(y_test, y_pred1)\nMAE1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Polynomial Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor2 = PolynomialFeatures(degree=3)\nX_poly = regressor2.fit_transform(X_train)\nregressor2.fit(X_poly, y_train)\n\nlinreg = LinearRegression()\nlinreg.fit(X_poly,y_train)\n\ny_pred2 = linreg.predict(regressor2.fit_transform(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Measure the result","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAE2 = mean_absolute_error(y_test, y_pred2)\nMAE2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\nregressor3 = DecisionTreeRegressor(random_state = 0)\nregressor3.fit(X_train, y_train)\n\ny_pred3 = regressor3.predict(X_test)\n\nMAE3 = mean_absolute_error(y_test, y_pred3)\nMAE3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Apply Grid Search","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'n_estimators': [2, 3, 5, 10, 15, 20, 30, 50, 75, 100, 500, 1000],\n              'max_leaf_nodes': [5, 10, 20, 35, 50, 100],\n              'random_state': [0]}\ngrid_search = GridSearchCV(estimator = RandomForestRegressor(),\n                           param_grid = parameters,\n                           scoring = 'neg_mean_absolute_error',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nprint(f\"Best MAE: {grid_search.best_score_ * (-1)}\")\nprint(f\"Best parameters: {grid_search.best_params_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor4 = RandomForestRegressor(n_estimators=100, max_leaf_nodes=35, random_state=0)\nregressor4.fit(X_train, y_train)\n\ny_pred4 = regressor4.predict(X_test)\n\nMAE4 = mean_absolute_error(y_test, y_pred4)\nMAE4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\n\nregressor5 = SVR(kernel = 'rbf')\nregressor5.fit(X_train, y_train)\n\ny_pred5 = regressor5.predict(X_test)\n\nMAE5 = mean_absolute_error(y_test, y_pred5)\nMAE5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### This MAE is too high to be true. We have forgotten that SVR model requires feature scaling before fitting!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nsc_X_train = sc_X.fit_transform(X_train)\nsc_y_train = sc_y.fit_transform(y_train.values.reshape(-1,1))\nsc_X_test = sc_X.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'C': [1, 5, 10, 20, 50, 100],\n              'kernel': ['rbf', 'linear', 'poly'],\n              'degree': [2, 3, 4]}\ngrid_search = GridSearchCV(estimator = SVR(),\n                           param_grid = parameters,\n                           scoring = 'neg_mean_absolute_error',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(sc_X_train, sc_y_train)\nprint(f\"Best MAE: {grid_search.best_score_ * (-1)}\")\nprint(f\"Best parameters: {grid_search.best_params_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor5 = SVR(kernel = 'rbf', C = 1)\nregressor5.fit(sc_X_train, sc_y_train)\n\ny_pred5 = regressor5.predict(sc_X_test)\ny_pred5 = sc_y.inverse_transform(y_pred5)\n\nMAE5 = mean_absolute_error(y_test, y_pred5)\nMAE5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'base_score': [0.1, 0.3, 0.5, 0.7, 1, 1.5, 2, 5, 10, 20],\n              'learning_rate': [0.001, 0.005, 0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5],\n              #'booster': ['gbtree', 'linear', 'dart'],\n              'n_estimators': [50, 100, 150, 200, 250, 300, 500, 750, 1000]}\n              #'max_depth': [3, 5]}\ngrid_search = GridSearchCV(estimator = XGBRegressor(),\n                           param_grid = parameters,\n                           scoring = 'neg_mean_absolute_error',\n                           cv = 2,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nprint(f\"Best MAE: {grid_search.best_score_ * (-1)}\")\nprint(f\"Best parameters: {grid_search.best_params_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor6 = XGBRegressor(learning_rate=0.01, n_estimators=300)\nregressor6.fit(X_train, y_train)\ny_pred6 = regressor6.predict(X_test)\nMAE6 = mean_absolute_error(y_test, y_pred6)\nMAE6","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regression Models Summary","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Summarize mean absolute error and R-squared","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = {'Multiple Linear': MAE1, 'Polynomial': MAE2, 'Decision Tree': MAE3,\n           'Random Forest': MAE4, 'SVR': MAE5, 'XGB': MAE6}\n\nfrom sklearn.metrics import r2_score\n\nsummary_R2 = {'Multiple Linear': r2_score(y_test,y_pred1), 'Polynomial': r2_score(y_test,y_pred2),\n             'Decision Tree': r2_score(y_test,y_pred3), 'Random Forest': r2_score(y_test,y_pred4),\n             'SVR ': r2_score(y_test,y_pred5), 'XGBoost': r2_score(y_test,y_pred6)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(15,5))\n\nax = f.add_subplot(121)\nplt.bar(summary.keys(), summary.values(), color='green');\nplt.title(\"Mean absolute error by model (the lower the better)\")\n\nax=f.add_subplot(122)\nplt.plot(summary_R2.keys(), summary_R2.values(), color='cyan');\nplt.title(\"R-Squared coefficient by model (the higher the better)\")\naxes = plt.gca()\naxes.set_ylim([0.5,1])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare MAE to the average value of the dependent variable\nround(100*MAE6/np.mean(y_test),2)\nround(100*MAE6/input.charges.mean(),2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusion: The best regression model that we've created for this dataset is:\n### XGBoost\n##### It returned mean absolute error of 2219, which is a deviation of about 16% of the average dependent variable value. It also has a high R-squared value of 0.90.\n##### Support Vector Machine also did a very good job, but was just narrowly outperformed. Random Forest Regression also did a relatively good job and completes the top 3.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\ninertia = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 0)\n    kmeans.fit(X)\n    inertia.append(kmeans.inertia_)\nplt.plot(range(1, 11), inertia);\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimal number of clusters is 3.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 0)\ny_kmeans = kmeans.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will need to have a look at the data and split into test and training set again, but this time the the dependent variable will be a categorical one - we will try to predict the region of the customer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XClass = input.iloc[:,[0,1,2,3,4,6]]\nyClass = input.iloc[:,5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Encode categorical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"LabelEncoder_XClass1 = LabelEncoder()\nLabelEncoder_XClass4 = LabelEncoder()\nLabelEncoder_yClass = LabelEncoder()\nXClass.iloc[:,1] = LabelEncoder_XClass1.fit_transform(XClass.iloc[:,1])\nXClass.iloc[:,4] = LabelEncoder_XClass4.fit_transform(XClass.iloc[:,4])\nyClass = LabelEncoder_yClass.fit_transform(yClass)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct_XClass = ColumnTransformer([('one_hot_encoder', OneHotEncoder(categories='auto'), [1,4])], remainder='passthrough')\nXClass = ct_XClass.fit_transform(XClass)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XClass = XClass[:,[0,1,3,5,6,7]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XClass_train, XClass_test, yClass_train, yClass_test = train_test_split(XClass, yClass, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_XClass = StandardScaler()\nXClass_train = sc_XClass.fit_transform(XClass_train)\nXClass_test = sc_XClass.transform(XClass_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Classification models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import cohen_kappa_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nclassifier1 = DecisionTreeClassifier(criterion = 'entropy', random_state = 1)\nclassifier1.fit(XClass_train, yClass_train)\nyClass_pred1 = classifier1.predict(XClass_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most of the classification model measurement tools are designed for binary classifications. One of the options for non-binary data is the Cohen Kappa score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kappa1 = cohen_kappa_score(yClass_test, yClass_pred1)\nkappa1\nacc1 = accuracy_score(yClass_test, yClass_pred1)\nacc1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'n_estimators': [50, 100, 150, 200, 250, 300, 500, 750, 1000], \n              'max_leaf_nodes': [5, 10, 20, 30, 50, 100, 300, 600, 800, 1000],\n              'criterion': ['gini', 'entropy'],\n              'max_depth': [3, 4, 6, 8, 9],\n              'random_state': [0]}\ngrid_search = GridSearchCV(estimator = RandomForestClassifier(),\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 2,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(XClass_train, yClass_train)\nprint(f\"Best Accuracy: {grid_search.best_score_}\")\nprint(f\"Best parameters: {grid_search.best_params_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier2 = RandomForestClassifier(n_estimators = 100)\nclassifier2.fit(XClass_train, yClass_train)\nyClass_pred2 = classifier2.predict(XClass_test)\nkappa2 = cohen_kappa_score(yClass_test, yClass_pred2)\nkappa2\nacc2 = accuracy_score(yClass_test, yClass_pred2)\nacc2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM (Kernel)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'kernel': ['rbf'], \n              'C': [1, 3, 5, 9, 10, 20, 25, 30, 40, 50, 75, 100, 200, 500, 1000],\n              'random_state': [0]}\ngrid_search = GridSearchCV(estimator = SVC(),\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(XClass_train, yClass_train)\nprint(f\"Best Accuracy: {grid_search.best_score_}\")\nprint(f\"Best parameters: {grid_search.best_params_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier3 = SVC(kernel = 'poly', C = 1, random_state = 0)\nclassifier3.fit(XClass_train, yClass_train)\nyClass_pred3 = classifier3.predict(XClass_test)\nkappa3 = cohen_kappa_score(yClass_test, yClass_pred3)\nkappa3\nacc3 = accuracy_score(yClass_test, yClass_pred3)\nacc3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier4 = LogisticRegression(random_state = 0)\nclassifier4.fit(XClass_train, yClass_train)\n\n# Predicting the Test set results\nyClass_pred4 = classifier4.predict(XClass_test)\nkappa4 = cohen_kappa_score(yClass_test, yClass_pred4)\nkappa4\nacc4 = accuracy_score(yClass_test, yClass_pred4)\nacc4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nclassifier5 = GaussianNB()\nclassifier5.fit(XClass_train, yClass_train)\nyClass_pred5 = classifier5.predict(XClass_test)\nkappa5 = cohen_kappa_score(yClass_test, yClass_pred5)\nkappa5\nacc5 = accuracy_score(yClass_test, yClass_pred5)\nacc5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-Nearest Neighbors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'n_neighbors': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \n              'p': [1, 2, 3, 5, 10, 20, 30, 50, 70, 90, 120, 150, 200]}\ngrid_search = GridSearchCV(estimator = KNeighborsClassifier(),\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(XClass_train, yClass_train)\nprint(f\"Best Accuracy: {grid_search.best_score_}\")\nprint(f\"Best parameters: {grid_search.best_params_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier6 = KNeighborsClassifier(n_neighbors = 4, metric = 'minkowski', p = 120)\nclassifier6.fit(XClass_train, yClass_train)\nyClass_pred6 = classifier6.predict(XClass_test)\nkappa6 = cohen_kappa_score(yClass_test, yClass_pred6)\nkappa6\nacc6 = accuracy_score(yClass_test, yClass_pred6)\nacc6","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier7 = XGBClassifier(base_score=0.1, n_estimators=2600, max_depth=2)\nclassifier7.fit(XClass_train, yClass_train)\nyClass_pred7 = classifier7.predict(XClass_test)\nkappa7 = cohen_kappa_score(yClass_test, yClass_pred7)\nkappa7\nacc7 = accuracy_score(yClass_test, yClass_pred7)\nacc7","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"summaryClass = {'Decision Tree': kappa1, 'Random Forest': kappa2, 'Kernel SVM': kappa3,\n               'Logistic Regression': kappa4, 'Naive Bayes': kappa5, 'KNN': kappa6, 'XGBoost': kappa7}\n\nclassmodels = []\nfor key in summaryClass.keys():\n    classmodels.append(key)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracies = [acc1, acc2, acc3, acc4, acc5, acc6, acc7]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.barplot(x=classmodels, y=accuracies);\nplt.title('Model Accuracy (the higher the better)')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The best perfomed model is:\n## XGBoost Classification\n##### It received the highest accuracy, much higher than the other models. However, it is still too low - only 44%. Therefore, none of the models is good enough to make reliable predictions for the region of the customers. This section was created just for training purposes as this my first data science project.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}