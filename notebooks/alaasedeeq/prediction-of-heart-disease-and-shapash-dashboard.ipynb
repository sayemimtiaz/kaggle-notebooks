{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <p style=\"background-color:yellow; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 10px 100px; color:black; hight:max\">  Upvote my work if you found it useful.ðŸŽ¯</p>\n\n# <p style=\"background-color:yellow; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 10px 100px; color:black; hight:max\"> Wait a few seconds until the notebook is ready</p>\n\n------------------------------------------\n------------------------------------------\n# <p style=\"background-color:#CCE3F2; font-family:newtimeroman; font-size:175%; text-align:center; border-radius: 15px 50px;\">Prediction of Heart Disease using Machine Learning Classification Algorithms ðŸ©º</p>\n------------------------------------------\n------------------------------------------\n\n<img src=\"https://www.biospace.com/getasset/ccf44985-b555-4b9e-aa80-891715d5540b/\" width=1000 hight=100><br>\n  ","metadata":{}},{"cell_type":"markdown","source":"<b><p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\">Introduction</p>\nThe objective of this project is to build a model to predict whether a person diagnose a heart disease based on pattern extracted from analysing 14 descriptive features found in Cleveland data set from UCI Machine Learning Repository.<br>\nThis project consists of three phases:\n    <ul>\n        <li>Phase I: Focuses on data preprocessing and exploration, as covered in this report.\n        <li>Phase II : The model building, validation and prediction are presented. \n        <li>Phase III : Build an interpretability web app to describe the machine learning model using Shapash dashboard.\n    </ul>\n<b><p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\">We have 13 descriptive features:</p>\n<ul><b>\n    <li>Age : Age of the patient\n    <li>Sex : Sex of the patient\n    <li>exang: exercise induced angina (1 = yes; 0 = no)\n    <li>ca: number of major vessels (0-3)\n    <li>cp : Chest Pain type\n        <ul>\n            <li>Value 1: typical angina\n            <li>Value 2: atypical angina\n            <li>Value 3: non-anginal pain\n            <li>Value 4: asymptomatic\n        </ul>\n    <li>trtbps : resting blood pressure (in mm Hg)\n    <li>chol : cholestoral in mg/dl fetched via BMI sensor\n    <li>oldpeak: ST depression induced by exercise relative to rest \n    <li>fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n    <li>rest_ecg : resting electrocardiographic results\n        <ul>\n            <li>Value 0: normal\n            <li>Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n            <li>Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n        </ul>\n    <li>thalach : maximum heart rate achieved\n    <li>slp : The slope of the peak exercise ST segment with 3 levels:\n        <ul>\n            <li>Up\n            <li>Flat\n            <li>Down\n        </ul>\n    <li>thall: The heart status as retrieved from Thallium test (Categorical with 3 levels-N(normal),FD(fixed defect), RD(reversible defect)\n    <li>target : 0= less chance of heart attack 1= more chance of heart attack\n    </ul>\n\n<p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\">Outline:</p>\n<ul>\n    <li><b><a href=\"#Phase I\">Phase I<a/>\n        <ul>\n            <li><a href=\"#head-1\">Data Pre-processing</a>\n            <li><a href=\"#head-2\">Setup and Basic EDA</a>  \n                <ul>\n                    <li><a href=\"#head-2-1\">Univariate Visualisation</a>\n                        <ul>\n                            <li><a href=\"#head-2-1-1\">Categorical Features</a>\n                                <ul>\n                                    <li><a href=\"#sex\">sex column</a>\n                                    <li><a href=\"#cp\">cp column</a>\n                                    <li><a href=\"#fbs\">fbs column</a>\n                                    <li><a href=\"#restecg\">restecg column</a>\n                                    <li><a href=\"#exng\">exng column</a>\n                                    <li><a href=\"#slp\">slp column</a>\n                                    <li><a href=\"#caa\">caa column</a>\n                                    <li><a href=\"#thall\">thall column</a>\n                                </ul>\n                            <li><a href=\"#head-2-1-2\">Numerical Features</a>\n                                <ul>\n                                    <li><a href=\"#age\">age column</a>\n                                    <li><a href=\"#trtbps\">trtbps column</a>\n                                    <li><a href=\"#chol\">chol column</a>\n                                    <li><a href=\"#thalachh\">thalachh column</a>\n                                    <li><a href=\"#oldpeak\">oldpeak column</a>\n                                </ul>\n                        </ul>\n                    <li><a href=\"#head-2-2\">Multivariate Visualisation</a>\n                        <ul>\n                            <li><a href=\"#sct_mtx\">Scatter Matrix for the data</a>\n                            <li><a href=\"#corr_mtx\">Correlation Matrix for the data</a>\n                            <li><a href=\"#thalach_age\">Maximum heart rate achieve vs Age</a>\n                            <li><a href=\"#oldpeak_age\">ST depression induced by exercise relative to rest vs. Age</a>\n                            <li><a href=\"#trestbps_age\">Resting blood pressure-in mm Hg on admission to the hospital</a>\n                            <li><a href=\"#viloin\">Violin plots for each column for each target grouped by age</a>\n                        </ul>\n                </ul>\n        </ul>\n        <li><b><a href=\"#Phase II\">Phase II:</a></b>\n        <ul>\n            <li><a href=\"#prep_ml\">Prepare the data for the machine learning model.</a>\n            <li><a href=\"#ml_models\">Comparing different Machine learning models.</a>\n            </ul>\n        <li><b><a href=\"#Phase III\">Phase III:</a></b>\n            <ul>\n                <li><a href=\"#shapash\">Build an interpretability web app to describe the machine learning model using Shapash dashboard.</a>\n            </ul>\n           ","metadata":{}},{"cell_type":"markdown","source":"<h1>Phase I</h1>\n<a id=\"Phase I\"></a>","metadata":{}},{"cell_type":"markdown","source":"## Data Pre-processing <a class=\"anchor\" id=\"head-1\"></a>","metadata":{}},{"cell_type":"markdown","source":"### Importing the Libraries","metadata":{}},{"cell_type":"code","source":"#main libraries\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import loguniform\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly \nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport plotly.io as pio\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks as cf\n\n#machine learning libraries:\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold, KFold, cross_validate, cross_val_score\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,make_scorer,classification_report\nfrom sklearn.preprocessing  import StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nimport eli5 # Feature importance evaluation\n\n\n# You can go offline on demand by using\ncf.go_offline() \n\n# To connect java script to your notebook\ninit_notebook_mode(connected=False)\n\n# set some display options:\nplt.rcParams['figure.dpi'] = 100\ncolors = px.colors.qualitative.Prism\npio.templates.default = \"plotly_white\"\n\n# see our files:\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Getting the Data","metadata":{}},{"cell_type":"code","source":"#import the data\ndf = pd.read_csv('../input/heart-disease-uci/heart.csv')\n\n#make a copy of the original data\ndf_orig = df.copy()\n\n#show the head of the data\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the shape of the data\nprint('This data contains {} rows and {} columns'.format(df.shape[0],df.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Exploration/Analysis","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#show the types of columns\ndf.dtypes.to_frame().rename(columns={0:'Column type'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#find the null values in each column\ndf.isnull().sum().to_frame().rename(columns={0:'Null values'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets take a look to the shape of columns\npd.set_option(\"display.float\", \"{:.4f}\".format)\ndf.skew().to_frame().rename(columns={0:'Skewness'}).sort_values('Skewness')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize columns have highest Skewness\nfig, axes = plt.subplots(2,3, figsize=(20, 8));\nfig.suptitle('Highest Skewness', fontsize=20);\n\nsns.kdeplot(df['fbs'], ax=axes[0,0],hue=df['target'],palette='viridis');\nsns.kdeplot(df['sex'], ax=axes[0,1],hue=df['target'],palette='viridis');\nsns.kdeplot(df['thalach'], ax=axes[0,2],hue=df['target'],palette='viridis');\nsns.kdeplot(df['slope'], ax=axes[1,0],hue=df['target'],palette='viridis');\nsns.kdeplot(df['thal'], ax=axes[1,1],hue=df['target'],palette='viridis');\nsns.kdeplot(df['ca'], ax=axes[1,2],hue=df['target'],palette='viridis');","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#describe our data\ndf[df.select_dtypes(exclude='object').columns].describe().\\\nstyle.background_gradient(axis=1,cmap=sns.light_palette('green', as_cmap=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets see the correlation between columns and target column\ncorr = df.corr()['target'].to_frame()\ncorr = corr.rename(columns={'target':'Correlation with target'})\ncorr.sort_values(by='Correlation with target',ascending=False)[1:]\\\n.style.background_gradient(axis=1,cmap=sns.light_palette('Pink', as_cmap=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>We don't have any null values \n<li>Target column have a higher correlation with:\n    <ul>\n        <li>exng with <b> -0.436757</b>\n        <li>cp with <b>0.433798</b>\n        <li>oldpeak with <b>-0.430696</b>\n        <li>thalachh with <b>0.421741</b>\n        <li>caa\twith <b>-0.391724</b>\n        <li>thall with <b>-0.344029</b>","metadata":{}},{"cell_type":"markdown","source":"# Setup and Basic EDA\n<a  id=\"head-2\"></a>","metadata":{}},{"cell_type":"code","source":"# lets define a function to plot a bar plot easily\n\ndef bar_plot(df,x,y,title,colors=None,text=None):\n    fig = px.bar(x=x,\n                 y=y,\n                 text=text,\n                 color_discrete_sequence=px.colors.qualitative.Prism,\n                 data_frame=df,\n                 color=colors,\n                 barmode='group',\n                 template=\"simple_white\")\n    \n    texts = [temp[col].values for col in y]\n    for i, t in enumerate(texts):\n        fig.data[i].text = t\n        fig.data[i].textposition = 'inside'\n\n    fig['layout'].title=title\n\n    for trace in fig.data:\n        trace.name = trace.name.replace('_',' ').capitalize()\n\n    fig.update_yaxes(tickprefix=\"\", showgrid=True)\n\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#replace 0 with 'less chance' and 1 value with 'more chance'\n\ndf['heart attack'] = df['target'].apply(lambda x: 'more chance' if x==1 else 'less chance')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Univariate Visualisation</h1><a id='head-2-1'></a>","metadata":{}},{"cell_type":"markdown","source":"<h2>Categorical Features</h2><a id='head-2-1-1'></a>","metadata":{}},{"cell_type":"code","source":"#let explore our columns\nprint('We have {} columns :\\n{}'.format(len(df.columns),df.columns.values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Sex column \n<a id='sex'></a>","metadata":{}},{"cell_type":"code","source":"temp = pd.DataFrame()\nfor sex in df['sex'].unique().tolist():\n    temp[sex] = df[df['sex']==sex]['heart attack'].value_counts()\n    \ntemp = temp.rename(columns={0:'Female',1:'Male'}).T\n\nfig = make_subplots(rows=2, cols=2, \n                    horizontal_spacing=0.2,\n                    specs=[[{\"type\": \"pie\",'rowspan':2}, {\"type\": \"bar\",'rowspan':2}],\n                           [             None          ,             None           ]])\n\nfig.add_trace(go.Pie(labels=temp.columns,\n                     sort=False,\n                     hole=0.3,\n                     showlegend=False,\n                     direction='clockwise',\n                     domain={'x': [0.15, 0.85], 'y': [0.15, 0.85]},\n                     textinfo='label+percent+text',\n                     values=temp.loc['Female'].values,\n                     textposition='inside',\n                     marker={'line': {'color': 'white', 'width': 1.5}},\n                     name='Female'),1,1)\n\nfig.add_trace(go.Pie(labels=temp.columns,\n                     values=temp.loc['Male'].values,\n                     sort=False,\n                     showlegend=False,\n                     direction='clockwise',\n                     textinfo='label+percent+text',\n                     domain={'x':[0.1,0.8],'y':[0,1]},\n                     hole=0.8,\n                     textposition='outside',\n                     marker={'line': {'color': 'white', 'width': 1.5}},\n                     name='Male'),1,1)\n\n\nfig.add_trace(go.Bar(x=temp.index,\n                     marker_color='salmon',\n                     y=temp['less chance'],\n                     name='less chance',\n                     text=temp['less chance'].values,\n                     textposition='auto'),1,2)\n\nfig.add_trace(go.Bar(x=temp.index,\n                     y=temp['more chance'],\n                     name='more chance',\n                     text=temp['more chance'].values,\n                     textposition='auto'),1,2)\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets make Sex column more clear to visualize\n\ntemp = pd.DataFrame()\nfor sex in df['sex'].unique().tolist():\n    temp[sex] = df[df['sex']==sex]['heart attack'].value_counts()\n\ntemp = temp.rename(columns={0:'Female',1:'Male'}).T.reset_index()\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp,\n         'index',\n         ['Total sum','less chance','more chance'],\n         title='Heart attack chance by sex')\n# temp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>Males are more likely to have a heart disease than Males.","metadata":{}},{"cell_type":"markdown","source":"###### cp (Chest Pain type) column\n<a id='cp'></a>","metadata":{}},{"cell_type":"code","source":"temp = pd.DataFrame()\nfor cp in df['cp'].unique().tolist():\n    temp[cp] = df[df['cp']==cp]['heart attack'].value_counts()\n\ntemp = temp.T.reset_index()\ntemp['index'] = pd.Series(['typical angina','atypical angina','non-anginal pain','asymptomatic'])\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp,\n         'index',\n         ['Total sum','less chance','more chance'],\n         title='Heart attack chance by chest Pain type')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>Most of our samples have a asymptomatic chest pain type.\n<li>Patients who have atypical angina are more likely to have heart attack.\n<li>Most of the healthy people have a non-anginal chest pain type.","metadata":{}},{"cell_type":"markdown","source":"##### fbs (fasting blood sugar > 120 mg/dl) columns\n<a id='fbs'></a>","metadata":{}},{"cell_type":"code","source":"temp = pd.DataFrame()\nfor fb in df['fbs'].unique().tolist():\n    temp[fb] = df[df['fbs']==fb]['heart attack'].value_counts()\n\ntemp = temp.T\ntemp.index = ['fasting blood sugar > 120 mg/dl','fasting blood sugar < 120 mg/dl']\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp.reset_index(),\n         'index',\n         ['Total sum','less chance','more chance'],\n         title='Heart attack chance by fasting blood sugar test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>Paple who have fasting blood sugar lower than 120 mg/dl are more likely to have heart attack.\n<li>Most of patients have fasting blood sugar lower than 120 mg/dl, compared to patients with low cholestoral level.","metadata":{}},{"cell_type":"markdown","source":"##### restecg (Resting electrocardiographic) column\n<a id='restecg'></a>","metadata":{}},{"cell_type":"code","source":"\ntemp = pd.DataFrame()\nfor rest in df['restecg'].unique().tolist():\n    temp[rest] = df[df['restecg']==rest]['heart attack'].dropna().value_counts()\n\ntemp.columns = ['Normal','ST-T wave abnormality','Probable left ventricular hypertrophy']\ntemp = temp.T.reset_index()\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp,\n         'index',\n         ['Total sum','less chance','more chance'],\n         title='Heart attack chance by Resting electrocardiographic ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>Most of patients have ST-T wave abnormality and a few of them have Probable left ventricular hypertrophy.\n<li>People who have ST-T wave abnormality are more likely to have less heart attack chance than Patients who have Probable left ventricular hypertrophy","metadata":{}},{"cell_type":"markdown","source":"###### exng (exercise induced angina) column (1 = yes; 0 = no)\n<a id='exng'></a>","metadata":{}},{"cell_type":"code","source":"temp = pd.DataFrame()\nfor exng in df['exang'].unique().tolist():\n    temp[exng] = df[df['exang']==exng]['heart attack'].value_counts()\n\ntemp.columns = ['Exercise induced angina','No exercise induced angina']\ntemp = temp.T.reset_index()\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp,\n         'index',\n         ['Total sum','less chance','more chance'],\n         title='Heart attack chance by exercise induced angina')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>People have Exercise induced angina are more likely to have heart attack.\n<li>Healthy people with no exercise induced angina have less chance of having a heart attack.\n<li>Exercise induced angina reflects a visualised difference for patients with and without heart attack, which means Exercise induced angina may be a major factor to diagnose a heart attack (Total sum is proportional with the chance being have a heart disease)","metadata":{}},{"cell_type":"markdown","source":"##### slp (The slope of the peak exercise ST segment) column  (categorical with 3 levels-Up, Flat, Down)\n<a id='slp'></a>","metadata":{}},{"cell_type":"code","source":"df['slope_category'] = df['slope'].replace({0:'Down',1:'Flat',2:'Up'})\n\ntemp = pd.DataFrame()\nfor slp in df['slope_category'].unique().tolist():\n    temp[slp] = df[df['slope_category']==slp]['heart attack'].value_counts()\n\ntemp = temp.T.reset_index()\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp,\n         'index',\n         ['Total sum','less chance','more chance'],\n         title='Heart attack chance by the slope of the peak exercise ST segment')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n\n<li>Most of patient have UP or Flat slope.\n<li>Patient who have UP slop are more likely to have heart attack.\n<li>The slope of the peak exercise ST segment also reflects a visualised difference for patients with and without heart attack, (Total sum is proportional with the chance being have a heart disease)","metadata":{}},{"cell_type":"markdown","source":"##### ca (number of major vessels) column\n<a id='ca'></a>","metadata":{}},{"cell_type":"code","source":"temp = pd.DataFrame()\nfor c in df['ca'].unique().tolist():\n    temp[c] = df[df['ca']==c]['heart attack'].value_counts()\n\ntemp = temp.T.reset_index()\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp,\n         'index',\n         ['Total sum','less chance','more chance'],\n         title='Heart attack chance by the Slope of the number of major vessels')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n\n<li>Most of people have zero number of number of major vessels.\n<li>Patient who have zero number of number of major vessels are more likely to have heart attack.","metadata":{}},{"cell_type":"markdown","source":"###### thall column \n<a id='thall'></a>","metadata":{}},{"cell_type":"code","source":"#thal (The Heart Status as Retrieved from Thallium) column\n\ntemp = pd.DataFrame()\nfor t in df['thal'].unique().tolist():\n    temp[t] = df[df['thal']==t]['heart attack'].value_counts()\n\ntemp = temp.T.reset_index()\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp,\n         'index',\n         ['Total sum','less chance','more chance'],\n         title='Heart attack chance by The Heart Status as Retrieved from Thallium')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n\n<li>Most of people have thal value 2.\n<li>Patient who have thal value 2 more likely to have heart attack.","metadata":{}},{"cell_type":"markdown","source":"<h2>Numerical Features</h2><a id=\"head-2-1-2\"></a>","metadata":{}},{"cell_type":"markdown","source":"##### Age column\n<a id='age'></a>","metadata":{}},{"cell_type":"code","source":"df['age_category'] = pd.cut(df['age'], bins=[0,39,49,59,69,77],labels=[\"29-39\",\"40-49\",\"50-59\",\"60-69\",\"70-79\"]).to_frame()\n\ntemp = pd.DataFrame()\nfor age in df['age_category'].unique().tolist():\n    temp[age] = df[df['age_category']==age]['heart attack'].value_counts()\n\ntemp = temp.T.reset_index()\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp,\n         'index',\n         ['Total sum','less chance','more chance'],\n         title='Heart attack chance by age')\n\n\nfig = make_subplots(rows=2, cols=2,\n                    specs=[[{\"colspan\": 2}, None],\n                           [{}, {}]],\n                    subplot_titles=('Age Histogram',\n                                    '(More chance)',\n                                    '(Less chance)'))\n\nfig.add_trace(go.Histogram(x=df['age']),\n              row=1, col=1)\n\nfig.add_trace(go.Histogram(x=df[df['heart attack']=='more chance']['age']),\n              row=2, col=1)\n\nfig.add_trace(go.Histogram(x=df[df['heart attack']=='less chance']['age']),\n              row=2, col=2)\n\nfig.update_layout(showlegend=False, title_text='Histogram for Age')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>Ages vary from 29 years to 77 years. \n<li>Age Histogram negative skewed, meaning that the average age is little lower than the median age.\n<li>Most of patients are 50-70 years. \n<li>There is no visualised difference in ages for patients with or without heart attack, which means age may not be a major factor to diagnose a heart attack (it have a correlation value -0.225439 with the output)","metadata":{}},{"cell_type":"markdown","source":"##### trtbps (Resting blood pressure) column\n<a id='trtbps'></a>","metadata":{}},{"cell_type":"code","source":"df['trtbps_category'] = pd.cut(df['trestbps'], bins=[0,90,120,200], labels=['Low blood pressure','Ideal blood pressure','High blood pressure'])\n\ntemp = pd.DataFrame()\nfor trtbps in df['trtbps_category'].unique().tolist():\n    temp[trtbps] = df[df['trtbps_category']==trtbps]['heart attack'].value_counts()\n\ntemp = temp.T.reset_index()\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp,\n         'index',\n         ['Total sum','less chance','more chance'],\n         title='Heart attack chance by resting blood pressure')\n\n\n\nfig = make_subplots(rows=2, cols=2,\n                    specs=[[{\"colspan\": 2}, None],\n                           [{}, {}]],\n                    subplot_titles=('Resting blood pressure Histogram',\n                                    '(More chance)',\n                                    '(Less chance)'))\n\nfig.add_trace(go.Histogram(x=df['trestbps']),\n              row=1, col=1)\n\nfig.add_trace(go.Histogram(x=df[df['heart attack']=='more chance']['trestbps']),\n              row=2, col=1)\n\nfig.add_trace(go.Histogram(x=df[df['heart attack']=='less chance']['trestbps']),\n              row=2, col=2)\n\nfig.update_layout(showlegend=False, title_text='Histogram for Resting blood pressure')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>Resting blood pressure is positive skewed, meaning that few patients have high blood pressure.\n<li>Also resting blood pressure doesn't reflect any visualised difference for patients with or without heart attack, which means resting blood pressure may not be a major factor to diagnose a heart attack (it have a correlation value -0.144931 with the output)\n<li>Most of patients have an extremely higher blood pressure, compared to patients with low blood pressure.","metadata":{}},{"cell_type":"markdown","source":"##### chol (cholestoral) column\n<a id='chol'></a>","metadata":{}},{"cell_type":"code","source":"df['chol_category'] = pd.cut(df['chol'], bins=[0,200,239,1000], labels=['Desirable','Borderline high','High'])\n\ntemp = pd.DataFrame()\nfor chol in df['chol_category'].unique().tolist():\n    temp[chol] = df[df['chol_category']==chol]['heart attack'].value_counts()\n\ntemp = temp.T.reset_index()\ntemp['Total sum'] = temp.sum(axis=1)\n\nbar_plot(temp,\n         'index',\n         ['Total sum','less chance','more chance'],\n         title='Heart attack chance by cholestoral test')\n\n\nfig = make_subplots(rows=2, cols=2,\n                    specs=[[{\"colspan\": 2}, None],\n                           [{}, {}]],\n                    subplot_titles=('Cholestoral Histogram',\n                                    '(More chance)',\n                                    '(Less chance)'))\n\nfig.add_trace(go.Histogram(x=df['chol']),\n              row=1, col=1)\n\nfig.add_trace(go.Histogram(x=df[df['heart attack']=='more chance']['chol']),\n              row=2, col=1)\n\nfig.add_trace(go.Histogram(x=df[df['heart attack']=='less chance']['chol']),\n              row=2, col=2)\n\nfig.update_layout(showlegend=False, title_text='Histogram for cholestoral')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>Cholestoral is positive skewed, meaning that few patients have high cholestoral.\n<li>Most of patients have high cholestoral level, compared to patients with low cholestoral level.","metadata":{}},{"cell_type":"markdown","source":"#####  thalachh (Maximum heart rate achieved) column\n<a id='thalachh'></a>","metadata":{}},{"cell_type":"code","source":"# Heart rate depends on age (changes by the age)\n# Lets explore each range of age separately\n\ntemp = pd.DataFrame(index=[['Average heart rate(target=0)','Average heart rate (target=1)']])\nfor age in df['age_category'].unique().tolist():\n    temp[age] = df[df['age_category']==age][['thalach','heart attack']].groupby('heart attack').mean().astype(int).values\n\ntemp.T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n\n<li>The average heart rate gets progressively slower as a person moves through childhood toward adolescence.","metadata":{}},{"cell_type":"markdown","source":"##### oldpeak (ST Depression Induced by Exercise Relative to Rest) column\n<a id='oldpeak'></a>","metadata":{}},{"cell_type":"code","source":"# oldpeak (ST Depression Induced by Exercise Relative to Rest) column\n\nfig = make_subplots(rows=2, cols=2,\n                    specs=[[{\"colspan\": 2}, None],\n                           [{}, {}]],\n                    subplot_titles=('OLDPEAK Histogram',\n                                    '(More chance)',\n                                    '(Less chance)'))\n\nfig.add_trace(go.Histogram(x=df['oldpeak']),\n              row=1, col=1)\n\nfig.add_trace(go.Histogram(x=df[df['heart attack']=='more chance']['oldpeak']),\n              row=2, col=1)\n\nfig.add_trace(go.Histogram(x=df[df['heart attack']=='less chance']['oldpeak']),\n              row=2, col=2)\n\nfig.update_layout(showlegend=False, title_text='Histogram for OLDPEAK')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li>The Histogram is highly skewed to the right meaning that most of patient have OLDPEAK values around zero.\n<li>OLDPEAK reflects a visualised difference for patients with and without heart attack, which means ST Depression Induced by Exercise Relative to Rest may be a major factor to diagnose a heart attack (it have a correlation value -0.430696 with the output)","metadata":{}},{"cell_type":"code","source":"#lets add sex_category \ndf['sex_category'] = df['sex'].apply(lambda x: 'Male' if x==1 else 'Female')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scatter Matrice\n<a id='scatter'></a>","metadata":{}},{"cell_type":"markdown","source":"<h2>Scatter Matrix</h2>\n<a id=\"sct_mtx\"></a>","metadata":{}},{"cell_type":"code","source":"#create a scatter plot for the columns that have a hih correlation with target column\nplt.figure();\nsns.set(style='whitegrid', context='talk', palette='viridis');\nsns.pairplot(data=df[['cp','thalach','exang','oldpeak','thal','slope','target']],\n             hue='target');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Correlation Matrix</h2>\n<a id=\"corr_mtx\"></a>","metadata":{}},{"cell_type":"code","source":"#Correlation Map\ncorr = df.corr()\n\ncorr.iplot(kind='heatmap',\n           colorscale='Blues',\n           hoverinfo='all',\n           layout = go.Layout(title='Correlation Heatmap for the correlation between our columns',\n                              titlefont=dict(size=20)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Maximum heart rate achieve vs. Age</h2>\n<a id=\"thalach_age\"></a>","metadata":{}},{"cell_type":"code","source":"# Maximum heart rate achieve vs Age\nfig = px.scatter(data_frame=df,\n                 y='age',\n                 x='thalach',\n                 color='heart attack')\n\nfig['layout'].update(title='Maximum heart rate achieve vs Age',\n                     titlefont=dict(size=20),\n                     xaxis=dict(title='Maximum heart rate achieve',titlefont=dict(size=18)),\n                     yaxis=dict(title='Age',titlefont=dict(size=18)))\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>ST depression induced by exercise relative to rest vs. Age</h2>\n<a id=\"oldpeak_age\"></a>","metadata":{}},{"cell_type":"code","source":"# Maximum heart rate achieve vs Age\nfig = px.scatter(data_frame=df,\n                 y='age',\n                 x='oldpeak',\n                 color='heart attack')\n\nfig['layout'].update(title='Maximum heart rate achieve vs Age',\n                     titlefont=dict(size=20),\n                     xaxis=dict(title='Maximum heart rate achieve',titlefont=dict(size=18)),\n                     yaxis=dict(title='Age',titlefont=dict(size=18)))\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Resting blood pressure-in mm Hg on admission to the hospital</h2>\n<a id=\"trestbps_age\"></a>","metadata":{}},{"cell_type":"code","source":"# Resting blood pressure-in mm Hg on admission to the hospital\n\nfig = px.scatter(data_frame=df,\n                 y='age',\n                 x='trestbps',\n                 color='heart attack')\n\nfig['layout'].update(title='Resting blood pressure vs Age',\n                     titlefont=dict(size=20),\n                     xaxis=dict(title='Resting blood pressure',titlefont=dict(size=18)),\n                     yaxis=dict(title='Age',titlefont=dict(size=18)))\n\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Violin plots for each column for each target grouped by age</h2>\n<a id=\"viloin\"></a>","metadata":{}},{"cell_type":"code","source":"#lets create a function to plot a multi-violin easily\n\ndef multi_violin(df,iter_col,dist_col,color_col='survived'):\n    if len(df[color_col].unique())!= 2:\n        return 'Maximun number of unique values in the color columns is 2'\n    i = 0\n    data = []\n    for ite in df[iter_col]:\n        data.append(go.Violin(x=df[df[iter_col]==ite][iter_col],\n                              y=df[df[color_col]==df[color_col].unique().tolist()[0]][dist_col],\n                              name=str(df[color_col].unique().tolist()[0]),\n                              jitter=0,\n                              meanline={'visible':True},\n                              line={\"color\": '#F78181'},\n                              side='negative',\n                              marker=dict(color= '#81F781'),\n                              showlegend=(i==0)))\n\n        data.append(go.Violin(x=df[df[iter_col]==ite][iter_col],\n                              y=df[df[color_col]==df[color_col].unique().tolist()[1]][dist_col],\n                               name=str(df[color_col].unique().tolist()[1]),\n                               jitter=0,\n                               meanline={'visible':True},\n                               line={\"color\": '#00FF40'},\n                               side='positive',\n                               marker=dict(color= '#81F781'),\n                               showlegend=(i==0)))\n        i+=1\n\n\n    layout = dict(title='Distribution of {} column for each {} colored by {}'.format(dist_col.replace('_',' ').title(),\n                                                                                     iter_col.replace('_',' ').title(),\n                                                                                     color_col.replace('_',' ').title()),\n                  width=1000,height=600,\n                  yaxis=dict(title='Distribution',titlefont=dict(size=20)))\n\n    iplot(dict(data=data,layout=layout))    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now lets visualize our continous columns","metadata":{}},{"cell_type":"code","source":"multi_violin(df=df,iter_col='cp',dist_col='age',color_col='sex_category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_violin(df=df,iter_col='age_category',dist_col='chol',color_col='sex_category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_violin(df=df,iter_col='age_category',dist_col='trestbps',color_col='sex_category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_violin(df=df,iter_col='age_category',dist_col='trestbps',color_col='heart attack')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_violin(df=df,iter_col='age_category',dist_col='thalach',color_col='heart attack')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multi_violin(df=df,iter_col='age_category',dist_col='oldpeak',color_col='sex_category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Phase II</h1><a id=\"Phase II\"></a>","metadata":{}},{"cell_type":"markdown","source":"<h2>Data Processing</h2>\n<h3>Prepare the data for the machine learning models</h3>\n<a id=\"prep_ml\"></a>","metadata":{}},{"cell_type":"markdown","source":"<p>First, we need to know which numerical features are most important?","metadata":{}},{"cell_type":"code","source":"#import the data\ndf = pd.read_csv('../input/heart-disease-uci/heart.csv')\n#lets see the correlation between columns and target column\ncorr = df.corr()['target'].to_frame()\ncorr = corr.rename(columns={'target':'Correlation with target'})\ncorr.sort_values(by='Correlation with target',ascending=False)[1:]\\\n.style.background_gradient(axis=1,cmap=sns.light_palette('green', as_cmap=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this list it is apparent that :\n<li>exng with <b> -0.436757</b>\n<li>cp with <b>0.433798</b>\n<li>oldpeak with <b>-0.430696</b>\n<li>thalachh with <b>0.421741</b>\n<li>caa\twith <b>-0.391724</b>\n<li>thall with <b>-0.344029</b><br>\nare the 6 most important numerical features.","metadata":{}},{"cell_type":"markdown","source":"<li>Before we proceed into modeling the data we notice that we have cateorical data that LabelEncoded but it will caase some problems.<br>\nThe problem here is since there are different numbers in the same column, <b>the model will misunderstand the data to be in some kind of order.</b>The model may derive a correlation like as the fbs number increases the chance of being have heart attack increases, <b>but this clearly may not be the scenario in some other data or the prediction set.</b>To overcome this problem, we use <b>One Hot Encoder.</b>","metadata":{}},{"cell_type":"code","source":"# first choose columns to train the model with\nnum_features = ['age','trestbps','chol','thalach','oldpeak']\ncat_features = ['cp','slope','thal','sex','exang','ca','fbs','restecg']\n\n# #convert our categorical columns to dummies instead of LabelEncoding\nnew_data = pd.DataFrame()\nnew_data[num_features] = df[num_features]\n\nfor col in cat_features:\n    dumm = pd.get_dummies(df_orig[col], \n                          prefix = col,\n                          dtype=float)\n    \n    new_data = pd.concat([new_data,dumm], axis=1)\n\n#scaling  our numeric columns\nstd = StandardScaler()\nfor col in num_features:\n    new_data[col] = pd.Series(std.fit_transform(df[col].values.reshape(-1,1)).reshape(-1))\n\n# Separate features and predicted value\nX = new_data\ny = df['target'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Comparing different Machine learning models</h2>\n<a id=\"ml_models\"></a>","metadata":{}},{"cell_type":"code","source":"# define models to test:\nbase_models = [(\"XGB_model\", XGBClassifier(random_state=42, n_jobs=-1,use_label_encoder=False)),  #XGBoost model\n               (\"SVM\",      SVC()),                                                               #Support Vector Machines\n               (\"LR_model\", LogisticRegression(random_state=42,n_jobs=-1)),                       #Logistic Regression model\n               (\"DT_model\", DecisionTreeClassifier(random_state=42)),                             #Decision tree model\n               (\"RF_model\", RandomForestClassifier(random_state=42, n_jobs=-1)),                  #Random Forest model\n               (\"Bagging_model\",BaggingClassifier(base_estimator=DecisionTreeClassifier(),        #Bagging model\n                                                 max_samples=30,\n                                                 n_estimators=500,\n                                                 n_jobs=-1,\n                                                 bootstrap=True,\n                                                 oob_score=True)),\n               (\"Random_subspaces_model\",BaggingClassifier(base_estimator=DecisionTreeClassifier(),#Random subspaces model\n                                                           n_estimators=100,\n                                                           bootstrap=False,\n                                                           max_samples=1.0,\n                                                           max_features=True,\n                                                           bootstrap_features=True,\n                                                           n_jobs=-1)),\n                (\"Random_Patches_model\", BaggingClassifier(base_estimator=DecisionTreeClassifier(),#Random Patches model\n                                                            n_estimators=100,\n                                                            bootstrap=True,\n                                                            max_samples=1.0,\n                                                            max_features=True,\n                                                            bootstrap_features=True,\n                                                            n_jobs=-1)),\n                (\"AdaBoost_model\",AdaBoostClassifier(DecisionTreeClassifier(),                      #AdaBoost model\n                                                    n_estimators=100,\n                                                    learning_rate=0.01)),\n                (\"GradientBoosting\",GradientBoostingClassifier(max_depth=2,                        #GradientBoosting model\n                                                              n_estimators=100))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>To evaluate the performance of any machine learning model we need to test it on some unseen data, based on the models \nperformance on unseen data we can say weather our model is :\n    <ul>\n        <li>Under-fitting.\n        <li>Over-fitting.\n        <li>Well generalized.\n    </ul>\n<b>Cross validation (CV)</b> is one of the technique used to test the effectiveness of a machine learning models, it is also a re-sampling procedure used to evaluate a model if we have a limited data.<br>\nTo perform CV we need to keep aside a sample/portion of the data on which is not used to train the model, later use this sample for testing/validating.<br>\nSo, k-fold cross validation is used for two main purposes:\n<ul>\n    <li>To tune hyper parameters.\n    <li>To better evaluate the performance of a model.","metadata":{}},{"cell_type":"code","source":"kfolds = 5   # it is better to be 1/(size of testing test)\nsplit = StratifiedKFold(n_splits=kfolds,\n                        shuffle=True, \n                        random_state=42)  # use shuffle to ensure random distribution of data\n\n# Preprocessing, fitting, making predictions and scoring for every model:\nmodels_data = {'min_score':{},'max_score':{},'mean_score':{},'std_dev':{}}\nfor name, model in base_models:\n    # get cross validation score for each model:\n    cv_results = cross_val_score(model, \n                                 X, y, \n                                 cv=split,\n                                 scoring=\"accuracy\",\n                                 n_jobs=-1)\n    \n    # output:\n    #To find the average of all the accuracies.\n    min_score = round(min(cv_results), 4)\n    models_data['min_score'][name] = min_score\n     \n    #To find the max accuracy of all the accuracies.\n    max_score = round(max(cv_results), 4)\n    models_data['max_score'][name] = max_score\n    \n    #To find the min accuracy of all the accuracies.\n    mean_score = round(np.mean(cv_results), 4)\n    models_data['mean_score'][name] = mean_score\n    \n    # let's find the standard deviation of the data to see degree of variance in the results obtained by our model.\n    std_dev = round(np.std(cv_results), 4)\n    models_data['std_dev'][name] = std_dev\n    \n    print(f\"{name} cross validation accuarcy score: {mean_score} +/- {std_dev} (std) ---> min: {min_score}, max: {max_score}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\">BaggingClassifier</a>:</h3><br>\n<li>Bagging and Pasting have the same algorithm, each model is trained using subsets but the way you chose the subsets changes:\n<ul>\n    <li>In Bagging you do sampling with replacement.\n    <li>In Pasting you do sampling without replacement.\n</ul>\nSo two algorithms allow training instances to be sampled several times accroce multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor.\n<li>In Bagging we control sampling by <b>(max_sample,Bootstrap).</b>\n    Random subspaces and Random Patches are extension to Bagging algorithm, but they do feature sampling instead of instances sampling, they are useful when we have a high dimensions inputs.<br>\n<ul>\n    <li>In Random subspaces you do sampling to both training and features.\n    <li>In Random Patches you keep all training instances <b>(Bootstrap=False, max_sample=1)</b>, but you do sampling feature by <b>(Bootstrap_feature=True)</b>.\n</ul>\n<h3><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\">AdaBoostClassifier</a>:</h3><br>\n<li>AdaBoost Combines a lot of week learners to make Classification <b>(week learners like stump in DecisionTree)</b>","metadata":{}},{"cell_type":"code","source":"models_df = pd.DataFrame(models_data).sort_values(by='mean_score',ascending=False)\nmodels_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'Mininam, Maximam and Mean score for each model'\n\nmodels_df.iplot(kind='bar', title=title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<li><b>Logistic Regression Performes better with mean accuracy 95.08% , and standard deviation 5.8%. This values of std is extremely low, which means that our model has a very low variance, which is actually very good since that means that the prediction that we obtained on one test set is not by chance. Rather, the model will perform more or less similar on all test sets as we will see.","metadata":{}},{"cell_type":"code","source":"accuracies = {}\nmodel = base_models\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfor name,model in base_models:\n    model.fit(X_train, y_train)\n    acc = model.score(X_test,y_test)*100\n    accuracies[name] = acc\n    print(\"{} Accuracy Score : {:.2f}%\".format(name,acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_res = pd.DataFrame(data=accuracies.items())\nmodels_res.columns = ['Model','Test score']\nmodels_res.sort_values('Test score',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model_df = models_df.join(models_res.set_index('Model'))\nnew_model_df['Test Score - Cross_Validation Score'] = new_model_df['Test score'] - new_model_df['mean_score']*100\nnew_model_df.sort_values('Test Score - Cross_Validation Score',ascending=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusions</h1><br>\n<b><li>Logistic Regression Performes more better with all test sets which is perfect !!\n   <li>Difference between cross validation score and test score is lower \n    <br><h3>Lets try to improve the model using grid search</h3>","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression()\n\n# define grid search\ngrid = dict(solver=['newton-cg', 'lbfgs', 'liblinear'],\n            penalty=['l2'],\n            C=np.linspace(0, 4, 50))\n\nclf = GridSearchCV(model, \n                   grid, \n                   cv=split,\n                   verbose = False,\n                   n_jobs=-1)\nbest_model = clf.fit(X_train, y_train)\nbest_parms = best_model.best_estimator_.get_params()\n\nprint('Best C : {}'.format(best_parms['C']))\nprint('Best solver : {}'.format(best_parms['solver']))   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_model = LogisticRegression(C=best_parms['C'],\n                             solver=best_parms['solver'])\n\nlr_model.fit(X_train, y_train)\n\nacc = lr_model.score(X_test, y_test)*100\n\nprint(\"Test Accuracy {:.2f}%\".format(acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>That's better !!!","metadata":{}},{"cell_type":"code","source":"#save the model in Pickle File\n\npickle.dump(lr_model, open('lr_model.sav', 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets try  KNN algorithms","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nk = []\nfor i in range(1,20):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train, y_train)\n    score = knn.score(X_test, y_test)\n    k.append(score)\n    \nknn_df = pd.DataFrame(columns=['Score'],index=list(range(1,20)),data=k)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'KNeighbor model score for number of neighbors from 1 to 20'\nfig = px.line(data_frame=knn_df.reset_index(),\n              title = title,\n              x='index', y='Score')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>So the best Kneighbor model will be achieved with number of neighbors 8","metadata":{}},{"cell_type":"code","source":"best_knn = KNeighborsClassifier(8)\nbest_knn.fit(X_train, y_train)\nscore = best_knn.score(X_test, y_test)*100\nprint(\"Test Accuracy {:.2f}%\".format(score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save the model in Pickle File\n\npickle.dump(best_knn, open('knn.sav', 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>Better than LogisticRegression !!","metadata":{}},{"cell_type":"markdown","source":"<h2>Now lets use <a href=\"https://github.com/MAIF/shapash\">shapash</a> library to give some visualization about the model.</h2>\n<a id=\"shapash\"></a>\n<P>It doesn't run with kaggle kernel, just remove the comment and run the codes after installing the library","metadata":{}},{"cell_type":"code","source":"# !pip install --ignore-installed shapash --user\n# !pip install shapash","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from shapash.explainer.smart_explainer import SmartExplainer\n# # first we need to define our columns\n# features_dict={'age': 'Age of the patient',\n#               'sex': 'Sex of the patient',\n#               'cp': 'Chest pain type (typical angina,atypical angina,non-anginal pain,asymptomatic',\n#               'trestbps': 'Resting blood pressure-in mm Hg',\n#               'chol': 'Serum cholesterol in mg/dl',\n#               'fbs': 'Fasting blood sugar > 120 mg/dl',\n#               'restecg': 'Resting electrocardiographic results',\n#               'thalach': 'Maximum heart rate achieved',\n#               'exang': 'Exercise induced angina',\n#               'oldpeak': 'ST depression induced by exercise relative to rest',\n#               'slope': 'The slope of the peak exercise ST segment',\n#               'ca': 'Number of major vessels (0-3)',\n#               'thal': 'The heart status as retrieved from Thallium test'}\n\n# xpl = SmartExplainer(features_dict=features_dict) # optional parameter, specifies label for features name","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xpl.compile(x=X_test,\n#             model=lr_model) #KNN is not supported by shapash","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Start WebApp\n# app = xpl.run_app(title_story='Heart Attack Prediction')\n\n# # Stop the WebApp after using it\n# # app.kill()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Export local explaination in DataFrame\n# summary_df= xpl.to_pandas(max_contrib=3, # Number Max of features to show in summary\n#                           threshold=5000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Save SmartExplainer in Pickle File\n# xpl.save('xpl.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <h1 align=\"center\">Thanks for reading</h1>\n<h2 align=\"center\" style='color:red' > If you like the notebook or learned something please upvote! </h2>\n<b><p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\">You can also see</p>\n<ul>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/data-exploration-and-visualization-uber-data'>Data exploration and visualization(Uber Data)</a><br>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/hotel-booking-eda-cufflinks-and-plotly'>Hotel booking EDA (Cufflinks and plotly)\n</a><br>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/suicide-rates-visualization-and-geographic-maps/edit/run/53135916'>Suicide Rates visualization and Geographic maps</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/superstore-data-analysis-with-plotly-clustering'>Superstore Data Analysis With Plotly(Clustering)</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/superstore-analysis-with-cufflinks-and-pandas'>Superstore Analysis With Cufflinks and pandas</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/learn-data-analysis-using-sql-and-pandas'>Learn Data Analysis using SQL and Pandas</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/european-soccer-database-with-sqlite3'>Chinook Questions with sqlite</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/chinook-questions-with-sqlite'>European soccer database with sqlite3</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/chinook-questions-with-sqlite'>Chinook data questions with sqlite3</a>\n</ul>","metadata":{}}]}