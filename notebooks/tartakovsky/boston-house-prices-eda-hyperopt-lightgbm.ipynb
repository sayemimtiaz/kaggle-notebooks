{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Import dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-loads all imports every time the cell is ran. \n%load_ext autoreload\n%autoreload 2\n\nimport pandas as pd\npd.options.display.float_format = '{:,.5f}'.format\n\nimport numpy as np\nfrom time import time\nfrom IPython.display import display\n\nfrom sklearn.model_selection import cross_validate, learning_curve, train_test_split, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import datasets\nfrom lightgbm import LGBMRegressor\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = datasets.load_boston()\nX = pd.DataFrame(data['data'], columns=data['feature_names'])\ny = pd.Series(data['target'], name='y')\ndisplay(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory data analysis"},{"metadata":{},"cell_type":"markdown","source":"Display the dataframe and it's description"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(y, bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_features = X.shape[1]\nn_cols = 4\nn_rows = np.ceil(n_features / n_cols).astype(int)\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows*3))\n\nfor i, col in enumerate(X.columns):\n    ax = axes[i//n_cols][i%n_cols]\n    ax.hist(X[col], bins=25)\n\n    ax.set_xlabel(X.columns[i])\n\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize scatterplots (xi, y) pairs**"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"n_features = X.shape[1]\npairs = [(X[col], y) for col in X.columns]\nn_cols = 4\nn_rows = np.ceil(n_features / n_cols).astype(int)\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(14, n_rows*3))\n\nfor i, pair in enumerate(pairs):\n    ax = axes[i//n_cols][i%n_cols]\n    ax.scatter(pair[0], pair[1], marker='x')\n\n    ax.set_xlabel(X.columns[i])\n    ax.set_ylabel('House price (in $1000s)')\n\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first impression from the plots above is that LSTAT and RM features have close-to-linear relationship with the target and can explain the significant part of the price."},{"metadata":{},"cell_type":"markdown","source":"### Prepare sets\n\nThe dataset is split initially into train and test sets using train_test_split.\n\nTrain set is further split into train and validation subsets using K-fold CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    random_state=1, test_size=0.2, shuffle=True\n)\n\ncv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Select metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = make_scorer(lambda a, b: np.sqrt(mean_squared_error(a, b)), greater_is_better=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Establish no skill performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy = DummyRegressor()\n\n# Cross validate dummy classifier to establish no skill performance\ndummy_res = cross_validate(dummy, X_train, y_train, scoring=rmse, \n                           return_train_score=True, cv=cv, n_jobs=-1)\n\ndummy_train_score = np.mean(dummy_res['train_score'])\ndummy_test_score = np.mean(dummy_res['test_score'])\n\ndisplay(\n    f'Train score, dummy: {-dummy_train_score:.2f}',\n    f'CV score, dummy: {-dummy_test_score:.2f}', \n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create prediction pipeline\nscaler = StandardScaler() \nclf = LinearRegression()\n\npipeline = make_pipeline(scaler, clf)\n\n\n# Fit the model, predict train and test sets\nres = cross_validate(pipeline, X_train, y_train, scoring=rmse, \n                     return_train_score=True, cv=cv, n_jobs=-1)\n\ntrain_score = np.mean(res['train_score'])\ntest_score = np.mean(res['test_score'])\n\ndisplay(\n    f'Mean train RMSE: {-train_score:.2f}',\n    f'Mean CV RMSE: {-test_score:.2f}', \n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Learning curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes, train_scores, test_scores = learning_curve(\n    pipeline, X, y, train_sizes=np.linspace(0.1, 0.8, 20),\n    random_state=1, shuffle=True, scoring=rmse\n)\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nsns.lineplot(x=train_sizes, y=train_scores_mean, label='-RMSE: train score', ax=ax)\nsns.lineplot(x=train_sizes, y=test_scores_mean, label='-RMSE: CV score', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's suppose that ideal performance is unreachable out of sample and estimate that bayes error is somewhere aroung ~1 RMSE. Combining that estimate with the results \n\n- No skill ~= 9 RMSE\n- 80/20 split linreg ~= 6 RMSE\n- Cross-validated test set linreg ~= 5 RMSE\n- Train set linreg ~= 4.5 RMSE\n- Estimated Bayes error ~= 1 RMSE\n- Ideal performance == 0 RMSE\n\nThat gives the following estimates for bias and variance:\n- ~3.5 RMSE worth of avoidable bias\n- ~0.5 RMSE worth of variance on CV\n- ~1.5 RMSE worth of variance on 80/20 split\n\nThat suggests that both bias and variance can be improved.\n\nAccesible ways to reduce bias:\n- Use more complex model \n- Engineer better features\n\nAccesible ways to reduce variance:\n- Use regularization\n- Drop irrelevant features\n\nIt's usually a good idea to deal with bias first and work on variance later.\nLet's try the easy way first and build a more complex model."},{"metadata":{},"cell_type":"markdown","source":"### Non-linear model"},{"metadata":{},"cell_type":"markdown","source":"Boosted trees have been one of the most succesful ensembles as of late. Let's try fitting LightGBM regressor."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create prediction pipeline\nscaler = StandardScaler() \nclf = LGBMRegressor()\n\npipeline = make_pipeline(scaler, clf)\n\n# Fit the model, predict train and test sets\nres = cross_validate(pipeline, X_train, y_train, scoring=rmse, \n                     return_train_score=True, cv=cv, n_jobs=-1)\n\ntrain_score = np.mean(res['train_score'])\ntest_score = np.mean(res['test_score'])\n\ndisplay(\n    f'Mean train RMSE: {-train_score:.2f}',\n    f'Mean CV RMSE: {-test_score:.2f}', \n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes, train_scores, test_scores = learning_curve(\n    pipeline, X, y, train_sizes=np.linspace(0.1, 0.8, 20),\n    random_state=1, shuffle=True, scoring=rmse\n)\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nsns.lineplot(x=train_sizes, y=train_scores_mean, label='-RMSE: train score', ax=ax)\nsns.lineplot(x=train_sizes, y=test_scores_mean, label='-RMSE: CV score', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LGBM indeed turned out to be quite good. Using more complex model, we have the following situation:\n\n- No skill ~= 9 RMSE\n- 80/20 split LGBM ~= 4.5 RMSE\n- Cross-validated test set LGBM ~= 3.5 RMSE\n- Train set LGBM ~= 1.5 RMSE\n- Estimated Bayes error ~= 1 RMSE\n- Ideal performance == 0 RMSE\n\nThat gives the following estimates for bias and variance:\n- ~0.5 RMSE worth of avoidable bias\n- ~2.0 RMSE worth of variance on CV\n- ~3.0 RMSE worth of variance on 80/20 split\n\nWe have managed to reduce the bias even below our initial estimate of unavoidable bias just by using the more complex model. Simultaneously, we have managed to reduce out of sample error significantly.\n\nNow the model is complex enough to express the train set, gives a good score on the test set, but still exhibits substantial variance.\n\nTime to try variance mitigation techniques."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create prediction pipeline\nscaler = StandardScaler() \nclf = LGBMRegressor(\n    reg_lambda=5,\n    reg_alpha=2,\n)\n\npipeline = make_pipeline(scaler, clf)\n\n# Fit the model, predict train and test sets\nres = cross_validate(pipeline, X_train, y_train, scoring=rmse, \n                     return_train_score=True, cv=cv, n_jobs=-1)\n\ntrain_score = np.mean(res['train_score'])\ntest_score = np.mean(res['test_score'])\n\ndisplay(\n    f'Mean train RMSE: {-train_score:.2f}',\n    f'Mean CV RMSE: {-test_score:.2f}', \n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By manually trying several values of regularization parameters, we can see how they affect the result:\n\nNo regularization\n- Mean train RMSE: 1.54\n- Mean CV RMSE: 3.56\n\nreg_lambda=5, reg_alpha=2\n- Mean train RMSE: 1.77\n- Mean CV RMSE: 3.57\n\nSo regularization indeed reduces variance in this case, but it does so by increasing train loss, not decreasing validation loss.\n\nLet's try tuning hyperparameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\nfrom hyperopt.pyll.base import scope\n\nspace = dict(\n    learning_rate = hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n    n_estimators = scope.int(hp.qloguniform('n_estimators', np.log(50), np.log(500), np.log(10))),\n    max_depth = scope.int(hp.quniform('max_depth', 2, 15, 1)),\n)\n\ndef objective(params):\n        \n        clf = make_pipeline(StandardScaler(), LGBMRegressor(**params))\n        cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)\n\n        res = cross_validate(clf, X_train, y_train, scoring=rmse, \n                             return_train_score=True, cv=cv, n_jobs=-1)\n\n        train_score = np.mean(res['train_score'])\n        test_score = np.mean(res['test_score']) - np.std(res['test_score'])\n\n#         print({ **params, 'loss': test_score})\n        result = dict(\n            params=params,\n            train_loss = -train_score,\n            # Hyperopt-required keys\n            loss = -test_score,\n            status = STATUS_OK,   \n        )\n        return result\n        \ntrials = Trials()\nbest = fmin(objective, space, algo=tpe.suggest, max_evals=50, trials=trials)\n# results = [trial['result'] for trial in trials]\nbest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hyperparameter optimization appears to reduce our loss on CV from 3.57 to 3.35. \n\nHowever, the search of hyperparameters was done on the same data and the same CV configuration as we are calculating the scores on.\n\nThat means that we actually don't know whether the best param set offers any improvement out of sample. I suspect it doesn't. Let's use our test set for the first time and see."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\nlinreg = make_pipeline(scaler, LinearRegression())\nlgbm = make_pipeline(scaler, LGBMRegressor())\nlgbm_tuned = make_pipeline(scaler, LGBMRegressor(\n    learning_rate=best['learning_rate'],\n    max_depth=int(best['max_depth']),\n    n_estimators=int(best['n_estimators']),\n))\n\nlinreg.fit(X_train, y_train)\nlgbm.fit(X_train, y_train)\nlgbm_tuned.fit(X_train, y_train)\n\nprint(\n    \"Linreg:\", -rmse(linreg, X_test, y_test),\n    \"\\nDefault LGBM:\", -rmse(lgbm, X_test, y_test),\n    \"\\nOptimized LGBM:\", -rmse(lgbm_tuned, X_test, y_test)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, optimized hyperparameters didn't remain the best out of sample and untuned LGBM performed slightly better.\n\nTherefore, RMSE = 3.066 is the best out of sample score achieved in this notebook on Boston house prices dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}