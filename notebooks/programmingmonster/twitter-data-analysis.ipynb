{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Gender classification with some meaningful predictions\n## The ML part of this code takes about 20 hours to train and learn providing about 80% of accuracy in test dataset. The code time was tested on i7-8750H at 3.9GHz on all 12 threads and 16GB of DDR4 RAM at 2800MHz.\n\n## The Neural network will take 5 Minutes and required to enable the GPU in kaggle accelerator\n\n## The part which is finding mistakes in this code takes about 20 minutes to run so be patient.\n\n## The rest of the code takes about 10 minutes to run excluding time for installing the libraries if not installed.\n\nThis is by no means the complete analysis. I will be improving it over the next week!!\n\nFor now this is just a barebones raw code with little to no explaination, more is comming soon!"},{"metadata":{"trusted":true},"cell_type":"code","source":"!/opt/conda/bin/python3.7 -m pip install --upgrade pip\n!pip install pandas\nimport pandas as pd\nimport numpy as np\nimport re\n!pip install numpy\nfrom numpy import mean\nfrom numpy import std\n!pip install sklearn\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import StackingClassifier\n!pip install seaborn\nimport seaborn as sn\n!pip install matplotlib\nfrom matplotlib import pyplot\n!pip install twython\n!pip install nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom collections import Counter\n!conda install -c asmeurer pattern -y\nfrom pattern.en import suggest\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/twitter-user-gender-classification/gender-classifier-DFE-791531.csv', encoding = 'ISO-8859-1')\ndataset = pd.DataFrame(dataset)\nprint(dataset.info())\ndf_discard = dataset.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above data we can see that the GENDER column has 19953 non-null values, which we are using as a dependant variable!\n# Cleaning of the data we have\nCleaning of dataset is important as we will have a better analysing of it"},{"metadata":{"trusted":true},"cell_type":"code","source":"################################################################################################################# Data Cleaning\ndataset.drop(['profileimage', 'tweet_id', '_golden', 'gender_gold', 'profile_yn_gold', 'profile_yn:confidence', 'profile_yn', 'user_timezone', 'fav_number', '_unit_state', '_trusted_judgments', '_last_judgment_at', '_unit_id', 'name', 'tweet_created', 'tweet_coord', 'link_color', 'sidebar_color', 'tweet_location'], axis = 1, inplace = True)\ndataset.drop(dataset[(dataset['gender'] != 'male') & (dataset['gender'] != 'female') & (dataset['gender'] != 'brand')].index, inplace = True) ### manually dropping the non-null but irrelevent values\nprint(dataset.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that in column GENDER we have just 18836 values inspite of 19953 non-null values in original dataset. This is becaues some of the values in GENDER column was \"nan\" and not null. so it had to be removed manually"},{"metadata":{},"cell_type":"markdown","source":"we will have to clean the text and description way of text representation"},{"metadata":{"trusted":true},"cell_type":"code","source":"################################################################################################################# Function for cleaning text & descriptions\nprint(\"Text before formatting is applied to it is as \\n\", dataset['text'].head(5))\nprint(\"Description before formatting is applied to it is as \\n\", dataset['description'].head(5))\ndef cleaning(s):\n    s = str(s)\n    s = s.lower()\n    s = re.sub('\\s\\W',' ',s)\n    s = re.sub('\\W,\\s',' ',s)\n    s = re.sub(r'[^\\w]', ' ', s)\n    s = re.sub(\"\\d+\", \"\", s)\n    s = re.sub('\\s+',' ',s)\n    s = re.sub('[!@#$_]', '', s)\n    s = s.replace(\"co\",\"\")\n    s = s.replace(\"https\",\"\")\n    s = s.replace(\",\",\"\")\n    s = s.replace(\"[\\w*\",\" \")\n    return s\ndataset['text'] = [cleaning(s) for s in dataset['text']]\nprint(\"\\nText after formatting is applied to it is as \\n\", dataset['text'].head(5))\ndataset['description'] = [cleaning(s) for s in dataset['description']]\nprint(\"Description after formatting is applied to it is as \\n\", dataset['description'].head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"you get the idea of what is going no here in TEXT and DESCRIPTION cleaning.\nnow it's time to convert dates into PANDS.DATETIME format"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The date and time as stored in dataset is\\n\", dataset['created'].head(10))\ndataset['created'] = pd.to_datetime(dataset['created'])\nprint(\"\\nThe date and time after conversions is\\n\", dataset['created'].head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dividing the cleaned dataset into test & train datasets\nWe will be dividing the dataset into Train & test datasets on the bases of GENDER:CONFIDANCE column value\nif GENDER:CONFIDANCE column value = 1 then the series goes to train set and tes set is whatever is lift"},{"metadata":{"trusted":true},"cell_type":"code","source":"################################################################################################################# Function to make new index with ordered numeric values\ndef correct_indexing(dataset):\n    dataset.insert(0, 'index', range(0, len(dataset)))\n    dataset.set_index(\"index\", inplace = True)\n    return\n\ndataset_store_gender_confodence_non1 = dataset.copy()\ndataset_store_gender_confodence_non1.drop(dataset_store_gender_confodence_non1[dataset_store_gender_confodence_non1['gender:confidence'] == 1].index, inplace = True)\ndataset.drop(dataset[(dataset['gender:confidence'] != 1)].index, inplace = True)\ndataset.drop(['gender:confidence'], axis = 1, inplace = True)\ndataset_store_gender_confodence_non1.drop(['gender:confidence'], axis = 1, inplace = True)\ncorrect_indexing(dataset)\ncorrect_indexing(dataset_store_gender_confodence_non1)\nprint(\"####################################The Train subset of dataset is\\n\", dataset.info())\nprint(\"####################################The Test subset of dataset is\\n\", dataset_store_gender_confodence_non1.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we have both Training dataset and Testing dataset. we will be using Training dataset to find meaningfull data from in next sections!"},{"metadata":{},"cell_type":"markdown","source":"# Data Visualisations"},{"metadata":{},"cell_type":"markdown","source":"## Plot of gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.value_counts(dataset['gender']).plot(kind=\"pie\", startangle = 90, shadow = True, radius = 1.2, autopct = '%1.1f%%')\npyplot.suptitle('Categorical Plotting of gender')\npyplot.xlabel(\"\")\npyplot.ylabel(\"\")\npyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like there are almost equal percentages of\ngenders and brands using twitter, we can predict that\nthe brands can also have a equal percentage of gender\nrunning them."},{"metadata":{},"cell_type":"markdown","source":"## Plot of tweet popularity"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.value_counts(dataset['created'].dt.year).sort_index().plot(kind=\"bar\")\npyplot.suptitle('Categorical Plotting of Twitter popularity')\npyplot.xlabel(\"Year\")\npyplot.ylabel(\"Number of tweets\")\npyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting enough the popularity of twitter\nskyrocketed in 2009. The rest of the data shows a\ngrowth slope at angle close to 45degree and the boom\nin 2009 can be a hoax but we will verify it in other\nvisualizations.\nInterestingly year 2010 was the least popular year\nfor twitter as there was the least growth rate in past 6\nyears from 2015. This fact can be confirmed in other\ngraphs too."},{"metadata":{},"cell_type":"markdown","source":"## Plot of tweets per Hours"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.value_counts(dataset['created'].dt.hour).sort_index().plot(kind=\"bar\")\npyplot.suptitle('Categorical Plotting of tweet war at what hour')\npyplot.xlabel(\"Hour Time\")\npyplot.ylabel(\"Number of tweets\")\npyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that what people do in their most\nproductive hours, they tweet. This graph upon refine\nstudy shows a sharp rise in morning usage of twitter\nand the decline in late afternoon or evening is not that\nsteep.\nMeanwhile we can make some guess as to what\nthis sharp morning curve means, in the morning news is\nthe most consumed media and this seems to be the\npattern. Twitter is the major news source for people\nusing it. "},{"metadata":{},"cell_type":"markdown","source":"## Plot of words use per year"},{"metadata":{"trusted":true},"cell_type":"code","source":"################################################################################################### Function to calculate the average based on 'column_dependent' &'column_working'\ndef average(column_working, column_dependent, info):\n    word_avg_dependent = []\n    word_avg = []\n    add = 0\n    count = pd.value_counts(column_dependent)\n    for i in count.index:\n        for j in range (0, len(column_working)):\n            if (column_dependent[j] == i):\n                temp = column_working[j]\n                if (info == \"Text\"):\n                    pieces = len(temp.split())\n                if (info == \"Num\"):\n                    pieces = temp\n                add = add + int(pieces)\n        word_avg_dependent.append(i)\n        word_avg.append(add)\n        add = 0\n        pieces = 0\n    return word_avg_dependent, word_avg\n\n################################################################################################### Plot of words use per year\nword_avg_year, word_avg = average(dataset['description'] + \" \" + dataset['text'], dataset['created'].dt.year, \"Text\")\npyplot.bar(word_avg_year, word_avg, tick_label = word_avg_year)\npyplot.suptitle('Categorical Plotting of word use per year')\npyplot.xlabel(\"Year\")\npyplot.ylabel(\"Words used\")\npyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The words used per year shows some of the\nfeatures which were seen in the popularity of twitter\ngraph confirming the boom in 2009. The word count has\nincreased per year in a very linear fashion.\nThe increase rate seems to be slow in recent years,\nbut this face can be verified in other graphs too.\nInterestingly the year 2010 is where the least words\nwere used in past 6 years from 2015 confirming the\n2010 least growth rate fact."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Plotting of word per tweet use per year"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_avg_year, word_avg = average(dataset['description'] + \" \" + dataset['text'], dataset['created'].dt.year, \"Text\")\nyears = pd.value_counts(dataset['created'].dt.year)\ni = 0\nfor j in years:\n    word_avg[i] = word_avg[i] / j\n    i = i + 1\npyplot.bar(word_avg_year, word_avg, tick_label = word_avg_year)\npyplot.suptitle('Categorical Plotting of word per tweet use per year')\npyplot.xlabel(\"Year\")\npyplot.ylabel(\"Words used per tweet\")\npyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The words used in a tweet is declining and the\ndecline per year rate is very slow. In span of 10 years\nthe average word per tweet count drowned down from\n30 to 25.\nThis seems to imply that people can say what they\nmean in less words, directly relating to increased\neducational qualities and quantities in the countries\ntwitter is used mostly"},{"metadata":{},"cell_type":"markdown","source":"## Categorical Plotting of text by gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_avg_gender, word_avg = average(dataset['text'], dataset['gender'], \"Text\")\npyplot.pie(word_avg, labels = word_avg_gender, startangle = 90, shadow = True, radius = 1.2, autopct = '%1.1f%%') \npyplot.suptitle('Categorical Plotting of text by gender')\npyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph Is in sync with gender graph and tells us\nthat the amount of text accumulated in twitter\ndatabase is equally contributed by male and females\nand brands"},{"metadata":{},"cell_type":"markdown","source":"## Categorical Plotting of tweet per gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_avg_gender, word_avg = average(dataset['tweet_count'], dataset['gender'], \"Num\")\npyplot.pie(word_avg, labels = word_avg_gender, startangle = 90, shadow = True, radius = 1.2, autopct = '%1.1f%%') \npyplot.suptitle('Categorical Plotting of tweet per gender')\npyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a interesting graph showing that the brands\ntweet almost as much as males and females combined.\nWe can make many conclusions here. Looking at\nprevious graphs one can say that the text weight of\nmale and female tweet is double than that of brands.\nWe can also conclude that the brands tweet mostly\nin the period of day when we discovered most activity\non twitter. We will confirm these facts in other\nvisualizations."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Plotting of retweet per year"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_avg_year, word_avg = average(dataset['retweet_count'], dataset['created'].dt.year, \"Num\")\npyplot.bar(word_avg_year, word_avg, tick_label = word_avg_year)\npyplot.suptitle('Categorical Plotting of retweet per year')\npyplot.xlabel(\"year\")\npyplot.ylabel(\"Number of retweets\")\npyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a visualization with some shocking\npredictions. The popularity boom in 2009 can be seen to\nbe due to massive shoot in retweet counts.\nThis can be related to a certain world spread event\nwhich was gaining fire in twitter. But that is just a\nprediction.\nRemoving 2009 the graph shows a very flattened\ncurve with a slight rise every year rate."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Plotting of retweet per gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_avg_gender, word_avg = average(dataset['retweet_count'], dataset['gender'], \"Num\")\npyplot.pie(word_avg, labels = word_avg_gender, startangle = 90, shadow = True, radius = 1.2, autopct = '%1.1f%%') \npyplot.suptitle('Categorical Plotting of retweet per gender')\npyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a visualization with some shocking\npredictions. The popularity boom in 2009 can be seen to\nbe due to massive shoot in retweet counts.\nThis can be related to a certain world spread event\nwhich was gaining fire in twitter. But that is just a\nprediction.\nRemoving 2009 the graph shows a very flattened\ncurve with a slight rise every year rate."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Plotting of gender per hour tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"################################################################################################## Function to plot line graph\ndef plot_x_per_y(legend1, legend2, legend3, y_per_x1, y_per_x2, y_per_x3, column1, column2, column3, column4):\n    plot = pyplot.plot(y_per_x1[column1], [y for y in y_per_x1[column2]], label = legend1)\n    plot = pyplot.plot(y_per_x2[column1], [y for y in y_per_x2[column3]], label = legend2)\n    plot = pyplot.plot(y_per_x3[column1], [y for y in y_per_x3[column4]], label = legend3)\n    return plot\n\n################################################################################################## Function to Create temporary dataset as per the graph needs\ndef x_per_y(y_check1, y_check2, y_check3, x, y, column1, column2, column3, column4):\n    x_now = pd.value_counts(x).sort_index()\n    a, b, c = 0, 0, 0\n    y_temp = []\n    y_per_x = pd.DataFrame(columns = [column1, column2, column3, column4])\n    for j in x_now.index:\n        y_temp = (y.loc[x == j])\n        for y_now in y_temp:\n            if y_now == y_check1:\n                a = a + 1\n            if y_now == y_check2:\n                b = b + 1\n            if y_now == y_check3:\n                c = c + 1\n        y_per_x.loc[j] = [j] + [a] + [b] + [c]\n        a, b, c = 0, 0, 0\n    plot_x_per_y(y_check1, y_check2, y_check3, y_per_x, y_per_x, y_per_x, column1, column2, column3, column4)\n    return\n\n################################################################################################## Categorical Plotting of gender per hour tweet\nplot = x_per_y( 'male', 'female', 'brand', dataset['created'].dt.hour, dataset['gender'], 'Hour', 'male', 'female', 'brand')\nplot = pyplot.suptitle('Categorical Plotting of gender per hour tweet')\nplot = pyplot.xlabel(\"Hour Time\")\nplot = pyplot.ylabel(\"Number of tweets\")\nplot = pyplot.legend()\nplot = pyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is how genders and brand behave in hourly\nmanner in a day.\nThis graph further confirms the fact that twitter is\nused as the news media for most of the user as the\nactivity in the morning for all gender and brand is\nalmost the same.\nSomething else can also be predicted which is\nfemales use twitter the most in working hours!\nFrom 12 in night to 5 in morning are the quietest\nhours on twitter in a day."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Plotting of gender per year tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = x_per_y( 'male', 'female', 'brand', dataset['created'].dt.year, dataset['gender'], 'Year', 'male', 'female', 'brand')\nplot = pyplot.suptitle('Categorical Plotting of gender per year tweet')\nplot = pyplot.xlabel(\"Year\")\nplot = pyplot.ylabel(\"Number of tweets\")\nplot = pyplot.legend()\nplot = pyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph is yet another confirmation of the 2009\npopularity boom. As seen clearly the popularity was\ndriven by male and females but brands donâ€™t contribute\nin that popularity much. Further strengthening the fact\nthat twitter is used to consume news media as brands\nusually use twitter for publishing media not consuming\nmedia.\nAt the end of the graph there seems to be more\nfemale activity than male, and the trend is followed by\nbrands too. We can predict this is due to recent gender\nequality that females in the field of business are now\nnor present full thn ever!"},{"metadata":{},"cell_type":"markdown","source":"## Categorical Plotting of gender emotion"},{"metadata":{"trusted":true},"cell_type":"code","source":"################################################################################################## Creating text per gender dataset\ntext_male, text_female, text_brand = '', '', ''\nfor i in dataset['text'].loc[dataset['gender'] == 'male'] + \" \" + dataset['description'].loc[dataset['gender'] == 'male']:\n    text_male = text_male + ' ' + i\nfor i in dataset['text'].loc[dataset['gender'] == 'female'] + \" \" + dataset['description'].loc[dataset['gender'] == 'female']:\n    text_female = text_female + ' ' + i\nfor i in dataset['text'].loc[dataset['gender'] == 'brand'] + \" \" + dataset['description'].loc[dataset['gender'] == 'brand']:\n    text_brand = text_brand + ' ' + i\n    \n################################################################################################## Function to convert any countable item into a dataset\ndef to_dataset_converter(counter, column_name):\n    temp, temp2 = [], []\n    for i in counter.keys():\n        temp.append(i)\n    for i in counter.values():\n        temp2.append(i)\n    dataset_temp = pd.DataFrame(columns = [column_name, 'count'])\n    for i in range(0, len(counter)):\n        dataset_temp.loc[i] = [temp[i]] + [temp2[i]]\n    dataset_temp.sort_values(column_name, axis = 0, ascending = True, inplace = True, na_position ='last')\n    dataset_temp.drop(dataset_temp[dataset_temp[column_name] == 'compound'].index, inplace = True)\n    return dataset_temp\n\n################################################################################################## Function for emotional analysis per gender\ndef emotional_analysis(text, column_name_1, column_name_2, column_name_3):\n    tokenize_words = word_tokenize(text)\n    clean_words=[]\n    for i in tokenize_words:\n        if i not in stopwords.words(\"english\"):\n            clean_words.append(i)\n    emotions = []\n    words_list = []\n    with open(\"../input/emotion/emotion.txt\",\"r\") as file:\n        for i in file:\n            temp = i.replace(\"\\n\",\"\")\n            temp = temp.strip()\n            temp = temp.replace(\" \",\"\")\n            temp = temp.replace(\",\",\"\")\n            temp = temp.replace(\"'\",\"\")\n            word, emotion = temp.split(\":\")\n            if word in clean_words:\n                emotions.append(emotion)\n                words_list.append(word)\n    counter_3 = Counter(words_list)\n    counter_2 = Counter(emotions)\n    counter_1 = SentimentIntensityAnalyzer().polarity_scores(text)\n    dataset_3 = to_dataset_converter(counter_3, column_name_3)\n    dataset_3.drop(dataset_3[(dataset_3['count'] == 1)].index, inplace = True)\n    dataset_2 = to_dataset_converter(counter_2, column_name_2)\n    dataset_1 = to_dataset_converter(counter_1, column_name_1)\n    return dataset_1, dataset_2, dataset_3\n\n################################################################################################## Gathering data for plotting graphs\nsentiments_dataset_male, emotions_dataset_male, words_dataset_male = emotional_analysis(text_male, 'sentiment' , 'emotion', 'words')\nsentiments_dataset_female, emotions_dataset_female, words_dataset_female = emotional_analysis(text_female, 'sentiment' , 'emotion', 'words')\nsentiments_dataset_brand, emotions_dataset_brand, words_dataset_brand = emotional_analysis(text_brand, 'sentiment' , 'emotion', 'words')\n\n################################################################################################## Categorical Plotting of gender emotion\nplot = plot_x_per_y('Emotion in males', 'Emotion in females', 'Emotion in brands', emotions_dataset_male, emotions_dataset_female, emotions_dataset_brand, 'emotion', 'count', 'count', 'count')\nplot = pyplot.suptitle('Categorical Plotting of gender emotion')\nplot = pyplot.xlabel(\"Emotion\")\nplot = pyplot.xticks(rotation = 90)\nplot = pyplot.ylabel(\"Proportions\")\nplot = pyplot.legend()\nplot = pyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical Plotting of words of emotion used by males"},{"metadata":{},"cell_type":"markdown","source":"Here we can see that the most common emotions\nused by both genders and brands are anger, attracted,\nfearful, happy and sad. Happy and sad being the most\ntwo prominent emotions expressed.\nEmotions like bored, average, cheated,\nembarrassed and lustful are not common among daily\ntweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"pyplot.pie(words_dataset_male['count'], labels = words_dataset_male['words'], startangle = 90, shadow = True, radius = 1.2, autopct = '%1.1f%%') \npyplot.suptitle('Categorical Plotting of words of emotion used by males')\npyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a graph which will be compared by the other\ntwo graphs shown below and it is very clear that the\nemotions expressed by males uses more different\nwords than females or brands.\nMales require more words to convey same\nemotions than females.\nProviding some support to fact that females getting\nmore into business and showing there presence in\nsociety as being more efficient in words."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Plotting of words of emotion used by females"},{"metadata":{"trusted":true},"cell_type":"code","source":"pyplot.pie(words_dataset_female['count'], labels = words_dataset_female['words'], startangle = 90, shadow = True, radius = 1.2, autopct = '%1.1f%%') \npyplot.suptitle('Categorical Plotting of words of emotion used by females')\npyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The words used by females to express emotions are\nless and consistent. This be an overall use of less words\nto convey a message and may have may significant\npredictions."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Plotting of words of emotion used by brands"},{"metadata":{"trusted":true},"cell_type":"code","source":"pyplot.pie(words_dataset_brand['count'], labels = words_dataset_brand['words'], startangle = 90, shadow = True, radius = 1.2, autopct = '%1.1f%%') \npyplot.suptitle('Categorical Plotting of words of emotion used by brand')\npyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Brands use the least word to convey emotions and\nhave high text in tweet ratio that genders which means\nthat brands have a very low density of emotional words\nin a tweet than genders."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Plotting of gender sentiment"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = plot_x_per_y('Sentiments in males', 'Sentiments in females', 'Sentiments in brands', sentiments_dataset_male, sentiments_dataset_female, sentiments_dataset_brand, 'sentiment', 'count', 'count', 'count')\nplot = pyplot.suptitle('Categorical Plotting of gender sentiment')\nplot = pyplot.xlabel(\"Sentiments\")\nplot = pyplot.ylabel(\"Proportion\")\nplot = plot = pyplot.legend()\nplot = pyplot.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The overall sentiment score of brands and genders\nis mostly neutral. The negative and positive ends only\nhave minimum information. But still genders and\nbrands tend to lean towards positive tweets rather than\nnegative tweets. "},{"metadata":{},"cell_type":"markdown","source":"## Categorical Plotting of typing mistakes per gender"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"################################################################################################## Function to check if spelling is correct ot not\ndef spelling_checker(text, breaker):\n    counter = 0\n    breaker_reach = 0\n    splits = text.split()\n    for split in splits:\n        word = re.compile(r\"(.)\\1{2,}\")\n        word_final = word.sub(r\"\\1\\1\", split)\n        correct_word = suggest(word_final)[0][0].replace(\"'\", \"\")\n        if correct_word != split:\n            counter += 1\n        breaker_reach += 1\n        if breaker == breaker_reach:\n            breaker_reach = 0\n            break\n    return counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################################################################################################## Collecting data of spelling mistakes per gender\ncounter_male_typo = spelling_checker(text_male, 4000)\ncounter_female_typo = spelling_checker(text_female, 4000)\ncounter_brand_typo = spelling_checker(text_brand, 4000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################################################################################################## Categorical Plotting of typing mistakes per gender\npyplot.bar(['male', 'female', 'brand'], [counter_male_typo, counter_female_typo, counter_brand_typo], tick_label = ['male', 'female', 'brand'])\npyplot.suptitle('Categorical Plotting of typing mistakes per gender')\npyplot.xlabel(\"Gender\")\npyplot.ylabel(\"Typing mistakes\")\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here you can see that females and brand make less\ntyping mistakes than males. The sample size of this\nresult was 4000 from each male, female and brands\nwords.\nThis shows that men are more de-focused or just\ntype fast while making more mistakes. Other than that\nwe can also conclude that the brands are run or at least\ntheir social accounts are handled mostly by females."},{"metadata":{},"cell_type":"markdown","source":"# Ensemble Classification Super Learner models deployment \"\"\"Prediction gender based on text\"\"\"\n\n## description will be put next week"},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################################################################################################### Classification models deployment \"\"\"Prediction gender based on text\"\"\"\n####################################################################################################################### Stacking\n#######################################################################################################################\n####################################################################################################################### get the dataset\nfile = open(r\"./Model Accuracy.txt\",\"w+\")\n\ndef get_dataset(dataset_y, dataset_x):\n    y = dataset_y.values\n    count_vectorizer = CountVectorizer(max_features = 4000, stop_words = \"english\")\n    sparce_matrix = count_vectorizer.fit_transform(dataset_x).toarray()\n    X = sparce_matrix\n    return X, y\n\n####################################################################################################################### get a stacking ensemble of models\ndef get_stacking():\n    ################################################################################################################### define the base models\n    level0 = list()\n    level0.append(('Logistic Regression', LogisticRegression()))\n    level0.append(('K Nearest Neighbour', KNeighborsClassifier()))\n    level0.append(('Decision Tree Classifier', DecisionTreeClassifier()))\n    level0.append(('Support Vector Classifier', SVC()))\n    level0.append(('Gaussian Navy Bayse', GaussianNB()))\n    level0.append(('ADA boost', AdaBoostClassifier()))\n    level0.append(('Bagging Classifier', BaggingClassifier(n_estimators = 10)))\n    level0.append(('Random Forest Classifier', RandomForestClassifier(n_estimators = 10)))\n    level0.append(('Extra Trees Classifier', ExtraTreesClassifier(n_estimators = 10)))\n    ################################################################################################################### define meta learner model\n    level1 = LogisticRegression()\n    ################################################################################################################### define the stacking ensemble\n    model = StackingClassifier(estimators = level0, final_estimator = level1, cv = 5)\n    return model\n\n####################################################################################################################### get a list of models to evaluate\ndef get_models():\n    models = dict()\n    models['Logistic Regression'] = LogisticRegression()\n    models['K Nearest Neighbour'] = KNeighborsClassifier()\n    models['Decision Tree Classifier'] = DecisionTreeClassifier()\n    models['Support Vector Classifier'] = SVC()\n    models['Gaussian Navy Bayse'] = GaussianNB()\n    models['ADA boost'] = AdaBoostClassifier()\n    models['Bagging Classifier'] = BaggingClassifier()\n    models['Random Forest Classifier'] = RandomForestClassifier()\n    models['Extra Trees Classifier'] = ExtraTreesClassifier()\n    models['Stacking'] = get_stacking()\n    return models\n\n####################################################################################################################### evaluate a give model using cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\n    scores = cross_val_score(model, X, y, scoring = 'accuracy', cv = cv, n_jobs = -1, error_score = 'raise')\n    return scores\n\n####################################################################################################################### get database and plot the result of all models \ndataset['gender'] = dataset['gender'].replace(['male', 'female', 'brand'],[0, 1, 2])\nX, y = get_dataset(dataset['gender'], dataset['text'])\nmodels = get_models()\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    file.writelines(str('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)) + \"\\n\"))\nprint(file)\npyplot.boxplot(results, labels = names, showmeans = True)\npyplot.xticks(rotation = 90)\npyplot.show()\n\n####################################################################################################################### predicting on new dataset\nmodel_test = get_stacking()\ndataset_store_gender_confodence_non1['gender'] = dataset_store_gender_confodence_non1['gender'].replace(['male', 'female', 'brand'],[0, 1, 2])\nX_test, y_test = get_dataset(dataset_store_gender_confodence_non1['gender'], dataset_store_gender_confodence_non1['text'])\nmodel_test.fit(X_test, y_test)\nscores_test = evaluate_model(model_test, X_test, y_test)\nprint('>%s %.3f (%.3f)' % ('stacking_pridection', mean(scores_test), std(scores_test)))\nyhat = model_test.predict(X_test)\n\nconfusion_test = confusion_matrix(y_test , yhat)\n\ndef accuracy(confusion_matrix):\n    diagonal_sum = confusion_matrix.trace()\n    sum_of_all_elements = confusion_matrix.sum()\n    return diagonal_sum / sum_of_all_elements\n\nfile.writelines(\"Accuracy of super learner stacking ML algorithm on test dataset is :-> \" + str(accuracy(confusion_test) * 100))\nsn.set(font_scale = 1.4)\nsn.heatmap(confusion_test, annot = True, annot_kws = {\"size\" : 16}, fmt = \"d\", xticklabels = ['male', 'female', 'brand'], yticklabels = ['male', 'female', 'brand'])\npyplot.xlabel('Predictions')\npyplot.ylabel('Actual')\npyplot.savefig(\"Confusion Matrix of the Ensemble learner model.png\", dpi = 300, bbox_inches = 'tight')\npyplot.show()\nfile.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network for prediction of gender based on text"},{"metadata":{},"cell_type":"markdown","source":"The function below will get the tokenized sentences form the text and descriptions"},{"metadata":{"trusted":true},"cell_type":"code","source":"############################################################## Get sequences of tokenised \ndef get_sequences(texts, vocab_length):\n    tokenizer = Tokenizer(num_words = vocab_length)\n    tokenizer.fit_on_texts(texts)\n    sequences = tokenizer.texts_to_sequences(texts)\n    max_seq_length = np.max([len(sequence) for sequence in sequences])\n    sequences = pad_sequences(sequences, maxlen = max_seq_length, padding = 'post')\n    return sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################################################## Get RGB values from the hexadecimal values\ndef hex_to_decimal(x):\n    try:\n        return np.int(x, 16)\n    except:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################################################## Get RGB values from The column\ndef get_rgb(colors):\n    r = colors.apply(lambda x: hex_to_decimal(x[0:2]))\n    g = colors.apply(lambda x: hex_to_decimal(x[2:4]))\n    b = colors.apply(lambda x: hex_to_decimal(x[4:6]))\n    return r, g, b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################################################## Prepairing complete dataset\ndef dataset_prepairing():\n    df = df_discard.copy()\n    df = df.drop(['_unit_id', 'name', 'profileimage', 'tweet_id'], axis = 1)\n    \n    missing_cols = df.columns[df.isna().mean() > 0.3]\n    df = df.drop(missing_cols, axis = 1)\n    \n    df.drop(df[(df['gender'] != 'male') & (df['gender'] != 'female') & (df['gender'] != 'brand')].index, inplace = True)\n    df.drop(df[(df['gender:confidence'] < 0.7)].index, inplace = True)\n    \n    judgment_nas = df[df['_last_judgment_at'].isna()].index\n    df = df.drop(judgment_nas, axis = 0).reset_index(drop = True)\n    df['description'] = df['description'].fillna('')\n    for column in ['_last_judgment_at', 'created', 'tweet_created']:\n        df[column] = pd.to_datetime(df[column])\n    \n    df['judgment_day'] = df['_last_judgment_at'].apply(lambda x: x.day)\n    df['judgment_hour'] = df['_last_judgment_at'].apply(lambda x: x.hour)\n    \n    df['created_year'] = df['created'].apply(lambda x: x.year)\n    df['created_month'] = df['created'].apply(lambda x: x.month)\n    df['created_day'] = df['created'].apply(lambda x: x.day)\n    df['created_hour'] = df['created'].apply(lambda x: x.hour)\n    \n    df['tweet_hour'] = df['tweet_created'].apply(lambda x: x.hour)\n    \n    df = df.drop(['_last_judgment_at', 'created', 'tweet_created'], axis = 1)\n    desc = get_sequences(df['description'], vocab_length = 20000)\n    tweets = get_sequences(df['text'], vocab_length = 20000)\n    \n    df = df.drop(['description', 'text'], axis = 1)\n    df = df.drop(['_golden', '_unit_state', '_trusted_judgments', 'profile_yn'], axis = 1)\n    df['link_red'], df['link_green'], df['link_blue'] = get_rgb(df['link_color'])\n    df['side_red'], df['side_green'], df['side_blue'] = get_rgb(df['sidebar_color'])\n    \n    df = df.drop(['link_color', 'sidebar_color'], axis = 1)\n    label_mapping = {'female': 0, 'male': 1, 'brand': 2}\n    df['gender'] = df['gender'].replace(label_mapping)\n    y = df['gender'].copy()\n    X = df.drop('gender', axis = 1).copy()\n    \n    scaler = StandardScaler()\n    X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\n    return desc, tweets, X, y\n\ndesc, tweets, X, y = dataset_prepairing()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################################################## Prepairing test dataset\nX_train, X_test, desc_train, desc_test, tweets_train, tweets_test, y_train, y_test = train_test_split(X, desc, tweets, y, train_size = 0.7, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making of the Neural network model for prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"############################################################## Function to load model\ndef build_model():\n    X_inputs = tf.keras.Input(shape=(X.shape[1],))\n    desc_inputs = tf.keras.Input(shape = (desc.shape[1], ))\n    tweet_inputs = tf.keras.Input(shape = (tweets.shape[1], ))\n    \n    # X\n    X_dense1 = tf.keras.layers.Dense(256, activation='relu')(X_inputs)\n    X_dense2 = tf.keras.layers.Dense(256, activation='relu')(X_dense1)\n\n    # desc\n    desc_embedding = tf.keras.layers.Embedding(input_dim = 20000, output_dim = 256, input_length = desc.shape[1])(desc_inputs)\n    desc_gru = tf.keras.layers.GRU(256, return_sequences = False)(desc_embedding)\n    desc_flatten = tf.keras.layers.Flatten()(desc_embedding)\n    desc_concat = tf.keras.layers.concatenate([desc_gru, desc_flatten])\n\n    # tweets\n    tweet_embedding = tf.keras.layers.Embedding(input_dim = 20000, output_dim = 256, input_length = tweets.shape[1])(tweet_inputs)\n    tweet_gru = tf.keras.layers.GRU(256, return_sequences = False)(tweet_embedding)\n    tweet_flatten = tf.keras.layers.Flatten()(tweet_embedding)\n    tweet_concat = tf.keras.layers.concatenate([tweet_gru, tweet_flatten])\n\n    concat = tf.keras.layers.concatenate([X_dense2, desc_concat, tweet_concat])\n\n    outputs = tf.keras.layers.Dense(3, activation='softmax')(concat)\n\n    model = tf.keras.Model(inputs = [X_inputs, desc_inputs, tweet_inputs], outputs = outputs)\n    return model\nmodel = build_model()\nprint(model.summary())\ntf.keras.utils.plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################################################## Training the model\nmodel.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\nbatch_size = 16\nepochs = 3\nhistory = model.fit([X_train, desc_train, tweets_train], y_train, validation_split = 0.2, batch_size = batch_size, epochs = epochs, callbacks = [tf.keras.callbacks.ModelCheckpoint('./model.h5', save_best_only = True, save_weights_only = True), tf.keras.callbacks.ReduceLROnPlateau()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############################################################## Predicting with the model\nmodel.load_weights('./model.h5')\nresults = model.evaluate([X_test, desc_test, tweets_test], y_test, verbose = 0)\nprint(\"Model Accuracy: {:.2f}%\".format(results[1] * 100))\ny_true = np.array(y_test)\n\ny_pred = model.predict([X_test, desc_test, tweets_test])\ny_pred = map(lambda x: np.argmax(x), y_pred)\ny_pred = np.array(list(y_pred))\ncm = confusion_matrix(y_true, y_pred)\nclr = classification_report(y_true, y_pred, target_names = ['Female', 'Male', 'Brand'])\npyplot.figure(figsize = (6, 6))\nsn.heatmap(cm, annot = True, fmt = 'g', cbar = False, cmap = 'Blues')\npyplot.xticks(np.arange(3) + 0.5, ['Female', 'Male', 'Brand'])\npyplot.yticks(np.arange(3) + 0.5, ['Female', 'Male', 'Brand'])\npyplot.xlabel(\"Predicted\")\npyplot.ylabel(\"Actual\")\npyplot.title(\"Confusion Matrix\")\npyplot.savefig(\"Confusion matrix.png\", dpi = 300, bbox_inches = 'tight')\npyplot.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}