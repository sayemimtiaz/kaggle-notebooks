{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Reddit NLP Posts Prediction ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport string\n\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nfrom wordcloud import WordCloud\nfrom tqdm.auto import tqdm\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nfrom sklearn.metrics import plot_roc_curve\nfrom numpy import interp\nfrom itertools import cycle\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# load data\n\ndata = pd.read_csv(\"../input/dataisbeautiful/r_dataisbeautiful_posts.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(data.isnull().sum() / len(data)) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see some columns having too many missing values. We will drop those.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del data['id']\ndel data['author_flair_text']\ndel data['removed_by']\ndel data['total_awards_received']\ndel data['awarders']\ndel data['created_utc']\ndel data['full_link']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have only the relevant information and no missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# check out numeric columns\n\ndata.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both columns look like they are heavily skewed when comparing the 75 percentile and max values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The score distribution is heavily skewed ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,5))\n\nsns.kdeplot(data['score'], shade=  True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(data[data['score'] < 10]), 'Posts with less than 10 votes')\nprint(len(data[data['score'] > 10]), 'Posts with more than 10 votes')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Same with the comments ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,5))\n\nsns.kdeplot(data['num_comments'], shade=  True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(data[data['num_comments'] < 10]), 'Posts with less than 10 comments')\nprint(len(data[data['num_comments'] > 10]), 'Posts with more than 10 comments')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# post with the most comments\n\ndata[data['score'] == data['score'].max()]['title'].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"These are useful cleaning functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef remove_line_breaks(text):\n    text = text.replace('\\r', ' ').replace('\\n', ' ')\n    return text\n\n#remove punctuation\ndef remove_punctuation(text):\n    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n    '''Escape all the characters in pattern except ASCII letters and numbers'''\n    tokens = word_tokenize(text)\n    tokens_zero_punctuation = []\n    for token in tokens:\n        if not re_replacements.match(token):\n            token = re_punctuation.sub(\" \", token)\n        tokens_zero_punctuation.append(token)\n    return ' '.join(tokens_zero_punctuation)\n\ndef remove_special_characters(text):\n    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n    return text\n\ndef lowercase(text):\n    text_low = [token.lower() for token in word_tokenize(text)]\n    return ' '.join(text_low)\n\ndef remove_stopwords(text):\n    stop = set(stopwords.words('english'))\n    word_tokens = nltk.word_tokenize(text)\n    text = \" \".join([word for word in word_tokens if word not in stop])\n    return text\n\n#remobe one character words\ndef remove_one_character_words(text):\n    '''Remove words from dataset that contain only 1 character'''\n    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      \n    return ' '.join(text_high_use)   \n    \n#%%\n# Stemming with 'Snowball stemmer\" package\ndef stem(text):\n    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        \n    return ' '.join(text_stemmed)\n\ndef lemma(text):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    word_tokens = nltk.word_tokenize(text)\n    text_lemma = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])       \n    return ' '.join(text_lemma)\n\n\n#break sentences to individual word list\ndef sentence_word(text):\n    word_tokens = nltk.word_tokenize(text)\n    return word_tokens\n#break paragraphs to sentence token \ndef paragraph_sentence(text):\n    sent_token = nltk.sent_tokenize(text)\n    return sent_token    \n\n\ndef tokenize(text):\n    \"\"\"Return a list of words in a text.\"\"\"\n    return re.findall(r'\\w+', text)\n\ndef remove_numbers(text):\n    no_nums = re.sub(r'\\d+', '', text)\n    return ''.join(no_nums)\n\n\n\ndef clean_text(text):\n    _steps = [\n    remove_line_breaks,\n    remove_one_character_words,\n    remove_special_characters,\n    lowercase,\n    remove_punctuation,\n    remove_stopwords,\n    stem,\n    remove_numbers\n]\n    for step in _steps:\n        text=step(text)\n    return text   \n#%%\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now using list comprehension we clean the titles and append the cleaned text as columns to the df.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['clean_title'] = pd.Series([clean_text(i) for i in tqdm(data['title'])])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"words = data[\"clean_title\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls = []\n\nfor i in words:\n    ls.append(str(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The wordcloud of Cthulhu/squidy thing for HP Lovecraft\nplt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"black\", max_words=1000, max_font_size= 200,  width=1600, height=800)\nwc.generate(\" \".join(ls))\nplt.title(\"Most discussed terms\", fontsize=20)\nplt.imshow(wc.recolor( colormap= 'viridis' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data, Visualization, USA and Time are popular terms.\nOriginal content seems to be very popular as well, as can be seen in the graph below.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Most popular posts","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"most_pop = data.sort_values('score', ascending =False)[['title', 'score']].head(12)\n\nmost_pop['score1'] = most_pop['score']/1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,25))\n\nsns.barplot(data = most_pop, y = 'title', x = 'score1', color = 'c')\nplt.xticks(fontsize=27, rotation=0)\nplt.yticks(fontsize=31, rotation=0)\nplt.xlabel('Votes in Thousands', fontsize = 21)\nplt.ylabel('')\nplt.title('Most popular posts', fontsize = 30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Roughly 75% of the most popular titles are Original Content","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_com = data.sort_values('num_comments', ascending =False)[['title', 'num_comments', 'author']].head(12)\nmost_com['num_comments1'] = most_com['num_comments']/1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(most_com)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data.reset_index()\nx[x['index'] == 92800]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_com = most_com[most_com.author != 'dinoignacio']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,25))\n\nsns.barplot(data = most_com, y = 'title', x = 'num_comments1', color = 'y')\nplt.xticks(fontsize=28, rotation=0)\nplt.yticks(fontsize=30, rotation=0)\nplt.xlabel('Comments in Thousands', fontsize = 21)\nplt.ylabel('')\nplt.title('Most commented posts', fontsize = 30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The titles with the most cpmments seem to be more controversial, which makes sense.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"most_com.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = data.sort_values('score', ascending =False)\n\nn['score1'] = n['score']/1000\nn['num_comments1'] = n['num_comments']/1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,15))\n\nsns.regplot(data = n, y = 'score1', x = 'num_comments1', color = 'purple')\nplt.xticks(fontsize=14, rotation=0)\nplt.yticks(fontsize=14, rotation=0)\nplt.xlabel('Comments in Thousands', fontsize = 15)\nplt.ylabel('Votes in Thousands')\nplt.title('Comments and votes', fontsize = 14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['num_comments'] == data['num_comments'].max()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Natural Language Processing ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Topic Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nimport nltk\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = SnowballStemmer('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming(token))\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['title'].iloc[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_sample = data['title'].iloc[1]\nprint('original document: ')\n\nwords = []\n\nfor word in doc_sample.split(' '):\n    words.append(word)\n    \n    \nprint(words)\nprint('\\n\\n tokenized and lemmatized document: ')\nprint(preprocess(doc_sample))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"University to 'univers' --> not too good","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['clean_title'] = data['clean_title'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = []\n\nfor i in data['clean_title']:\n        words.append(i.split(' '))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create the dictionary\n\n\n\n--> every unique word in titles","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary = gensim.corpora.Dictionary(words)\n\ncount = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter out tokens in the dictionary by their frequency.\n\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Create Corpus -> term document frequency\n\ndoc2bow() simply counts the number of occurrences of each distinct word, \nconverts the word to its integer word ID and returns the result as a sparse vector.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_corpus = [dictionary.doc2bow(doc) for doc in words]\nbow_corpus[4310]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_doc_4310 = bow_corpus[4310]\n\nfor i in range(len(bow_doc_4310)):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n                                               dictionary[bow_doc_4310[i][0]], \nbow_doc_4310[i][1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TF/IDF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim import corpora, models\n\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\n\nfrom pprint import pprint\n\nfor doc in corpus_tfidf:\n    pprint(doc)\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore(bow_corpus,\n                                       num_topics=10,\n                                       id2word=dictionary,\n                                       passes=2,\n                                       workers=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now we show the output of the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, topic in lda_model.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The parallelization uses multiprocessing; in case this doesnâ€™t work for you for some reason, try the gensim.models.ldamodel.LdaModel class which is an equivalent, but more straightforward and single-core implementation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf,\n                                             num_topics=10,\n                                             id2word=dictionary,\n                                             passes=2,\n                                             workers=4)\n\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in the output, we should definitely also clean the text for links --> 'https:' ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Lets check out with how much certainty the model predicts a new title to belong to one of the created topics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"unseen_document = 'How a Pentagon deal became an identity crisis for Google'\nbow_vector = dictionary.doc2bow(preprocess(unseen_document))\n\nfor index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['over_18'] = data['over_18'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['over_18'] = pd.Categorical(data['over_18']) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check class balance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(data['over_18'].value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am not sure if its possible to get reliable predictions with one class being only represented in 0.5% of the titles.\n\nI tried resampling, but the results at the end look way too good to be true.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Separate majority and minority classes\ndf_majority = data[data.over_18==0]\ndf_minority = data[data.over_18==1]\n \n# Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=180000) # reproducible results\n \n# Combine majority class with upsampled minority class\ndata_n = pd.concat([df_majority, df_minority_upsampled])\n \n# Display new class counts\ndata_n['over_18'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(data_n['over_18'].value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, naive_bayes, svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_text = data_n['clean_title']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create TF/IDF again","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer()\ntfidf = vectorizer.fit_transform(processed_text)\nprint(tfidf.shape)\nprint('\\n')\n#print(vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_n['over_18']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data_n['over_18']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(tfidf, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the training dataset on the NB classifier\nNaive = naive_bayes.MultinomialNB()\nNaive.fit(X_train_tf,y_train_tf)\n# predict the labels on validation dataset\npredictions_NB_tf = Naive.predict(X_test_tf)\n# Use accuracy_score function to get the accuracy\nprint(\"Naive Bayes Accuracy -> \",accuracy_score(predictions_NB_tf, y_test_tf)*100)\nprint(classification_report(predictions_NB_tf,y_test_tf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logmodel = LogisticRegression()\nlogmodel.fit(X_train_tf, y_train_tf)\n\npredictions_LR_tf = logmodel.predict(X_test_tf)\n\nprint(\"LR Accuracy -> \",accuracy_score(predictions_LR_tf, y_test_tf)*100)\nprint(classification_report(predictions_LR_tf,y_test_tf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the training dataset on the KNN classifier from sklearn.neighbors import KNeighborsClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X_train_tf, y_train_tf)\npredictions_KNN_tf = neigh.predict(X_test_tf)\nprint(\"KNN Accuracy -> \",accuracy_score(predictions_KNN_tf, y_test_tf)*100)\nprint(classification_report(predictions_KNN_tf,y_test_tf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nsvm = svm.SVC()\nsvm.fit(X_train_tf, y_train_tf)\npredictions_SVM_tf = svm.predict(X_test_tf)\nprint(\"SVM Accuracy -> \",accuracy_score(predictions_SVM_tf, y_test_tf)*100)\nprint(classification_report(predictions_SVM_tf,y_test_tf))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}