{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport spacy\nfrom spacy import displacy\nfrom spacy.util import minibatch, compounding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing dataset\ndf = pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv\",encoding = \"Latin-1\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shape of dataset\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for null values\ndf.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Datatypes of each column\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Sentiment.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.replace(to_replace=\"Extremely Negative\", value=\"Negative\", inplace=True)\ndf.replace(to_replace=\"Extremely Positive\", value=\"Positive\", inplace=True)\ndf.replace(to_replace=\"Neutral\", value=\"Negative\", inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Sentiment.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf.Sentiment = le.fit_transform(df.Sentiment)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Sentiment.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df.Sentiment)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = df[df.Sentiment==1][:18000]\ndf2 = df[df.Sentiment==0][:18000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df1.append(df2)\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train[[\"OriginalTweet\",\"Sentiment\"]]\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df_train.Sentiment)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenization\nspacy_tok = spacy.load('en_core_web_sm')\nsample_tweet = df_train.OriginalTweet[23]\nsample_tweet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parsed_tweet = spacy_tok(sample_tweet)\nparsed_tweet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/tylerneylon/explacy/master/explacy.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import explacy\nexplacy.print_parse_info(spacy_tok,'Covid-19 has various Symptoms') # text for example","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explacy.print_parse_info(spacy_tok,df_train.OriginalTweet[23])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_text = pd.DataFrame()\n\nfor i, token in enumerate(parsed_tweet):\n    tokenized_text.loc[i, 'text'] = token.text\n    tokenized_text.loc[i, 'lemma'] = token.lemma_,\n    tokenized_text.loc[i, 'pos'] = token.pos_\n    tokenized_text.loc[i, 'tag'] = token.tag_\n    tokenized_text.loc[i, 'dep'] = token.dep_\n    tokenized_text.loc[i, 'shape'] = token.shape_\n    tokenized_text.loc[i, 'is_alpha'] = token.is_alpha\n    tokenized_text.loc[i, 'is_stop'] = token.is_stop\n    tokenized_text.loc[i, 'is_punctuation'] = token.is_punct\n\ntokenized_text[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spacy.explain('GPE')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" **Dependency Parsing**:\n* Syntactic Parsing or Dependency parsing is process of identifying sentences and assigning a syntactic structure to it. As in subject combined with object makes a sentence. Spacy provides a sparse tree which can be used to generate this structure.","metadata":{}},{"cell_type":"markdown","source":"**Sentence Boundary Detection**:\n* Figuring out where sentences start and ends is important in NLP.","metadata":{}},{"cell_type":"code","source":"sentence_spans = list(parsed_tweet)\nsentence_spans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"displacy.render(parsed_tweet, style='dep',jupyter=True, options={\"distance\":140})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install scattertext\nimport scattertext as st\nnlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['parsed'] = df_train.OriginalTweet.apply(nlp)\ncorpus = st.CorpusFromParsedDocuments(df_train,category_col=\"Sentiment\", parsed_col=\"parsed\").build()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SpaCy Text Categorizer**:\n* We will train convolutional neural network text classifier on our Coronavirus Tweets using spaCy's new TextCategorizer component.\n* SpaCy provides classification model with multiple labels,non_mutually exclusive labels.The TextCategorizer uses its own CNN to balance weights and other pipeline components.","metadata":{}},{"cell_type":"code","source":"df_train['tuples'] = df_train.apply(lambda row: (row[\"OriginalTweet\"], row[\"Sentiment\"]), axis=1)\ntrain = df_train[\"tuples\"].tolist()\ntrain[:6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#functions from spaCy documentation\ndef load_data(limit=0, split=0.8):\n    train_data = train\n    np.random.shuffle(train_data)\n    train_data = train_data[-limit:]\n    texts, labels = zip(*train_data)\n    cats = [{'POSITIVE': bool(y)} for y in labels]\n    split = int(len(train_data) * split)\n    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n\ndef evaluate(tokenizer, textcat, texts, cats):\n    docs = (tokenizer(text) for text in texts)\n    tp = 1e-8  # True positives\n    fp = 1e-8  # False positives\n    fn = 1e-8  # False negatives\n    tn = 1e-8  # True negatives\n    for i, doc in enumerate(textcat.pipe(docs)):\n        gold = cats[i]\n        for label, score in doc.cats.items():\n            if label not in gold:\n                continue\n            if score >= 0.5 and gold[label] >= 0.5:\n                tp += 1.\n            elif score >= 0.5 and gold[label] < 0.5:\n                fp += 1.\n            elif score < 0.5 and gold[label] < 0.5:\n                tn += 1\n            elif score < 0.5 and gold[label] >= 0.5:\n                fn += 1\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f_score = 2 * (precision * recall) / (precision + recall)\n    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n\n#(\"Number of texts to train from\",\"t\" , int)\nn_texts=30000\n#You can increase texts count if you have more computational power.\n\n#(\"Number of training iterations\", \"n\", int))\nn_iter=10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add the text classifier to the pipeline if it doesn't exist\n# nlp.create_pipe works for built-ins that are registered with spaCy\nif 'textcat' not in nlp.pipe_names:\n    textcat = nlp.create_pipe('textcat')\n    nlp.add_pipe(textcat, last=True)\n# otherwise, get it, so we can add labels to it\nelse:\n    textcat = nlp.get_pipe('textcat')\n\n# add label to text classifier\ntextcat.add_label('POSITIVE')\n\n# load the dataset\nprint(\"Loading Covid Tweets data...\")\n(train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\nprint(\"Using {} examples ({} training, {} evaluation)\"\n      .format(n_texts, len(train_texts), len(dev_texts)))\ntrain_data = list(zip(train_texts,\n                      [{'cats': cats} for cats in train_cats]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\nwith nlp.disable_pipes(*other_pipes):  # only train textcat\n    optimizer = nlp.begin_training()\n    print(\"Training the model...\")\n    print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n    for i in range(n_iter):\n        losses = {}\n        # batch up the examples using spaCy's minibatch\n        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n                       losses=losses)\n        with textcat.model.use_params(optimizer.averages):\n            # evaluate on the dev data split off in load_data()\n            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n        print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n              .format(losses['textcat'], scores['textcat_p'],\n                      scores['textcat_r'], scores['textcat_f']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_text1 = \"Mercedes is going to launch its new Car this weekend.\"\ntest_text2 = \"Coronavirus is grown to mutate itself.\"\ndoc = nlp(test_text1)\ntest_text1, doc.cats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"OriginalTweet\"][2900]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc3 = nlp(df_train[\"OriginalTweet\"][2900])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"OriginalTweet\"][2900], doc3.cats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"OriginalTweet\"][26770]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc4 = nlp(df_train[\"OriginalTweet\"][26770])\ndf_train[\"OriginalTweet\"][26770], doc4.cats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc5 = nlp(df_train[\"OriginalTweet\"][12500])\ndf_train[\"OriginalTweet\"][12500], doc5.cats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Now, we can apply this model to our Test dataset and get respective results whether tweets are related to coronavirus or not.","metadata":{}},{"cell_type":"markdown","source":"Author: Purvit Vashishtha","metadata":{}}]}