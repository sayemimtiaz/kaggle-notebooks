{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport torch\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import Input, Lambda, Dense\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.backend as K\nfrom tqdm import tqdm_notebook\n\nimport helper_functions as hf\nimport meta_cleaning as mc\nimport eda_text as et\nimport config \nimport viz_plot as vp\nimport word_cloud_prep as wcp\nimport covid_clustering as  cc\n\n#import biobert_embedding as be\n#import spacy\nimport matplotlib.pylab as plt\nimport plotly.express as px\nfrom collections import defaultdict\nfrom timeit import default_timer as timer\nfrom IPython.display import Image\n#import tabulate\n\n#spacytokenizer = spacy.tokenizer.Tokenizer(be.nlp.vocab)\n\n# Any results you write to the current directory are saved as output.\nROOTDIR = \"../input\"\nDATADIR = os.path.join(ROOTDIR, 'CORD-19-research-challenge')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"../input/covid-image/cov.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"> **\"Nothing in life is to be feared, it is only to be understood. Now is the time to understand more, so that we may fear less.\" **\n--Marie Curie\n\nIn the midst of this crisis, we are a team of data scientists who would like to put our skills into good use, to shed light by providing a tool to the scientific community using NLP to answer key questions from the scientific literature. \n\nThis tool we are building here is a search engine based on the similarity score calculated on embeddings using [Universal Sentence Encoder](https://arxiv.org/abs/1803.11175) model and its pretrained weights can be found on tensorflow hub via this [link](https://tfhub.dev/google/universal-sentence-encoder/4). We also provide a clustering among all articles so that users can get some insights by navigating between articles that are similar.\n\nWe attempt to write a kernel that is more like an article. If you want to get your hand dirty, please feel free to have a look at the enclosed utility scripts."},{"metadata":{},"cell_type":"markdown","source":"# Parsing\nWe first would like to include information from the json files to get a complete dataset. The following steps parse documents from Biorxiv, comm use subset and non-comm use subset as indicated the CORD-19-research-challenge."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_meta = pd.read_csv(os.path.join(DATADIR, \"metadata.csv\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parsing Biorxiv articles**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"biorxiv_path = os.path.join(DATADIR, \"biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/\")\ndf_biorxiv = mc.parse_biorxiv(biorxiv_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parsing Comm use subset articles**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"comm_subset_path = os.path.join(\n    DATADIR, \"comm_use_subset/comm_use_subset/pdf_json/\"\n)\ndf_comm = mc.parse_comm(comm_subset_path)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parsing Non Comm use subset articles**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"noncomm_subset_path = os.path.join(\n    DATADIR, \"noncomm_use_subset/noncomm_use_subset/pdf_json/\"\n)\ndf_noncomm = mc.parse_noncomm(noncomm_subset_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Merge dataset**\n\nWe proceed by merging all the dataset using the unique key \"sha\", and check if there are some duplicates in our dataset."},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"df_meta.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Convert publish time to type publish_date \ndf_meta[\"publish_date\"] =  pd.to_datetime(df_meta[\"publish_time\"])\ndf_merge = mc.merge_datasets(df_meta, df_biorxiv, df_comm, df_noncomm)\ndf_merge_impute = mc.impute_columns(df_merge)\ndf_meta_comp = mc.drop_duplicates(df_merge_impute)\ndf_meta_comp.index.name =\"row_id\"\ndf_meta_comp.to_csv(\"df_meta_comp.csv\")\ndel df_meta, df_merge_impute, df_comm, df_noncomm, df_biorxiv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A peek on the completed meta dataset."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"df_meta_comp.head()\ndf_meta_comp.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA\n## Understanding variables \nBefore working on the search engine, we perform some data exploration to get a hint of what data are awaiting us.\n\nWe first have a look at the missing values. This helps us first understand the variables; and subsequently help us in choosing the variables to work with."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mc.plot_missing_value_barchart(df_meta_comp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above count plot, we can identify the meaning behind each variable as follows, most of them are self-explanatory:\n\n* ID numbers:\n    * cord_uid,\n    * doi,\n    * pubmed_id (has been exlcuded),\n    * pmcid (has been exlcuded),\n    * WHO #Covidence (has been exlcuded),\n    * Microsoft Academic Paper ID (has been exlcuded),\n    * sha (only this is important to retrieve data from jsons)\n* url: link to the article\n* source_x: source of the article\n* has_full_text: (boolean) indicate whether the article has full text\n* journal\n* authors_list\n* first_author: the first ofauthor of the article\n* last_author: the author who is in the last position, corresponds normally to the chief of the research institute\n* affiliations\n* bibliography\n* raw_bibliography : bibliography in its raw position\n* title\n* abstract\n* text\n* publish_date\n* full_text_file: path to the json file (has been excluded)\n\nA lot of them are related to IDs, here we are only interested in one which is sha which will act as a unique key to retrieve the articles. Among other variables that are useful are url for documentation purpose, source_x, journal, authors, title, abstract and publish_time.\n\nNote that there are several articles (15 of them) that do not contain title. By right, they should be exlcuded from this dataset as they are not informative."},{"metadata":{},"cell_type":"markdown","source":"## On the authors"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_authors = df_meta_comp[\"authors\"].apply(mc.author_feats)\nmc.plot_num_author_distrib(df_authors[\"num_authors\"])\ndel df_authors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nowadays, an article is often an contribution of several people, most likely within a range of 2 to 30."},{"metadata":{},"cell_type":"markdown","source":"## On the sources"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mc.plot_article_sources_distrib(df_meta_comp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe there are many contributions for PubMed (PMC) and Elsevier, and the rest not as many, it might be dependant on the establishment of these journals. Here is summary of these journals.\n- **PubMed CentralÂ® (PMC)**: free full-text archive of biomedical and life sciences journal literature at the U.S, about 5.9 MILLION Articles are archived in PMC.\n- **Elsevier**: Dutch publishing and analytics company specializing in scientific, technical, and medical content.\n- **WHO**: World Health Organization\n- **biorxiv**: the preprint server for biology. \n- **merdxiv**: the preprint server for medical health science\n- **CZI**: Chan Zuckerberg Initiative"},{"metadata":{},"cell_type":"markdown","source":"## On the publish date"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_publish_date = mc.groupby_publish_date(df_meta_comp)\nmc.plot_publish_date_distrib(df_publish_date)\ndel df_publish_date","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that there are spikes of publication at the end of the year. Do they only concern certain journals?\n\nLooking at this time plot, we strongly suspect that there is a dependance between the publish date and the sources, let's have a look at that to confirm this hypothesis. We shall only look at the articles from 2002 onwards, as articles before this year are not as frequent."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_date_source = mc.gropuby_date_source(df_meta_comp)\nmc.plot_publish_date_wrt_sources(df_date_source)\ndel df_date_source","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While we see the publication of several articles fairly recently, eg WHO, medrxiv and CZI, PubMed and Elsevier on the other hand have contributions since 2002, and they are the ones which have spikes of publication at the end of the year.\n\nFor publications prior to 2002, topics are mostly about on ILI (Influence-like illnesses) as SARS virus has not known yet.\n\nIn the coming days/weeks, we'll see an avalanche of papers submitted to biorxiv and medrxic as they don't require any peer reviewing to get published. That implies that the to-be-developed search engine has to be scalable."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## EDA on text data\n**On the title**\n\nWe perform a light EDA on text data, the NLP preprcocessing pipeline includes the following steps:\n- lowercase\n- remove puncuations\n- remove https\n- remove stopwords"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_preprocess_title = df_meta_comp[\"title\"].apply(\n        lambda x: et.nlp_preprocess(x, config.PUNCT_DICT)\n    )\n    \ndf_wc_title = et.corpora_freq(df_preprocess_title)\net.plot_distrib(df_wc_title, \"title\");\ndel df_preprocess_title","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that the top words are dominated by influenza-like illnesses and COVID-19 related vocabularies, eg virus, respiratory, coronavirus. It is also noted that the vocab is also related to research terms, such as clinical, study, analysis. There is no doubt that we are dealing with COVID-19 literature."},{"metadata":{},"cell_type":"markdown","source":"**On the abstract**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_preprocess_abstract = df_meta_comp[\"abstract\"].apply(\n        lambda x: et.nlp_preprocess(x, config.PUNCT_DICT)\n    )\ndf_wc_abstract = et.corpora_freq(df_preprocess_abstract)\net.plot_distrib(df_wc_abstract, \"abstract\");\n\ndel df_preprocess_abstract","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The word count plot above shows a richer vocab that does not always seem specified to the medical research literature at hand, eg also, may, can, however, not, abstract ... By right, if we were handling the corpus with the bag-of-words approach, these words should be removed. This plot has been insightful in allowing us to build our strategy in our following studies."},{"metadata":{},"cell_type":"markdown","source":"**On the text**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_preprocess_text = df_meta_comp[\"text\"].apply(\n        lambda x: et.nlp_preprocess_text(x, config.PUNCT_DICT)\n)\ndf_wc_text = et.corpora_freq(df_preprocess_text)\net.plot_distrib(df_wc_text, \"text\");\ndel df_preprocess_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If the abstract has provided us with some uninformative words, we observe that this type of vocab is even more present in the text."},{"metadata":{},"cell_type":"markdown","source":"## On the affiliation"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_process_affiliation = df_meta_comp[\"affiliations\"].apply(et.process_affiliations)\n\ndf_wc_affiliation = et.corpora_freq(df_process_affiliation, affiliation=True)\nax = et.plot_distrib(df_wc_affiliation.iloc[1:], \"affiliation\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=90);\ndel df_process_affiliation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The top 10 institutitions that have contributed to this dataset are mostly Chinese institutions. This makes sense as China is where the first-wave of COVID-19 has taken place. In summary, the contribution to this dataset is a global effort."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Embedding\nIn this section, we will proceed with the embeddings of our text data. The idea of embedding is to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data. The embedded data are also in a lower dimension as compared to only using classical methods, such as count or tfidf vectorizations Once we have the embeddings, we use them as latent variables for further processing, eg clusterisation, similarity computation with embeded queries... Here, we focus on the aforementioned tasks.\n\nNow comes the question of what embedding to be used. We have previously written a kernel on applying [BioBERT embeddings](https://www.kaggle.com/pmlee2017/biobert-embedding/), however without fine-tuning, the performance is not what we expected. In this kernel, we are using USE embeddings and perform a comparison between using only title and both title and abstract."},{"metadata":{},"cell_type":"markdown","source":"## Universal Sentence Encoder in a nutshell\n\nThere are some key elements a reader ought to know while using USE embeddings.\n- The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. \n- It is trained on a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks, such as semantic similarity, and classification.\n- The input is variable length English text and the output is a 512 dimensional vector\n- The USE model is trained with a deep averaging network (DAN) encoder.\n\n### Semantic Similarity\nThe task at hand is to find similar documents by evaluating semantic similarity. Semantic similarity is a measure of the degree to which two pieces of text carry the same meaning. As shown in the following image, text of a document is first embedded by a model. And by computing similarity scores across the embeddings of all documents in the corpus, we are able to use this score accordingly to rank the documents.\n\n\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"../input/semantic-similarity/Capture decran 2020-04-16 a 16.10.28.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load USE model\nUSE model can be obtained from this [Tensorflow Hub](https://tfhub.dev/google/universal-sentence-encoder/4) page. It takes a minute to load the model, and the inference is relatively fast on ~50000 articles. \n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\nembed = hub.load(module_url)\n\ndef UniversalEmbedding(x):\n    return embed(tf.squeeze(tf.cast(x, tf.string)))\n\ninput_text = Input(shape=(1,), dtype=tf.string)\nembedding = Lambda(UniversalEmbedding, output_shape=(512, ))(input_text)\nmodel = Model(inputs=[input_text], outputs=embedding)\n\n# define some parameters\nnsamples = 5000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## USE on title\n\nIn this section, we carry out the analysis only on titles."},{"metadata":{"trusted":true},"cell_type":"code","source":"title_len = df_meta_comp[\"title\"].apply(lambda x: len(x.split()) if not pd.isnull(x) else np.NaN)\nmean_title_len = np.nanmean(title_len)\nprint(\"Mean title length: %.1f words\" %mean_title_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_title = df_meta_comp[\"title\"].dropna()\n\n# keep id\nid_titles = df_meta_comp[~df_meta_comp.title.isna()].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"embeddings_titles = model.predict(df_title, batch_size=1024, verbose=1)\ndf_embedding_title = pd.DataFrame(embeddings_titles, index = id_titles)\ndf_embedding_title.name = \"row_id\"\nembeddings_titles = np.concatenate((id_titles.values.reshape(-1, 1), embeddings_titles), axis=1)\nnp.save('embeddings_titles.npy', embeddings_titles)\ndel embeddings_titles\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clustering\nWith embeddings done, we can proceed with clustering. We use MiniBatch Kmeans to cluster our datapoints, this again goes with our aim of performing all computations in batch. Then we perform a PCA projection on 3D. "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"n_clusters = np.arange(4, 32, 4)\ndf_clusters = cc.miniBatchClustering(df_embedding_title, None, n_clusters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To determine the optimal number of clusters, we use the Silhouette score to measure how similar an object is to its own cluster (cohesion) compared to other clusters (separation), and this metric gives a value between -1 to 1 with -1 meaning the datapoint does not belong to the attributed cluster at all, and 1 indicating good clustering."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"import matplotlib.pylab as plt\nfig, axes = plt.subplots(3, 3, figsize=(12, 16))\n\nsample_title = df_embedding_title.sample(nsamples, random_state=42)\nidx_title = sample_title.index\ncc.plot_silhouette_graph(\n        df_embedding_title.sample(nsamples, random_state=42),\n        df_clusters.loc[idx_title],\n        n_clusters,\n        fig,\n        axes,\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the average Silhouette score, we observe that the value is rather low, but again this score should not be the hard and fast rule in determining the optimum number of clusters. Looking closely to the Silhouette plot, we observe that a lot of the datapoints are wrongly attributed but their Silhouette scores are contained between -0.1 and 0 signifying they are not that badly attributed. Of course, looking at the positive side, the Silhouette scores are not that high either. \n\nFor low number of clusters (4-8), we observe that cluster size is rather uneven, and potentially they need to be divided into more sub clusters. Hence higher number of clusters is prioritised.\n\nThe fact that many datapoints are attributed to the wrong cluster might indicate that a lot of articles are cross-discipline, and therefore don't belong to only one cluster. \n\nFrom this analysis, we have seen that the Silhouette graph is not very conclusive. We pursue by measuring another metric which is the Calinski-Harabasz score, it is defined as ratio between the within-cluster dispersion and the between-cluster dispersion.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_calinski = cc.evaluate_metric_score(df_embedding_title, df_clusters, metric=\"calinski\")\ndf_calinski[\"n_cluster\"] = n_clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ax = cc.plot_metric_vs_cluster(df_calinski)\nx = df_calinski['n_cluster'].values\ny = df_calinski['metric'].values\n\na1 = (y[1]-y[0])/(x[1]-x[0])\nb1 =1270\n\ndef f(x, a, b):\n    return a*x+b\n\nx_asympt= np.arange(4., 13.)\ny1 = f(x_asympt, a1, b1)\n\nax.axhline(300,  color=\"black\", linestyle=\"dashed\", linewidth=1)\nax.axvline(11, ymax=.6, color=\"red\", linestyle=\"dashed\", linewidth=1)\n\nax.plot(x_asympt, y1, color=\"black\", linestyle=\"dashed\", linewidth=1)\nax.set_ylabel(\"Calinksi-Harabasz score\");\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By plotting the Calinski-Harabasez score wrt number of clusters, we observe an elbow. Though it's not a very precise method of determining the optimal number of clusters, but it at least provides us with some elements of response. The optimal number of clusters determined here is 11. "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n### Visualization\nAnother way to check if our datapoints are being correctly clustered, we can visualize in lower dimensions. We choose to perform a PCA with 3 components to have a projection in 3D. Before carrying out the PCA projection, we sphereize our datapoints (the embeddings) by shifting each datapoint by the centroid and make it unit norm. By doing this the datapoints will be better distributed in the spherical space.\n\n**Give it a spin at the 3D plot, and hover over the datapoints to get some information.**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X = vp.sphereize(df_embedding_title.values)\nindex = df_embedding_title.index\ncolors = px.colors.qualitative.Alphabet + px.colors.qualitative.Plotly\ndf_pca = vp.compute_pca(X, index)\ndf_resampled = vp.resample(df_pca, df_meta_comp, df_clusters, colors, nsamples=nsamples, n_cluster=24)\ninfo_title = vp.prepare_info(df_resampled)\nvp.plot_tsne(df_resampled, \"X_0\", \"X_1\", info_title, var_z=\"X_2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that the clustering is rather coherent. In order to evaluate the relevance of clusters, our next step is to seek experts."},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud\nWe further plot the wordcloud for each cluster to help us with the interpretation. Since the vocab of USE are wordpieces, it doesn't make too much sense to use them for our wordcloud. Therefore, we proceed with the conventional NLP preprocessing pipeline: Lemmatokenization, Stopword removal, Punctuation removal, and Count vectorization. \n\nWe first perform a preprocessing on the general corpus to identify the top words and then consider them as stopwords."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_title_cluster = df_clusters.merge(\n        df_title, left_index=True, right_index=True\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_title_cluster[\"title_process\"] = df_title_cluster[\"title\"].apply(\n        lambda x: et.nlp_preprocess(str(x), config.PUNCT_DICT)\n        if not pd.isnull(x) else \"\")\n\ndf_title_wc = wcp.prepare_word_cloud(df_title_cluster[\"title_process\"])\n# get rid of top 10 words\nextra_stopwords = df_title_wc.head(10).keys().tolist()\n\nextra_stopwords += [\"covid\", \"sars\", \"infectious\", \"19\", \"volume\", \"index\", \"chapter\", \"volume\", \"1\", \"de\", \"la\"]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we loop over all clusters and preprocess the text in order to give coherent vocabularies for the wordclouds.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"wc_title = defaultdict(int)\nncluster = 12\nnclusters = sorted(\n        df_title_cluster[\"labels_cluster_%d\" % ncluster].unique().tolist()\n    )\n\nfor k in nclusters:\n    temp = df_title_cluster[df_title_cluster[\"labels_cluster_%d\" % ncluster] == k][\n        \"title_process\"\n    ]\n\n    try:\n        wc_title[k] = wcp.prepare_word_cloud(temp, extra_stopwords)\n    except ValueError:\n        pass\n\nn_top = 50\nfig, axes = plt.subplots(4, 3, figsize=(16, 16))\nax = axes.flatten()\nwcp.plot_word_cloud(wc_title, ax)\nplt.tight_layout(w_pad=2.0)\ndel wc_title, df_title_cluster, df_title_wc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above each word cloud is the top 3 word occurences. Topics are quite distinctive knowing that we have only used title as input. Among the topics, we have syndromes, detection and analysus, genetics (rna), receptor and inhibitor...\n"},{"metadata":{},"cell_type":"markdown","source":"## USE on title and abstract\nFor this task, we concatenate title and abstract. The following procedure ressembles that of embedding on title."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_title_abstract = df_meta_comp['title'] + \"\\n\" + df_meta_comp['abstract'].fillna('') \ndf_title_abstract = df_title_abstract.dropna()\nid_title_abstract = df_title_abstract[~df_title_abstract.isna()].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"title_abstract_len = df_title_abstract.apply(lambda x: len(x.split()) if not pd.isnull(x) else np.NaN)\nmean_title_abstract_len = np.nanmean(title_abstract_len)\nprint(\"Mean title abstract length: %.1f words\" %mean_title_abstract_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_title_abstract = model.predict(df_title_abstract, batch_size=1024, verbose=1)\ndf_embedding_title_abstract = pd.DataFrame(embeddings_title_abstract, index = id_title_abstract)\ndf_embedding_title_abstract.name = \"row_id\"\nembeddings_title_abstract = np.concatenate((id_title_abstract.values.reshape(-1, 1), embeddings_title_abstract), axis=1)\nnp.save('embeddings_title_abstract.npy', embeddings_title_abstract)\ndel embeddings_title_abstract","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"n_clusters = np.arange(4, 30, 4)\ndf_clusters_title_abstract = cc.miniBatchClustering(df_embedding_title_abstract, None, n_clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(12, 16))\n\nsample_title_abstract = df_embedding_title_abstract.sample(nsamples, random_state=42)\nidx_title_abstract = sample_title_abstract.index\ncc.plot_silhouette_graph(\n        df_embedding_title_abstract.sample(nsamples, random_state=42),\n        df_clusters_title_abstract.loc[idx_title_abstract],\n        n_clusters,\n        fig,\n        axes,\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These Silhouette plots look a little bit different as compared to only using title, the values are higher and the errors in cluster attribution seem to be lower, however this analysis still does not allow us to determine the optimal number of clusters. Let's have a look at the Calinski-Harabasz score."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_calinski_title_abstract = cc.evaluate_metric_score(df_embedding_title_abstract, df_clusters_title_abstract, metric=\"calinski\")\ndf_calinski_title_abstract[\"n_cluster\"] = n_clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nax = cc.plot_metric_vs_cluster(df_calinski_title_abstract)\nx = df_calinski_title_abstract['n_cluster'].values\ny = df_calinski_title_abstract['metric'].values\n\na1 = (y[1]-y[0])/(x[1]-x[0])\nb1 = 2350\n\ndef f(x, a, b):\n    return a*x+b\n\nx_asympt= np.arange(2., 14.)\ny1 = f(x_asympt, a1, b1)\n\nax.axhline(500,  color=\"black\", linestyle=\"dashed\", linewidth=1)\nax.axvline(12, ymax=.5, color=\"red\", linestyle=\"dashed\", linewidth=1)\nax.plot(x_asympt, y1, color=\"black\", linestyle=\"dashed\", linewidth=1)\n\nax.set_ylabel(\"Calinski-Harabasz score\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization \nProjection with PCA."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X_title_abstract = vp.sphereize(df_embedding_title_abstract.values)\nindex_title_abstract = df_embedding_title_abstract.index\ncolors = px.colors.qualitative.Alphabet\ndf_pca_title_abstract = vp.compute_pca(X_title_abstract, index_title_abstract)\ndf_resampled_title_abstract = vp.resample(df_pca_title_abstract, df_meta_comp, \n                                          df_clusters_title_abstract, colors, nsamples=nsamples, n_cluster=12 )\ninfo_title_abstract = vp.prepare_info(df_resampled_title_abstract)\nvp.plot_tsne(df_resampled_title_abstract, \"X_0\", \"X_1\", info_title_abstract, var_z=\"X_2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this visualization, we observe that more distinctive clusters than the case with only title. This indicates that including abstract adds more information, and thus leading to a better clustering."},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud\nWe plot the wordcloud for each cluster."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_title_abstract.name = \"title_abstract\"\ndf_title_abstract_cluster = df_clusters_title_abstract.merge(\n        df_title_abstract, left_index=True, right_index=True\n    )\ndf_title_abstract_cluster[\"title_abstract_process\"] = df_title_abstract_cluster[\"title_abstract\"].apply(\n        lambda x: et.nlp_preprocess(str(x), config.PUNCT_DICT)\n        if not pd.isnull(x) else \"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_title_abstract_wc = wcp.prepare_word_cloud(df_title_abstract_cluster[\"title_abstract_process\"])\n# since there are more words, we consider the top 20 words as stop words\nextra_stopwords_title_abstract = df_title_abstract_wc.head(20).keys().tolist()\n\nextra_stopwords_title_abstract += [\"covid\", \"sars\", \"infectious\", \"19\", \"may\", \"can\", \"volume\", \"index\", \"chapter\", \"volume\", \"used\", \"also\", \"de\", \"la\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"wc_title_abstract = defaultdict(int)\nncluster = 12\nnclusters = sorted(\n        df_title_abstract_cluster[\"labels_cluster_%d\" % ncluster].unique().tolist()\n    )\n\nfor k in nclusters:\n    temp = df_title_abstract_cluster[df_title_abstract_cluster[\"labels_cluster_%d\" % ncluster] == k][\n        \"title_abstract_process\"\n    ]\n\n    try:\n        wc_title_abstract[k] = wcp.prepare_word_cloud(temp, extra_stopwords_title_abstract)\n    except ValueError:\n        pass\n\nn_top = 50\nfig, axes = plt.subplots(4, 3, figsize=(16, 16))\nax = axes.flatten()\nwcp.plot_word_cloud(wc_title_abstract, ax)\nplt.tight_layout(w_pad=2.0)\ndel wc_title_abstract, df_title_abstract_cluster, df_title_abstract_wc ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As compared to the previous study using only title, we observe that the vocab is richer and the topics are also more distinctive. We can easily identify some coronavirus (Covid-19) related topics such as detection and samples, vaccines devopement, the genes (sequence, replication, genome structure and method), the outbreak in China, public care system, data and modeling, treatments and effects..."},{"metadata":{},"cell_type":"markdown","source":"# Demo\nIn order to have an idea on how the difference between the two studies, we have prepared a demo to have a sense of the performance using ipywidgets. You are looking at results compared with embeddings from title only."},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual, Textarea, Layout","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"text_area_layout = Layout(width=\"70%\", height=\"50px\")\ntext_area = widgets.Textarea(value=\"Incubation periods\", placeholder=\"Enter your text here.\", layout=text_area_layout)\n\nint_slider_layout = Layout(width=\"50%\")\nint_slider = widgets.IntSlider(description=\"Select number of results to show\",\n                               min=1, \n                               max=40, \n                               value=10, \n                               layout=int_slider_layout,\n                               style={'description_width': 'initial'}\n                              )\n\nradio_buttons_layout = Layout(width=\"50%\")\nradio_buttons = widgets.RadioButtons(description=\"select embeddings\", \n                                     value='title', \n                                     options=['title', 'title+abstract'],\n                                     style={'description_width': 'initial'},\n                                     layout=radio_buttons_layout\n                                    )\n\ntoggle_button = widgets.ToggleButton(value=True)\n\ncheckbox = widgets.Checkbox(value=False, description='Show abstracts', disabled=False, indent=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"@interact\ndef plot_search_results(emb=radio_buttons, n=int_slider, show_abstracts=checkbox, text=text_area):\n    if text.strip() != '':\n        if emb == \"title\": \n            embs = df_embedding_title\n        elif emb == \"title+abstract\":\n            embs = df_embedding_title_abstract\n      \n        print(f\"Displaying {n} most similar results for \\n{text} ...\\n\")\n        \n        embedding_text = embed([text])\n        embedding_text = embedding_text.numpy()\n        similarities = np.inner(embedding_text, embs.values)\n\n        indices = np.argsort(similarities)[0]\n        indices = indices[::-1][:n]\n\n        row_ids = embs.iloc[indices].index\n        row_ids = list(map(int, row_ids))\n\n        for i, (row_id, index) in enumerate(zip(row_ids, indices)):\n\n            title = df_meta_comp.loc[row_id]['title']\n            abstract = df_meta_comp.loc[row_id]['abstract']\n            print(f'result {i} title : {title}')\n            print(f'similarity : {similarities[0][index]}')\n            \n            if show_abstracts:\n                print('')\n                print(f'result {i} abstract : {abstract}')\n\n            print('----' )\n    else:\n        print('no query, no results baby.')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By testing with some queries, it is observed that first five topics are rather relevant. In terms of their similarities scores, which is a measure of confidence, they are very often capped at 0.65. This might be due to the fact that the model is trained on a general text corpus, and not specific enough to scientific articles, and even less to articles related to Covid-19."},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\nIn this kernel, we have used USE embeddings to embed our text data, and using this to calculate the similarity with the query entered by a user. We have performed clustering on the datapoints and have used PCA to reduce the number of dimensions and visualize them in three dimensions. With word cloud, we have been able to get an idea of the topic of each cluster. And finally, we build a demo to test out two settings, one with the embeddings on title only and the other one on both title and abstract.\n\nWe have observed some differences while comparing the returned results between using only title and both title and abstract. From the point of view of a search engine, users often only use several keywords as queries, hence it is more reasonable to compare with embeddings infered from only title because they are also short and precise. On the other hand, the embeddings with title and abstract have proven to be useful in clustering because they contain more information.\n\nThe takeaway message here is that embeddings with title are useful for query searching, and embeddings with title and abstract are useful for clustering.\n\n# Next steps\n\nFrom this study, while embeddings using USE have been able to recommend some relevant articles for the input queries, there is still room for improvement as we have seen that the confidence of the model given by the similarity score is not exceptional, we will proceed by fine tuning the USE model on the CORD19 research challenge corpus, and as a comparison we will carry out the same task with the BioBERT model. Stay tuned for more ðŸ˜€."},{"metadata":{},"cell_type":"markdown","source":"Here is a list of entries to the CORD19 research challenge that use this method to extract information:\n* [What is known about transmission, incubation, and environmental stability?](https://www.kaggle.com/pmlee2017/use-semantic-similarity-transmission-incubation/) \n* [What do we know about virus genetics, origin, and evolution? ](https://www.kaggle.com/pmlee2017/use-semantic-similarity-virus-genetics)  \n* [What do we know about COVID-19 risk factors?](https://www.kaggle.com/pmlee2017/use-semantic-similarity-risk-factors/)\n* [What do we know about vaccines and therapeutics? What has been published concerning research and development and evaluation efforts of vaccines and therapeutics?](https://www.kaggle.com/pmlee2017/use-semantic-similarity-therapeutics/)\n* [What has been published about medical care?](https://www.kaggle.com/pmlee2017/use-semantic-similarity-medical-care)\n* [What do we know about non-pharmaceutical interventions?](https://www.kaggle.com/pmlee2017/use-semantic-similarity-non-pharma-interv)\n* [What do we know about diagnostics and surveillance?](https://www.kaggle.com/pmlee2017/use-semantic-similarity-diagnostics-surveillance) \n* [What has been published about ethical and social science considerations?](https://www.kaggle.com/pmlee2017/use-semantic-similarity-ethics) \n* [What has been published about information sharing and inter-sectoral collaboration?](https://www.kaggle.com/pmlee2017/use-semantic-similarity-sharing-collaboration) \n\nEnjoy the read! ðŸ˜ƒ"},{"metadata":{},"cell_type":"markdown","source":"# References\n* [Ipywidgets](https://ipywidgets.readthedocs.io/en/stable/)\n* [Universal Sentence Encoder](https://arxiv.org/abs/1803.11175)\n* [Colab for USE embeddings](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb#scrollTo=K_3uevjRUgpo)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}