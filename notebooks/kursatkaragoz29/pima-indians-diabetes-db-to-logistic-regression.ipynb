{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# INTRODUCTION\n* Bu kernelde amacımız veri setini kullanarak bir hastanın diyabet olup olmadığını tahmin etmektir. Tahmin etme işlemini Logistic Regression ile yaparak 0 veya 1 sınıf etiketlerine ulaşıcağız.\n\n        Bu kenelde, pima-indians-diabetes data seti ile Linear Learning gerçekleştirilmiştir.\n        Features:\n        Pregnancies: Gebelik Sayısı\n        Glucose: Oral glukoz tolerans testinde glikoz konsantrasyonu değeri.\n        BloogPressure: Kan Değeri(mm Hg)\n        SkinThickness: Cilt Kalınlığı(mm)\n        Insulin: 2 saatlik serum insulini(mu U/ml)\n        BMI:  Vücut Kütle indeksi\n        DiabetesPedigreeFunction: Diyabet soyağacı işlevi\n        Age: Yaş\n        Outcome: Diyabet olup olmaması 1 veya 0\n\n  \n\n<br>\n1.) [Data Reading and Data Pre-Processing](#1)<br>\n2.) [Train Test Split Data](#2)<br>\n3.) [Parameter Initialize and Sigmoid Function](#3)<br>\n4.) [Forward and Back Propagation Function](#4)<br>\n5.) [Update Function for Parameters (Weight,Bias)](#5)<br>\n6.) [Predict Method](#6)<br>\n7.) [Logistic Regression (test main)](#7)<br>\n8.) [Logistic Regression with Sklearn Library](#8)<br>\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a><br>\n# Data Reading and Data Pre-Processing"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dependent variables, class label for train dataset\ny=data[\"Outcome\"].values  #array\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Independent variables, traindataset\nx_data = data.drop([\"Outcome\"],axis=1)\nx_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalization Dataset\n# normalized data = (X - X_MİN) / (X_MAX - X_MİN)\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data))\nx.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n# Train Test Split Data\n    Train Test Split data==> 80% of data set for Train, 20% of data set for Test\n    train_test_split method'unun içine x ve y eğitim için verildi.\n    test_size=0.2 parametresi ilede veri setinin %20'sini test olarak ayrıldı.\n    random_state parametresi ile \"42\" sayısını id olarak tut ve aynı işlem  birdaha yapılırsa:\n    aynı bölümleme işlemini yaparak aynı sonuçlara ulaşmamızı bize sağla.\n    bu method sonucunda oluşacak resultlarıda belirtilen değişkenlere aktar.\n    x'in %80 x_train , x'in %20'si x_test ; y'nin %80'i y_train, y'nin %20'si y_test\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\n# Feature ve onlara ait değerlerin yeri değiştirildi.\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"Changed of Features and Values place.\")\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n# Parameter Initialize and Sigmoid Function\n\n    Dimension Parameter: Features count ==> dimension = 8\n    \n    Sigmoid Function : f(x) = 1 / ( 1 + (e ^ -x)\n    Initialize weight = 0.01 for each data\n    Initialize bias = 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize\n# w = Create matrix array and all array values are 0.01\n# np.exp = exponent method in numpy library\ndef initialize_weights_and_bias(dimension):\n    #initialize\n    w = np.full((dimension,1),0.01) \n    b=0.0\n    return w,b\n\ndef sigmoid(z):\n    y_head = 1 / (1 + np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n# Forward and Back Propagation Function\n\n*  **z = bias + px1*w1 + px2*w2 + ... + pxn*wn **\n*  **loss function = -(1 - y) * log(1- y_head) - y * log(y_head)**\n*  **cost function = sum(loss value) / train dataset sample count**\n   \n         Her bir weightin, kendisine ait her bir x_train ile çarpılması gerekir.\n         \n         Örneğin:\n         \n         x_train,y_train = Öğrenme veri seti, w=weightler ==> (30,1) * (30,455) matrixlerinin çarpması olamaz.\n         \n             1. matrix'in sütunu ile 2. matrix'in satırı birbirine uymalıdır => (1,30) * (30,455) olmalıdır.\n             \n         Bu işlem sonucunda da (1,455) lik bir matrix elde edilir. ==> np.dot(w.T,x_train,y_train)\n         \n         Forward - Backward İşleminde yapılacaklar:\n         \n      *forward için:*\n      \n           1.) x_train değerleri ile ağırlıkları(weights) çarp ve bias ekle.\n           2.) y_head değerini sigmoid function ile hesapla.\n           3.) loss function formulünden yola çıkarak loss değerini hesapla\n           4.) cost functionu hesapla => sum(loss) / sample_count\n\n      *Backward için:*\n      \n           1.)backward işleminde gerekli weighte göre türev al.\n           2.)backward işleminde gerekli bias'a göre türev al\n           cost ve gradients(derivative_weight, derivative_bias) return et."},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss =  -(1 - y_train) * np.log(1 - y_head) -y_train * np.log(y_head)\n    cost = (np.sum(loss)) / x_train.shape[1]\n    \n    #backward propagation\n    derivative_weight = (np.dot(x_train, ((y_head - y_train).T))) / x_train.shape[1] #derivative based on weight\n    derivative_bias = np.sum(y_head - y_train ) / x_train.shape[1] #derivative based on bias\n    \n    #weight and bias are derivates kept in dictionary(gradients)\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\":derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n# Update Function\n\n     Update işlemi arka arkaya forward propagation ve backward propagatin işleminin n defa yapılması işlemidir.\n     Bu nedenle parametrelerimiz:\n         güncellenecek weightler             : w                    => parameter 1\n         güncellenecek biaslar               : b                    => parameter 2\n         forward için x_train input1         : x_train              => inputs features values\n         forward için y_train input_label    : y_train              => inputs class labels\n         slope için learning_rate değeri     : learning_rate        => hyper parameter 1\n         forward ve backward tekrar sayısı   : number_of_iteration  => hyper parameter 2\n     Not: Gradients, weight ve bias'ın türevlerini tutar.\n     costları tutarız çünkü nuber_of_itearation sayısını belirlemek için.\n     learning_rate fazla veya az olursa öğrenme işlemi kazaya uğrayabilir.\n     cost2'nin pek bir işlevi yoktur sadece her 10 adımda bir costları tutuyoruz \n     Bunun sebebi costları 10'ar adımda bir plot ettirmektir."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n#parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.009,number_of_iterarion = 200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n# Predict Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict Method\n\ndef predict(w,b,x_test):\n    z=sigmoid(np.dot(w.T,x_test) + b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a>\n# Logistic Regression (test main)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    \n    #initialize\n    dimension =  x_train.shape[0]  # that is 4096\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    \n    #update method for forward and backward propagation\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n\n    # Print train/test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test\nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 5, num_iterations = 300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a>\n# Logistic Regression with Sklearn Library"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"Test Accuracy : {}\".format(lr.score(x_test.T,y_test.T)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\nprint(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}