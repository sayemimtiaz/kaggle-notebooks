{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffecbfed07a596e4b9e100bcf2538d37a41541c9"},"cell_type":"markdown","source":"### Introduction\n\nDecision Trees are classification methods that are able to extract simple rules about the data features which are inferred from the input\ndataset. Several algorithms for decision tree induction are available in the literature. Scikit-learn contains the implementation of the CART (Classification and Regression Trees) induction algorithm.\n\nFirst, we use make_classificaton to create an artificial classification dataset. This convenience function allows fine-grained control over the characteristics of the dataset it produces. We create a dataset with 1,000 instances. Of the 100 features, 20 are informative; the remainder are redundant combinations of the information features, or noise. We then train and evaluate a single decision tree, followed by a random forest with 10 trees. The random forest's F1 precision, recall, and F1 scores are greater."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport graphviz\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n#Setting random seed\nseed = 10\nX, y = make_classification(\nn_samples=1000, n_features=100, n_informative=20,\nn_clusters_per_class=2,\nrandom_state=11)\nX_train, X_test, y_train, y_test = train_test_split(X, y,\nrandom_state=11)\nclf = DecisionTreeClassifier(random_state=11)\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\nprint(classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"994afd14819379b03cb44c07d53e6bcc3b935756"},"cell_type":"markdown","source":"### Decison Tree Application on Iris Dataset\n\nHere, we apply decision tree classifier on Iris dataset. First, we import all the libraries needed for this example. Scikit-learn does not implement any post-prunning step. So, to avoid overfitting, we can control the tree size with the parameters min_samples_leaf, min_samples_split and max_depth. "},{"metadata":{"trusted":true,"_uuid":"533e1f761749389dbffe4c539ff80b07f0a0f4f1"},"cell_type":"code","source":"# Decision Tree on Iris Dataset\n\nimport pandas as pd\nimport graphviz\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn import datasets \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n# Setting random seed.\nseed = 10\n\n# Loading Iris dataset.\ndata = pd.read_csv('../input/iris/Iris.csv')\nprint(data.head())\n# Creating a LabelEncoder and fitting it to the dataset labels.\nle = LabelEncoder()\nle.fit(data['Species'].values)\n# Converting dataset str labels to int labels.\ny = le.transform(data['Species'].values)\n# Extracting the instances data.\nX = data.drop('Species', axis=1).values\n# Splitting into train and test sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, stratify=y, random_state=seed)\n\n# Creating a DecisionTreeClassifier.\n# The criterion parameter indicates the measure used (possible values: 'gini' for the Gini index and\n# 'entropy' for the information gain).\n# The min_samples_leaf parameter indicates the minimum of objects required at a leaf node.\n# The min_samples_split parameter indicates the minimum number of objects required to split an internal node.\n# The max_depth parameter controls the maximum tree depth. Setting this parameter to None will grow the\n# tree until all leaves are pure or until all leaves contain less than min_samples_split samples.\ntree = DecisionTreeClassifier(criterion='gini',\nmin_samples_leaf=5,\nmin_samples_split=5,\nmax_depth=None,\nrandom_state=seed)\ntree.fit(X_train, y_train)\ny_pred = tree.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('DecisionTreeClassifier accuracy score: {}'.format(accuracy))\n\ndef plot_tree(tree, dataframe, label_col, label_encoder, plot_title):\n    label_names = pd.unique(dataframe[label_col])\n    # Obtaining plot data.\n    graph_data = export_graphviz(tree, feature_names=dataframe.drop(label_col, axis=1).columns,\n    class_names=label_names,filled=True,rounded=True, out_file=None)\n    # Generating plot.\n    graph = graphviz.Source(graph_data)\n    graph.render(plot_title)\n    return graph\n\ntree_graph = plot_tree(tree, data, 'Species', le, 'Iris')\ntree_graph","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d479ce294f05e4795e0795beead83a00c72c902c"},"cell_type":"markdown","source":"### Dataset with Categorical Features\n\nUnfortunately, the DecisionTreeClassifier class does not handle categorical features directly. So, we might consider to transform them to\ndummy variables. However, this approach must be taken with a grain of salt because decision trees tend to overfit on data\nwith a large number of features.\n\nHere we build two trees with different depth to analyze overfitting, and deal with overfitting by building a smaller tree. We can observe that the second tree is almost as accurate as the first one. Apparently both trees are able to handle the mushroom data pretty well. The second three might be preferred, since it is a simpler and computationally cheaper model. Finally, we plot the second tree."},{"metadata":{"trusted":true,"_uuid":"0cf3c7cab6fac95395b6b8d14ef70f739f9dd851"},"cell_type":"code","source":"import pandas as pd\nimport graphviz\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn import datasets \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Setting random seed.\nseed = 10\n\n# Loading Mushroom dataset.\ndata = pd.read_csv('../input/mushroom-classification/mushrooms.csv')\n# We drop the 'stalk-root' feature because it is the only one containing missing values.\ndata = data.drop('stalk-root', axis=1)\n# Creating a new DataFrame representation for each feature as dummy variables.\ndummies = [pd.get_dummies(data[c]) for c in data.drop('class', axis=1).columns]\n# Concatenating all DataFrames containing dummy variables.\nbinary_data = pd.concat(dummies, axis=1)\n# Getting binary_data as a numpy.array.\nX = binary_data.values\n# Getting the labels.\nle = LabelEncoder()\ny = le.fit_transform(data['class'].values)\n# Splitting the binary dataset into train and test sets.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, stratify=y, random_state=seed)\n\n# Creating a DecisionTreeClassifier.\ntree = DecisionTreeClassifier(criterion='gini', min_samples_leaf=5, min_samples_split=5, max_depth=None,\nrandom_state=seed)\ntree.fit(X_train, y_train)\n\n# Prediction and Accuracy\ny_pred = tree.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint('DecisionTreeClassifier accuracy score: {}'.format(accuracy))\n\nprint('DecisionTreeClassifier max_depth: {}'.format(tree.tree_.max_depth))\n\n#What if we fit a decision tree with a smaller depth?\ntree = DecisionTreeClassifier(criterion='gini',\nmin_samples_leaf=5,\nmin_samples_split=5,\nmax_depth=3,\nrandom_state=seed)\ntree.fit(X_train, y_train)\ny_pred = tree.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint('DecisionTreeClassifier accuracy score: {}'.format(accuracy))\n\n# Appending 'label' column to binary DataFrame.\nbinary_data['class'] = data['class']\ntree_graph = plot_tree(tree, binary_data, 'class', le, 'Mushroom')\ntree_graph\n\n# Feature Importance\nprint(\"Number of Features :\", tree.n_features_,\", number of classes :\\n\",tree.n_classes_)\nprint(\"Feature Importance :\\n\",tree.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00c008ddfcf8af2fb20a312d1b9151780c4aa47a"},"cell_type":"markdown","source":"### Bagging\n\nBootstrap aggregating, or bagging, is an ensemble meta-algorithm that can reduce the variance in an estimator. Bagging can be used in classification and regression tasks. When the component estimators are regressors, the ensemble averages their predictions. When the component estimators are classifiers, the ensemble returns the mode class.\n\nBagging independently fits multiple models on variants of the training data. The training data variants are created using a procedure called bootstrap resampling. Often it is necessary to estimate a parameter of an unknown probability distribution using only a sample of the distribution. \n\nBagging is a useful meta-algorithm for estimators that have high variance and low bias, such as decision trees. In fact, bagged decision tree ensembles are used so often and successfully that the combination has its own name: the random forest. \n\nThe number of trees in the forest is an important hyperparameter. Increasing the number of trees improves the model's performance at the cost of computational complexity.\n\nRegularization techniques, such as pruning or requiring a minimum number of training instances per leaf node, are less important when training trees for forests than they are for training a single estimator, as bagging provides regularization. "},{"metadata":{"trusted":true,"_uuid":"eb19d925af48312ac7b7c1c3edee74a20e81797e"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n#Setting random seed\nseed = 10\n\n# Dataset Creation\nX, y = make_classification(n_samples=1000, n_features=100, n_informative=20,\nn_clusters_per_class=2,random_state=11)\n\n# Train Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state=11)\n\nclf = RandomForestClassifier(n_estimators=10, random_state=11)\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\nprint(classification_report(y_test, predictions))\n\n# Feature Importance\nprint(\"Number of Features :\", clf.n_features_,\", number of classes :\\n\",clf.n_classes_)\nprint(\"Feature Importance :\\n\",clf.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f80915d79c6c7b0689d6708d4220f0efc5a37a6"},"cell_type":"markdown","source":"### Boosting\n\nBoosting is a family of ensemble methods that are primarily used to reduce the bias of an estimator. Boosting can be used in classification and regression tasks. Like bagging, boosting creates ensembles of homogeneous estimators. \n\nWe will focus our discussion of boosting on one of the most popular boosting\nalgorithms, AdaBoost. AdaBoost is an iterative algorithm that was formulated by Yoav Freund and Robert Schapire in 1995. It's name is a portmanteau of adaptive boosting. \n\nOn the first iteration, AdaBoost assigns equal weights to all\nof the training instances and then trains a weak learner. A weak learner (or weak classifier, weak predictor, and so on), is defined only as an estimator that performs slightly better than random chance, such as a decision tree with one or a small number of nodes. Weak learners are often, but not necessarily,\nsimple models. A strong learner, in contrast, is defined as an estimator that is arbitrarily better than a weak learner. \n\nMost boosting algorithms, including AdaBoost, can use any base estimator as a weak\nlearner. On subsequent iterations, AdaBoost increases the weights of training instances that the previous iteration's weak learner predicted incorrectly and decreases the weights of the instances that were predicted correctly. It then trains another weak learner on the re-weighted instances. \n\nSubsequent learners increasingly focus on instances that the ensemble predicts incorrectly. The algorithm terminates when it achieves perfect performance, or after a specified number of iterations. The ensemble predicts the weighted sum of the base estimators' predictions.\n\nscikit-learn implements a variety of boosting meta-estimators for classification and regression tasks, including AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, and GradientBoostingRegressor. \n\nIn the following example, we train an AdaBoostClassifier for an artificial dataset created using the make_classification convenience function. We then plot the accuracy of the ensemble as the number of base estimators increases. We compare the ensemble's accuracy with the accuracy of a single decision tree"},{"metadata":{"trusted":true,"_uuid":"289076e74fd9ecf24a47573ab3e9db90afa6e8aa"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# Dataset creation\nX, y = make_classification(\nn_samples=1000, n_features=50, n_informative=30,\nn_clusters_per_class=3,\nrandom_state=11)\n\n#Train/Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11)\n\n#Model Creation\ntree_clf = DecisionTreeClassifier(random_state=11)\ntree_clf.fit(X_train, y_train)\nprint('Decision tree accuracy: %s' % tree_clf.score(X_test, y_test))\n\n# When an argument for the base_estimator parameter is not passed, the default DecisionTreeClassifier is used\nclf = AdaBoostClassifier(n_estimators=50, random_state=11)\nclf.fit(X_train, y_train)\naccuracies=[]\naccuracies.append(clf.score(X_test, y_test))\nplt.title('Ensemble Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of base estimators in ensemble')\nplt.plot(range(1, 51), [accuracy for accuracy in clf.staged_score(X_test, y_test)])\n\n# Feature Importance\nprint(\"Number of Features :\", tree_clf.n_features_,\", number of classes :\\n\",tree_clf.n_classes_)\nprint(\"Feature Importance :\\n\",tree_clf.feature_importances_)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f229a2f93a4efbd9e504c2cce1816598d8f2bb6"},"cell_type":"markdown","source":"### Conclusion\n\nThis is an indepth tutorial covering basic decision tree classifier, bagging, and boosting techniques. It shows you how you can print decision tree for visual analysis. Another important application of decision tree is feature selection which is done by printing feature importances calculated using decision trees. \n\n### Note: \nPlease don't forget to like, share the tutorial and provide useful feedback in comments."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}