{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"0\"></a>\n![Rains](https://media.giphy.com/media/xUPGcCYOXmcQqhVQli/giphy.gif)"},{"metadata":{},"cell_type":"markdown","source":"# Will it rain tomorrow? "},{"metadata":{},"cell_type":"markdown","source":"- This dataset contains daily weather observations from numerous Australian weather stations.\n\n- Classification problem: Did it rain the next day? Yes (1) or No (0).\n\n- For this problem I will be using **Logistic Regression** and **XGBoost**"},{"metadata":{},"cell_type":"markdown","source":"## Contents\n1. [ Importing Libraries and Data](#1)\n1. [ Exploratory Data Analysis](#2)\n1. [ Data Cleaning](#3)\n    - [ Handling Numericals](#3.1)\n    - [ Handling Categoricals](#3.2)\n1. [ Splitting data](#4)\n1. [ Feature Scaling](#5)\n1. [ Model Training](#6)\n    - [ Logistic Regression](#6.1)\n    - [ XGBoost](#6.2)\n1. [ Comparing Models](#7)\n1. [ Conclusion](#8)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Importing Libraries and Data <a id=\"1\"></a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport math\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Exploratory Data Analysis <a id=\"2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"#### Viewing shape of the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Dataframe shape:', df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Viewing information (Variable data types and counts)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Previewing first 5 rows of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Finding percentage of missing data in each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"def null_percentage(data):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent Missing'])\n\nprint('''Null values:\\n\n{}'''.format(null_percentage(df)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualising missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = df.columns\ncolours = ['#000099', '#ffff00']\nsns.heatmap(df[cols].isnull(), cmap=sns.color_palette(colours))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dropping columns with over 30% missing data and un-needed RISK_MM column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['Sunshine', 'Evaporation', 'Cloud3pm', 'Cloud9am', 'RISK_MM'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Seperating numerical and categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_cols = [var for var in df.columns if df[var].dtype=='f8']\ncategorical_cols = [var for var in df.columns if df[var].dtype=='O']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Numerical Columns: \\n{}\\n'.format(numerical_cols))\nprint('Categorical Columns: \\n{}\\n'.format(categorical_cols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Viewing statistical properties of numericals"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Data cleaning <a id=\"3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"#### Viewing cardinality of categoricals"},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in categorical_cols:\n    print(var, ' has {} unique values'.format(len(df[var].unique())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling of Date column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Date'] = pd.to_datetime(df['Date'])\n\n# Extracting Year, Month and Day from Date Column\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['Day'] = df['Date'].dt.day\n\n# Dropping original Date column\ndf.drop('Date', inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reviewing date changes\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Reviewing categoricals"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = [var for var in df.columns if df[var].dtype=='O']\n\ncategorical_nulls = df[categorical_cols].isnull().sum()\n\nfor var in categorical_cols:\n    print(var, ' has: \\n{} unique values\\n {} null values\\n'.format(len(df[var].unique()), categorical_nulls[var]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Location variable\n* 49 unique values\n* 0 null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def location_percentage(data):\n    total = df['Location'].value_counts()\n    percent = round(df['Location'].value_counts()/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','%'])\n\nprint('''Location Values:\n{}'''.format(location_percentage(df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Location dummies\nlocation_dummies = pd.get_dummies(df.Location, drop_first=True).head()\nlocation_dummies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### WindGustDir variable\n* 17 unique values\n* 9330 null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def WindGustDir_percentage(data):\n    total = df['WindGustDir'].value_counts()\n    percent = round(df['WindGustDir'].value_counts()/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','%'])\n\nWindGustDir_null = df['WindGustDir'].isnull().sum() / len(df['WindGustDir'])\n\nprint('''WindGustDir Values:\n{}\nNull percentage = {}%'''.format(WindGustDir_percentage(df), round(WindGustDir_null * 100, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# WidGustDir Dummies\nWindGustDir_dummies = pd.get_dummies(df.WindGustDir, drop_first=True, dummy_na=True)\nWindGustDir_dummies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### WindDir9am variable\n* 17 unique values\n* 10013 null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def WindDir9am_percentage(data):\n    total = df['WindDir9am'].value_counts()\n    percent = round(df['WindDir9am'].value_counts()/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','%'])\n\nWindDir9am_null = df['WindDir9am'].isnull().sum() / len(df['WindDir9am'])\n\nprint('''WindDir9am Values:\n{}\nNull percentage = {}%'''.format(WindDir9am_percentage(df), round(WindDir9am_null * 100, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# WindDir9am Dummies\nWindDir9am_dummies = pd.get_dummies(df.WindDir9am, drop_first=True, dummy_na=True)\nWindDir9am_dummies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### WindDir3pm variable\n* 17 unique values\n* 3778 null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def WindDir3pm_percentage(data):\n    total = df['WindDir3pm'].value_counts()\n    percent = round(df['WindDir3pm'].value_counts()/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','%'])\n\nWindDir3pm_null = df['WindDir3pm'].isnull().sum() / len(df['WindDir3pm'])\n\nprint('''WindDir3pm Values:\n{}\nNull percentage = {}%'''.format(WindDir3pm_percentage(df), round(WindDir3pm_null * 100, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# WindDir3pm Dummies\nWindDir3pm_dummies = pd.get_dummies(df.WindDir3pm, drop_first=True, dummy_na=True)\nWindDir3pm_dummies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RainToday variable\n* 3 unique values\n* 1406 null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def RainToday_percentage(data):\n    total = df['RainToday'].value_counts()\n    percent = round(df['RainToday'].value_counts()/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','%'])\n\nRainToday_null = df['RainToday'].isnull().sum() / len(df['RainToday'])\n\nprint('''RainToday Values:\n{}\nNull percentage = {}%'''.format(RainToday_percentage(df), round(RainToday_null * 100, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RainToday Dummies\nRainToday_dummies = pd.get_dummies(df.RainToday, drop_first=True, dummy_na=True)\nRainToday_dummies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RainTomorrow variable\n* 2 unique values\n* 0 null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exploring 'RainTomorrow' values (labels)\ndef rain_tomorrow_percentage(data):\n    total = df['RainTomorrow'].value_counts()\n    percent = round(df['RainTomorrow'].value_counts()/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','%'])\n\nprint('''RainTomorrow Values:\n{}'''.format(rain_tomorrow_percentage(df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax2 = plt.subplots(figsize=(5, 5))\nax2 = sns.countplot(x=\"RainTomorrow\", data=df, palette='Blues')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1. Handling Numericals<a id=\"3.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[numerical_cols].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(df[numerical_cols].describe(), 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we can see from the difference in 75% percentile and max values, it is likely we have outliers in:\n# 'Rainfall', 'WindSpeed9am' and 'WindSpeed3pm'\n\n# Plotting suspected outliers\nplt.figure(figsize=(15,10))\n\nplt.subplot(3, 1, 2)\nfig = sns.boxplot(x='Rainfall', data=df)\nfig.set_title('')\n\nplt.subplot(3, 2, 2)\nfig = sns.boxplot(x='WindSpeed9am', data=df)\nfig.set_title('')\n\nplt.subplot(3, 2, 1)\nfig = sns.boxplot(x='WindSpeed3pm', data=df)\nfig.set_title('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ##### As we can see from the boxplots, all suspected variables contain outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Histograms to check skew\n\nplt.figure(figsize=(15,10))\n\nplt.subplot(3, 1, 2)\nfig = df['Rainfall'].hist(bins=20)\nfig.set_xlabel('Rainfall')\n\nplt.subplot(3, 2, 2)\nfig = df['WindSpeed9am'].hist(bins=20)\nfig.set_xlabel('WindSpeed9am')\n\nplt.subplot(3, 2, 1)\nfig = df['WindSpeed3pm'].hist(bins=20)\nfig.set_xlabel('WindSpeed3pm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Removing outliers using IQR"},{"metadata":{"trusted":true},"cell_type":"code","source":"#IQR for Rainfall\nQ1 = df['Rainfall'].quantile(0.25)\nQ3 = df['Rainfall'].quantile(0.75)\nIQR = Q3 - Q1\nLower_bound = Q1 - (IQR * 1.5)\nUpper_bound = Q3 + (IQR * 1.5)\nprint('Rainfall has outliers: < {} or > {}'.format(Lower_bound, Upper_bound))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing Rainfall outliers\ndf = df[~((df['Rainfall'] < - 1.20) |(df['Rainfall'] > 2.0))]\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#IQR for WindSpeed9am\nQ1 = df['WindSpeed9am'].quantile(0.25)\nQ3 = df['WindSpeed9am'].quantile(0.75)\nIQR = Q3 - Q1\nLower_bound = Q1 - (IQR * 1.5)\nUpper_bound = Q3 + (IQR * 1.5)\nprint('WindSpeed9am has outliers: < {} or > {}'.format(Lower_bound, Upper_bound))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing WindSpeed9am outliers\ndf = df[~((df['WindSpeed9am'] < - 11.0) |(df['WindSpeed9am'] > 37.0))]\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#IQR for WindSpeed3pm\nQ1 = df['WindSpeed3pm'].quantile(0.25)\nQ3 = df['WindSpeed3pm'].quantile(0.75)\nIQR = Q3 - Q1\nLower_bound = Q1 - (IQR * 1.5)\nUpper_bound = Q3 + (IQR * 1.5)\nprint('WindSpeed3pm has outliers: < {} or > {}'.format(Lower_bound, Upper_bound))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing WindSpeed3pm outliers\ndf = df[~((df['WindSpeed3pm'] < - 3.5) |(df['WindSpeed3pm'] > 40.5))]\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reviewing Histograms after outlier removal\nplt.figure(figsize=(15,10))\n\nplt.subplot(3, 1, 2)\nfig = df['Rainfall'].hist(bins=20)\nfig.set_xlabel('Rainfall')\n\nplt.subplot(3, 2, 2)\nfig = df['WindSpeed9am'].hist(bins=20)\nfig.set_xlabel('WindSpeed9am')\n\nplt.subplot(3, 2, 1)\nfig = df['WindSpeed3pm'].hist(bins=20)\nfig.set_xlabel('WindSpeed3pm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ##### As we can see our columns are a generally less skewed with the outliers removed, Rainfall is still skewed due to most values being '0'"},{"metadata":{},"cell_type":"markdown","source":"#### Taking care of nulls"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing number of nulls \npd.DataFrame(df[numerical_cols].isnull().sum().sort_values(ascending=False)).head(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling null numericals with mean\nfor col in numerical_cols:\n    mean = df[col].mean()\n    df[col].fillna(mean, inplace=True)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Viewing correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = df.corr()\n\nplt.figure(figsize=(16,12))\nplt.title('Correlation Heatmap of Rain in Australia Dataset')\nax = sns.heatmap(corr_matrix, square=True, annot=True, fmt='.2f', linecolor='white')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nax.set_yticklabels(ax.get_yticklabels(), rotation=30)           \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2. Handling Categoricals<a id=\"3.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Replacing categoricals with dummies of each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df, pd.get_dummies(df['Location'],dummy_na=True, prefix='Location', columns=categorical_cols)],axis=1).drop(['Location'],axis=1)\ndf = pd.concat([df, pd.get_dummies(df['WindGustDir'],dummy_na=True, prefix='WindGustDir', columns=categorical_cols)],axis=1).drop(['WindGustDir'],axis=1)\ndf = pd.concat([df, pd.get_dummies(df['WindDir9am'],dummy_na=True, prefix='WindDir9am', columns=categorical_cols)],axis=1).drop(['WindDir9am'],axis=1)\ndf = pd.concat([df, pd.get_dummies(df['WindDir3pm'],dummy_na=True, prefix='WindDir3pm', columns=categorical_cols)],axis=1).drop(['WindDir3pm'],axis=1)\ndf = pd.concat([df, pd.get_dummies(df['RainToday'],dummy_na=True, prefix='RainToday', columns=categorical_cols)],axis=1).drop(['RainToday'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Viewing data with dummies"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Splitting data<a id=\"4\"></a>"},{"metadata":{},"cell_type":"markdown","source":"#### Splitting data into X and y variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('RainTomorrow', axis=1)\ny = df['RainTomorrow']\n\nprint('''X Shape: {}\ny Shape: {}'''.format(X.shape, pd.DataFrame(y).shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Splitting data into training and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n# Checking shapes of each set\nprint('''X train: {}\nX test: {}\ny train: {}\ny test: {}'''.format(X_train.shape, X_test.shape, pd.DataFrame(y_train).shape, pd.DataFrame(y_test).shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Feature Scaling<a id=\"5\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\ncols = pd.DataFrame(X_train).columns\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns= cols)\nX_test = pd.DataFrame(scaler.transform(X_test), columns=cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing scaled training set\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing scaled test set\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Model Training<a id=\"6\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nrandom_state = 42","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1. Logistic Regression<a id=\"6.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg = LogisticRegression(random_state=random_state)\nlog_reg.fit(X_train, y_train)\n\nlog_reg_pred = log_reg.predict(X_test)\n\nlog_reg_cm = confusion_matrix(y_test, log_reg_pred)\n\nprint('Model accuracy score:\\n{}\\nConfusion Matrix:\\n{}'. format(round(accuracy_score(y_test, log_reg_pred), 4), log_reg_cm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for over/under fitting\nprint('Training set score: \\n{}\\nTest set score: \\n{}'.format(round(log_reg.score(X_train, y_train), 4), round(log_reg.score(X_test, y_test), 4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using grid-search to find better parameters for Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'C' : [1, 25, 50, 75, 100]}\n\nlog_reg_2 = LogisticRegression(random_state=random_state, solver='lbfgs')\n\ngrid_search_log = GridSearchCV(log_reg_2, param_grid, scoring=\"roc_auc\", cv=5)\n\ngrid_search_log.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best Parameters:\\n{}'.format(grid_search_log.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using our best parameters\nlog_reg_3 = LogisticRegression(random_state=random_state, C=50)\nlog_reg_3.fit(X_train, y_train)\n\nlog_reg_3_pred = log_reg_3.predict(X_test)\n\nlog_reg_3_cm = confusion_matrix(y_test, log_reg_3_pred)\n\nprint('Model accuracy score:\\n{}\\nConfusion Matrix:\\n{}'. format(round(accuracy_score(y_test, log_reg_3_pred), 4), log_reg_3_cm))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2 XGBoost<a id=\"6.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_pred = xgb.predict(X_test)\n\nxgb_cm = confusion_matrix(y_test, xgb_pred)\n\nprint('Model accuracy score:\\n{}\\nConfusion Matrix:\\n{}'. format(round(accuracy_score(y_test, xgb_pred), 4), xgb_cm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for over/under fitting\nprint('Training set score: \\n{}\\nTest set score: \\n{}'.format(round(xgb.score(X_train, y_train), 4), round(xgb.score(X_test, y_test), 4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using grid-search to find better parameters for XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n     'eta'    : [0.01, 0.15, 0.30 ] ,\n     'max_depth'        : [ 3, 6, 9],\n     'min_child_weight' : [ 1, 3, 5],\n     'gamma'            : [ 0.0, 0.2, 0.4 ]\n     }\n\nxgb_2 = XGBClassifier(random_state=random_state)\n\ngrid_search = GridSearchCV(xgb_2, param_grid, n_jobs=4, scoring=\"roc_auc\", cv=5)\n\ngrid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best Parameters:\\n{}'.format(grid_search.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using our best parameters\nxgb_3 = XGBClassifier(eta=0.01, gamma=0.4, max_depth=9, min_child_weight=3)\nxgb_3.fit(X_train, y_train)\n\nxgb_3_pred = xgb_3.predict(X_test)\n\nxgb_3_cm = confusion_matrix(y_test, xgb_3_pred)\n\nprint('Model accuracy score:\\n{}\\nConfusion Matrix:\\n{}'. format(round(accuracy_score(y_test, xgb_3_pred), 4), xgb_3_cm))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Comparing models<a id=\"7\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy scores of each model\nlog_reg_acc = round(accuracy_score(y_test, log_reg_pred), 4)\nlog_reg_3_acc = round(accuracy_score(y_test, log_reg_3_pred), 4)\nxgb_acc = round(accuracy_score(y_test, xgb_pred), 4)\nxgb_3_acc = round(accuracy_score(y_test, xgb_3_pred), 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating dataframe showing accuracy scores of each model\ncompare = {'Model': ['Logistic Regression Original', 'Logistic Regression Tuned', 'XGBoost Original', 'XGBoost Tuned'],\n          'Accuracy score': [log_reg_acc, log_reg_3_acc, xgb_acc, xgb_3_acc]}\n\npd.DataFrame(data=compare)\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Conclusion<a id=\"8\"></a>"},{"metadata":{},"cell_type":"markdown","source":"As we can see from the comparison table, our best model is the XGBoost model in which we used our best parameters from grid search. This model gives us a high accuracy of 87.85%. From these results we can conclude that we have a reliable model for predicting future rainy days given we know todays data."},{"metadata":{},"cell_type":"markdown","source":"[Back to top](#0)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}