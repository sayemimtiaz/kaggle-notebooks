{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Table Of Content</h3>\n","metadata":{}},{"cell_type":"markdown","source":"$\\text{Table Of Content}$\n\n* [1. Introduction](#1)\n    * [1.1 Import Libraries](#1.1)\n    * [1.2 Load data](#1.2)\n* [2. Exploratory Data Analysis(EDA)](#2)\n    * [2.1 Overview](#2.1)\n* [3. Feature Engineering](#3)\n    * [3.1 Creating The Major Genre Feature](#3.1)\n* [4. Model Selection,fitting and prediction](#4)\n    * [4.1 Selecting The Target Feature And The Predictior Features](#4.1)\n    * [4.2 Linear Regression Cross Validation Test](#4.2)\n    * [4.3 Decision Tree Cross Validation Test](#4.3)\n    * [4.4 Random Forest Cross Validation Test](#4.4)\n    * [4.5 KNN Cross Validation Test](#4.5)\n    * [4.6 Model Blending](#4.6)    \n    ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nsns.set_style('darkgrid')\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\ndef RMSE(y,yhat):\n    return np.sqrt(mean_squared_error(y,yhat))\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nplt.rc('figure',figsize=(20,11))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n","metadata":{}},{"cell_type":"code","source":"s_data =pd.read_csv('/kaggle/input/top50spotify2019/top50.csv',encoding='ISO-8859-1')\ns_data.head(5)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s_data.drop(s_data.columns[0] ,axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis</h3>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>Overview </span></p>","metadata":{}},{"cell_type":"code","source":"info = s_data.describe()\ninfo.loc['skew'] =s_data.skew()\ninfo.loc['kurtosis'] =s_data.kurt()\ninfo.loc['median'] =s_data.median()\ninfo.loc['iqr'] = info.loc['75%']-info.loc['25%']\ninfo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>So We have no missing values which are great and all of our features are correctly labeled, also we already can see that Ed Sheeran has the most songs in the top 50 list! </span></p>","metadata":{}},{"cell_type":"code","source":"number_of_unique_artists = len(s_data['Artist.Name'].value_counts().to_list())\nnumber_of_unique_genres = len(s_data['Genre'].value_counts().to_list())\nprint(\"Number Of Unique Artists: \",number_of_unique_artists,' | ',' Number Of Unique Genres: ',number_of_unique_genres)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### \n\n\n<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>In terms of encoding our artists and our genres we see one-hot encoding is a bad idea here considering the cardinality of those features therefore we will dummy encode the genres and replace the artist names with numerical values closer to the model selection stage </span></p>","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=[[0.0, \"rgb(165,0,38)\"],\n                [0.1111111111111111, \"rgb(215,48,39)\"],\n                [0.2222222222222222, \"rgb(244,109,67)\"],\n                [0.3333333333333333, \"rgb(253,174,97)\"],\n                [0.4444444444444444, \"rgb(254,224,144)\"],\n                [0.5555555555555556, \"rgb(224,243,248)\"],\n                [0.6666666666666666, \"rgb(171,217,233)\"],\n                [0.7777777777777778, \"rgb(116,173,209)\"],\n                [0.8888888888888888, \"rgb(69,117,180)\"],\n                [1.0, \"rgb(49,54,149)\"]]\n\ns_val =s_data.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =s_data.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Locations That Contribute The Most To Our Cut-Offs\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>Let us take a side not here and remember that Popularity, which prediction of it is one of our goals, is correlated mostly with beats per minute, valence, and speechless </span></p>","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>What music genres dominate the top 50 songs? Extracting The Main Genre From Our Sub Genres</span></h2>","metadata":{}},{"cell_type":"code","source":"ax = sns.countplot(s_data['Genre'],palette='Greens')\nax.set_xticklabels(ax.get_xticklabels(),rotation=90,size=13)\nax.set_title('Distribution Of Genres Across Our Data',fontsize=16)\nax.patches[2].set_fc('r')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>Interesting we can see that the dance-pop genre is the genre of which there are the most songs in our data, but if we take a close look we can see that there is an interesting feature hidden in our data, and its that most of the genres we have in our data are sub-genres of major music genres. We see that we have the pop that is the main genre and a few sub-genres of pop lets create a new feature of the main genre and observe our findings. </span></p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<a id=\"3.1\"></a>\n","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Extracting The Main Genre From Our Sub-Genres</span></h2>","metadata":{}},{"cell_type":"code","source":"main_genres = ['rock','pop','blues','hip hop','jazz','reggae','techno','trap','regga','rap','r&b']\ndef check_genre(sir):\n    for word in main_genres:\n        if sir.find(word) != -1:\n            if word == 'rap':\n                return 'hip hop'\n            else:\n                return word\n    return sir\n\ns_data['Main.Genre'] = s_data['Genre'].apply(check_genre)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>Now that we have our new feature which shows us the main genres of music lets take a quick look again at our distribution and make some assumptions </span></p>","metadata":{}},{"cell_type":"code","source":"ax = sns.countplot(s_data['Main.Genre'],palette='Greens',order=s_data['Main.Genre'].value_counts().index)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90,size=13)\nax.set_title('Distribution Of Genres Across Our Data',fontsize=16)\nax.patches[0].set_fc('r')\nax.patches[1].set_fc((0.75,0,0))\nax.patches[2].set_fc((0.50,0,0))\nplt.legend({'Most Frequent Music Genre':0},prop={'size':'16'})\nplt.show()\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>As we see the top 3 music major music genres are&nbsp;</span></p>\n<ol>\n    <li style=\"text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 24px;\">Pop</span></span></li>\n    <li style=\"text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 24px;\"><span style=\"color: rgb(0, 0, 0); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">Hip Hop</span> <br></span></span></li>\n    <li style=\"text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 24px;\">Latin</span></span></li>\n</ol>\n<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>where the most popular sub-genre is dance-pop</span></p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>Another important point to observe is which artist dominates the top 50 songs? </span></p>","metadata":{}},{"cell_type":"code","source":"ax = sns.countplot(s_data['Artist.Name'],palette='Greens',order = s_data['Artist.Name'].value_counts().index,label='Top Artist')\nax.set_xticklabels(ax.get_xticklabels(),rotation=90,size=13)\nax.set_title('Distribution Of Genres Across Our Data',fontsize=16)\nax.patches[0].set_fc('r')\nplt.legend(prop={'size':'16'})\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Its quite clear that our top artist is Ed Sheeran with 4 of his songs in the top 50 list followed by The Chainsmokers, Shawn Mendes, Post Malone, Sech, Marshmello, Billie Elish, J Bavlin, Lil Nas, and Ariana Grande with 2 songs at the top 50 list alike the other artist which only have 1 song. </span></p>\n<p><br></p>","metadata":{}},{"cell_type":"code","source":"#Our Top 10 Artist And Top 10 Genres \ntop_10_artist = s_data['Artist.Name'].value_counts()[:10]\ntop_10_genres = s_data['Genre'].value_counts()[:10]\ntop_10_songs = s_data.iloc[s_data['Popularity'].nlargest(10).index,:]\ntop_10_artist.to_frame()\ntop_10_songs","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.distplot(s_data['Popularity'],hist_kws={'color':'r'},kde_kws={'color':'g','lw':'6'})\ntextstr = '\\n'.join(\n    \n        (   r'$\\mu=%.2f$' % (s_data['Popularity'].mean(),)\n          , r'$\\mathrm{median}=%.2f$' % (s_data['Popularity'].median(),)\n          , r'$\\sigma=%.2f$' % (s_data['Popularity'].std(),)\n          , r'Skew=%.2f' % (s_data['Popularity'].skew(),)\n          , r'Kurtosis=%.2f' % (s_data['Popularity'].kurt(),)\n\n        )\n    \n                  )\n\nprops = dict(boxstyle='round', facecolor='red', alpha=0.5)\nax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=17,\n        verticalalignment='top', bbox=props)\nax.set_title('Distribution Of Popularity Scores In Our Data',fontsize=16)\nax.set_xlabel('Popularity',fontsize=16)\nplt.legend()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'> So we see that we have a negative skew in our popularity scores, and our mean score is 87.5 with a standard deviation value of 4.49; in the following steps of our kernel, we would like to transform our data and center it around the mean and obtain a more normal distribution using the z transformation. Hopefully making our model better.</span></p>\n<p><br></p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Next, let us take a closer look at the artists in our data; we have a minimal data set of just 50 samples; my goals currently will be to collect more basic information on the artists who create those new features and take a look at all the data concerning the artist and hopefully uncover an underlying connection between the top 10 genres and the top 10 artists</span></p>\n<p><br></p>","metadata":{}},{"cell_type":"code","source":"gender = {'Ed Sheeran':'M','The Chainsmokers':'Group','Shawn Mendes':'M','Post Malone':'M','Sech':'M','Marshmello':'M','Billie Eilish':'F','J Balvin':'M',\n         'Lil Nas X':'M','Ariana Grande':'F','Daddy Yankee':'M','Y2K':'M','DJ Snake':'M','Lewis Capaldi':'M','Chris Brown':'M','Khalid':'M','Lizzo':'F','Lauv':'M',\n         'Kygo':'M','Ali Gatie':'M','Lady Gaga':'F','Bad Bunny':'M','Lunay':'M','Sam Smith':'M','Anuel AA':'M','Nicky Jam':'M','Lil Tecca':'M','ROSAL√çA':'F','Young Thug':'M',\n         'Martin Garrix':'M','Katy Perry':'F','Jhay Cortez':'M','Drake':'M','Tones and I':'F','Taylor Swift':'F','Jonas Brothers':'Group','MEDUZA':'M','Maluma':'M'}\ns_data['Artist.Gender'] = s_data['Artist.Name'].apply(lambda x: gender[x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_10_arist_df = s_data[s_data['Artist.Name'].isin(top_10_artist.index)]\nsia = SentimentIntensityAnalyzer()\ntop_10_arist_df['Track_Name_Sentiment.c'] = top_10_arist_df['Track.Name'].apply(lambda x: sia.polarity_scores(x)['compound'])","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,11))\nax = sns.countplot(top_10_arist_df['Artist.Gender'])\nax.set_title('Distribution Of Gender Among The Top 10 Artists',fontsize=17)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>In our top 50 data set, most of the top 10 artists are male in general, it applies to all 50 songs in our dataset.</span></p>\n<p style=\"text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 24px;\"><br></span></span></p>\n<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>let us try and find out what do the top 10 songs share in common </span></p>","metadata":{}},{"cell_type":"code","source":"fig = go.Figure(data=[go.Table(\n    header=dict(values=list([f'<b>{x}<b>' for x in top_10_songs.columns]),\n                fill_color='royalblue',\n                font_color='white',\n                font_size=13,\n                align='left'),\n    cells=dict(values=[top_10_songs[col] for col in top_10_songs.columns],\n               fill_color='azure',\n               align='left'))\n])\nfig.update_layout(title='Top 10 Songs')\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ex.scatter_polar(s_data,theta='Main.Genre',r='Beats.Per.Minute',color ='Popularity',title='Spread of different genre popularity according to beats per minute')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ex.density_heatmap(s_data,x='Beats.Per.Minute',y='Popularity',title='Popularity counts according to BPM ')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_f = top_10_songs.columns[3:13]\ncor = top_10_songs.corr()\nax = sns.distplot((top_10_songs['Danceability']-top_10_songs['Danceability'].mean())/top_10_songs['Danceability'].std(),hist=False,label='Danceability')\nax = sns.distplot((top_10_songs['Energy']-top_10_songs['Energy'].mean())/top_10_songs['Energy'].std(),hist=False,label='Energy')\nax = sns.distplot((top_10_songs['Valence.']-top_10_songs['Valence.'].mean())/top_10_songs['Valence.'].std(),hist=False,label='Valence')\nax = sns.distplot((top_10_songs['Length.']-top_10_songs['Length.'].mean())/top_10_songs['Length.'].std(),hist=False,label='Length')\nax = sns.distplot((top_10_songs['Beats.Per.Minute']-top_10_songs['Beats.Per.Minute'].mean())/top_10_songs['Beats.Per.Minute'].std(),hist=False,label='Beats.Per.Minute')\nax.set_xlabel('Tansformed Distribution',fontsize=16)\nax.set_title('Normalized Distributions Of The Most Significant Features In Our Top 10 Songs',fontsize=16)\nplt.legend(prop={'size':'20'})\nplt.show()\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>We can observe that for the features we tested above the variance of the normalized distribution is quite normal and there is a strong resemblance across these features in our top 10 songs </span></p>","metadata":{}},{"cell_type":"code","source":"sd_data = s_data.copy()\ngeners_one = pd.get_dummies(sd_data['Main.Genre'],prefix='Genre')\ngeners_one = geners_one[geners_one.columns[1:]]\nsd_data = pd.concat([sd_data,geners_one],axis=1)\nsd_data = sd_data.drop(columns='Main.Genre')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sd_data['Track_Name_Sentiment'] = sd_data['Track.Name'].apply(lambda x: sia.polarity_scores(x)['compound'])\nsd_data['Track_Name_Length'] = sd_data['Track.Name'].apply(lambda x: len(x))\nsd_data['Genre'] =sd_data['Genre'].astype('category').cat.codes\nsd_data['Artist.Gender'] =sd_data['Artist.Gender'].astype('category').cat.codes\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=[[0.0, \"rgb(165,0,38)\"],\n                [0.1111111111111111, \"rgb(215,48,39)\"],\n                [0.2222222222222222, \"rgb(244,109,67)\"],\n                [0.3333333333333333, \"rgb(253,174,97)\"],\n                [0.4444444444444444, \"rgb(254,224,144)\"],\n                [0.5555555555555556, \"rgb(224,243,248)\"],\n                [0.6666666666666666, \"rgb(171,217,233)\"],\n                [0.7777777777777778, \"rgb(116,173,209)\"],\n                [0.8888888888888888, \"rgb(69,117,180)\"],\n                [1.0, \"rgb(49,54,149)\"]]\n\ns_val =sd_data.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =sd_data.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Locations That Contribute The Most To Our Cut-Offs\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>Heatmap for the new features we created, we can see that some of the features we added have a significantly higher correlation with popularity than the original features in the data, lest check how those features span to decide which type of regression to use</span></p>","metadata":{}},{"cell_type":"code","source":"p_correaltion=['Speechiness.','Beats.Per.Minute','Valence.','Genre','Genre_hip hop']\nfig,axs = plt.subplots(2,2)\nfig.set_figheight(15)\nfig.set_figwidth(15)\nsns.regplot(y=sd_data['Popularity'],x=sd_data[p_correaltion[1]],ax=axs[0,1],color='r')\nsns.regplot(y=sd_data['Popularity'],x=sd_data[p_correaltion[0]],ax=axs[0,0])\nsns.regplot(y=sd_data['Popularity'],x=sd_data[p_correaltion[2]],ax=axs[1,0],color='g')\nsns.regplot(y=sd_data['Popularity'],x=sd_data[p_correaltion[3]],ax=axs[1,1],color='c')\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Model Selection And Evaluation</h3>\n\n\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Let us select a model to predict the popularity of a song. our data set is very small so we must use cross-validation hopefully avoiding overfitting</span></p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.1\"></a>\n\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Selecting The Target Feature And The Predictor Features</span></p>","metadata":{}},{"cell_type":"code","source":"y = sd_data['Popularity']\nX = sd_data[['Speechiness.','Beats.Per.Minute','Valence.','Genre','Genre_hip hop','Genre_escape room','Genre_r&b']]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Knn Pipe\nKnn_Pipe =  Pipeline(steps=[('scaler',StandardScaler()),('DT',KNeighborsRegressor(n_neighbors=5))])\nKnn_scores = np.sqrt(-1*cross_val_score(Knn_Pipe,X,y,cv=5,scoring='neg_mean_squared_error'))\nKnn_Pipe.fit(X,y)\n\n#Random Forest Pipe\nRF_Pipe =  Pipeline(steps=[('scaler',StandardScaler()),('DT',RandomForestRegressor(max_leaf_nodes=14,n_estimators=20,random_state=42))])\nRF_scores = np.sqrt(-1*cross_val_score(RF_Pipe,X,y,cv=5,scoring='neg_mean_squared_error'))\nRF_Pipe.fit(X,y)\n\n#Decision Tree Pipe\nDT_Pipe =  Pipeline(steps=[('scaler',StandardScaler()),('DT',DecisionTreeRegressor(max_leaf_nodes=10))])\nDT_scores = np.sqrt(-1*cross_val_score(DT_Pipe,X,y,cv=5,scoring='neg_mean_squared_error'))\nDT_Pipe.fit(X,y)\n\n#Linear Regression Pipe\nLR_pipe = Pipeline(steps=[('scaler',StandardScaler()),('poly',PolynomialFeatures(degree=1)),('LinearRegression',LinearRegression())])\nLR_scores = np.sqrt(-1*cross_val_score(LR_pipe,X,y,cv=5,scoring='neg_mean_squared_error'))\nLR_pipe.fit(X,y)\n\npred = LR_pipe.predict(X)\nmse = np.sqrt(mean_squared_error(pred,y))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=4, cols=1,shared_xaxes=True,subplot_titles=('Random Forest Cross Val Scores',\n                                                                     'Decision Tree Cross Val Scores',\n                                                                    'Linear Regression Cross Val Scores',\n                                                                    'KNN Cross Val Scores'))\n\nfig.add_trace(\n    go.Scatter(x = np.arange(1,len(RF_scores)+1),y=RF_scores,mode='lines+markers',name='Random Forest'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x = np.arange(1,len(DT_scores)+1),y=DT_scores,mode='lines+markers',name='Decision Tree'),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(x = np.arange(1,len(LR_scores)+1),y=LR_scores,mode='lines+markers',name='Linear Regression'),\n    row=3, col=1\n)\nfig.add_trace(\n    go.Scatter(x = np.arange(1,len(Knn_scores)+1),y=Knn_scores,mode='lines+markers',name='KNN'),\n    row=4, col=1\n)\n\n\n#Fold Means\nfig.add_shape(type=\"line\",\n    x0=1, y0=np.mean(RF_scores), x1=5, y1=np.mean(RF_scores),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x1', \n        yref='y1'\n)\n\nfig.add_shape(type=\"line\",\n    x0=1, y0=np.mean(DT_scores), x1=5, y1=np.mean(DT_scores),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\nfig.add_shape(type=\"line\",\n    x0=1, y0=np.mean(LR_scores), x1=5, y1=np.mean(LR_scores),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x3', \n        yref='y3'\n)\n\nfig.add_shape(type=\"line\",\n    x0=1, y0=np.mean(Knn_scores), x1=5, y1=np.mean(Knn_scores),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x4', \n        yref='y4'\n)\n\n\nfig.update_layout(height=700, width=900, title_text=\"Different Model 5 Fold Cross Validation\")\nfig.update_yaxes(title_text=\"RMSE\")\nfig.update_xaxes(title_text=\"Fold #\")\n\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure(data=[go.Table(header=dict(values=['<b>Model<b>', '<b>Root Mean Squared Error<b>'],\n                                           line_color='darkslategray',\n    fill_color='whitesmoke',\n    align=['center','center'],\n    font=dict(color='black', size=18),\n    height=40),\n                               \n                 cells=dict(values=[['<b>Random Forest<b>', '<b>Decision Tree<b>','<b>Linear Regression<b>','<b>KNN<b>'],\n                                    [np.round(RF_scores.mean(),2), \n                                     np.round(DT_scores.mean(),2),\n                                     np.round(mse,2),\n                                     np.round(Knn_scores.mean(),2) \n                                    ]]))\n                     ])\n\nfig.update_layout(title='Model Result On Original Data (Without Upsampling)')\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.6\"></a>\n\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Model Blending</h3>","metadata":{}},{"cell_type":"code","source":"pred = LR_pipe.predict(X)*0.2 + RF_Pipe.predict(X)*0.3 + 0.4* DT_Pipe.predict(X) + Knn_Pipe.predict(X)*0.1\nrmse = RMSE(pred,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax=sns.lineplot(x=np.arange(0,len(y)),y=y,label = 'Actual Popularity Value')\nax = sns.lineplot(x=np.arange(0,len(y)),y=pred,label = 'Predicted Popularity Value')\nax.set_xlabel('Song Index',fontsize=16)\nax.set_ylabel('Popularity',fontsize=16)\nprop3 = dict(boxstyle='round',facecolor='orange',alpha=0.5)\nax.text(0.05, 0.25, 'RMSE : {:.2f}'.format(rmse), transform=ax.transAxes, fontsize=27,\n        verticalalignment='top', bbox=prop3)\nplt.legend(prop={'size':'20'})\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output=pd.DataFrame({'Prediction':pred,'Actual':y})\nfig = make_subplots(\n    rows=3, cols=2,subplot_titles=('','Actual','Predictions','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"table\",\"rowspan\": 3}     ,{\"type\": \"scatter\"}] ,\n           [None                               ,{\"type\": \"scatter\"}]            ,           \n           [None                               ,{\"type\": \"scatter\"}]                           \n          ]\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Actual\"])),\n        y=output[\"Actual\"],\n        mode=\"markers\",\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"],\n        mode=\"markers\",\n    ),\n    row=2, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"]-output[\"Actual\"],\n        mode=\"markers\",\n    ),\n    row=3, col=2\n)\n\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['Prediction','Actual'],\n            font=dict(size=10),\n            align=\"left\"\n        ),\n        cells=dict(\n            values=[output[k].tolist() for k in output.columns],\n            align = \"left\")\n    ),\n    row=1, col=1\n)\n\n\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=(output[\"Prediction\"]-output[\"Actual\"]).mean(), x1=len(output[\"Prediction\"]), y1=(output[\"Prediction\"]-output[\"Actual\"]).mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x3', \n        yref='y3'\n)\n\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"Prediction Evaluation\",\n)\n\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Visable Heteroscedasticity Evaluation</h3>","metadata":{}},{"cell_type":"code","source":"plt.title('Residuals of predicted values',fontsize=(18))\nsns.residplot(pred,y)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}