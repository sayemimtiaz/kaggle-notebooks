{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Table Of Content</h1>\n\n\n* [1. Introduction](#1)\n    * [1.1 Libraries And Utilities](#1.1)\n    * [1.2 Data Loading](#1.2)\n* [2. Exploratory Data Analysis(EDA)](#2)\n* [3. Data Preprocessing](#3)\n    * [3.1 Data Upsampling Using SMOTE](#3.1)\n    * [3.2 Principal Component Analysis Of One Hot Encoded Data](#3.2)  \n* [4. Model Selection And Evaluation](#4) \n    * [4.1 Cross Validation](#4.1)\n    * [4.2 Model Evaluation](#4.2)\n    * [4.3 Model Evaluation On Original Data (Before Upsampling)](#4.3)\n* [5. Results](#5) \n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities</h3>","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.offline as pyo\npyo.init_notebook_mode()\nsns.set_style('darkgrid')\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score as f1\nfrom sklearn.metrics import confusion_matrix\nimport scikitplot as skplt\n\nplt.rc('figure',figsize=(18,9))\n%pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1.2\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Loading</h3>\n","metadata":{}},{"cell_type":"code","source":"c_data = pd.read_csv('/kaggle/input/credit-card-customers/BankChurners.csv')\nc_data = c_data[c_data.columns[:-2]]\nc_data.head(3)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis</h1>\n","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Customer_Age'],name='Age Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Customer_Age'],name='Age Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of Customer Ages\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We can see that the distribution of customer ages in our dataset follows a fairly normal distribution; thus, further use of the age feature can be done with the normality assumption.</span></p>","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(\n    rows=2, cols=2,subplot_titles=('','<b>Platinum Card Holders','<b>Blue Card Holders<b>','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"pie\",\"rowspan\": 2}       ,{\"type\": \"pie\"}] ,\n           [None                               ,{\"type\": \"pie\"}]            ,                                      \n          ]\n)\n\nfig.add_trace(\n    go.Pie(values=c_data.Gender.value_counts().values,labels=['<b>Female<b>','<b>Male<b>'],hole=0.3,pull=[0,0.3]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Pie(\n        labels=['Female Platinum Card Holders','Male Platinum Card Holders'],\n        values=c_data.query('Card_Category==\"Platinum\"').Gender.value_counts().values,\n        pull=[0,0.05,0.5],\n        hole=0.3\n        \n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Pie(\n        labels=['Female Blue Card Holders','Male Blue Card Holders'],\n        values=c_data.query('Card_Category==\"Blue\"').Gender.value_counts().values,\n        pull=[0,0.2,0.5],\n        hole=0.3\n    ),\n    row=2, col=2\n)\n\n\n\nfig.update_layout(\n    height=800,\n    showlegend=True,\n    title_text=\"<b>Distribution Of Gender And Different Card Statuses<b>\",\n)\n\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>More samples of females in our dataset are compared to males, but the percentage of difference is not that significant, so we can say that genders are uniformly distributed.</span></p>","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Dependent_count'],name='Dependent count Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Dependent_count'],name='Dependent count Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of Dependent counts (close family size)\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The distribution of Dependent counts is fairly normally distributed with a slight right skew.</span></p>","metadata":{}},{"cell_type":"code","source":"ex.pie(c_data,names='Education_Level',title='Propotion Of Education Levels',hole=0.33)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>If most of the customers with unknown education status lack any education, we can state that more than 70% of the customers have a formal education level. About 35% have a higher level of education.</span></p>","metadata":{}},{"cell_type":"code","source":"ex.pie(c_data,names='Marital_Status',title='Propotion Of Different Marriage Statuses',hole=0.33)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Almost half of the bank customers are married, and interestingly enough, almost the entire other half are single customers. only about 7% of the customers are divorced, which is surprising considering the worldwide divorce rate statistics! (let me know where the bank is located and sign me up!)</span></p>","metadata":{}},{"cell_type":"code","source":"ex.pie(c_data,names='Income_Category',title='Propotion Of Different Income Levels',hole=0.33)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ex.pie(c_data,names='Card_Category',title='Propotion Of Different Card Categories',hole=0.33)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Months_on_book'],name='Months on book Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Months_on_book'],name='Months on book Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of months the customer is part of the bank\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Kurtosis of Months on book features is : {}'.format(c_data['Months_on_book'].kurt()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>We have a low kurtosis value pointing to a very flat shaped distribution (as shown in the plots above as well), meaning we cannot assume normality of the feature.</span></p>","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Total_Relationship_Count'],name='Total no. of products Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Total_Relationship_Count'],name='Total no. of products Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of Total no. of products held by the customer\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The distribution of the total number of products held by the customer seems closer to a uniform distribution and may appear useless as a predictor for churn status.</span></p>","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Months_Inactive_12_mon'],name='number of months inactive Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Months_Inactive_12_mon'],name='number of months inactive Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of the number of months inactive in the last 12 months\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Credit_Limit'],name='Credit_Limit Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Credit_Limit'],name='Credit_Limit Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of the Credit Limit\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Total_Trans_Amt'],name='Total_Trans_Amt Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Total_Trans_Amt'],name='Total_Trans_Amt Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of the Total Transaction Amount (Last 12 months)\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We see that the distribution of the total transactions (Last 12 months) displays a multimodal distribution, meaning we have some underlying groups in our data; it can be an interesting experiment to try and cluster the different groups and view the similarities between them and what describes best the different groups which create the different modes in our distribution.</span></p>","metadata":{}},{"cell_type":"code","source":"ex.pie(c_data,names='Attrition_Flag',title='Proportion of churn vs not churn customers',hole=0.33)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>As we can see, only 16% of the data samples represent churn customers; in the following steps, I will use SMOTE to upsample the churn samples to match them with the regular customer sample size to give the later selected models a better chance of catching on small details which will almost definitely be missed out with such a size difference.</span></p>","metadata":{}},{"cell_type":"code","source":"c_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Data Preprocessing</h1>\n","metadata":{}},{"cell_type":"code","source":"c_data.Attrition_Flag = c_data.Attrition_Flag.replace({'Attrited Customer':1,'Existing Customer':0})\nc_data.Gender = c_data.Gender.replace({'F':1,'M':0})\nc_data = pd.concat([c_data,pd.get_dummies(c_data['Education_Level']).drop(columns=['Unknown'])],axis=1)\nc_data = pd.concat([c_data,pd.get_dummies(c_data['Income_Category']).drop(columns=['Unknown'])],axis=1)\nc_data = pd.concat([c_data,pd.get_dummies(c_data['Marital_Status']).drop(columns=['Unknown'])],axis=1)\nc_data = pd.concat([c_data,pd.get_dummies(c_data['Card_Category']).drop(columns=['Platinum'])],axis=1)\nc_data.drop(columns = ['Education_Level','Income_Category','Marital_Status','Card_Category','CLIENTNUM'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Here we one hot encode all the categorical features describing different statuses of a customer.</span></p>","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =c_data.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=0.7,ygap=0.7,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =c_data.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=0.7,ygap=0.7,colorscale=colorscale),\n    row=2, col=1\n)\nfig.update_layout(\n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=16,\n        font_family=\"Rockwell\"\n    )\n)\nfig.update_layout(height=700, width=900, title_text=\"Numeric Correaltions\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Upsampling Using SMOTE</h3>\n","metadata":{}},{"cell_type":"code","source":"oversample = SMOTE()\nX, y = oversample.fit_resample(c_data[c_data.columns[1:]], c_data[c_data.columns[0]])\nusampled_df = X.assign(Churn = y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ohe_data =usampled_df[usampled_df.columns[15:-1]].copy()\n\nusampled_df = usampled_df.drop(columns=usampled_df.columns[15:-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =usampled_df.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =usampled_df.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\nfig.update_layout(\n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=16,\n        font_family=\"Rockwell\"\n    )\n)\nfig.update_layout(height=700, width=900, title_text=\"Upsmapled Correlations\")\nfig.show()\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Principal Component Analysis Of One Hot Encoded Data </h3>\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We will use principal component analysis to reduce the dimensionality of the one-hot encoded categorical variables losing some of the variances, but simultaneously, using a couple of principal components instead of tens of one-hot encoded features will help me construct a better model.</span></p>","metadata":{}},{"cell_type":"code","source":"\nN_COMPONENTS = 4\n\npca_model = PCA(n_components = N_COMPONENTS )\n\npc_matrix = pca_model.fit_transform(ohe_data)\n\nevr = pca_model.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\", \n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(     title='Explained Variance Using {} Dimensions'.format(N_COMPONENTS))\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"usampled_df_with_pcs = pd.concat([usampled_df,pd.DataFrame(pc_matrix,columns=['PC-{}'.format(i) for i in range(0,N_COMPONENTS)])],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = ex.scatter_matrix(\n    usampled_df_with_pcs[['PC-{}'.format(i) for i in range(0,N_COMPONENTS)]].values,\n    color=usampled_df_with_pcs.Credit_Limit,\n    dimensions=range(N_COMPONENTS),\n    labels={str(i):'PC-{}'.format(i) for i in range(0,N_COMPONENTS)},\n    title=f'Total Explained Variance: {total_var:.2f}%')\n\nfig.update_traces(diagonal_visible=False)\nfig.update_layout(\n    coloraxis_colorbar=dict(\n        title=\"Credit_Limit\",\n    ),\n)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\n\n\ns_val =usampled_df_with_pcs.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =usampled_df_with_pcs.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\nfig.update_layout(\n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=16,\n        font_family=\"Rockwell\"\n    )\n)\nfig.update_layout(height=700, width=900, title_text=\"Upsmapled Correlations With PC\\'s\")\nfig.show()\n\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Model Selection And Evaluation</h1>\n","metadata":{}},{"cell_type":"code","source":"X_features = ['Total_Trans_Ct','PC-3','PC-1','PC-0','PC-2','Total_Ct_Chng_Q4_Q1','Total_Relationship_Count']\n\nX = usampled_df_with_pcs[X_features]\ny = usampled_df_with_pcs['Churn']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x,test_x,train_y,test_y = train_test_split(X,y,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.1\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Cross Validation</h3>\n","metadata":{}},{"cell_type":"code","source":"rf_pipe = Pipeline(steps =[ ('scale',StandardScaler()), (\"RF\",RandomForestClassifier(random_state=42)) ])\nada_pipe = Pipeline(steps =[ ('scale',StandardScaler()), (\"RF\",AdaBoostClassifier(random_state=42,learning_rate=0.7)) ])\nsvm_pipe = Pipeline(steps =[ ('scale',StandardScaler()), (\"RF\",SVC(random_state=42,kernel='rbf')) ])\n\n\nf1_cross_val_scores = cross_val_score(rf_pipe,train_x,train_y,cv=5,scoring='f1')\nada_f1_cross_val_scores=cross_val_score(ada_pipe,train_x,train_y,cv=5,scoring='f1')\nsvm_f1_cross_val_scores=cross_val_score(svm_pipe,train_x,train_y,cv=5,scoring='f1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=3, cols=1,shared_xaxes=True,subplot_titles=('Random Forest Cross Val Scores',\n                                                                     'Adaboost Cross Val Scores',\n                                                                    'SVM Cross Val Scores'))\n\nfig.add_trace(\n    go.Scatter(x=list(range(0,len(f1_cross_val_scores))),y=f1_cross_val_scores,name='Random Forest'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=list(range(0,len(ada_f1_cross_val_scores))),y=ada_f1_cross_val_scores,name='Adaboost'),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(x=list(range(0,len(svm_f1_cross_val_scores))),y=svm_f1_cross_val_scores,name='SVM'),\n    row=3, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Different Model 5 Fold Cross Validation\")\nfig.update_yaxes(title_text=\"F1 Score\")\nfig.update_xaxes(title_text=\"Fold #\")\n\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Model Evaluation</h3>\n","metadata":{}},{"cell_type":"code","source":"rf_pipe.fit(train_x,train_y)\nrf_prediction = rf_pipe.predict(test_x)\n\nada_pipe.fit(train_x,train_y)\nada_prediction = ada_pipe.predict(test_x)\n\nsvm_pipe.fit(train_x,train_y)\nsvm_prediction = svm_pipe.predict(test_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure(data=[go.Table(header=dict(values=['<b>Model<b>', '<b>F1 Score On Test Data<b>'],\n                                           line_color='darkslategray',\n    fill_color='whitesmoke',\n    align=['center','center'],\n    font=dict(color='black', size=18),\n    height=40),\n                               \n                 cells=dict(values=[['<b>Random Forest<b>', '<b>AdaBoost<b>','<b>SVM<b>'], [np.round(f1(rf_prediction,test_y),2), \n                                                                          np.round(f1(ada_prediction,test_y),2),\n                                                                          np.round(f1(svm_prediction,test_y),2)]]))\n                     ])\n\nfig.update_layout(title='Model Results On Test Data')\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.3\"></a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Model Evaluation On Original Data (Before Upsampling)</h3>\n","metadata":{}},{"cell_type":"code","source":"ohe_data =c_data[c_data.columns[16:]].copy()\npc_matrix = pca_model.fit_transform(ohe_data)\noriginal_df_with_pcs = pd.concat([c_data,pd.DataFrame(pc_matrix,columns=['PC-{}'.format(i) for i in range(0,N_COMPONENTS)])],axis=1)\n\nunsampled_data_prediction_RF = rf_pipe.predict(original_df_with_pcs[X_features])\nunsampled_data_prediction_ADA = ada_pipe.predict(original_df_with_pcs[X_features])\nunsampled_data_prediction_SVM = svm_pipe.predict(original_df_with_pcs[X_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure(data=[go.Table(header=dict(values=['<b>Model<b>', '<b>F1 Score On Original Data (Before Upsampling)<b>'],\n                                           line_color='darkslategray',\n    fill_color='whitesmoke',\n    align=['center','center'],\n    font=dict(color='black', size=18),\n    height=40),\n                               \n                 cells=dict(values=[['<b>Random Forest<b>', '<b>AdaBoost<b>','<b>SVM<b>'], [np.round(f1(unsampled_data_prediction_RF,original_df_with_pcs['Attrition_Flag']),2), \n                                                                          np.round(f1(unsampled_data_prediction_ADA,original_df_with_pcs['Attrition_Flag']),2),\n                                                                          np.round(f1(unsampled_data_prediction_SVM,original_df_with_pcs['Attrition_Flag']),2)]]))\n                     ])\n\nfig.update_layout(title='Model Result On Original Data (Without Upsampling)')\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Results</h1>\n","metadata":{}},{"cell_type":"code","source":"z=confusion_matrix(unsampled_data_prediction_RF,original_df_with_pcs['Attrition_Flag'])\nfig = ff.create_annotated_heatmap(z, x=['Not Churn','Churn'], y=['Predicted Not Churn','Predicted Churn'], colorscale='Fall',xgap=3,ygap=3)\nfig['data'][0]['showscale'] = True\nfig.update_layout(title='Prediction On Original Data With Random Forest Model Confusion Matrix')\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unsampled_data_prediction_RF = rf_pipe.predict_proba(original_df_with_pcs[X_features])\nskplt.metrics.plot_precision_recall(original_df_with_pcs['Attrition_Flag'], unsampled_data_prediction_RF)\nplt.legend(prop={'size': 20})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}