{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom scipy import stats\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.offline as pyo\npyo.init_notebook_mode()\nimport re\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.feature_selection import chi2,f_classif,SelectKBest\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nplt.rc('figure',figsize=(20,11))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://hivelife.com/wp-content/uploads/2019/09/Banner_TokyoTopRamen.jpg)\n\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Introduction</h1>\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-size: 22px; font-family: \"Times New Roman\", Times, serif;'>Ramen is without arguing a popular &apos;fast food&apos; not only in Japan but in many western countries as well but what differs between the many brands available what makes a certain type of ramen better than others, what features have the largest effect on the rating of the product, is it the company that produces the ramen? or maybe its the country of origin.&nbsp;</span></p>\n<p style=\"text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 22px;\">We can see that there are many questions to be investigated, and using the data we have in this notebook; we can try and answer some of those questions.</span></span></p>\n<p style=\"text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 22px;\">Before diving into the EDA and modeling, let&apos;s first clearly define our goals in this kernel, what are some questions we will try to answer using the data at hand, and what we will try to predict.&nbsp;</span></span></p>\n<p><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></p>\n<h1 style=\"text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><strong>&nbsp;Question We Will Investigate:</strong></span></h1>\n<p><span style='font-family: \"Times New Roman\", Times, serif; font-size: 22px;'>1) What country produces the highest amount of ramen products&nbsp;</span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">2) Whats the country with the highest rating products&nbsp;</span></span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">3) Does the product style affect the score directly? Does a certain kind of product style mean a lower score?</span></span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">4) Is there a significant difference between brand average scores?</span></span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p><span style='font-family: \"Times New Roman\", Times, serif; font-size: 22px;'>5) Do the scores follow a normal distribution, or are the scores skewed? What&apos;s the brand that skews the distribution?&nbsp;</span></p>\n<p><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></p>\n<h1 style=\"text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><strong>Feature Engineering Goals:</strong></span></h1>\n<p><span style='font-family: \"Times New Roman\", Times, serif; font-size: 22px;'>1) Extract the meat type from the variety feature</span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">2) Extract the noodle type from the variety feature</span></span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">3) Extract the number of words in the various feature</span></span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">4) Extract the presence of the word &nbsp;&apos;Flavor&apos; from the variety feature</span></span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">5) Extract the presence of the word &nbsp;&apos;Spicy&apos; to declare a product as spicy or not</span></span></p>\n<p><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></p>\n<p><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></p>\n<h1 style=\"text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><strong>&nbsp;Modeling And Prediction Goals:</strong></span></h1>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 22px;'>Our main goal as for prediction making will be to predict the ranking of a &apos;new&apos; ramen product based on the feature we may have before the rating like the variety feature, the brand, the country of origin, the new features will create, etc. so there is no risk of data leakage.</span></p>\n<p style=\"text-align: center;\"><span style=\"font-size: 22px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></span></p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 22px;'>I think we are ready now to dive into our noodle bowl of information!</span></p>\n<p><span style=\"font-family: 'Times New Roman', Times, serif;\"><br></span></p>\n<p><br></p>","metadata":{}},{"cell_type":"code","source":"r_data = pd.read_csv('/kaggle/input/ramen-ratings/ramen-ratings.csv')\nr_data.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Preparation And Feature Engineering</h1>\n","metadata":{}},{"cell_type":"code","source":"n_data = r_data.copy()\nfig = make_subplots(\n    rows=2, cols=2,\n    shared_xaxes=True,\n    vertical_spacing=0.03,\n    specs=[[{\"type\": \"heatmap\",'rowspan':2},{\"type\": \"table\",'rowspan':2}],\n           [None             ,None],\n          ]\n)\n\nfig.add_trace(\n    go.Heatmap(\n        z=n_data.isna().T.astype(int),\n        x=np.arange(0,len(n_data)),\n        y=n_data.columns,\n        showscale =False\n    ),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Table(\n    header=dict(values=list(['Feature Name','Number Of Missing']),\n                fill_color='royalblue',\n                font_color='white',\n                font_size=13,\n                align='left'),\n    cells=dict(values=[n_data.columns,n_data.isna().sum().to_frame()[0]],\n               fill_color='azure',\n               font_size=14,\n               align='left')),\n    row=1, col=2\n)\n\n\n\n\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"Proportion Of Missing Values\",\n)\nfig.update_yaxes(title_text=\"Sentiment Strength\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r_data.Style.fillna(r_data.Style.mode()[0],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"text-align: center;\">Only 2 values are missing from the &apos;Style&apos; feature. To place those values, we will use the mode, as for the &apos;Top Ten&apos; feature, it has more than 95% of values missing, I will not rely on this feature during my analysis and prediction.</h2>","metadata":{}},{"cell_type":"code","source":"Style_dict = {r_data.Style.unique()[i]: i+1 for i in range(0,len(r_data.Style.unique()))}\nBrand_dict = {r_data.Brand.unique()[i]: i+1 for i in range(0,len(r_data.Brand.unique()))}\nCountry_dict = {r_data.Country.unique()[i]: i+1 for i in range(0,len(r_data.Country.unique()))}\n\norg_data = r_data.copy()\nr_data.Style.replace(Style_dict,inplace=True)\nr_data.Brand.replace(Brand_dict,inplace=True)\nr_data.Country.replace(Country_dict,inplace=True)\nr_data = r_data[r_data['Stars'] != 'Unrated']\nr_data.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Variety Feature Extraction\n\n#number of words extraction\ndef number_of_words(sir):\n    return len(sir.split(' '))\nr_data['Number_Of_Words'] = r_data.Variety.apply(number_of_words)\n\nmeat_types = ['chicken','beef','duck','pork','shrimp','turkey']\nnoodle_types = ['udon','soba','ramen','egg','shirataki','hokkien','noodles']\n\ndef contain_flavor(sir):\n    stn = sir.lower()\n    return 1 if 'flavor' in stn else 0\n\ndef meat_scanner(sir):\n    stn = sir.lower()\n    for meat in meat_types:\n        if meat in stn:\n            return meat\n    return 'unspecified'\n\ndef noodle_scanner(sir):\n    stn = sir.lower()\n    for noodle in noodle_types:\n        if noodle in stn:\n            return noodle\n    return 'unspecified'\n    \n    \ndef is_spicy(sir):\n    stn = sir.lower()\n    spc = ['spicy','hot','flaming','chili']\n    for t in spc:\n        if t in stn:\n            return 1\n    return 0\n\nr_data['Contains_Flavor'] = r_data.Variety.apply(contain_flavor)\nr_data['Meat_Type'] = r_data.Variety.apply(meat_scanner)\nr_data['Noodle_Type'] = r_data.Variety.apply(noodle_scanner)\nr_data['Is_Spicy'] = r_data.Variety.apply(is_spicy)\n\nmeat_type_dict = {r_data['Meat_Type'].unique()[i]: i+1 for i in range(0,len(r_data['Meat_Type'].unique()))}\nnoodle_type_dict = {r_data['Noodle_Type'].unique()[i]: i+1 for i in range(0,len(r_data['Noodle_Type'].unique()))}\n\nr_data['Meat_Type'].replace(meat_type_dict,inplace=True)\nr_data['Noodle_Type'].replace(noodle_type_dict,inplace=True)\n\n\n\nw_data = r_data[['Brand','Style','Country','Number_Of_Words','Contains_Flavor','Meat_Type',\n                'Noodle_Type','Is_Spicy','Stars']].copy()\n\nw_data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"text-align: center;\">We created all the features we aimed to create as well as encoding all the categorical features (later we will divide some of our features into &apos;dummy&apos; features depending on what we find out in our EDA) </h2>\n<p><br></p>","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis</h1>\n","metadata":{}},{"cell_type":"code","source":"Brand_dict = {key : val for val,key in Brand_dict.items()}\nCountry_dict = {key : val for val,key in Country_dict.items()}\nStyle_dict = {key : val for val,key in Style_dict.items()}\nnoodle_type_dict= {key : val for val,key in noodle_type_dict.items()}\nmeat_type_dict =  {key : val for val,key in meat_type_dict.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_25_brands =w_data['Brand'].value_counts().sort_values(ascending=False)[:25]\nax = sns.countplot(w_data[w_data['Brand'].isin(top_25_brands.index)]['Brand'],order = top_25_brands.index,\n                  palette='mako')\n\n\nax.set_xticklabels([Brand_dict[int(ax.get_xticklabels()[i].get_text())] for i in range(0,25)],rotation=90)\n\nax.patches[0].set_fc('r')\nax.patches[1].set_fc((0.7,0.0,0.0))\nax.patches[2].set_fc((0.5,0.0,0.0))\nax.set_title('Distribution of the number of prodcuts made by different brands [Top 25]',fontsize=18)\nplt.show()\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"text-align: center;\">We see that that in our data there is a company which makes the most of the product,&apos; Nissin&apos; dominates our sample population of products having more than 350 different products in the dataset we see that if we look at the next 2 leading companies, they are fairly close to each other in the number of products they sale. As we go down our list, we see that the number of products decreases in a fairly balanced slope, meaning except &apos;Nissin,&apos; which dominates all the other companies, does not have as sharp a difference compared to &apos;Nissan.&apos;</h2>\n<p><br></p>","metadata":{}},{"cell_type":"code","source":"w_data.Stars = w_data.Stars.astype('float64')\npivot = w_data.groupby(by='Country').sum()\npivot = pivot.sort_values(by='Stars',ascending=False)\ntop_25_country_by_score = pivot[:25]\n\nax = sns.barplot(pivot.index,pivot.Stars,order = top_25_country_by_score.index,palette='mako')\nax.set_xticklabels([Country_dict[int(ax.get_xticklabels()[i].get_text())] for i in range(0,25)],rotation=90)\nax.patches[0].set_fc('r')\nax.patches[1].set_fc((0.7,0.0,0.0))\nax.patches[2].set_fc((0.5,0.0,0.0))\nax.set_title('Distribution of prodcuts made by different countries [Top 25]',fontsize=18)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"> &apos;Japan,&apos; &apos;South Korea&apos; and &apos;USA&apos; are the countries with the highest amount of stars meaning they produce the most products and most of their products are fairly-highly rated.</span></h2>\n<p><br></p>","metadata":{}},{"cell_type":"code","source":"for t in Style_dict.keys():\n    ax = sns.distplot(w_data[w_data['Style']==t]['Stars'],hist=False,label = Style_dict[t],kde_kws={'lw':4} )\nplt.legend(prop={'size':20})\nax.set_title('Distributions of different ramen style rankings ',fontsize=18)\n\nplt.show()","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"> We can see that most ramen style rankings are bimodal distributed between 3/4 and 5 except &apos;Box&apos; style, which is bimodally distributed between 1 and 5, meaning most of the &apos;Boxed&apos; ramen is highly rated with 4.2-4.4 stars on average. We also see that our data is negatively skewed, meaning most of the ramen products are rated with 3+ stars</span></h2>\n<p><br></p>","metadata":{}},{"cell_type":"code","source":"pivot = w_data.groupby(by='Style').mean()\npivot = pivot.sort_values(by='Stars',ascending=False)\n\n\nax = sns.barplot(x=pivot.index,y=pivot['Stars'],order=pivot.index,palette='mako')\nax.set_xticklabels([Style_dict[int(ax.get_xticklabels()[i].get_text())] for i in range(0,7)],rotation=90)\nax.patches[0].set_fc('r')\nax.patches[1].set_fc((0.7,0.0,0.0))\nax.patches[2].set_fc((0.5,0.0,0.0))\nax.set_title('Distribution of average rankings across different styles',fontsize=18)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.boxplot(x='Style',y='Stars',hue='Meat_Type',data=w_data)\nl = plt.legend()\nfor i in range(0,6):\n    l.get_texts()[i].set_text(list(meat_type_dict.values())[i])\n\n    \nax.set_title('Distribution of rankings of different meat types across different styles',fontsize=18)    \nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### At most styles pork and beef are usually rated higher compared to chicken and shrimp","metadata":{}},{"cell_type":"code","source":"labels = {x+1:list(noodle_type_dict.values())[x] for x in range(0,len(noodle_type_dict.values()))}\ntmp = w_data.copy()\ntmp.Noodle_Type = tmp.Noodle_Type.replace(labels)\nex.box(tmp,x='Style',y='Stars',color='Noodle_Type',title='Distribution of rankings of different noodle types across different styles',height=800,\n      labels=labels)\n\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Udon And Ramen Usually Get Higher Rankings ","metadata":{}},{"cell_type":"code","source":"ax = sns.distplot(w_data[w_data['Is_Spicy']==1]['Stars'],hist=False,label = \"Spicy\",kde_kws={'lw':4} )\nax = sns.distplot(w_data[w_data['Is_Spicy']==0]['Stars'],hist=False,label = \"Not Spicy\",kde_kws={'lw':4} )\n\n\nplt.legend(prop={'size':20})\n\nax.set_title('Distribution of stars in spicy and not spicy products',fontsize=18)    \nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.distplot(w_data[w_data['Contains_Flavor']==1]['Stars'],hist=False,label = \"Flavor In Var\",kde_kws={'lw':4} )\nax = sns.distplot(w_data[w_data['Contains_Flavor']==0]['Stars'],hist=False,label = \"Flavor Not In Var\",kde_kws={'lw':4} )\n\nplt.legend(prop={'size':20})\n\n\nax.set_title('Distribution of stars in labels containg the word Flavor and not',fontsize=18)    \nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.jointplot(w_data['Stars'],w_data['Number_Of_Words'],kind='kde',cmap='mako',height=14)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meats = pd.get_dummies(w_data['Meat_Type'])\nmeats = meats.rename(meat_type_dict,axis=1)\nw_data = w_data.join(meats[1:])\nw_data.drop('Meat_Type',axis=1,inplace=True)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"style_ohe = pd.get_dummies(org_data.Style).drop(columns=['Tray'])\nw_data = pd.concat([w_data,style_ohe],axis=1)\n\ncorrs = w_data.corr('pearson')\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=[[0.0, \"rgb(165,0,38)\"],\n                [0.1111111111111111, \"rgb(215,48,39)\"],\n                [0.2222222222222222, \"rgb(244,109,67)\"],\n                [0.3333333333333333, \"rgb(253,174,97)\"],\n                [0.4444444444444444, \"rgb(254,224,144)\"],\n                [0.5555555555555556, \"rgb(224,243,248)\"],\n                [0.6666666666666666, \"rgb(171,217,233)\"],\n                [0.7777777777777778, \"rgb(116,173,209)\"],\n                [0.8888888888888888, \"rgb(69,117,180)\"],\n                [1.0, \"rgb(49,54,149)\"]]\n\nsd_data = w_data.copy()\n\ns_val =sd_data.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =sd_data.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Correaltions Between Our Numeric Features\")\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Model Selection And Evaluation</h1>\n","metadata":{}},{"cell_type":"code","source":"selector = SelectKBest(f_classif,k=5)\nw_data = w_data.fillna(0)\ncols = [w_data.columns[i] for i in range(0,len(w_data.columns)) if w_data.columns[i] != 'Stars']\nX = selector.fit_transform(w_data[cols],w_data['Stars'])\n\nselected_columns = [cols[i] for i in range(0,len(cols)) if selector.get_support()[i] == True]\nselected_columns\n\nx_train,x_test,y_train,y_test = train_test_split(X,w_data['Stars'])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Linear Regression\nLR_pipe = Pipeline(steps=[('scale',StandardScaler()),('model',LinearRegression())])\nLR_cv_scores = -1*cross_val_score(LR_pipe,x_train,y_train,cv=5,scoring='neg_mean_squared_error')\n\nLR_pipe.fit(x_train,y_train)\nLR_predictions = LR_pipe.predict(x_test)\nLR_score = np.sqrt(mean_squared_error(LR_predictions,y_test))\n\n#RandomForest\nRF_pipe = Pipeline(steps=[('scale',StandardScaler()),('model',RandomForestRegressor(random_state=42,\n                                                                                    n_estimators=60,max_leaf_nodes=22))])\nRF_cv_scores = -1*cross_val_score(RF_pipe,x_train,y_train,cv=5,scoring='neg_mean_squared_error')\n\nRF_pipe.fit(x_train,y_train)\nRF_predictions = RF_pipe.predict(x_test)\nRF_score = np.sqrt(mean_squared_error(RF_predictions,y_test))\n\n\n#AdaBoost\nADA_pipe = Pipeline(steps=[('scale',StandardScaler()),('model',AdaBoostRegressor(random_state=42,\n                                                                                learning_rate=0.3,n_estimators=30))])\nADA_pipe.fit(x_train,y_train)\nADA_predictions = ADA_pipe.predict(x_test)\nada_cv_scores = -1*cross_val_score(ADA_pipe,x_train,y_train,cv=5,scoring='neg_mean_squared_error')\nada_score = np.sqrt(mean_squared_error(ADA_predictions,y_test))\n\n\n#Knn Pipeline\nKNN_pipe = Pipeline(steps=[('model',KNeighborsRegressor(n_neighbors=24))])\nknn_cv_scores = -1*cross_val_score(KNN_pipe,x_train,y_train,cv=5,scoring='neg_mean_squared_error')\nKNN_pipe.fit(X,w_data['Stars'])\n\n#XGB \nxgb_model = XGBRegressor(n_estimators=300,learning_rate=0.03,random_state=42)\nxgb_model.fit(x_train,y_train,early_stopping_rounds=7,eval_set=[(x_test[:5],y_test[:5])],verbose=False)\npredictions = xgb_model.predict(x_test)\nxgb_score = np.sqrt(mean_squared_error(predictions,y_test))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=4, cols=1,shared_xaxes=True,subplot_titles=('Random Forest Cross Val Scores',\n                                                                     'Linear Regression Cross Val Scores',\n                                                                    'AdaBoost Cross Val Scores',\n                                                                    'KNN Cross Val Scores'))\n\nfig.add_trace(\n    go.Scatter(x=np.arange(0,5),y=RF_cv_scores,mode='lines+markers',name='Random Forest'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=np.arange(0,5),y=LR_cv_scores,mode='lines+markers',name='Linear Regression'),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(x=np.arange(0,5),y=ada_cv_scores,mode='lines+markers',name='AdaBoost'),\n    row=3, col=1\n)\nfig.add_trace(\n    go.Scatter(x=np.arange(0,5),y=knn_cv_scores,mode='lines+markers',name='KNN'),\n    row=4, col=1\n)\n\n\n#Fold Means\nfig.add_shape(type=\"line\",\n    x0=0, y0=np.mean(RF_cv_scores), x1=5, y1=np.mean(RF_cv_scores),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x1', \n        yref='y1'\n)\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=np.mean(LR_cv_scores), x1=5, y1=np.mean(LR_cv_scores),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=np.mean(ada_cv_scores), x1=5, y1=np.mean(ada_cv_scores),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x3', \n        yref='y3'\n)\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=np.mean(knn_cv_scores), x1=5, y1=np.mean(knn_cv_scores),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x4', \n        yref='y4'\n)\n\n\nfig.update_layout(height=700, width=900, title_text=\"Different Model 5 Fold Cross Validation\")\nfig.update_yaxes(title_text=\"RMSE\")\nfig.update_xaxes(title_text=\"Fold #\")\n\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text Feature Extratction\n\nNUMBER_OF_COMPONENTS=190\n\ndef preprocess_text(txt):\n    ppt = txt.lower()\n    ppt = ' '.join([word for word in re.findall(r'\\w+',ppt) if len(word) >1 and word not in list(STOPWORDS)])\n    return ppt\n    \nVariety = org_data['Variety'].apply(preprocess_text)\n\nvectorizer = CountVectorizer()\nsp_va_matrix = vectorizer.fit_transform(Variety)\nt_SVD = TruncatedSVD(n_components = NUMBER_OF_COMPONENTS)\ncp_matrix = t_SVD.fit_transform(sp_va_matrix)\n\ncu_sum = np.cumsum(t_SVD.explained_variance_ratio_)\n\ntr1 = go.Scatter(x=np.arange(0,len(cu_sum)),y=cu_sum,name='Explained Variance')\nlayout = dict(title=f'Explained Variance Using {NUMBER_OF_COMPONENTS} Components',xaxis_title='Number Of Components',yaxis_title='Explained Variance')\ngo.Figure(data=[tr1],layout=layout)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CV_Model = LinearRegression()\nCV_Model.fit(cp_matrix,w_data['Stars'])\nText_Based_Prediction = CV_Model.predict(cp_matrix)\n\nX = np.append(X,Text_Based_Prediction.reshape((2580,1)),axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model.fit(X,w_data['Stars'])\nADA_pipe.fit(X,w_data['Stars'])\nLR_pipe.fit(X,w_data['Stars'])\nRF_pipe.fit(X,w_data['Stars'])\nKNN_pipe.fit(X,w_data['Stars'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glb = xgb_model.predict(X)\nada = ADA_pipe.predict(X)\nrf = RF_pipe.predict(X)\nlr = LR_pipe.predict(X)\nknn = KNN_pipe.predict(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax=sns.lineplot(y=w_data['Stars'],x=np.arange(0,w_data.shape[0]),label='Actual')\nax=sns.lineplot(y=glb,x=np.arange(0,w_data.shape[0]),label='XGB Prediction')\nax=sns.lineplot(y=ada,x=np.arange(0,w_data.shape[0]),label='ADABoost Prediction')\nax=sns.lineplot(y=rf,x=np.arange(0,w_data.shape[0]),label='RandomForest Prediction')\nax=sns.lineplot(y=lr,x=np.arange(0,w_data.shape[0]),label='LinearRegression Prediction')\nax=sns.lineplot(y=lr,x=np.arange(0,w_data.shape[0]),label='KNN Prediction')\n\n\n\nax.set_title('Predicted values against real values',fontsize=20)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">Our Model Blending Goes As Follows : $$Blended  =(XGB*0.6 + RandoForest*0.21 + KNN*0.2)$$</span></h2>\n<p><br></p>","metadata":{}},{"cell_type":"code","source":"fp = (glb*0.60+rf*0.21 + knn*0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('RMSE Of Stacked Predictions: ',np.sqrt(mean_squared_error(w_data['Stars'],fp)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax=sns.lineplot(y=w_data['Stars'],x=np.arange(0,w_data.shape[0]),label='Actual')\nax=sns.lineplot(y=fp,x=np.arange(0,w_data.shape[0]),label='Stacked Prediction')\n\nax.set_title('Final Prediction values against real values',fontsize=20)\nplt.legend()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output=pd.DataFrame({'Prediction':fp,'Actual':w_data['Stars']})\nfig = make_subplots(\n    rows=3, cols=2,subplot_titles=('','Actual','Predictions','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"table\",\"rowspan\": 3}     ,{\"type\": \"scatter\"}] ,\n           [None                               ,{\"type\": \"scatter\"}]            ,           \n           [None                               ,{\"type\": \"scatter\"}]                           \n          ]\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Actual\"])),\n        y=output[\"Actual\"],\n        mode=\"markers\",\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"],\n        mode=\"markers\",\n    ),\n    row=2, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"]-output[\"Actual\"],\n        mode=\"markers\",\n    ),\n    row=3, col=2\n)\n\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['Prediction','Actual'],\n            font=dict(size=10),\n            align=\"left\"\n        ),\n        cells=dict(\n            values=[output[k].tolist() for k in output.columns],\n            align = \"left\")\n    ),\n    row=1, col=1\n)\n\n\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=(output[\"Prediction\"]-output[\"Actual\"]).mean(), x1=len(output[\"Prediction\"]), y1=(output[\"Prediction\"]-output[\"Actual\"]).mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x3', \n        yref='y3'\n)\n\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"Prediction Evaluation\",\n)\n\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Stacked Model Residual Plot',fontsize=16)\nsns.residplot(w_data['Stars'],fp)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}