{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords   # to get collection of stopwords\nfrom sklearn.model_selection import train_test_split       # for splitting dataset\nfrom tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\nfrom tensorflow.keras.models import Sequential     # the model\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\nfrom tensorflow.keras.callbacks import ModelCheckpoint   # save model\nfrom tensorflow.keras.models import load_model   # load saved model\nimport re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Stop Word is a commonly used words in a sentence, usually a search engine is programmed to ignore this words (i.e. \"the\", \"a\", \"an\", \"of\", etc.)\n#Declaring the english stop words\nenglish_stops = set(stopwords.words('english'))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nLoad and Clean Dataset\nIn the original dataset, the reviews are still dirty. There are still html tags, numbers, uppercase, and punctuations. This will not be good for training, so in load_dataset() function, beside loading the dataset using pandas, I also pre-process the reviews by removing html tags, non alphabet (punctuations and numbers), stop words, and lower case all of the reviews.\n\nEncode Sentiments\nIn the same function, I also encode the sentiments into integers (0 and 1). Where 0 is for negative sentiments and 1 is for positive sentiments.","metadata":{}},{"cell_type":"code","source":"def load_dataset():\n    df = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n    x_data = df['review']       # Reviews/Input\n    y_data = df['sentiment']    # Sentiment/Output\n\n    # PRE-PROCESS REVIEW\n    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n    \n    # ENCODE SENTIMENT -> 0 & 1\n    y_data = y_data.replace('positive', 1)\n    y_data = y_data.replace('negative', 0)\n\n    return x_data, y_data\n\nx_data, y_data = load_dataset()\n\nprint('Reviews')\nprint(x_data, '\\n')\nprint('Sentiment')\nprint(y_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split Dataset\nIn this work, I decided to split the data into 80% of Training and 20% of Testing set using train_test_split method from Scikit-Learn. By using this method, it automatically shuffles the dataset. We need to shuffle the data because in the original dataset, the reviews and sentiments are in order, where they list positive reviews first and then negative reviews. By shuffling the data, it will be distributed equally in the model, so it will be more accurate for predictions.","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n\nprint('Train Set')\nprint(x_train, '\\n')\nprint(x_test, '\\n')\nprint('Test Set')\nprint(y_train, '\\n')\nprint(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function for getting the maximum review length, by calculating the mean of all the reviews length (using numpy.mean)","metadata":{}},{"cell_type":"code","source":"def get_max_length():\n    review_length = []\n    for review in x_train:\n        review_length.append(len(review))\n\n    return int(np.ceil(np.mean(review_length)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tokenize and Pad/Truncate Reviews\nA Neural Network only accepts numeric data, so we need to encode the reviews. I use tensorflow.keras.preprocessing.text.Tokenizer to encode the reviews into integers, where each unique word is automatically indexed (using fit_on_texts method) based on x_train.\nx_train and x_test is converted into integers using texts_to_sequences method.\n\nEach reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length) using tensorflow.keras.preprocessing.sequence.pad_sequences.\n\npost, pad or truncate the words in the back of a sentence\npre, pad or truncate the words in front of a sentence","metadata":{}},{"cell_type":"code","source":"# ENCODE REVIEW\ntoken = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\ntoken.fit_on_texts(x_train)\nx_train = token.texts_to_sequences(x_train)\nx_test = token.texts_to_sequences(x_test)\n\nmax_length = get_max_length()\n\nx_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\nx_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n\ntotal_words = len(token.word_index) + 1   # add 1 because of 0 padding\n\nprint('Encoded X Train\\n', x_train, '\\n')\nprint('Encoded X Test\\n', x_test, '\\n')\nprint('Maximum review length: ', max_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Build Architecture/Model\nEmbedding Layer: in simple terms, it creates word vectors of each word in the word_index and group words that are related or have similar meaning by analyzing other words around them.\n\nLSTM Layer: to make a decision to keep or throw away data by considering the current input, previous output, and previous memory. There are some important components in LSTM.\n\nForget Gate, decides information is to be kept or thrown away\nInput Gate, updates cell state by passing previous output and current input into sigmoid activation function\nCell State, calculate new cell state, it is multiplied by forget vector (drop value if multiplied by a near 0), add it with the output from input gate to update the cell state value.\nOuput Gate, decides the next hidden state and used for predictions\nDense Layer: compute the input with the weight matrix and bias (optional), and using an activation function. I use Sigmoid activation function for this work because the output is only 0 or 1.\n\nThe optimizer is Adam and the loss function is Binary Crossentropy because again the output is only 0 and 1, which is a binary number.","metadata":{}},{"cell_type":"code","source":"# ARCHITECTURE\nEMBED_DIM = 32\nLSTM_OUT = 64\n\nmodel = Sequential()\nmodel.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\nmodel.add(LSTM(LSTM_OUT))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training\nFor training, it is simple. We only need to fit our x_train (input) and y_train (output/label) data. For this training, I use a mini-batch learning method with a batch_size of 128 and 5 epochs.\n\nAlso, I added a callback called checkpoint to save the model locally for every epoch if its accuracy improved from the previous epoch.","metadata":{}},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(\n    'models/LSTM.h5',\n    monitor='accuracy',\n    save_best_only=True,\n    verbose=1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train, y_train, batch_size = 128, epochs = 5, callbacks=[checkpoint])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing\nTo evaluate the model, we need to predict the sentiment using our x_test data and comparing the predictions with y_test (expected output) data. Then, we calculate the accuracy of the model by dividing numbers of correct prediction with the total data. Resulted an accuracy of 86.63%","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict_classes(x_test, batch_size = 128)\n\ntrue = 0\nfor i, y in enumerate(y_test):\n    if y == y_pred[i]:\n        true += 1\n\nprint('Correct Prediction: {}'.format(true))\nprint('Wrong Prediction: {}'.format(len(y_pred) - true))\nprint('Accuracy: {}'.format(true/len(y_pred)*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Saved Model\nLoad saved model and use it to predict a movie review statement's sentiment (positive or negative).","metadata":{}},{"cell_type":"code","source":"loaded_model = load_model('models/LSTM.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Receives a review as an input to be predicted","metadata":{}},{"cell_type":"code","source":"review = str(input('Movie Review: '))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Movie Review: Nothing was typical about this. Everything was beautifully done in this movie, the story, the flow, the scenario, everything. I highly recommend it for mystery lovers, for anyone who wants to watch a good movie!\nThe input must be pre processed before it is passed to the model to be predicted","metadata":{}},{"cell_type":"code","source":"# Pre-process input\nregex = re.compile(r'[^a-zA-Z\\s]')\nreview = regex.sub('', review)\nprint('Cleaned: ', review)\n\nwords = review.split(' ')\nfiltered = [w for w in words if w not in english_stops]\nfiltered = ' '.join(filtered)\nfiltered = [filtered.lower()]\n\nprint('Filtered: ', filtered)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cleaned:  Nothing was typical about this Everything was beautifully done in this movie the story the flow the scenario everything I highly recommend it for mystery lovers for anyone who wants to watch a good movie\nFiltered:  ['nothing typical everything beautifully done movie story flow scenario everything i highly recommend mystery lovers anyone wants watch good movie']\nOnce again, we need to tokenize and encode the words. I use the tokenizer which was previously declared because we want to encode the words based on words that are known by the model.","metadata":{}},{"cell_type":"code","source":"tokenize_words = token.texts_to_sequences(filtered)\ntokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\nprint(tokenize_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the result of the prediction which shows the confidence score of the review statement.","metadata":{}},{"cell_type":"code","source":"result = loaded_model.predict(tokenize_words)\nprint(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  If the confidence score is close to 0, then the statement is negative. On the other hand, if the confidence score is close to 1, then the statement is positive. I use a threshold of 0.7 to determine which confidence score is positive and negative, so if it is equal or greater than 0.7, it is positive and if it is less than 0.7, it is negative","metadata":{}},{"cell_type":"code","source":"if result >= 0.7:\n    print('positive')\nelse:\n    print('negative')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}