{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ** Mushroom Classification Test Classifiers Evaluation\nI created this classifiers evaluation mainly as a submission requirement for the class Data Mining MIT504 and also my personal training on learning the concepts of data mining for Future practices.\n\n**Overview**\n\nThis dataset consists of 8124 records of the physical characteristics of gilled mushrooms in the Agaricus and Lepiota families, along with their edibility. The classification task is to determine the edibility given the physical characteristics of the mushrooms.\n\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like \"leaflets three, let it be'' for Poisonous Oak and Ivy.\n\nThanks to sir @Raven Duran for sharing his thoughs, it helps me to grasp the basic concept of data science workflow on supervised learning.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-datasets-images/478/974/557711140aeab7ca242d1e903c4e058e/dataset-cover.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**I. Initialization:**\nLets try to load our dataset csv file","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To Begin with** =\nLets add some Libraries: \n*import numpy as np* - for python,\n*import pandas as pd* - parsing our dataset,\n*import seabor as sns* - graphical visualization","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# This is so we can check the files we have in the current system. Most Kaggle Notebook files are found under kaggle/input\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 1: ** Lets try to display the list of samples on our dataset using Data Frame Shape","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load csv file that we found\n# Take note that this data set is just 365kb, so we are confident in putting everything in memory. \n# Fortunately Kaggle gives a provision of 16gb of running memory for Free! \n# So if your data set gets to more than that, I suggest you either breakdown the data or you use a database instead\n# as it will be more manageable for larger sets of data\ndf = pd.read_csv('/kaggle/input/mushroom-classification/mushrooms.csv') # df usually is used to abbreviate \"Data Frame\" from pandas library\n\n\n# Remember we are using Python 3 in this Notebook, so the last statement will be evaluated as string for output if any.\n# If you want to explicitly print, you can use the \"print()\" function\n\n# Shape shows you the # of samples and the number of features / cols you have in your data set.\nprint(f'Data Frame Shape (rows, columns): {df.shape}') \n\n# This method is one of your basic tool to quickly view data. It shows first 5 rows of a frame\ndf.head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I. Quick Data Analysis and Exploration","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I try to balance myour dataset for me to see how many results on the dataset per class(edible or poisonous)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df, x=\"class\").set_title(\"Class Outcome - Edible-e/Poisonous-p\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see in the above chart ^, the dataset is not balance. This is very important as this is where we can see if our dataset needs balancing to clean up or correct biases or possibility of overfitting. But on my dataset case, I don't really need to balance since the dataset has a large set of values/data with minimal differences. So will skip on balancing the dataset!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(data=df, x=\"bruises\", y=\"odor\", hue=\"class\", palette=\"bright\", height=6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In corelation to their physical characteristics:\n\nIn here we just plotted the Mushrooms by their physical characteristics (odor and bruises). This is just to visualize the dataset we have in corelation to their characteristics. In here we can observe that the Poisonous(p) class has a higher counts rather that the edible(e)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(data=df, x=\"population\", y=\"habitat\", hue=\"class\", palette=\"bright\", height=6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In corelation to their Physical Characteristics:\n\nIn here we plotted the mushrooms by their environment. You can see on the plot above that it doesn't differ on that counts of which the mushroom grows to a specific location or nature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# II. Data Preparation, Balancing and Cleanup","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if there are any null values\ndf.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove null values\ndf = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if there are any null values\ndf.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# III. Classifier Setups and Build Model\n\n# **A. Import Necessary Libraries for Scoring and Evaluation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import required libraries for performance metrics\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now this is the custom function (c.sir Raven) on doing matrix caculation to the Confusion Matrix to get the TP, FP, TN and FN respectively. This function returns an ordered list of the performance measures.\n\nTP is True Positives FP is False Positives TN is True Negative FN is False Negative","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_performance_measures(actual, prediction):\n    matrix = confusion_matrix(actual, prediction)\n    FP = matrix.sum(axis=0) - np.diag(matrix)  \n    FN = matrix.sum(axis=1) - np.diag(matrix)\n    TP = np.diag(matrix)\n    TN = matrix.sum() - (FP + FN + TP)\n\n    return(TP, FP, TN, FN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below were custom scorers. A scorer is basically a benchmark of how well your model performs given an actual result and a predicted result.\n\nThere are actually already a comprehensive list of scorers built in Python but for the purpose of learning, we will be creating 2 custom scorers in this notebook: sensitivity and specificity. The rest we just rely on the library to do its work to save up space in this notebook.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Custom Scorers\n\n# # Sensitivity, hit rate, recall, or true positive rate\n# TPR = TP/(TP+FN)\n# # Specificity or true negative rate\n# TNR = TN/(TN+FP) \n# # Precision or positive predictive value\n# PPV = TP/(TP+FP)\n# # Negative predictive value\n# NPV = TN/(TN+FN)\n# # Fall out or false positive rate\n# FPR = FP/(FP+TN)\n# # False negative rate\n# FNR = FN/(TP+FN)\n# # False discovery rate\n# FDR = FP/(TP+FP)\n\n# # Overall accuracy\n# ACC = (TP+TN)/(TP+FP+FN+TN)\n\n# Also remember:\n# specificity = true negative rate\n# sensitivity = true positive rate\n\ndef sensitivity_score(y_true, y_pred, mode=\"multiclass\"):\n    if mode == \"multiclass\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TPR = (TP/(TP+FN)).mean()\n    elif mode == \"binary\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TPR = (TP/(TP+FN))[1] # Since the [0] part is the index\n    else:\n        raise Exception(\"Mode not recognized!\")\n    \n    return TPR\n\ndef specificity_score(y_true, y_pred, mode=\"multiclass\"):\n    if mode == \"multiclass\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TNR = (TN/(TN+FP)).mean()\n    elif mode == \"binary\":\n        TP, FP, TN, FN = get_performance_measures(y_true, y_pred)\n        TNR = (TN/(TN+FP))[1]\n    else:\n        raise Exception(\"Mode not recognized!\")\n    \n    return TNR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **B. Setup Scorers**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# To know what everaging to use: https://stats.stackexchange.com/questions/156923/should-i-make-decisions-based-on-micro-averaged-or-macro-averaged-evaluation-mea#:~:text=So%2C%20micro%2Daveraged%20measures%20add,is%20more%20like%20an%20average.\n\n\nscoring = {\n            'accuracy':make_scorer(accuracy_score), \n            'precision':make_scorer(precision_score, average='weighted'),\n            'f1_score':make_scorer(f1_score, average='weighted'),\n            'recall':make_scorer(recall_score, average='weighted'), \n            'sensitvity':make_scorer(sensitivity_score, mode=\"binary\"), \n            'specificity':make_scorer(specificity_score, mode=\"binary\"), \n           }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **C. Setting up our Classifiers**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import required libraries for machine learning classifiers\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.naive_bayes import GaussianNB #Naive Bayes\nfrom sklearn.linear_model import LogisticRegression #Logistic Regression\nfrom sklearn.svm import LinearSVC # Support Vector Machine\nfrom sklearn.neighbors import KNeighborsClassifier #K-nearest Neighbors\nfrom sklearn.cluster import KMeans #K-means\n\n# Instantiate the machine learning classifiers\ndecisionTreeClassifier_model = DecisionTreeClassifier()\ngaussianNB_model = GaussianNB()\nlogisticRegression_model = LogisticRegression(max_iter=10000)\nlinearSVC_model = LinearSVC(dual=False)\nkNeighbors_model = KNeighborsClassifier()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make a custom evaluation function to easily aggregate our score results. You can actually do this manually, but by making a function; we can easily reuse it easily for future use.\n\nThe beauty of this function is that it is very versatile and can be use not only for this data set but also to other data sets as well as long as you format them properly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# features = data frame set that contain your features that will be used as input to see if prediction is equal to actual result\n# target = data frame set (1 column usually) that will contain your target or actual results.\n# folds = this is added so we can easily change the number of folds we want to do with our data set.\n# folding is a technique to minimise overfitting and therefore make our model more accurate.\ndef models_evaluation(features, target, folds):    \n    # Perform cross-validation to each machine learning classifier\ndecisionTreeClassifier_result = cross_validate(decisionTreeClassifier_model, features, target, cv=folds, scoring=scoring)\ngaussianNB_result = cross_validate(gaussianNB_model, features, target, cv=folds, scoring=scoring)\nlogisticRegression_result = cross_validate(logisticRegression_model, features, target, cv=folds, scoring=scoring)\nlinearSVC_result = cross_validate(linearSVC_model, features, target, cv=folds, scoring=scoring)\nkNeighbors_result = cross_validate(kNeighbors_model, features, target, cv=folds, scoring=scoring)\n    # kMeans_result = cross_validate(kMeans_model, features, target, cv=folds, scoring=scoring)\n\n    # Create a data frame with the models perfoamnce metrics scores\n    models_scores_table = pd.DataFrame({\n      'Decision Tree':[\n                        decisionTreeClassifier_result['test_accuracy'].mean(),\n                        decisionTreeClassifier_result['test_precision'].mean(),\n                        decisionTreeClassifier_result['test_recall'].mean(),\n                        decisionTreeClassifier_result['test_sensitvity'].mean(),\n                        decisionTreeClassifier_result['test_specificity'].mean(),\n                        decisionTreeClassifier_result['test_f1_score'].mean()\n                       ],\n\n      'Gaussian Naive Bayes':[\n                                gaussianNB_result['test_accuracy'].mean(),\n                                gaussianNB_result['test_precision'].mean(),\n                                gaussianNB_result['test_recall'].mean(),\n                                gaussianNB_result['test_sensitvity'].mean(),\n                                gaussianNB_result['test_specificity'].mean(),\n                                gaussianNB_result['test_f1_score'].mean()\n                              ],\n\n      'Logistic Regression':[\n                                logisticRegression_result['test_accuracy'].mean(),\n                                logisticRegression_result['test_precision'].mean(),\n                                logisticRegression_result['test_recall'].mean(),\n                                logisticRegression_result['test_sensitvity'].mean(),\n                                logisticRegression_result['test_specificity'].mean(),\n                                logisticRegression_result['test_f1_score'].mean()\n                            ],\n\n      'Support Vector Classifier':[\n                                    linearSVC_result['test_accuracy'].mean(),\n                                    linearSVC_result['test_precision'].mean(),\n                                    linearSVC_result['test_recall'].mean(),\n                                    linearSVC_result['test_sensitvity'].mean(),\n                                    linearSVC_result['test_specificity'].mean(),\n                                    linearSVC_result['test_f1_score'].mean()\n                                   ],\n\n       'K-nearest Neighbors':[\n                        kNeighbors_result['test_accuracy'].mean(),\n                        kNeighbors_result['test_precision'].mean(),\n                        kNeighbors_result['test_recall'].mean(),\n                        kNeighbors_result['test_sensitvity'].mean(),\n                        kNeighbors_result['test_specificity'].mean(),\n                        kNeighbors_result['test_f1_score'].mean()\n                       ],\n\n      },\n\n      index=['Accuracy', 'Precision', 'Recall', 'Sensitivity', 'Specificity', 'F1 Score', ])\n    \n    # Return models performance metrics scores data frame\n    return(models_scores_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try to look at our data frame again one last time\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **D. Preparing Features and Targets**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Actually what we are doing here is that we are just dropping the Species column since that is our class\n# and the remaining columns will then be our features (eg. inputs to come up to a class)\n# axis 0 basically means to drop all of that column\nfeatures = df.drop(columns=\"class\", axis=0)\n\n# Now let's see what features looks like\nfeatures\n\n# Don't mind the left hand side, those are just index mainly used for viewing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Specify target column\n# Now we try to get the frame of only our target. Which is the \"Class\" column\ntarget = df[\"class\"]\n\n# Do note that csv files are also zero-index, that means a row starts from zero.\ntarget","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# IV. Running our Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluationResult = models_evaluation(features, target, 5)\nview = evaluationResult\nview = view.rename_axis('Test Type').reset_index() #Add the index names to the column. This will be used for our presentation\n\n# https://pandas.pydata.org/docs/reference/api/pandas.melt.html\n# Re-Organizing our dataframe to fit our view need\nview = view.melt(var_name='Classifier', value_name='Value', id_vars='Test Type')\n# result\nsns.catplot(data=view, x=\"Test Type\", y=\"Value\", hue=\"Classifier\", kind='bar', palette=\"bright\", alpha=0.8, legend=True, height=5, margin_titles=True, aspect=2)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}