{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dependencies and Data"},{"metadata":{},"cell_type":"markdown","source":"Here as the manually selected features code we import all the required dependencies and also load the dataset. \nBut unlike previously i'm not manually selecting the features instead i allow the Recursive Feature Elimination(RFE) algorithm to eliminate the least important features and keep the rest to train the model.\n\nThe RFE algorithm only works with SVC with linear kernel and works all the time for support vector regression(SVR).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib notebook\nimport pandas as pd\nimport seaborn as sn\nimport numpy as np\nfrom sklearn.neural_network import MLPClassifier   #Multi-Layer Perceptron Classifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nimport matplotlib.pyplot as plt     \nfrom prettytable import from_csv    #To draw tables\n\ndataset=pd.read_csv('/kaggle/input/heart-disease-uci/heart.csv')\n\nprint(dataset.head(5))     #prints first 5 values for all columns\n\narray=dataset.values\ndata=array[:,0:13]\nlabels=array[:,13]\n#model1=MLPClassifier(activation='relu',solver='lbfgs',alpha=1e-5,random_state=1,) #doesnot work with neural networks\nmodel2=SVC(kernel='linear')    \nmodel3=LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature reduction using RFE"},{"metadata":{},"cell_type":"markdown","source":"The Recursive Feature Elimination(RFE) algorithm is implemented using the sklearn in-built method for RFE :- RFE(model,number of features required)\n\n\nThe RFE method does not seem to work with the neural network model, so i eliminnate the MLPClassifer model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\n#rfe1=RFE(model1,6)  \nrfe2=RFE(model2,6)    #here we wish to keep 6 most important features\nrfe3=RFE(model3,6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the data into train data and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(data,labels,test_size=0.2,random_state=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting the data and finding the ranked features"},{"metadata":{},"cell_type":"markdown","source":"Here since we have reduced the feature from the original dataset, we have to fit the rfe values not to the main model.\n\nFirst, we will try for the SVC classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"#k1=rfe1.fit(data,labels)\nk2=rfe2.fit(x_train,y_train)\nprint(\"Num Features: \",k2.n_features_)\nprint(\"Selected Features: \",k2.support_)\nprint(\"Feature Ranking: \",k2.ranking_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see the features with 'True' value in 'Selected Features' or have '1' in corresponding index place are the 6 most preffered features for SVC model.\n\nHere the positions(considering first index value to be 0) 1,2,8,9,11,12 are said to be the most important features.\n\nThe corresponding data are \"sex\",\"cp\",\"exang\",\"oldpeak\",\"ca\",\"thal\".\n\nThese data are said to be give the best results for our data classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"k3=rfe3.fit(x_train,y_train)\nprint(\"Num Features: \",k3.n_features_)\nprint(\"Selected Features: \",k3.support_)\nprint(\"Feature Ranking: \",k3.ranking_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, for the Logistic regression model, the selected features are in the index position 1,2,8,9,10,11\n\nThe corresponding data being \"sex\",\"cp\",\"exang\",\"oldpeak\",\"slope\",\"ca\".\n"},{"metadata":{},"cell_type":"markdown","source":"## Accuracy for test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"p2=k2.predict(x_test)\nprint(\"Accuracy of the SVC model on unseen data is \"+str(accuracy_score(p2,y_test)*100)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the accuracy of the SVC model has imporved. \n\nThe manually selected features with the same linear kernel gave SVC a test data accuracy of 81.9672131147541 %."},{"metadata":{"trusted":true},"cell_type":"code","source":"p3=k3.predict(x_test)\nprint(\"Accuracy of the SVC model on unseen data is \"+str(accuracy_score(p3,y_test)*100)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see the accuracy of the Logistic regression model has reduced after using the feature elimination technique.\n\nThe accuracy with manually selected features was 90.1639344262295 %.\n\nThis means that feature selection RFE algorithm works best on SVC of the three selected algorithms( MLP,SVC and Logistic Regression) having said that RFE technique doesnot work on MLP. "},{"metadata":{},"cell_type":"markdown","source":"## Confusion matrix for SVC and Logisitic Regression after RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm1=confusion_matrix(y_test,p2)\ncm2=confusion_matrix(y_test,p3)\n\nplt.figure()\nplt.title(\"Confusion Matrix for SVC model after RFE\")\ndf_cm = pd.DataFrame(cm1, range(2),range(2))\nsn.set(font_scale=1.4)     #for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}) # font size\n\nplt.figure()\nplt.title(\"Confusion Matrix for Logistic Regression model after RFE\")\ndf_cm = pd.DataFrame(cm2, range(2),range(2))\nsn.set(font_scale=1.4)     #for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}) # font size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do compare it with the confusion matrices of SVC and Logistic Regression without RFE."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}