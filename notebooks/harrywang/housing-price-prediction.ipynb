{"cells":[{"metadata":{"_cell_guid":"db00efbb-83bf-4291-9d03-806e04fb172e","_uuid":"bf0a1a372d8b3539b3f5b64b6160de862cdba2c3"},"cell_type":"markdown","source":"Most of the code in this notebook is from https://github.com/ageron/handson-ml. This is a great book - please buy the book to support the author.\n\nThis notebook can serve as a good training tutorial for novice data scientists.\n\nThe 10-Step Machine Learning Project Workflow (My Version):\n1. Define business object\n2. Make sense of the data from a high level\n    - data types (number, text, object, etc.)\n    - continuous/discrete\n    - basic stats (min, max, std, median, etc.) using boxplot\n    - frequency via histogram\n    - scales and distributions of different features\n3. Create the traning and test sets using proper sampling methods, e.g., random vs. stratified\n4. Correlation analysis (pair-wise and attribute combinations)\n5. Data cleaning (missing data, outliers, data errors)\n6. Data transformation via pipelines (categorical text to number using one hot encoding, feature scaling via normalization/standardization, feature combinations)\n7. Train and cross validate different models and select the most promising one (Linear Regression, Decision Tree, and Random Forest were tried in this tutorial)\n8. Fine tune the model using trying different combinations of hyperparameters\n9. Evaluate the model with best estimators in the test set\n10. Launch, monitor, and refresh the model and system"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","scrolled":false,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"# loading data\ndata_path = \"../input/housing.csv\"\nhousing = pd.read_csv(data_path)\n\n# see the basic info\nhousing.info()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b49f8321-dc46-4b76-9a75-3e5e6a51f1d5","_uuid":"46853c813bed59faa5898515448809318199c8bc"},"cell_type":"markdown","source":"What information should we get from `info()`\n- total observations: 20640: make sure you understand what is each observation. In this case, each observation is the data about a district\n- total columns (features): 10\n- data type of each feature: 9 numbers and 1 object (will handle later)\n- meaning of each feature: it is very important to work with domain expert to fully understand each feature\n- any null values (e.g., total_bedrooms is 20433, which indicates null values - will handle later)"},{"metadata":{"_cell_guid":"1f1c703b-8bcb-4de8-8a0b-a8417f632a5a","scrolled":true,"_uuid":"630644f793ed0e3f9297ae494539a727c24f1ab1","trusted":false},"cell_type":"code","source":"housing.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ffdbd075-0f51-4de2-81e6-c84ee256e561","_uuid":"7d7b082e2421495610ef60591cc097ab514ae62a"},"cell_type":"markdown","source":"## Data Types\n`head()` shows the valuse of top rows, which gives more idea on data types. Pandas guessed the data types of features when reading in the data, which may not always work. In this dataset, you can see ocean_proximity feature is text. Sometimes, the price feature may also be object type becaue the raw data has $ sign, in which case you need to convert the data type from object to float64 if you want to use this feature in the model."},{"metadata":{"_cell_guid":"6880a522-94d7-409e-8234-9fffa393b285","_uuid":"cfb563121e2ca78709203b9c6cbbfca237e88ac7"},"cell_type":"markdown","source":"## Basic Stats\n`describe()` shows a summary of numerial features, which can be visualized using boxplots and histograms. `value_counts()` can be used to generate a summary of categorical features."},{"metadata":{"_cell_guid":"c28f2ea3-7735-458c-a4ca-749d751da239","_uuid":"6b6c05c0400ec70bc5f7e82337dbbed4690507f5","trusted":false},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"44ff7bf4-6c17-46a6-8d9e-af0684836059","_uuid":"d93c407c87049bcace672abc8b6cad5fbc27878e","trusted":false},"cell_type":"code","source":"# boxplot \nhousing.boxplot(['median_house_value'], figsize=(10, 10))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d5b4ed7d-f927-4f6b-af8b-40121ad35b34","_uuid":"fb3a2350ccad322f62e33e1fab160f358b104b67","trusted":false},"cell_type":"code","source":"# histogram\nhousing.hist(bins=50, figsize=(15, 15))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d31a4f47-a2f1-40b0-8f60-c8c7ff80c6e8","_uuid":"da7ee409c9f8fbd379d70bd54abcf5fc790554e8"},"cell_type":"markdown","source":"\nGiven that `.botplot()` and `.hist()` only handle numerical features. We cannot forget ocean_proximity, which is object type (no need to change to string)."},{"metadata":{"_cell_guid":"3647db42-ddd2-40aa-aa04-da7f38c77f36","_uuid":"d9a3a52af4d4449ef1ac66656cb73cc7795bdf9c","trusted":false},"cell_type":"code","source":"housing['ocean_proximity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5dc65421-e679-4cf5-8d7b-57d8eec7761b","_uuid":"cb15a204d73352978a9e98f32b8e09fdc6b4ca5a","trusted":false},"cell_type":"code","source":"op_count = housing['ocean_proximity'].value_counts()\nplt.figure(figsize=(10,5))\nsns.barplot(op_count.index, op_count.values, alpha=0.7)\nplt.title('Ocean Proximity Summary')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Ocean Proximity', fontsize=12)\nplt.show()\n# housing['ocean_proximity'].value_counts().hist()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ababde1f-a4e1-40e7-a089-e63934e19c36","_uuid":"5a0fcc70b05cf4e11f83dbcef8c5353b010d37b2"},"cell_type":"markdown","source":"## Make Sence of the Data\nWhat are the typical things we can learn from the basic statistics with visualizaitons? \n1. Do the data make sence? scan each column and see whether the data make sense at a high level. \n    - longitude, latitude and house median age seem to be OK. \n    - total rooms and total bedrooms are in hundreds and thousands - this does make sense given each row is a district\n    - population seems to be OK but you want to know what's the unit for the number, e.g., thousands? millions? households are numbers of people living together, which is OK. Households mean is 499 and population mean is 1425, so you can tell that population is just the total number of people in each district not in thousands or millions. \n    - median income is apparently problematic - how can mean income be 3.87? Actually, this is because median income has been scaled and capped between 15.0001 (max) and  0.4999 (min). Preprocessed attributes like this are common. Knowning how the data is calculated is critical.\n    - median house value data is OK and this is our target variable, i.e., we want to build a model to predict this value.\n \n2. Feature scaling: you have noticed that the features have very different scales, which we need to handle later\n3. Distribution: from the histograms, we can tell many of them are skewed, i.e., having a long tail on one side or the other. In many cases, we need to transform the data so that they have more bell-shaped distributions."},{"metadata":{"collapsed":true,"_cell_guid":"e66b67ae-660b-47df-9422-b8bcfca8edd7","_uuid":"8a3035ab1ea8aa11367a489f1078c4ecc48169c7"},"cell_type":"markdown","source":"## Create Training, Validation, and Test Data Sets\nTo avoid [data snooping bias](https://en.wikipedia.org/wiki/Data_dredging), you should set aside 20% of the data as the test set without further exploriation of the data. "},{"metadata":{"_cell_guid":"ff81030b-907a-488a-9ab4-ea0bd7907824","_uuid":"18073beffb4d3a4efcf3e6416f81a1ac09cb74f4","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\nprint(len(train_set), \"train + \", len(test_set), \"test\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8a05ecd7-c41e-459d-bc60-92fd763a8b12","_uuid":"692febfe53ae4a6cc471681a0dc6acd83d44b8e8","trusted":false},"cell_type":"code","source":"# check whether the test set is the same for every run\ntest_set.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1d61bd85-d3ff-4be4-b19e-4d4527686b3b","_uuid":"1fae59bf211e77a35e95838f3f0b3fc4cf4d29d0"},"cell_type":"markdown","source":"The above function will generate a different test set every time you run the program, which is undesirable. A few potential solutions:\n\n1. save the test set and training set for the first run into csv files\n2. set a fixed random number generator seed so that the random numbers are always the same (we use this approach in this notebook using `random_state=42`)\n\n1 and 2 won't work if new data is loaded. The book proposed a method of hashing the identifier of the data to get the same test set (see page 50 for details). \n"},{"metadata":{"_cell_guid":"0daa39c2-7660-44c1-bc89-e7af8b20ef83","_uuid":"2cf3eb165cf5b69fb0d0bcc9881478ee4ff6b977"},"cell_type":"markdown","source":"## About Sampling\nIn order to make sure that the test set is a good representation of the overall population. We may want to consider different sampling techniques:\n\n- random sampling (what we used above): OK if the dataset is large enough (how large is enough?)\n- stratified sampling: the population is devided into homogeneous subgroups called *strata* and the correct instances are sampled from each *stratum*  (such as for a survey of 1000 people, given US population has 51.3% female and 48.7% male, 513 female and 487 male should be surveyed instead of pure random sampling) \n\nSuppose you learned that median income is very important for predicting median housing prices. You may want to use stratified sampling for the test set. To do that, you first need to change median income from a continuse attribute to a categorical attribute. As shown in the histogram below, we can see most of the income are around 2 and 5, and some are beyond. "},{"metadata":{"_cell_guid":"941e1ee3-092e-498e-8890-c9610de0a88e","_uuid":"0a164d205b5476b652ba2ef41bfaea412eb819d7","trusted":false},"cell_type":"code","source":"housing['median_income'].hist()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"03b10ea2-f8fe-4bb3-8ee0-358583299bae","_uuid":"a543259c25859b3a7a954cc3e71fb846277a35ba"},"cell_type":"markdown","source":"We limit the number of categories by dividing the median income by 1.5 and merge all the income greater than 5 into 5. Then, we can use stratified sampling."},{"metadata":{"_cell_guid":"777b39ec-5eb9-4756-b8af-7e0b9008b44d","_uuid":"ee2fda47a58118e2d96f5c30b783c0c3eaf34fef","trusted":false},"cell_type":"code","source":"housing['income_cat'] = np.ceil(housing['median_income']/1.5)\n# DataFrame.where(cond, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=False, raise_on_error=None)\n# Where cond is True, keep the original value. Where False, replace with corresponding value from other\nhousing['income_cat'].where(housing['income_cat']<5, 5.0, inplace=True)\nhousing['income_cat'].hist()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d1604ef6-2b00-417e-9006-81ad94389dd1","_uuid":"5dbd1482938b92afacbdecd3de621d90d099db89","trusted":false},"cell_type":"code","source":"# stratified sampling based on income categories\nfrom sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing['income_cat']):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]\n\nstrat_test_set.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"82dac523-e36d-487b-a8b4-31030af6fced","_uuid":"bb7f4c56d5fe9c06e1db986d4a2b5c03a892a950"},"cell_type":"markdown","source":"We can compare the different sampling results for the test set by comparing them to the overall population distribution as follows. As you can see, stratified sampling's distributions are much more similar to the overall distributions compared with random sampling"},{"metadata":{"_cell_guid":"a3113e0f-3921-406d-8710-be51df274172","_uuid":"156fcb1f9a8312834865a4d2c3fc680e76124025","trusted":false},"cell_type":"code","source":"housing['income_cat'].value_counts() / len(housing)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e69d3bd5-5ec4-405b-b3c2-96c3041e7386","_uuid":"282a4b5f4c573aad97faba6bd8adfb8844fbed5f","trusted":false},"cell_type":"code","source":"strat_test_set['income_cat'].value_counts() / len(strat_test_set)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a5cc95fa-e2fc-4495-aa56-7b607b575ac3","_uuid":"8ff373e8e6b2ecea9969f3f338a9971a0aa7a798","trusted":false},"cell_type":"code","source":"# we need to do the random sampling again to include income_cat column\nfrom sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ntest_set['income_cat'].value_counts() / len(test_set)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"d95f7c24-b8a0-4765-97a7-53563eb463f6","_uuid":"f98bed743dc3b7ca9d5c1ada93f56d213cce6d99","trusted":false},"cell_type":"code","source":"# drop the income_cat attributes\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9167ef9e-56fd-4ed0-a270-b788e7781cbf","_uuid":"fbbbd95b87c311df51dafb0131343ff959ccfd62","trusted":false},"cell_type":"code","source":"# check the dropping result\nstrat_test_set.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6b3e92ef-bb62-435d-909d-918242a43bb5","_uuid":"31fae695cee2a02c587130c0ea086285fddc9849"},"cell_type":"markdown","source":"Now you should set aside the test set and only use the training set. When the training set is too large, you can also create an exploration set out of the training set to make the initial analysis fast. we create a copy of the training set as follows:"},{"metadata":{"_cell_guid":"78f55f8c-4142-4159-a9ef-807c00a7dc27","_uuid":"b6e124964a39c6f6f6f0550e45ed5d0c15a7d7b6","trusted":false},"cell_type":"code","source":"housing = strat_train_set.copy()\nhousing.info()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a2fd29ec-2939-46ab-8afc-c02dff2cf2e6","_uuid":"0c25677ff2e64bb71bb10d851ab886af2a42dfb9"},"cell_type":"markdown","source":"### Additional Visualizations for Data Exploration\nThe following geographical data visualizations show that the price is related to the location and population density."},{"metadata":{"_cell_guid":"71595dae-6db8-45ee-ad13-bd66e509486e","_uuid":"5246af5648505229fcdf9cdc8b9ab63deba67776","trusted":false},"cell_type":"code","source":"housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3600a06a-483a-4089-9059-f5f052bf7379","_uuid":"c3a496891013e9b715b90fd368e53105ce735144","trusted":false},"cell_type":"code","source":"# option s: radius of each circle represent the population/100\n# option c: color represents the median price\nhousing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4, \n    s=housing['population']/100, label='population', figsize=(10,7), \n    c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2ec535a8-89d0-42a2-b960-477d653131f2","_uuid":"7b4bea83faa759761430bac0f3d4a5cf52212f63"},"cell_type":"markdown","source":"### Correlation Analysis\nWe want to further explore the data to look for correlations between different attributes. correlation coefficient is between -1 and 1, representing negative and positive correlations. 0 means there is no liner correlation. Correlation is said to be linear if the **ratio of change** is constant, otherwise is non-linear. Visual inspection of data is very important, which cannot be replaced by simply looking at the correlation coefficients, i.e., check out Anscombe's quartet: https://en.wikipedia.org/wiki/Anscombe%27s_quartet"},{"metadata":{"_cell_guid":"ba195d33-cfb6-4014-b58f-a6dfd7880335","_uuid":"db950f10e2cae5139311e20389f2dfe976d52829","trusted":false},"cell_type":"code","source":"# Anscombe's quartet: https://seaborn.pydata.org/examples/anscombes_quartet.html\nsns.set(style=\"ticks\")\nanscombe = pd.read_csv(\"../input/anscombe.csv\")\n\n# Show the results of a linear regression within each dataset\nsns.lmplot(x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\", data=anscombe,\n           col_wrap=2, ci=None, palette=\"muted\", size=4,\n           scatter_kws={\"s\": 50, \"alpha\": 1})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4af21b33-b350-420e-af52-f328c2bfbe99","_uuid":"3b2fb3c1f0a3ea1443f49f7de2d7463b0a97c986","trusted":false},"cell_type":"code","source":"# Pearson's r, aka, standard correlation coefficient for every pair\ncorr_matrix = housing.corr()\n# Check the how much each attribute correlates with the median house value\ncorr_matrix['median_house_value'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cdfbb263-843b-4dac-b448-26f69f97a5fd","_uuid":"cb1a1f8c3af889e45a3bdb66a5370c008c34411f","trusted":false},"cell_type":"code","source":"from pandas.tools.plotting import scatter_matrix\nattributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']\nscatter_matrix(housing[attributes], figsize=(12,12))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"5cfc8d48-b50a-40e2-948c-436cf7fbfd03","_uuid":"037e979a849d82a6239a2cf855e2c3932167374d"},"cell_type":"markdown","source":"We can see that median_income is a promising attribute to predict median_house_value. A close-up of the scatterplot is as follows:"},{"metadata":{"_cell_guid":"5b9f7601-708d-413c-ae76-47a28f12e9c6","_uuid":"524b69679a4b4a9ccb838e473e50ac3001d1f625","trusted":false},"cell_type":"code","source":"housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.2, figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"56d8c76a-a07d-495b-a390-7bcfe58fa4ec","_uuid":"aadb245af44f43598fdb8b471a500bc143efd7f6"},"cell_type":"markdown","source":"We can see there are a number of \"horizontal lines\" in the plot: one clear one at $500,000, one at $450,000, another one at $350,000, and a few other ones. Try to find out why that is happening. If you cannot figure out the reason, removing those data points (if not too many) might be a good idea before feeding the data to the algorithms. "},{"metadata":{"_cell_guid":"b1b82d9b-73ad-463c-83aa-6a7e00971d35","_uuid":"c0742bf6798c7fc3f30ebfe3722b56585c8f3fe0"},"cell_type":"markdown","source":"### Attribute Combinations\nSometime, the combinations of attributes are more meaningful and interesting in terms of solving the business problems, e.g.,\n- rooms per household: total # of rooms per district is not useful but rooms per household may be interesting\n- bedroom/total room ratio\n- population per household"},{"metadata":{"_cell_guid":"7a94f687-7045-4688-a529-549d567900ec","_uuid":"b12a658bc9002f2f6880a1b2eb64b28b7b260a30","trusted":false},"cell_type":"code","source":"# calculated attributes\nhousing['rooms_per_household'] = housing['total_rooms']/housing['households']\nhousing['bedrooms_per_room'] = housing['total_bedrooms']/housing['total_rooms']\nhousing['population_per_household'] = housing['population']/housing['households']\n\n# checkout the correlations again\ncorr_matrix = housing.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3fd2032b-b4f5-4029-bb27-3159da0eaa9e","_uuid":"cabd99787ffe46b29f3434e571b5fdded3d58a6d"},"cell_type":"markdown","source":"Two findings after combining attributes:\n1. rooms_per_household is slightly more correlated (0.146285) with house value than total_rooms (0.135097)\n2. bedrooms_per_room is much more correlated (-0.259984) than total_rooms (0.135097) and total_bedrooms (0.047689): houses with lower bedroom/room ratio is more expensive: this sort of make sense, more expensive houses may have more offices, dens, playrooms, etc. "},{"metadata":{"_cell_guid":"04d35a6c-d829-4591-aa69-a84a7c325492","_uuid":"201fe75e6ebcac0c3a5f7315836776310e18d354"},"cell_type":"markdown","source":"### Data Cleaning and Transformation\nTypically, data need to be cleaned and transformed before trying different ML algorithms.\n#### missing data in one attribute. \nThree ways to handle this:\n1.   remove the observations with missing values using `dropna()`; \n```\nhousing.dropna(subset=['total_bedrooms']\n```\n2.  remove the entire attribute using `drop()`;\n```\nhousing.drop('total_bedrooms', axis=1)\n```\n3. set/impute the missing values using `fillna()`\n```\nmedian = housing['total_bedroom'].median()\nhousing['total_bedrooms'].fillna(median, inplace=True)\n```"},{"metadata":{"_cell_guid":"f2de01cd-bf87-40da-9b61-7414343d89e6","_uuid":"b2fcc8c51e882b70df464f353873dcabe8783bcb"},"cell_type":"markdown","source":"### Seperate the predictors (independent variables) and labels (target/dependent variables)\nWe want to create a clean training set first."},{"metadata":{"_cell_guid":"387a6d89-5a5a-472c-8d63-857d7dfd7ac9","_uuid":"ee579bb8d522ce4f5e7acf8380462564761bd5a2","trusted":false},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"25aac2f5-101d-458f-8f05-5a6e21299530","_uuid":"0502ccc173d784983bebab10a9f7410288545b60","trusted":false},"cell_type":"code","source":"housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop target labels for training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy() # this is the target label vector\nhousing.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"824c6366-cb22-44ea-98a7-4372f011dcb4","_uuid":"4eb5ab1ad2469dbb2329d900d0c5bf22b0d10f9f","trusted":false},"cell_type":"code","source":"# using Scikit-Learn Imputer\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy='median')\n\n# remove non-numerical attributes for Imputer by making a copy of the dataframe\nhousing_num = housing.drop('ocean_proximity', axis=1)\n\nimputer.fit(housing_num)  # this computes median for each attributes and store the result in statistics_ variable\nimputer.statistics_  # same result as housing_num.median().values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"840dee1a-9d1a-47a6-856b-46bef53f7974","_uuid":"a3ca348320c428d58b9c7b1dcc2c7f5b9cdddcfa","trusted":false},"cell_type":"code","source":"# see attributes with missing values\nhousing_num.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"365eed43-4e35-40ff-9163-4602759be8df","scrolled":false,"_uuid":"1707f31d9a2fe9eabfae387d5e0cd59d54c42070","trusted":false},"cell_type":"code","source":"x = imputer.transform(housing_num)  # this is a Numpy array\nhousing_tr = pd.DataFrame(x, columns=housing_num.columns)  # change a Numpy array to a DataFrame\nhousing_tr.info()  # no missing values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b2d65178-66dd-4ce8-a674-ab6ef8a5e4d6","_uuid":"55b8d5f52fd86a9f8451ca4c7faad4f0e3aba660"},"cell_type":"markdown","source":"### Text and Categorial Attributes\nMost ML algorithms work with numbers better. Therefore, we often need to convert text attributes into numerical attributes. For ocean_proximity, we have two ways to handle this problem:\n1. map each category to a number, such as \"<1H OCEAN\" is 0, \"INLAND\" is 1, 'NEAR OCEAN' is 4, etc. The problem with this solution is that ML algorithm may think 4 is greater than 0, which could cause problem.\n2. To address the problem in 1, we can also create a binary variable for each attribute, which is called one-hot encoding (only one is 1 hot, all others are 0 cold)"},{"metadata":{"_cell_guid":"3e1eb0c6-0587-4d21-a809-9828c47c4f38","_uuid":"aea67bcb76c83f85303eb5dccabcfae3901eee86","trusted":false},"cell_type":"code","source":"# Approach 1\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nhousing_cat = housing['ocean_proximity']\nhousing_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8a4c75d5-ac14-4e45-a8fe-b44f67b72b06","_uuid":"d7ff64d2836382456cdb192faa607e5dbe7b7e13","trusted":false},"cell_type":"code","source":"housing_cat_encoded = encoder.fit_transform(housing_cat)\nhousing_cat_encoded","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e92bb83b-ffa3-408a-a5b9-79e694ebfa16","_uuid":"7179edbddf315680ccfe865404fe33c12b4ab8e0","trusted":false},"cell_type":"code","source":"print(encoder.classes_)  # '<1H OCEAN' is 0, 'INLAND' is 1, etc.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bfdc24e1-48f9-4fdf-ac1c-0cfb618a433d","_uuid":"df9a729a0ded87b696a58ca346e1ef07a21af048","trusted":false},"cell_type":"code","source":"# Approach 2\n# reshape\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()  # don't forget the ()!!!\nhousing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))  # this returns a sparse SciPy matrix\nhousing_cat_1hot.toarray()  # convert the sparse matrix to numpy array","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c7b00738-dbf6-429c-8a54-51f835e315ab","_uuid":"6980028b8310cc298517265f4942c78e4a4b7c9f","trusted":false},"cell_type":"code","source":"# Combine Approch 1 and 2 in one shot\nfrom sklearn.preprocessing import LabelBinarizer\nencoder = LabelBinarizer()\nhousing_cat_1hot = encoder.fit_transform(housing_cat)\nhousing_cat_1hot","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8a47b8c0-b1f6-40b7-92b3-00a1a1252342","_uuid":"da0379b001297c81242a13754afb580c3d2d469d"},"cell_type":"markdown","source":"### Custom Transformers\nYou may need to develop custom transformers - you can just write a simple function for that or if you want your translormer work with Scikit-Learn, you need to develope the transformers as a class."},{"metadata":{"_cell_guid":"2a9794f3-c076-4d58-a642-2605827023dc","_uuid":"7dcb24efee14625eeacc8f2554a993c752193e73","trusted":false},"cell_type":"code","source":"# A custom transformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6  # hardcoded just for this dataset\n\nclass CombineAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True):\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        rooms_per_household = X[:, rooms_ix]/ X[:, household_ix]\n        population_per_household = X[:, population_ix] / X[:, household_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\n            \nhousing.head()  # note that rooms_per_household, and population_per_household already calculated before","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"736c3f9f-20f8-4a7e-8e38-5414c36f3a1e","_uuid":"02fbee95f49816fa3488f31d88239841c05733f4","trusted":false},"cell_type":"code","source":"attr_adder = CombineAttributesAdder(add_bedrooms_per_room=False)  # add_bedrooms_per_room is called a hyperparameter\nhousing_extra_attribs = attr_adder.transform(housing.values)  # housing.values is the numpy N-array representation of the DataFrame\nhousing_extra_attribs","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b19edaac-26eb-49e6-8640-f360925c1353","scrolled":true,"_uuid":"8eec12aa94adbfe6e928a83d0643f369dcc8d2c3","trusted":false},"cell_type":"code","source":"# check the stats of the training set for feature scaling\nhousing_tr.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d8fb74ea-2a75-4c1e-99a8-f504afd94469","_uuid":"5b3a4f2471ae9c902da13ba41ba30e3a3c00e60d"},"cell_type":"markdown","source":"### Feature Scaling\nTypically, ML algorithms don't perform well when the input numerial attributes have very different scales. For example, in this housing dataset (shown above), you can see median_income ranges from 0.4999 to 15 while total rooms is between 6 and 39320. Note that scaling the target values is typically not required. \n\nTwo major scaling methods (two different scalers):\n- normalization (aka, min-max scaling): values are rescaled to between 0 and 1 using (value-min)/(max-min)\n- standardization: values are rescaled to have unit variance: (value - mean)/variance\n\nNormalization can be dramatically affected by outliner data while standardization handles outliers very well. "},{"metadata":{"collapsed":true,"_cell_guid":"b6361e5f-a3d5-45d7-abed-11d5062688f3","_uuid":"12c5f0a5edfa0e2dc746eb5e44b15d255aa10e96","trusted":false},"cell_type":"code","source":"# Transformation Pipeline\n# name/estimator pairs for pipeline steps\n# each estimator except the last one must be transformers with fit_transform() method\n# pipeline fit() calls each fit_transform() of each estimator and fit() for the last estimator\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n    ('imputer',Imputer(strategy='median')),\n    ('attribs_adder', CombineAttributesAdder()),\n    ('std_scaler', StandardScaler()),\n])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6f1dc1d4-af46-481d-913e-7c518f3d48f6","_uuid":"2c61d7bbe7471442466cd5496d119fa8825081df"},"cell_type":"markdown","source":"Scikit-Learn only handles Numpy arrays not Pandas Dataframes, we can create another transformer so that we can feed the pipeline a DataFrame"},{"metadata":{"collapsed":true,"_cell_guid":"015d518b-6d49-429a-919e-59842ea97b3b","_uuid":"2cce4be70cb11aa70fc16f8dbb4126de7f1c6f5b","trusted":false},"cell_type":"code","source":"# this is the fix to the problem at https://stackoverflow.com/questions/46162855/fit-transform-takes-2-positional-arguments-but-3-were-given-with-labelbinarize\n# CategoricalEncoder should be used instead of LabelEncoder in the latest version of Scikit-Learn\n# Definition of the CategoricalEncoder class, copied from PR #9151.\n# Just run this cell, or copy it to your code, do not try to understand it (yet).\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8ca15fff-2867-4f26-ac2e-caa2661b3ca9","_uuid":"e243a1e53b6f0ac59bf3f9a222f212e55c01b004","trusted":false},"cell_type":"code","source":"# given a list of attributes names, this transformer converts the dataframe to a numpy array\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X[self.attribute_names].values","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"484af4d6-b347-4f38-84b2-6cd69954712c","_uuid":"cf105759306d13fbe3da596f7214c28955f55b59","trusted":false},"cell_type":"code","source":"# create two pipelines and use feture union to join them\nnum_attribs = list(housing_num)\ncat_attribs = ['ocean_proximity']\n\nnum_pipeline = Pipeline([\n    ('selector', DataFrameSelector(num_attribs)),\n    ('imputer', Imputer(strategy='median')),\n    ('attribs_adder', CombineAttributesAdder()),\n    ('std_scaler', StandardScaler()),\n])\n\ncat_pipeline = Pipeline([\n    ('selector', DataFrameSelector(cat_attribs)),\n    ('label_binarizer', CategoricalEncoder()),\n    # ('label_binarizer', LabelBinarizer()),  # LabelBinarizer does not work this way with last Scikit-Learn\n])\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f27a157c-eda9-424d-8699-bd951af1650b","_uuid":"631b839ec06de641e35f0b68538dc7ac8b2c92f6","trusted":false},"cell_type":"code","source":"housing_num_tr = num_pipeline.fit_transform(housing)\nhousing_num_tr.shape\nnum_attribs","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bb8f8b86-e2f2-43c7-b3b4-58fb90511718","_uuid":"f281e5f9a1ebf1e3156deb2e7ec5c8f14a11df38","trusted":false},"cell_type":"code","source":"housing_cat_tr = cat_pipeline.fit_transform(housing)\nhousing_cat_tr","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f40d7d18-8200-4760-8f5f-4bc21321de7c","_uuid":"6c4e35d7fac37abfcdc77298d47c22731eb00109","trusted":false},"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion\n\nfull_pipeline = FeatureUnion(transformer_list=[\n    ('num_pipeline', num_pipeline),\n    ('cat_pipeline', cat_pipeline),\n])\n\n# run the whole pipeline\nhousing_prepared = full_pipeline.fit_transform(housing)\nhousing_prepared.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"932afe85-ebd9-44f6-90cd-d5372ebaddd2","_uuid":"62a100e0cac4fb655d8efe3ad934e7c19e18259e"},"cell_type":"markdown","source":"### Select and Train a Model\nWe are going to try Linear Regression, Decision Tree, and Random Forest models."},{"metadata":{"_cell_guid":"c9afd3fc-b59b-421c-ba63-e6594629f02c","_uuid":"42c3f81ece6a53b5f251dd97f52bbe3d03e1e6b9","trusted":false},"cell_type":"code","source":"# Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)  # housing_prepared are independent variables and housing_labels are dependent variables","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"98dbea01-a653-47ff-b458-f1b57839da55","_uuid":"6be340d78308ac9b311e3590df096ef6ca1662fd","trusted":false},"cell_type":"code","source":"# test out the linear regression model\nsome_data = housing.iloc[:5]  # choose the first five observations\nsome_labels = housing_labels.iloc[:5]\nsome_data","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6ce2830b-af34-42aa-a61d-7e2b5da2cdf3","_uuid":"fc85590e8aabbbe63bb0bc92aba13b99f2fca4cf","trusted":false},"cell_type":"code","source":"some_data_prepared = full_pipeline.transform(some_data)\nsome_data_prepared\nprint('Actual Prices:', list(some_labels))  # actual prices","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b48064d-e2ee-4a62-97bd-01872e930ba6","_uuid":"9840c8c83ca91e39bcd214b3e5b725ab07088d08","trusted":false},"cell_type":"code","source":"# print predicted prices\nprint('Predicted Prices:', lin_reg.predict(some_data_prepared))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a0f09240-ae1c-4187-a389-7f9e67f8c23a","_uuid":"444c46d9cea6c76396a1b8f8d2820454390e51eb"},"cell_type":"markdown","source":"As you can see, the sample predictions are not very good, i.e., the first one is off by (286600-210644)/286600 ~= 26.5%! Let's calculate the RMSE (root-mean-square error) on the whole training set."},{"metadata":{"_cell_guid":"9f66f27a-dadc-4ea0-9968-834e81926d45","_uuid":"3e03914a8c3bf274c7c3a904c5e9f6bba41ec9e1","trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dc8f7b85-1b28-4c4e-97fd-0f8f2bf681c9","_uuid":"c2c879a054a825ec6e48e8a519564b89916bcd38","trusted":false},"cell_type":"code","source":"housing_labels.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b9a4162a-b978-465b-b889-a61789e2dfa5","_uuid":"0bf417daca5e1399299fbae480d831de9b073b51"},"cell_type":"markdown","source":"25% and 75% quantile are `$120,000`  and  `$264,000` respectively, which means 50% of the house prices are between those two values. Therefore, $6,8628 error is not very good, which is a typical example of **underfitting**.\n\nThree ways you can potentially improve the results:\n1. try a different model\n2. try better features, i.e., feature engineering\n3. reduce the constraints on the model is any (for example, if the model is regularized)"},{"metadata":{"_cell_guid":"811ed0a0-2674-4204-8016-6def39f46a17","_uuid":"a3867c9d2347e7bf4cc825d037bcadd44c0583f3","trusted":false},"cell_type":"code","source":"# Try Decision Tree\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"64436baf-c382-49e1-a780-c08e27fd11d7","_uuid":"51ad5326a7657d30e8980f69d8e096eb0c7cd920"},"cell_type":"markdown","source":"No error at all! This is a typical example of **overfitting**. You cannot use the same set of data for both training and validation. Cross-Validation (CV) can help with model validation.\n\nScikit-Learn CV features expect a utility function (greater is better) than a cost function (lower is better), which is the reason for having `-scores`:"},{"metadata":{"collapsed":true,"_cell_guid":"7dd05ddd-a709-4a79-8920-df32a8400645","_uuid":"1421162c51bd6bcb15e05e93b8c364c06b1b27b9","trusted":false},"cell_type":"code","source":"# 10-fold cross validation\nfrom sklearn.model_selection import cross_val_score\n\n# for decision tree\ntree_scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\ntree_rmse_scores = np.sqrt(-tree_scores)\n\n# for linear regression\nlin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1f2af684-c577-460c-9f39-ce5f134dc357","_uuid":"2d1445771d2cfbf95938ae7eb3ee71cb7b10a544","trusted":false},"cell_type":"code","source":"print('Scores:', tree_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"be8a7f66-b248-4eb5-a380-2d062b1d3576","_uuid":"efc5e4e9222c1059fd3ad07358bdfbf046f04e7c","trusted":false},"cell_type":"code","source":"print('Mean:', tree_rmse_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"96485224-a64e-4609-99f0-71bbafff6300","_uuid":"69c1cfb746c86b0e24ea65896fa02acfcdf39c1d","trusted":false},"cell_type":"code","source":"print('Standard Deviation:', tree_rmse_scores.std())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"29480118-59aa-439c-914f-b23714673b31","_uuid":"749994535f5949d0f6b497d2ff4bda20da54097c","trusted":false},"cell_type":"code","source":"print('Scores:', lin_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c81ac484-9252-45f7-aa03-b07c4d7376ff","_uuid":"9cbfdd82fc8b982d396bf840b363f4b9cc9c0ddf","trusted":false},"cell_type":"code","source":"print('Mean:', lin_rmse_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cde8fbbc-9644-49ee-8395-52ec702c5c24","scrolled":false,"_uuid":"f77704ddd4abffce639833964ba84031bbd3e706","trusted":false},"cell_type":"code","source":"print('Standard Deviation:', lin_rmse_scores.std())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b32fe0fb-84fe-488d-8830-f33913fa2bc2","_uuid":"9bc97deb1e0a5adfe816317b725aa6b10e537ee2"},"cell_type":"markdown","source":"Now, Decision Tree Model's performance is actually worse than the Linear Regression Model: mean rmse 71493 vs. 69051 (the numbers differ everytime you run the models)"},{"metadata":{"_cell_guid":"266b8bfe-e277-44a7-8d3c-3e6c77101ce9","_uuid":"e97f302c4ab871ce359cb98ab6d8ea63c8ae5372","trusted":false},"cell_type":"code","source":"# Try Random Forest, which is an Ensemble Learning model\nfrom sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring='neg_mean_squared_error', cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\nprint('Mean:', forest_rmse_scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a1b56f74-ef1a-4b62-9586-0d22ee6a533c","_uuid":"bb74b674b649642710269e41bb23f1df3d4f4f25"},"cell_type":"markdown","source":"Random Forest is much better that the previous two models. You can try other models, such as SVM, NN, etc. We assume you choose Random Forest as the model and discuss how you can fine tune the model for better performance. \n"},{"metadata":{"_cell_guid":"13e78f06-3e6e-44b2-a26c-81f4a30e1318","_uuid":"f96ed0fefb8873d46217f5b3c936cf91e6e1f2b9"},"cell_type":"markdown","source":"### Fine Tune the Model\nThere are different ways you can fine tune your model:\n1. try different combinations of hyperparameters of a model: \n    a. the following example trys 18 different combinations of hyperparameters and find the best one\n    b. you can also use Randomized Search to try more combinations when the search space is very large\n2. combine the best performing models"},{"metadata":{"_cell_guid":"0ff1d832-23b4-4d28-9170-56a117632c1d","_uuid":"bedc716a898798c0e561ef11a873778634fec17d","trusted":false},"cell_type":"code","source":"# use GridSearch to find best hyperparameter combinations\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators':[3, 10, 30], 'max_features': [2, 4, 6, 8]},  # try 3x4=12 combinations\n    {'bootstrap': [False], 'n_estimators':[3, 10], 'max_features': [2, 3, 4]},  # try 2x3=6 combinations\n]\n\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')  # each model is trained 5 times, so (12+6)*5 = 80 rounds of training in total\ngrid_search.fit(housing_prepared, housing_labels)\ngrid_search.best_params_  # best parameters","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b686f35f-1bb3-4b38-b235-d1cee88de630","_uuid":"b8a176643b9d7590d0e5bab26a6de72a8de366d0","trusted":false},"cell_type":"code","source":"grid_search.best_estimator_  # best estimators","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"445493b3-92ea-45ca-a847-0f481336d220","_uuid":"1aef221e5f179796989ea9d1738012637eebddb7","trusted":false},"cell_type":"code","source":"# The importance of the features\nfeature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c36d54fb-5fe2-4c67-a9bf-09ab67b51ada","_uuid":"3ca3aee50b4754203f5576cb78f3698c04150b7f","trusted":false},"cell_type":"code","source":"extra_attribs = ['rooms_per_hhold', 'pop_per_hhold', 'bedrooms_per_room']\ncat_one_hot_attribs = list(encoder.classes_)\nattributes = num_attribs = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b2cfeb2e-4d00-47f5-b0a5-296b61f14bc4","_uuid":"d960681098d397773bdb199f012322145ffc7d4a"},"cell_type":"markdown","source":"Based on the feature importance, you can choose to drop some features such as the last four ocean proximity features to simplify the model. In the following example, the performanc on the test set is actually better than the validation set."},{"metadata":{"_cell_guid":"42d2431e-49f7-4b8f-aef6-3490cbd8625b","_uuid":"15689a515c1c8173463663978cba6099e50bcac2"},"cell_type":"markdown","source":"### Evaluation via the Test Set\nThis step is to see how the model performs on unknow data. As long as the result is not way off from the validation result, you should go ahead lauch the model."},{"metadata":{"collapsed":true,"_cell_guid":"bba44def-e55c-4d9b-b5b7-a2d3c9e13c56","_uuid":"300cc0b473f4a293bc02887bd396839f24addad6","trusted":false},"cell_type":"code","source":"final_model = grid_search.best_estimator_  # best model\n\n# see the best rmse on the validation set\nbest_valiation_score = grid_search.best_score_\nbest_validation_rmse = np.sqrt(-best_valiation_score)\nbest_validation_rmse","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"833f4fd2-d806-4369-956c-2530ef606837","_uuid":"9b77dd26834654d0dafd30287e16de815af3b1f9","trusted":false},"cell_type":"code","source":"# see the final rmse on the test set\nX_test = strat_test_set.drop('median_house_value', axis=1)\ny_test = strat_test_set['median_house_value'].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)  # note DO NOT USE fit_transform!! not need to fit anymore\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_rmse","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"77ea9ea6-237d-4530-be01-792ae0e04328","_uuid":"c085c2587b3b21f76d9ea595738b6de227de5d07"},"cell_type":"markdown","source":"### Present the Final Model\nIt's very important to present the final model. The author's suggestion on an easy-to-remember statements is very good!! For example, our final solution is to use Random Forest Model and the median income is the number one predictor of housing prices."},{"metadata":{"_cell_guid":"5570e941-7da1-4a9c-86b7-2f63b779acd4","_uuid":"1572891f0794c714040c536cc6d053d41ef6d759"},"cell_type":"markdown","source":"### Lauch, Monitor and Maintain the System\nThings to monitor:\n1. Performance \n2. Input Data Quality\n\nContinues evaluations via Human evaluations and Training new models using fresh data."}],"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","name":"python","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","mimetype":"text/x-python","file_extension":".py","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}