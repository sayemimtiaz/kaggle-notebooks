{"cells":[{"metadata":{},"cell_type":"markdown","source":"Did you know?\n\n\n**A person can be turned into a diamond!!**\n\nWant to live on for eternity? With the advancement of lab-grown diamond technology, some companies are now offering to turn the ashes of a deceased love one into a diamond gem that can worn as jewelry by those they left behind. Swiss-based Algordanza has been providing memorial diamonds since 2003, and now operates in 30 different countries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\nfrom PIL import Image\nsns.set(style=\"ticks\", color_codes=True)\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"diamond=pd.read_csv('diamonds.csv')\ndiamond.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image.open('Anglo-DiamondAnatomy_03.JPG')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diamond.drop('Unnamed: 0',axis=1,inplace=True)\n\nprint('Numnber of records in dataset are {} and Features are {}.  '.format(*diamond.shape))\nprint(\"\\nAny missing sample in set:\",diamond.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diamond.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Categorical column's details :\")\ndiamond.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nfor i,feature in enumerate(diamond.select_dtypes(include='O').columns.tolist(),1):\n    plt.subplot(1,3,i)\n    diamond[feature].value_counts(normalize=True).plot.bar()\n    plt.title(feature)\n\nplt.figure(figsize=(20,5))\nfor i,feature in enumerate(diamond.select_dtypes(include='O').columns.tolist(),1):\n    plt.subplot(1,3,i)\n    sns.boxplot(x=feature,y='price',data=diamond,palette='husl')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **CUT** \n* Most of the cut present in dataset are of 'Ideal' type with around 40% but median price for them are lowest.\n* 'Fair' cut is least found and hence has high price range comparitively.\n\n**COLOR** \n* G,E,F are easily available colors and economical too.\n* I,J color type will cost you more bucks :)\n\n**CLARITY** \n* SI1 and VS2 are the most often found clarities in diamonds but need to pay more for SI1 clarity than VS2.\n\n**Note :**\n* Price exception are for all cases irrespective of cut,color,clarity.\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"cut_clarity=pd.pivot_table(diamond, values='price', columns='cut', index='clarity', aggfunc='median')\ncut_color=pd.pivot_table(diamond, values='price', columns='cut', index='color', aggfunc='median')\nclarity_color=pd.pivot_table(diamond, values='price', columns='clarity', index='color', aggfunc='median')\n\nprint(\"Combination of Color,Cut,Clarity with median price \")\nplt.figure(figsize=(20,5))\nplt.subplot(131)\nsns.heatmap(cut_clarity,cmap='Blues')\n\nplt.subplot(132)\nsns.heatmap(cut_color,cmap='Greys')\n\nplt.subplot(133)\nsns.heatmap(clarity_color,cmap='Blues')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Combination of Premium cut,SI1 clarity and H,I,J color makes diamond costly.\n* IF cut generally doesn't attract more money, but its combination with D color were sold foe higher median price.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\ndiamond.corr()['price'].sort_values()[:-1].plot.barh()\nplt.title('Order of dependence of price on Numerical Features')\nplt.xlabel('Correaltion coefficient with Price')\nplt.ylabel('Feature')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Carat and x,y,z feature are deciding factor for price calculation. \n* Table and depth has very little effect in price of diamond.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"list=['carat', 'depth', 'table', 'x', 'y', 'z']\nplt.figure(figsize=(15,10))\nfor i,feature in enumerate(list,1):\n    plt.subplot(2,3,i)\n    sns.scatterplot(diamond[feature],diamond['price'])\n    plt.title(feature)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Trend of price with carat,x,y,z can be easily seen from scatterplots.\n* While Depth and Table concentrated more in their particular range and have no strong pattern with price.\n* Some outliers are clearly visible in scatterplots and need to be treated.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"list=['carat', 'depth', 'table', 'x', 'y', 'z']\nplt.figure(figsize=(15,8))\nfor i,feature in enumerate(diamond.select_dtypes(exclude='O').columns.tolist(),1):\n    plt.subplot(3,3,i)\n    plt.hist(diamond[feature],bins=100)\n    plt.title(feature)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Carat, price and x covers a wide distribution with significant skewness.\n* Certainly our numerical features are not normally distributed.","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sns.heatmap(diamond.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Lot of interdependency can be seen between features, as multiple features have high correlation coefficient.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1) Raw OLS model\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"diamond_d=pd.get_dummies(diamond,columns=diamond.select_dtypes(include='O').columns.tolist(),drop_first=True)\n\nX=diamond_d.drop('price',axis=1)\ny=diamond.price\n\nXc = sm.add_constant(X)\nlin_reg = sm.OLS(y,Xc).fit()\nlin_reg.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_results = pd.DataFrame(columns=['Description','R2','Adjusted_R2','MSE_Model','MSE_Resid'])\n\ndescription = \"Raw OLS\"\nr2=round(lin_reg.rsquared,3)\nadjR2=round(lin_reg.rsquared_adj,3)\nmsem=round(lin_reg.mse_model,2)\nmser=round(lin_reg.mse_resid,2)\ndf_results = pd.concat([df_results,pd.DataFrame(np.array([description,r2,adjR2,msem,mser]).reshape(1,-1),\n            columns=['Description','R2','Adjusted_R2','MSE_Model','MSE_Resid'])], axis=0)\n\ndf_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2) OLS model after log transformation of Price\n","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X\ny_log=y.apply(np.log)\n\nXc = sm.add_constant(X)\nlin_reg_log = sm.OLS(y_log,Xc).fit()\nlin_reg_log.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"description = \"OLS + Log Price\"\nr2=round(lin_reg_log.rsquared,3)\nadjR2=round(lin_reg_log.rsquared_adj,3)\nmsem=round(lin_reg_log.mse_model,2)\nmser=round(lin_reg_log.mse_resid,2)\ndf_results = pd.concat([df_results,pd.DataFrame(np.array([description,r2,adjR2,msem,mser]).reshape(1,-1),\n            columns=['Description','R2','Adjusted_R2','MSE_Model','MSE_Resid'])], axis=0)\n\ndf_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking the Asumptions of Linear Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For Linear Regression, we need to check if the 5 major assumptions hold.\n\n1. No Auto correlation\n2. Normality of error terms\n3. Linearity of variables \n4. No Heteroscedacity\n5. No strong MultiCollinearity\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Assumption 1- No autocorrelation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\nTest done : Durbin- Watson Test.\n\n- It's value ranges from 0-4. \n- If the value of Durbin- Watson is Between 0-2, it's known as Positive Autocorrelation.\n- If the value ranges from 2-4, it is known as Negative autocorrelation.\n- If the value is exactly 2 (generally can take range from 1.5 to 2.5), it means No Autocorrelation .\n- For a good linear model, it should have low or no autocorrelation.\n\nwe can see here the values of dublin watson test (From OLS summary) : \n\n1.190 (1st case),\n\n1.533 (2nd case)\n\nHence no autocorrelation before and after log transformation of price.\n    \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.tsa.api as smt\n\nacf = smt.graphics.plot_acf(lin_reg.resid, lags=40 , alpha=0.05)\nacf.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph below, we can easily see that there is no autocorrelation.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Assumption 2- Normality of Residuals","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Test done : Jarque Bera test.\n\n* The Jarque–Bera test is a goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution.\n* A large value for the jarque-bera test indicates non normality.\n    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nprint('JB value before log Transformation of price :',stats.jarque_bera(lin_reg.resid)[0])\nprint('JB value  after log Transformation of price :',stats.jarque_bera(lin_reg_log.resid)[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The critical chi square value at the 5% level of significance is 5.99. \n* In both case the computed value of the JB statistic is greater than 5.99. \n* Thus we reject the null hypothesis and hence the error terms are not normally distributed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,4))\nplt.subplot(121)\nplt.title('Before log Transformation of price')\nsns.distplot(lin_reg.resid)\nplt.subplot(122)\nplt.title('After log Transformation of price')\nsns.distplot(lin_reg_log.resid)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Residuals have high kurtosis value, which makes it non linear.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Asssumption 3 - Linearity of residuals\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Test done : Rainbow test ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* It is done to check the linearity of the residuals for a linear regression model.\n* Linearity of residuals is preferred.\n\n $$ H_{0} : Residuals ~are~ linear $$\n $$ H_{1} : Residuals~ are ~not ~linear $$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\n#checking 50% of fraction of data, since sample of data can be linear.\nlstat1,pvalue1=sm.stats.diagnostic.linear_rainbow(res=lin_reg, frac=0.5)  \nlstat2,pvalue2=sm.stats.diagnostic.linear_rainbow(res=lin_reg_log, frac=0.5)  \n\nprint('The pvalue before log Transformation of price :',round(pvalue1,3))\nprint('The pvalue after log Transformation of price :',round(pvalue2,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As pvalue > 0.05, We failed to reject H0, Hence residuals are linear.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.stats as stats\nimport pylab\nfrom statsmodels.graphics.gofplots import ProbPlot\n\nprint('Probability Plot:\\n 1)Before\\n 2)After log Transformation of price')\nplt.figure(figsize=(15,5))\nplt.subplot(121)\nst_residual = lin_reg.get_influence().resid_studentized_internal\nstats.probplot(st_residual, dist=\"norm\", plot = pylab)\n\nplt.subplot(122)\nst_residual = lin_reg_log.get_influence().resid_studentized_internal\nstats.probplot(st_residual, dist=\"norm\", plot = pylab)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Points are symmetrically distributed around a diagonal line in both case(more after log Transformation of price).\n* Hence linearity of residuals can be seen.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The mean value of residuals should be zero.\\n')\nprint('The mean value of residuals before log Transformation of price :',lin_reg.resid.mean())\nprint('The mean value of residuals after log Transformation of price :',lin_reg_log.resid.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Very much close to zero in both case, So linearity is present.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Assumption 4 -  Homoscedasticity_test \n\nTest done : Goldfeld Test\n\n* If the variance of the residuals are symmetrically distributed across the regression line , then the data is said to homoscedastic and the residuals should be homoscedastic.\n* This test is based on the hytpothesis testing where null and alternate hypothesis are:\n$$ H_{0} : \\sigma_{u_{i}}~is~constant~across~the~range~of~data $$\n\n$$ H_{a} : \\sigma_{u_{i}}~is~not~constant~across~the~range~of~data $$\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.stats.api as sms\n\npvalue1 = sms.het_goldfeldquandt(lin_reg.resid, lin_reg.model.exog)[1]\npvalue2 = sms.het_goldfeldquandt(lin_reg_log.resid, lin_reg_log.model.exog)[1]\n\nprint('The pvalue before log Transformation of price :',round(pvalue1,3))\nprint('The pvalue after log Transformation of price :',round(pvalue2,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, p value > 0.05 so we failed to reject null hypothesis, it is homoscedasticity distribution.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Assumption 5- NO  MULTI COLLINEARITY","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* As, we can see from the second warning of OLS model itself that Multicollinearity is present in the dataset.\n* It can also be stated by noting the high conditional number.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = [variance_inflation_factor(Xc.values, i) for i in range(Xc.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=X.columns).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The interpretation of VIF is as follows: the square root of a given variable’s VIF shows how much larger the standard error is, compared with what it would be if that predictor were uncorrelated with the other features in the model. If no features are correlated, then all values for VIF will be 1.\n* So, multicollinearity exists here as vif values are high.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##  Dropping feature with highest VIF value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diamond_d2=diamond_d.drop('x',axis=1)\n\nX2=diamond_d2.drop('price',axis=1)\ny=diamond.price\n\nXc2 = sm.add_constant(X2)\nlin_reg2 = sm.OLS(y,Xc2).fit()\n\ndescription = \"OLS + without X feature \"\nr2=round(lin_reg2.rsquared,3)\nadjR2=round(lin_reg2.rsquared_adj,3)\nmsem=round(lin_reg2.mse_model,2)\nmser=round(lin_reg2.mse_resid,2)\ndf_results = pd.concat([df_results,pd.DataFrame(np.array([description,r2,adjR2,msem,mser]).reshape(1,-1),\n            columns=['Description','R2','Adjusted_R2','MSE_Model','MSE_Resid'])], axis=0)\n\ndf_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## OLS + without X feature + Log Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X2\ny_log\n\nXc3 = sm.add_constant(X2)\nlin_reg3 = sm.OLS(y_log,Xc3).fit()\n\ndescription = \"OLS + without X feature + Log Price\"\nr2=round(lin_reg3.rsquared,3)\nadjR2=round(lin_reg3.rsquared_adj,3)\nmsem=round(lin_reg3.mse_model,2)\nmser=round(lin_reg3.mse_resid,2)\ndf_results = pd.concat([df_results,pd.DataFrame(np.array([description,r2,adjR2,msem,mser]).reshape(1,-1),\n            columns=['Description','R2','Adjusted_R2','MSE_Model','MSE_Resid'])], axis=0)\n\ndf_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check VIF again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\npd.set_option('Display.max_columns',None)\nvif = [variance_inflation_factor(Xc3.values, i) for i in range(Xc3.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=X2.columns).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Still multicolinearity can be seen in some features, so we will be dropping those feature and will check performance of model again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## OLS + No multicolinear Feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diamond_d4=diamond_d.drop(['z','cut_Ideal','clarity_SI1'],axis=1)\n\nX4=diamond_d4.drop('price',axis=1)\ny=diamond.price\n\nXc4 = sm.add_constant(X4)\nlin_reg4 = sm.OLS(y,Xc4).fit()\n\ndescription = \"OLS + No multicolinear Feature \"\nr2=round(lin_reg4.rsquared,3)\nadjR2=round(lin_reg4.rsquared_adj,3)\nmsem=round(lin_reg4.mse_model,2)\nmser=round(lin_reg4.mse_resid,2)\ndf_results = pd.concat([df_results,pd.DataFrame(np.array([description,r2,adjR2,msem,mser]).reshape(1,-1),\n            columns=['Description','R2','Adjusted_R2','MSE_Model','MSE_Resid'])], axis=0)\n\ndf_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## OLS + No multicolinear Feature + Log Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X4\ny_log\n\nXc5 = sm.add_constant(X4)\nlin_reg5 = sm.OLS(y_log,Xc5).fit()\n\ndescription = \"OLS + No multicolinear Feature + Log Price\"\nr2=round(lin_reg5.rsquared,3)\nadjR2=round(lin_reg5.rsquared_adj,3)\nmsem=round(lin_reg5.mse_model,2)\nmser=round(lin_reg5.mse_resid,2)\ndf_results = pd.concat([df_results,pd.DataFrame(np.array([description,r2,adjR2,msem,mser]).reshape(1,-1),\n            columns=['Description','R2','Adjusted_R2','MSE_Model','MSE_Resid'])], axis=0)\n\ndf_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From above results we can infer that taking log transformation on price feature is giving better results in all cases.\n* R2 and AdjustedR2 values are almost similar.\n* OLS model with Log of price can be considered for further studies as it is giving best result in predicting price of diamonds.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Do Upvote if you liked my work.**\n\n**Happy learning!!**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}