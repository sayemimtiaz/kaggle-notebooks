{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Prediction of Credit Card Default\n\n### Tutorial for beginners in classification analysis using scikit-learn."},{"metadata":{},"cell_type":"markdown","source":"This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\n\n\n# Table of Content\n\n* [Objectives](#obj)\n* [Importing packages and loading data](#imp)\n* [Feature Engineering](#fe)\n* [Exploratory Data Analysis (EDA)](#eda)\n    * [Mapping the target: categorizing](#map)\n    * [Descriptive Statistics](#stat)\n    * [Standardizing and plotting features](#std)\n    * [Correlation](#corr)\n* [Machine Learning: Classification models](#ml)\n    * [Feature Selection](#fs)\n    * [Spliting the data: train and test](#sp)\n    * [Logistic Regression (original data)](#lr1)\n    * [Logistic Regression (standardized features)](#lr2)\n    * [Logistic Regression (most important features)](#lr3)\n    * [ExtraTree-decision](#tree)\n    * [Random-Forest Classifier](#rf)\n* [Comparison of model performance](#sum)\n    * [Receiver operating characteristic (ROC) Curve](#roc)\n    * [Mean Accuracy (coss-validation)](#ac)\n    * [Precision, Recall, F1-score](#m)"},{"metadata":{},"cell_type":"markdown","source":"<a id='obj'></a>\n## Objectives:<br>\n-       Identify the key drivers that determine the likelihood of credit card default.\n-       Predict the likelihood of credit card default for customers of the Bank."},{"metadata":{},"cell_type":"markdown","source":"<a id='imp'></a>\n## Importing packages and loading data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# here we will import the libraries used for machine learning\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy.stats import randint\nimport pandas as pd # data processing, CSV file I/O, data manipulation \nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph. \nfrom pandas import set_option\nplt.style.use('ggplot') # nice plots\n\nfrom sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.linear_model import LogisticRegression # to apply the Logistic regression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import KFold # for cross validation\nfrom sklearn.model_selection import GridSearchCV # for tuning parameter\nfrom sklearn.model_selection import RandomizedSearchCV  # Randomized search on hyper parameters.\nfrom sklearn.preprocessing import StandardScaler # for normalization\nfrom sklearn.pipeline import Pipeline \nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn import metrics # for the check the error and accuracy of the model\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport os\n#print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/UCI_Credit_Card.csv')\ndata.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are variables that need to be converted to categories:\n* __SEX:__ Gender   \n                    1 = male \n                    2 = female\n* __EDUCATION:__     \n                     1 = graduate school \n                     2 = university \n                     3 = high school \n                     4 = others \n                     5 = unknown \n                     6 = unknown\n* __MARRIAGE:__ Marital status \n                    1 = married\n                    2 = single\n                    3 = others\n* __PAY_0,2,3,4,5,6:__ Repayment status in September 2005, August 2005, July 2005, June 2005, May 2005, April 2005 (respectivey)\n                    -2= no consumption\n                    -1= pay duly\n                    1 = payment delay for one month\n                    2 = payment delay for two months\n                    ... \n                    8 = payment delay for eight months\n                    9 = payment delay for nine months and above"},{"metadata":{},"cell_type":"markdown","source":"<a id='fe'></a>\n## Feature engineering\n\nThe data has been already encoded and cleaned. However, some categorical data have repeated categories. For instance, the variable ‘education’ has three categories with similar information:<br>\n4: others, 5: unknown, and 6: unknown<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.rename(columns={\"default.payment.next.month\": \"Default\"}, inplace=True)\ndata.drop('ID', axis = 1, inplace =True) # drop column \"ID\"\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating features and target\ny = data.Default     # target default=1 or non-default=0\nfeatures = data.drop('Default', axis = 1, inplace = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['EDUCATION'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The categories 4:others, 5:unknown, and 6:unknown can be grouped into a single class '4'."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['EDUCATION']=np.where(data['EDUCATION'] == 5, 4, data['EDUCATION'])\ndata['EDUCATION']=np.where(data['EDUCATION'] == 6, 4, data['EDUCATION'])\ndata['EDUCATION']=np.where(data['EDUCATION'] == 0, 4, data['EDUCATION'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After grouping, the education column has the following categories:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['EDUCATION'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"Similarly, the column 'marriage' should have three categories: 1 = married, 2 = single, 3 = others but it contains a category '0' which will be joined to the category '3'."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['MARRIAGE'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['MARRIAGE']=np.where(data['MARRIAGE'] == 0, 3, data['MARRIAGE'])\ndata['MARRIAGE'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='eda'></a>\n## Exploratory Data Analysis (EDA)\n\n<a id='map'></a>\n### Mapping the target: categorizing\n\nFrom this sample of 30,000 credit card holders, there were 6,636 default credit cards; that is, the proportion of default in the data is 22,1%."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The frequency of defaults\nyes = data.Default.sum()\nno = len(data)-yes\n\n# Percentage\nyes_perc = round(yes/len(data)*100, 1)\nno_perc = round(no/len(data)*100, 1)\n\nimport sys \nplt.figure(figsize=(7,4))\nsns.set_context('notebook', font_scale=1.2)\nsns.countplot('Default',data=data, palette=\"Blues\")\nplt.annotate('Non-default: {}'.format(no), xy=(-0.3, 15000), xytext=(-0.3, 3000), size=12)\nplt.annotate('Default: {}'.format(yes), xy=(0.7, 15000), xytext=(0.7, 3000), size=12)\nplt.annotate(str(no_perc)+\" %\", xy=(-0.3, 15000), xytext=(-0.1, 8000), size=12)\nplt.annotate(str(yes_perc)+\" %\", xy=(0.7, 15000), xytext=(0.9, 8000), size=12)\nplt.title('COUNT OF CREDIT CARDS', size=14)\n#Removing the frame\nplt.box(False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='stat'></a>\n### Descriptive Statistics\nThe table below shows the descriptive statistics of the variables of this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"set_option('display.width', 100)\nset_option('precision', 2)\n\nprint(\"SUMMARY STATISTICS OF NUMERIC COLUMNS\")\nprint()\nprint(data.describe().T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average value for the amount of credit card limit is 167,484 NT dollars. The standard deviation is 129,747 NT dollars, ranging from 10,000 to 1M NT dollars.<br>\nEducation level is mostly graduate school (1) and university (2). Most of the clients are either marrined or single (less frequent the other status). Average age is 35.5 years, with a standard deviation of 9.2 years."},{"metadata":{},"cell_type":"markdown","source":"### Frequency of explanatory variables by defaulted and non-defaulted cards"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new dataframe with categorical variables\nsubset = data[['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', \n               'PAY_5', 'PAY_6', 'Default']]\n\nf, axes = plt.subplots(3, 3, figsize=(20, 15), facecolor='white')\nf.suptitle('FREQUENCY OF CATEGORICAL VARIABLES (BY TARGET)')\nax1 = sns.countplot(x=\"SEX\", hue=\"Default\", data=subset, palette=\"Blues\", ax=axes[0,0])\nax2 = sns.countplot(x=\"EDUCATION\", hue=\"Default\", data=subset, palette=\"Blues\",ax=axes[0,1])\nax3 = sns.countplot(x=\"MARRIAGE\", hue=\"Default\", data=subset, palette=\"Blues\",ax=axes[0,2])\nax4 = sns.countplot(x=\"PAY_0\", hue=\"Default\", data=subset, palette=\"Blues\", ax=axes[1,0])\nax5 = sns.countplot(x=\"PAY_2\", hue=\"Default\", data=subset, palette=\"Blues\", ax=axes[1,1])\nax6 = sns.countplot(x=\"PAY_3\", hue=\"Default\", data=subset, palette=\"Blues\", ax=axes[1,2])\nax7 = sns.countplot(x=\"PAY_4\", hue=\"Default\", data=subset, palette=\"Blues\", ax=axes[2,0])\nax8 = sns.countplot(x=\"PAY_5\", hue=\"Default\", data=subset, palette=\"Blues\", ax=axes[2,1])\nax9 = sns.countplot(x=\"PAY_6\", hue=\"Default\", data=subset, palette=\"Blues\", ax=axes[2,2]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1 = list(data[data['Default'] == 1]['LIMIT_BAL'])\nx2 = list(data[data['Default'] == 0]['LIMIT_BAL'])\n\nplt.figure(figsize=(12,4))\nsns.set_context('notebook', font_scale=1.2)\n#sns.set_color_codes(\"pastel\")\nplt.hist([x1, x2], bins = 40, normed=False, color=['steelblue', 'lightblue'])\nplt.xlim([0,600000])\nplt.legend(['Yes', 'No'], title = 'Default', loc='upper right', facecolor='white')\nplt.xlabel('Limit Balance (NT dollar)')\nplt.ylabel('Frequency')\nplt.title('LIMIT BALANCE HISTOGRAM BY TYPE OF CREDIT CARD', SIZE=15)\nplt.box(False)\nplt.savefig('ImageName', format='png', dpi=200, transparent=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 30,000 credit card clients.\n\nThe average value for the amount of credit card limit is 167,484 NT dollars. The standard deviation is 129,747 NT dollars, ranging from 10,000 to 1M NT dollars.\n\nEducation level is mostly graduate school and university.\n\nMost of the clients are either marrined or single (less frequent the other status).\n\nAverage age is 35.5 years, with a standard deviation of 9.2.\n\nAs the value 0 for default payment means 'not default' and value 1 means 'default', the mean of 0.221 means that there are 22.1% of credit card contracts that will default next month (will verify this in the next sections of this analysis)."},{"metadata":{"trusted":true},"cell_type":"code","source":"Repayment = data[['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']]\n\nRepayment = pd.concat([y,Repayment],axis=1)\nRepayment = pd.melt(Repayment,id_vars=\"Default\",\n                    var_name=\"Repayment_Status\",\n                    value_name='value')\n\nplt.figure(figsize=(10,5))\nsns.set_context('notebook', font_scale=1.2)\nsns.boxplot(y=\"value\", x=\"Repayment_Status\", hue=\"Default\", data=Repayment, palette='Blues')\nplt.legend(loc='best', title= 'Default', facecolor='white')\nplt.xlim([-1.5,5.5])\nplt.title('REPAYMENT STATUS - BOXPLOT', size=14)\nplt.box(False)\nplt.savefig('ImageName', format='png', dpi=200);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that PAY_0 (Repayment status in September) and PAY_2 (Repayment status in August)  have more discriminatory power the repayment status in other months."},{"metadata":{},"cell_type":"markdown","source":"<a id='std'></a>\n### Standardizing and plotting the data\nThe boxplot below reveals that features are in different scales and units. Many models use some form of distance when making predictions, and therefore, it is recommended to normalize the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"## data are distributed in a wide range (below), need to be normalizded.\nplt.figure(figsize=(15,3))\nax= data.drop('Default', axis=1).boxplot(data.columns.name, rot=90)\noutliers = dict(markerfacecolor='b', marker='p')\nax= features.boxplot(features.columns.name, rot=90, flierprops=outliers)\nplt.xticks(size=12)\nax.set_ylim([-5000,100000])\nplt.box(False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Standardization of data was performed; i.e, all features are centered around zero and have variance one. Features were plotted again, using a violin plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"stdX = (features - features.mean()) / (features.std())              # standardization\ndata_st = pd.concat([y,stdX.iloc[:,:]],axis=1)\ndata_st = pd.melt(data_st,id_vars=\"Default\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,10))\nsns.set_context('notebook', font_scale=1)\nsns.violinplot(y=\"value\", x=\"features\", hue=\"Default\", data=data_st,split=True, \n               inner=\"quart\", palette='Blues')\nplt.legend(loc=4, title= 'Default', facecolor='white')\nplt.ylim([-3,3])\nplt.title('STANDARDIZED FEATURES - VIOLIN PLOT', size=14)\nplt.box(False)\nplt.savefig('ImageName', format='png', dpi=200, transparent=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='corr'></a>\n### Correlation\nA correlation matrix of all variables is shown in the heatmap below. The only feature with a notable positive correlation with the dependent variable ‘Default’ is re-payment status during the last month (September). The highest negative correlation with default occurs with Limit_Balance, indicating that customers with lower limit balance are more likely to default. It can also be observed that some variables are highly correlated to each other, that is the case of the amount of bill statement and the repayment status in different months.\n\nLooking at correlations matrix, defined via Pearson function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#  looking at correlations matrix, defined via Pearson function  \ncorr = data.corr() # .corr is used to find corelation\nf,ax = plt.subplots(figsize=(8, 7))\nsns.heatmap(corr, cbar = True,  square = True, annot = False, fmt= '.1f', \n            xticklabels= True, yticklabels= True\n            ,cmap=\"coolwarm\", linewidths=.5, ax=ax)\nplt.title('CORRELATION MATRIX - HEATMAP', size=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The heatmat shows that features are correlated with each other (collinearity), such us like PAY_0,2,3,4,5,6 and BILL_AMT1,2,3,4,5,6. In those cases, the correlation is positive.\n\nUncorrelated data are poentially more useful: discriminatory!\n\n**What do correlations mean?**<br>\n\nLets separately fit correlated and uncorrelated data via linear regression: "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='LIMIT_BAL', y= 'BILL_AMT2', data = data, hue ='Default', \n           palette='coolwarm')\nplt.title('Linear Regression: distinguishing between Default and Non-default', size=16)\n\n\nsns.lmplot(x='BILL_AMT1', y= 'BILL_AMT2', data = data, hue ='Default', \n           palette='coolwarm')\nplt.title('Linear Regression: Cannot distinguish between Default and Non-default', size=16);\n\nprint('Uncorrelated data are poentially more useful: discrimentory!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='ml'></a>\n## Machine Learning: Classification models\n\nThe classification models used for this analysis are: Logistic Regression, Decision Tree and Random Forest Classifier.<br>\n\nTo build machine learning models the original data was divided into features (X) and target (y) and then split into train (80%) and test (20%) sets. Thus, the algorithms would be trained on one set of data and tested out on a completely different set of data (not seen before by the algorithm).\n\n<a id='sp'></a>\n### Spliting the data into train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Original dataset\nX = data.drop('Default', axis=1)  \ny = data['Default']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset with standardized features\nXstd_train, Xstd_test, ystd_train, ystd_test = train_test_split(stdX,y, test_size=0.2, stratify=y,\n                                                                random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='fs'></a>\n### Feature Selection\n\n#### Recursive Feature Elimination\nRecursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_FEATURES = 3\nmodel = LogisticRegression()\nrfe_stand = RFE(model, NUM_FEATURES)\nfit_stand = rfe_stand.fit(stdX, y)\n#print(\"St Model Num Features:\", fit_stand.n_features_)\n#print(\"St Model Selected Features:\", fit_stand.support_)\nprint(\"Std Model Feature Ranking:\", fit_stand.ranking_)\n# calculate the score for the selected features\nscore_stand = rfe_stand.score(stdX,y)\nprint(\"Standardized Model Score with selected features is: %f (%f)\" % (score_stand.mean(), score_stand.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = np.array(features.columns)\nprint('Most important features (RFE): %s'% feature_names[rfe_stand.support_])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Repayment status in September (PAY_0)\n* Amount of bill statement in September (BILL_AMT1)\n* Amount of previous payments in August (PAY_AMT2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset with three most important features\nXimp = stdX[['PAY_0', 'BILL_AMT1', 'PAY_AMT2']]\nX_tr, X_t, y_tr, y_t = train_test_split(Ximp,y, test_size=0.2, stratify=y, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='lr1'></a>\n### Logistic Regression (original data)\n\nLogistic Regression is one of the simplest algorithms which estimates the relationship between one dependent binary variable and independent variables, computing the probability of occurrence of an event. The regulation parameter C controls the trade-off between increasing complexity (overfitting) and keeping the model simple (underfitting). For large values of C, the power of regulation is reduced and the model increases its complexity, thus overfitting the data.<br>\n\nThe parameter ‘C’ was tuned using RandomizedSearchCV( ) for the different datasets: original, standardized and with most important features. Once the parameter ‘C’ was defined for each dataset, the logistic regression model initiated and then fitted to the training data, as it was described in the methodology."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup the hyperparameter grid, (not scaled data)\nparam_grid = {'C': np.logspace(-5, 8, 15)}\n\n# Instantiate a logistic regression classifier\nlogreg = LogisticRegression()\n\n# Instantiate the RandomizedSearchCV object\nlogreg_cv = RandomizedSearchCV(logreg,param_grid , cv=5, random_state=0)\n\n# Fit it to the data\nlogreg_cv.fit(X_train, y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression(C=0.00005, random_state=0)\nLR.fit(X_train, y_train)\ny_pred = LR.predict(X_test)\nprint('Accuracy:', metrics.accuracy_score(y_pred,y_test))\n\n## 5-fold cross-validation \ncv_scores =cross_val_score(LR, X, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint()\nprint(classification_report(y_test, y_pred))\nprint()\nprint(\"Average 5-Fold CV Score: {}\".format(round(np.mean(cv_scores),4)),\n      \", Standard deviation: {}\".format(round(np.std(cv_scores),4)))\n\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(y_test,LR.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix - Logistic Regression\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model has not power predicting default credit cards. However, it can be observed that the average accuracy of the model is about 78%, which demonstrates that this metrics is not appropriate for the evaluation of this problem. "},{"metadata":{},"cell_type":"markdown","source":"<a id='lr2'></a>\n### Logistic Regression (standardized features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the RandomizedSearchCV object:\nlogreg_cv_std = RandomizedSearchCV(logreg,param_grid , cv=5, random_state=0)\n\n# Fit it to the standardized data\nlogreg_cv_std.fit(Xstd_train, ystd_train)\n\n# Print the tuned parameters \nprint(\"Tuned Logistic Regression Parameters with standardized features: {}\".format(logreg_cv_std.best_params_)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LRS = LogisticRegression(C=3.73, random_state=0)\nLRS.fit(Xstd_train, ystd_train)\ny_pred = LRS.predict(Xstd_test)\nprint('Accuracy:', metrics.accuracy_score(y_pred,ystd_test))\n\n## 5-fold cross-validation \ncv_scores =cross_val_score(LRS, stdX, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint()\nprint(classification_report(ystd_test, y_pred))\nprint()\nprint(\"Average 5-Fold CV Score: {}\".format(round(np.mean(cv_scores),4)),\n      \", Standard deviation: {}\".format(round(np.std(cv_scores),4)))\n\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(ystd_test,LRS.predict(Xstd_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix - Logistic Regression with standardized data\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance of the logistic regression model standardized data improved compared to the model built with the original dataset. By using the standardized dataset, the model is able to predict defaults; however, with a very low recall (0.24)."},{"metadata":{},"cell_type":"markdown","source":"<a id='lr3'></a>\n### Logistic Regression (most important features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_imp = LogisticRegression(C=3.73, random_state=0)\nLR_imp.fit(X_tr, y_tr)\ny_pred = LR_imp.predict(X_t)\nprint('Accuracy:', metrics.accuracy_score(y_pred,y_t))\n\n## 5-fold cross-validation \ncv_scores =cross_val_score(LR_imp, Ximp, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint()\nprint(classification_report(y_t, y_pred))\nprint()\nprint(\"Average 5-Fold CV Score: {}\".format(round(np.mean(cv_scores),4)),\n      \", Standard deviation: {}\".format(round(np.std(cv_scores),4)))\n\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(y_t,LR_imp.predict(X_t))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix - Logistic Regression (most important features)\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When using just important features for building the model, there is a slight improvement in performance with respect to the previous model. Therefore, taking into account three features only, the model has the same predictive power than using 24 features. Thus, future strategies should be focused on: repayment status in September (PAY_0), amount of bill statement in September (BILL_AMT1), and amount of previous payments in August (PAY_AMT2)."},{"metadata":{},"cell_type":"markdown","source":"<a id='tree'></a>\n### Decision Tree Classifier\nDecision Tree is another very popular algorithm for classification problems because it is easy to interpret and understand. An internal node represents a feature, the branch represents a decision rule, and each leaf node represents the outcome. Some advantages of decision trees are that they require less data preprocessing, i.e., no need to normalize features. However, noisy data can be easily overfitted and results in biased results when the data set is imbalanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [1,2,3,4,5,6,7,8,9],\n              \"max_features\": [1,2,3,4,5,6,7,8,9],\n              \"min_samples_leaf\": [1,2,3,4,5,6,7,8,9],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_distributions=param_dist, cv=5, random_state=0)\n\n# Fit it to the data\ntree_cv.fit(X_train, y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Tree = DecisionTreeClassifier(criterion= 'gini', max_depth= 7, \n                                     max_features= 9, min_samples_leaf= 2, \n                                     random_state=0)\nTree.fit(X_train, y_train)\ny_pred = Tree.predict(X_test)\nprint('Accuracy:', metrics.accuracy_score(y_pred,y_test))\n\n## 5-fold cross-validation \ncv_scores =cross_val_score(Tree, X, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint()\nprint(classification_report(y_test, y_pred))\nprint()\nprint(\"Average 5-Fold CV Score: {}\".format(round(np.mean(cv_scores),4)), \n      \", Standard deviation: {}\".format(round(np.std(cv_scores),4)))\n\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(y_test,Tree.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix - Decision Tree\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance of the decision tree model improved compared to the logistic regression model showed previously. However, the recall is still low (0.33)."},{"metadata":{},"cell_type":"markdown","source":"<a id='rf'></a>\n### Random Forest Classifier\nRandom forest classifier is comprised of multiple decision trees. It creates different random subset of decision trees from the training set as its predictors and selects the best solution by means of voting. As a result, the Random Forest model avoids overfitting problems. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the random grid\nparam_dist = {'n_estimators': [50,100,150,200,250],\n               \"max_features\": [1,2,3,4,5,6,7,8,9],\n               'max_depth': [1,2,3,4,5,6,7,8,9],\n               \"criterion\": [\"gini\", \"entropy\"]}\n\nrf = RandomForestClassifier()\n\nrf_cv = RandomizedSearchCV(rf, param_distributions = param_dist, \n                           cv = 5, random_state=0, n_jobs = -1)\n\nrf_cv.fit(X, y)\n\nprint(\"Tuned Random Forest Parameters: %s\" % (rf_cv.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ran = RandomForestClassifier(criterion= 'gini', max_depth= 6, \n                                     max_features= 5, n_estimators= 150, \n                                     random_state=0)\nRan.fit(X_train, y_train)\ny_pred = Ran.predict(X_test)\nprint('Accuracy:', metrics.accuracy_score(y_pred,y_test))\n\n## 5-fold cross-validation \ncv_scores =cross_val_score(Ran, X, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint()\nprint(classification_report(y_test, y_pred))\nprint()\nprint(\"Average 5-Fold CV Score: {}\".format(round(np.mean(cv_scores),4)),\n      \", Standard deviation: {}\".format(round(np.std(cv_scores),4)))\n\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(y_test,Ran.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix - Random Forest\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='sum'></a>\n## Comparison of model performance\nThe metrics used to evaluate performance of the different models: accuracy, precision, recall, f1-score, AUC (ROC), and confusion matrix were employed."},{"metadata":{},"cell_type":"markdown","source":"<a id='roc'></a>\n### Receiver operating characteristic (ROC) Curve\nReceiver Operating Characteristic (ROC) curve is a plot of the true positive rate against the false positive rate. The AUC is the Area Under Curve. If the AUC is high, the model is better distinguishing between positive and negative class. The ROC curve is plotted with “True Positive Rate” or Recall (on the y-axis) against the “False Positive Rate” (on the x-axis). When the AUC is 0.5 means that the model has no discrimination capacity to distinguish between positive and negative class.\n\nThe Receiver operating characteristic (ROC) Curve with the respective area under the curve (AUC) are shown below for each model."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba_RF = Ran.predict_proba(X_test)[::,1]\nfpr1, tpr1, _ = metrics.roc_curve(y_test,  y_pred_proba_RF)\nauc1 = metrics.roc_auc_score(y_test, y_pred_proba_RF)\n\ny_pred_proba_DT = Tree.predict_proba(X_test)[::,1]\nfpr2, tpr2, _ = metrics.roc_curve(y_test,  y_pred_proba_DT)\nauc2 = metrics.roc_auc_score(y_test, y_pred_proba_DT)\n\ny_pred_proba_LR = LR.predict_proba(X_test)[::,1]\nfpr3, tpr3, _ = metrics.roc_curve(y_test,  y_pred_proba_LR)\nauc3 = metrics.roc_auc_score(y_test, y_pred_proba_LR)\n\ny_pred_proba_LRS = LRS.predict_proba(Xstd_test)[::,1]\nfpr4, tpr4, _ = metrics.roc_curve(ystd_test,  y_pred_proba_LRS)\nauc4 = metrics.roc_auc_score(ystd_test, y_pred_proba_LRS)\n\ny_pred_proba_LRimp = LR_imp.predict_proba(X_t)[::,1]\nfpr5, tpr5, _ = metrics.roc_curve(y_t,  y_pred_proba_LRimp)\nauc5 = metrics.roc_auc_score(y_t, y_pred_proba_LRimp)\n\nplt.figure(figsize=(10,7))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr1,tpr1,label=\"Random Forest, auc=\"+str(round(auc1,2)))\nplt.plot(fpr2,tpr2,label=\"Decision Tree, auc=\"+str(round(auc2,2)))\nplt.plot(fpr3,tpr3,label=\"LogReg, auc=\"+str(round(auc3,2)))\nplt.plot(fpr4,tpr4,label=\"LogReg(std), auc=\"+str(round(auc4,2)))\nplt.plot(fpr5,tpr5,label=\"LogReg(Std&Imp), auc=\"+str(round(auc5,2)))\nplt.legend(loc=4, title='Models', facecolor='white')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC', size=15)\nplt.box(False)\nplt.savefig('ImageName', format='png', dpi=200, transparent=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest AUC is obtained for the Random Forest Classifier model, with a value of 0.77. This means there is 77% chance that the model will be able to distinguish between default class and non-default class."},{"metadata":{},"cell_type":"markdown","source":"<a id='ac'></a>\n### Mean Accuracy (coss-validation)\nAccuracy is the ratio of correctly predicted observation to the overall observations and it is one of the most intuitive measurements of performance. However, a high accuracy rate does not always mean we have a perfect model. In fact, it only works well when the datasets are symmetric. It can be misleading when classes are imbalanced.\n\nUsing K-fold cross-validation it is possible to obtain less biased models and avoid overfitting the data. In this case it was used a 5-fold cross-validation, as shown in the code below.\n\ncv_scores = cross_val_score(Model, X, y, cv=5)\n\nAfter cross-validation there are five values of accuracy, so it was calculated the mean and standard deviation of all results."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Append different models\nmodels = []\n\n# Logistic Regression\nmodels.append(('LogReg',\n               LogisticRegression(C=3.73, random_state=0),'none'))\n\n# Logistic Regression (with standardized data)\nmodels.append(('LogReg(Std)',\n               LogisticRegression(C=3.73, random_state=0),'Std'))\n\n# Logistic Regression with standardized and important features\nmodels.append(('LogReg(Std&Imp)',\n               LogisticRegression(C=3.73, random_state=0),'imp'))\n\n# Decision Tree\nmodels.append(('Decision Tree', \n              DecisionTreeClassifier(criterion= 'entropy', max_depth= 4, \n                                     max_features= 7, min_samples_leaf= 8, \n                                     random_state=0),'none'))\n\n# Random Forest Classifier\nmodels.append(('Random Forest', \n              RandomForestClassifier(criterion= 'gini', max_depth= 6, \n                                     max_features= 5, n_estimators= 150, \n                                     random_state=0), 'none'))\n\n# Evaluate each model\nresults = []\nnames = []\nscoring = 'accuracy'\n\nfor name, model, Std in models:\n    if Std == 'Std':\n        cv_results = cross_val_score(model, stdX, y, cv=5, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)       \n    elif Std == 'none':\n        cv_results = cross_val_score(model, X, y, cv=5, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n    else:\n        cv_results = cross_val_score(model, Ximp, y, cv=5, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot\n# Plot all the accuracy results vs. each model \n#(model type on the x-axis and accuracy on the y-axis).\nfig = pyplot.figure(figsize=(10,5))\nsns.set_context('notebook', font_scale=1.1)\nfig.suptitle('Algorithm Comparison - Accuracy (cv=5)')\nax = fig.add_subplot(111)\npyplot.boxplot(results, showmeans=True)\nax.set_xticklabels(names)\nax.set_ylabel('Accuracy')\nax.set_ylim([0.75,1])\nplt.box(False)\nplt.savefig('ImageName', format='png', dpi=200, transparent=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best accuracy is obtained for the Random Forest Classifier with a mean accuracy of 0.82, yet it is the model with higher variation (0.0096). In general, all models have comparable mean accuracy. Nevertheless, because the classes are imbalanced (the proportion of non-default credit cards is higher than default) this metric is misleading. Furthermore, accuracy does not consider the rate of false positives (non-default credits cards that were predicted as default) and false negatives (default credit cards that were incorrectly predicted as non-default). Both cases have negative impact on the bank, since false positives leads to unsatisfied customers and false negatives leads to financial loss. "},{"metadata":{},"cell_type":"markdown","source":"<a id='m'></a>\n### Precision, Recall, F1-score\nThe precision of a model is the ratio TP / (TP + FP). In this case, it is the ability of the classifier not to label as positive a sample that is negative. Precision is a good metric to use when the costs of false positive (FP) is high.\n\nThe recall of a model is the ratio TP / (TP + FN). In this case, it is the ability of the classifier to find all the positive class. Recall is a good metric to use when the cost associated with false negative (FN) is high. In this classification problem there is a high cost for the bank when a default credit card is predicted as non-default, since no actions can be taken. Thus, recall is one important metric to pay attention to.\n\nF1-score is a weighted average of precision and recall. Thus, it considers FP and FN. This metric is very useful when we have uneven class distribution, as it seeks a balance between precision and recall."},{"metadata":{"trusted":true},"cell_type":"code","source":"from astropy.table import Table, Column\ndata_rows = [('Logistic Regression', 'Standardized', 0.79, 0.81, 0.77),\n              ('Logistic Regression', 'Important features', 0.79, 0.81, 0.78),\n              ('Decision Tree', 'original', 0.80, 0.82, 0.79),\n             ('Random Forest', 'original', 0.80, 0.82, 0.80)\n            ]\nt = Table(rows=data_rows, names=('Model', 'Data', 'Precision', 'Recall', 'F1'))\nprint(t)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}