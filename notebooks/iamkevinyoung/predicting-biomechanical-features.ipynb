{"cells":[{"metadata":{"collapsed":true,"_uuid":"e4c99cd6b67d62fc92679fb0e4713f713b695d0e"},"cell_type":"markdown","source":"# Predicting whether a patient's biomechanical features are normal or abnormal\nby Kevin Young\n\nI will be using the dataset provided by UCI Machine Learning.\n\nThe data contains 310 instances of patients' features with six biomechanical attributes which come from the shape and orientation of the pelvis and lumbar spine:\n\n- pelvic incidence\n- pelvic tilt\n- lumbar lordosis angle\n- sacral slope\n- pelvic radius\n- grade of spondylolisthesis"},{"metadata":{"_uuid":"beffd06d9b33accedc381eb5355d8058d80ce7c3"},"cell_type":"markdown","source":"## Preparing the data\nFirst I import the data file into a Pandas dataframe. Fortunatley, there are no missing values in the data set, so we won't have to do any cleaning."},{"metadata":{"trusted":true,"_uuid":"9ba471f1468cf4b035c5196afca9097cde7ade55"},"cell_type":"code","source":"import pandas as pd\n\npatients_data = pd.read_csv('../input/column_2C_weka.csv')\npatients_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f40a932616da6788b416f17d718da8d226fd69f4"},"cell_type":"markdown","source":"Now I convert the dataframes into numpy arrays to be used by scikit_learn. We have one array that contains the class, another array with the feature data and another array with the feature name labels."},{"metadata":{"trusted":true,"_uuid":"baf4c219057992143d46b2dfcbf597018185e2a4"},"cell_type":"code","source":"all_features = patients_data[['pelvic_incidence', 'pelvic_tilt numeric', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis']].values\n\nall_classes = patients_data['class'].values\n\nfeature_names = ['pelvic_incidence', 'pelvic_tilt numeric', 'lumbar_lordosis_angle', 'sacral_slope', 'pelvic_radius', 'degree_spondylolisthesis']\n\nall_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ac3a43d727b00b4e5a968b99107cdce3fb81fb5"},"cell_type":"markdown","source":"Now I will need to normalise the input data."},{"metadata":{"trusted":true,"_uuid":"1a54cc87a670cb310e50ab59894f15cb541875ac"},"cell_type":"code","source":"from sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler()\nall_features_scaled = scaler.fit_transform(all_features)\nall_features_scaled","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b59860e3cd8049afe06f6bf0d069697ae885aea"},"cell_type":"markdown","source":"## Logistic Regression\n\nGiven this is just a binary classification problem, I will first try logistic regression and see how high this accuracy is."},{"metadata":{"trusted":true,"_uuid":"df54e2005567b51c9ea58627f3be311682b56609"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nclf = LogisticRegression()\ncv_scores = cross_val_score(clf, all_features_scaled, all_classes, cv=10)\ncv_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"682ab99b6ee5cbaed8082ca47b078dd76b359bb9"},"cell_type":"markdown","source":"## Decision Trees\n\nBefore using the DecisionTreeClassifier, I will create a train/test split of the data - 80% for training and 20% for testing."},{"metadata":{"trusted":true,"_uuid":"037ad4870f035afec3ecf896f1c3357dd6ae4fd2"},"cell_type":"code","source":"import numpy\nfrom sklearn.model_selection import train_test_split\n\nnumpy.random.seed(1234)\n\n(training_inputs, testing_inputs, training_classes, testing_classes) = train_test_split(all_features_scaled, all_classes, train_size= 0.8, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9897c60f788017ed4bcb71ee1dec604805dd18b"},"cell_type":"markdown","source":"Now I fit a DecisionTreeClassifier to the training data."},{"metadata":{"trusted":true,"_uuid":"7d6ff4f9de76f63cb0d2e6f5b93a7c4d4b13090d"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(random_state=1)\n\nclf.fit(training_inputs, training_classes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f22d4cd814237606a7d3b7d5b61369b8ab233869"},"cell_type":"markdown","source":"Measuring the accuracy of the decision tree model using the test data."},{"metadata":{"trusted":true,"_uuid":"36710d5d495157c5b47c6ce45626465f19a64010"},"cell_type":"code","source":"clf.score(testing_inputs, testing_classes)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"b4f934e9b0a2fa0f1105c73dca9a42ffdf3ba77c"},"cell_type":"markdown","source":"Now I am trying K-Fold cross validation to further help avoid overfitting (K=10)."},{"metadata":{"trusted":true,"_uuid":"cd10a8e9228049a3cd48f61efc1577658e5a2c35"},"cell_type":"code","source":"clf = DecisionTreeClassifier(random_state=1)\n\ncv_scores = cross_val_score(clf, all_features_scaled, all_classes, cv=10)\n\ncv_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94ea370b3f334beb2f373f3d46081da51ebd3c80"},"cell_type":"markdown","source":"Now I will also try a RandomForestClassifier to see if that accuracy is any better."},{"metadata":{"trusted":true,"_uuid":"443b7c1034896258d42ab1182934b61e64b272c6"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=10, random_state=1)\ncv_scores = cross_val_score(clf, all_features_scaled, all_classes, cv=10)\n\ncv_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbafeb4cd8a81b617ad5b9bce4420b28acde6819"},"cell_type":"markdown","source":"## Support Vector Machines\n\nsvm.SVC has different kernels which may vary in performance. I will try linear, rbf, sigmoid and poly and see which results in the highest accuracy."},{"metadata":{"trusted":true,"_uuid":"07a9053505fcbb7826c3440b82bb46b951feffab"},"cell_type":"code","source":"from sklearn import svm\n\nC = 1.0\nsvc = svm.SVC(kernel='linear', C=C)\ncv_scores = cross_val_score(svc, all_features_scaled, all_classes, cv=10)\ncv_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c2512e9a5fd075f5903cbf3b0ab1b8f3e070c85"},"cell_type":"code","source":"svc = svm.SVC(kernel='rbf', C=C)\ncv_scores = cross_val_score(svc, all_features_scaled, all_classes, cv=10)\ncv_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7671cea8195f162085d548b14384beb7268f37a2"},"cell_type":"code","source":"svc = svm.SVC(kernel='sigmoid', C=C)\ncv_scores = cross_val_score(svc, all_features_scaled, all_classes, cv=10)\ncv_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7472cb1cc956b703aa1b46591c46a0b0ca982750"},"cell_type":"code","source":"svc = svm.SVC(kernel='poly', C=C)\ncv_scores = cross_val_score(svc, all_features_scaled, all_classes, cv=10)\ncv_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b0ccad4d8879769315440f8eb7c37a5bfb13fd8"},"cell_type":"markdown","source":"## K-Nearest Neighbours\n\nIn attempt to find the best value of K, I will use a for loop to iterate through different values of K from 1 to 50 and compare the resulting accuracy from each value of K."},{"metadata":{"trusted":true,"_uuid":"d2552d4776e5acbe51c67378180124ab3f00caeb"},"cell_type":"code","source":"from sklearn import neighbors\n\nfor i in range(1, 51):\n    clf = neighbors.KNeighborsClassifier(n_neighbors= i)\n    cv_scores = cross_val_score(clf, all_features_scaled, all_classes, cv=10)\n    print(i, \":\", cv_scores.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"975adc668a8829186e1b975d22e51a7583a1d10b"},"cell_type":"markdown","source":"## Naive Bayes"},{"metadata":{"trusted":true,"_uuid":"95680a462813edfea1cf9662a5f440558fc4c816"},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nscaler = preprocessing.MinMaxScaler()\nall_features_minmax = scaler.fit_transform(all_features)\n\nclf = MultinomialNB()\ncv_scores = cross_val_score(clf, all_features_minmax, all_classes, cv=10)\n\ncv_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cef181ed08fb5d8e11a1e17da2311076de6a583"},"cell_type":"markdown","source":"## Neural Networks"},{"metadata":{"trusted":true,"_uuid":"0bb92c2d1feec830273386e6060cfa2cc150add3"},"cell_type":"code","source":"from tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n\ndef create_model():\n    model = Sequential()\n    model.add(Dense(10, input_dim=6, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(6, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nestimator = KerasClassifier(build_fn=create_model, epochs=100, verbose=0)\ncv_scores = cross_val_score(estimator, all_features_scaled, all_classes, cv=10)\ncv_scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6b5909927304b136a1cd710dcb41b5413fca620"},"cell_type":"markdown","source":"## Conclusion\n\nIt seems the best machine learning model was the model using a SVM with a linear kernel hyperparameter. It yielded the highest accuracy being 83.5%."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}