{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n**Recently, Covid-19 is spreading around the globe. There is an outbreak in almost everywhere around the world. There are lots of researches focusing on analyzing and predicting the diffusing of this kind of fatal virus, the same as I did in this project. The whole project could be separated into two main parts, EDA (Exploratory Data Analysis) and Prediction.**\n\n**The first part includes data aggregation and data visualization. Because of the convenience of invoking, all the actions have been done in Pandas and Numpy. There is another type of data processing building in Pyspark at the top of Databricks.**\n\n**The second part is the biggest difference compared with other same kinds of work. Normally, if people want to involve deep learning, only the LSTM model will be picked. But I also drew on the experience of transfer learning and built my own LSTM base model for Covid-19 in terms of the SARS-2003 dataset.**\n\n**However, given that the lacks of data from both SARS and Covid-19 are irreversible, the final performance for transfer learning model is a little bit weak and it only shows the learning capacity from the base model, but the adjustable ability according to Covid-19 dataset is not enough. So there is quite a long way for this project to use transfer learning in a real prediction data science case.**\n\n\n>#  <font color='Blue'>Contents :</font>\n>1. [Necessary libraries](#0)\n>1. [Data Injection](#1)\n>1. [Data Aggregation & Data Visualization](#2)\n>>    1. [World Epidemic Progress ](#2.1)\n>>    1. [Global Case Map ](#2.2)\n>>    1. [Pie Chart of Global Distribution ](#2.3)\n>>    1. [Comparison between SARS and Covid-19 dataset ](#2.4)\n>>        1. [Confirmed Percentage](#2.41)\n>>        1. [Recovered Percentage](#2.42)\n>1. [Perdiction](#3)\n>>    1. [Base Model](#3.1)\n>>        1. [Feature Extraction](#3.11)\n>>        1. [Compile the Sars Model](#3.12)\n>>        1. [Train the Sars Model](#3.13)\n>>        1. [Learning Curves](#3.14)\n>>    1. [Fine Tuning](#3.2)\n>>        1. [Format the data](#3.21)\n>>        1. [Load the base model](#3.22)\n>>        1. [Freeze the bottom LSTM layers](#3.23)\n>>        1. [Re-train the model with Covid-19 dataset](#3.24)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"0\"></a> <br>\n# Necessary Libraries\n* **Numpy:** Linear algebra\n* **Pandas:** Data processing and aggregation\n* **Matplotlib:** Simple visualization\n* **Plotly:** Interactive plots - World Epidemic Progress \n* **Datetime:** Time data manuplation - Data Analysis, Predictions \n* **Sklearn:** Machine Learning\n* **Keras:** Deep learning - Predictions, LSTM","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime\nimport requests\nimport warnings\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport plotly.express as px\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.models import model_from_json\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n# Data Injection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid19=pd.read_csv('/kaggle/input/novel-corona-virus-2019-dataset/covid_19_data.csv')\ndf_covid19.drop(['SNo','Last Update','Province/State'],axis=1,inplace = True)\ndf_covid19['ObservationDate']=pd.to_datetime(df_covid19['ObservationDate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Because there are several updates in the same day, we need a groupby function to merge the data in the same day \ndf_covid19 = df_covid19.groupby([\"ObservationDate\",\"Country/Region\"],as_index = False).sum()\ndf_covid19_compare = df_covid19\ndf_covid19 = df_covid19.set_index('ObservationDate')\ndf_covid19.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sars = pd.read_csv('../input/sars-2003-complete-dataset-clean/sars_2003_complete_dataset_clean.csv')\ndf_sars.rename(columns={'Date':'ObservationDate', 'Country':'Country/Region', 'Cumulative number of case(s)':'Confirmed', 'Number of deaths':'Deaths','Number recovered':'Recovered' }, inplace=True)\ndf_sars['ObservationDate']=pd.to_datetime(df_sars['ObservationDate'])\ndf_sars_compare = df_sars\ndf_sars = df_sars.set_index('ObservationDate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Sars_CA = df_sars[df_sars['Country/Region'] == 'China']\nSars_CA.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n# Data Aggregation & Data Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2.1\"></a> <br>\n## World Epidemic Progress ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_new=pd.read_csv('/kaggle/input/novel-corona-virus-2019-dataset/covid_19_data.csv')\ncovid19_new['Active'] = covid19_new['Confirmed'] - covid19_new['Deaths'] - covid19_new['Recovered']\ncovid19_new[\"ObservationDate\"] = pd.to_datetime(covid19_new[\"ObservationDate\"])\nprint(\"Active Cases Column Added Successfully\")\ncovid19_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wep = covid19_new.groupby([\"ObservationDate\",\"Country/Region\"])[\"Confirmed\",\"Deaths\",\"Recovered\"].max()\nwep = wep.reset_index()\nwep[\"ObservationDate\"] = wep[\"ObservationDate\"].dt.strftime(\"%m,%d,%Y\")\nwep[\"Country\"] = wep[\"Country/Region\"]\n\nchoro_map = px.choropleth(wep, \n                          locations= \"Country\", \n                          locationmode = \"country names\",\n                          color = \"Confirmed\", \n                          hover_name = \"Country/Region\",\n                          projection = \"natural earth\",\n                          animation_frame = \"ObservationDate\",\n                          color_continuous_scale = \"Blues\",\n                          range_color = [10000,200000])\nchoro_map.update_layout(\n    title_text = 'Global Spread of Coronavirus',\n    title_x = 0.5,\n    geo=dict(\n        showframe = False,\n        showcoastlines = False,\n    ))\n    \nchoro_map.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"covid19_new.rename(columns={'ObservationDate':'Date', 'Country/Region':'Country', 'Province/State':'Province' }, inplace=True)\ncovid19_new['Date']=pd.to_datetime(covid19_new['Date'])\n\nmaxdate=max(covid19_new['Date'])\n\nfondate=maxdate.strftime(\"%Y-%m-%d\")\nprint(\"The last observation date is {}\".format(fondate))\nondate = format(fondate)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2.2\"></a> <br>\n## Global Case Map ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"date_list1 = list(covid19_new[\"Date\"].unique())\nconfirmed = []\ndeaths = []\nrecovered = []\nactive = []\nfor i in date_list1:\n    x = covid19_new[covid19_new[\"Date\"] == i]\n    confirmed.append(sum(x[\"Confirmed\"]))\n    deaths.append(sum(x[\"Deaths\"]))\n    recovered.append(sum(x[\"Recovered\"]))\n    active.append(sum(x[\"Active\"]))\ndata_glob = pd.DataFrame(list(zip(date_list1,confirmed,deaths,recovered,active)),columns = [\"Date\",\"Confirmed\",\"Deaths\",\"Recovered\",\"Active\"])\ndata_glob.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objs as go \ntrace1 = go.Scatter(\nx = data_glob[\"Date\"],\ny = data_glob[\"Confirmed\"],\nmode = \"lines\",\nname = \"Confirmed\",\nline = dict(width = 2.5),\nmarker = dict(color = [0, 1, 2, 3])\n)\n\ntrace2 = go.Scatter(\nx = data_glob[\"Date\"],\ny = data_glob[\"Deaths\"],\nmode = \"lines\",\nname = \"Deaths\",\nline = dict(width = 2.5),\nmarker = dict(color = [0, 1, 2, 3])\n)\n\ntrace3 = go.Scatter(\nx = data_glob[\"Date\"],\ny = data_glob[\"Recovered\"],\nmode = \"lines\",\nname = \"Recovered\",\nline = dict(width = 2.5),    \nmarker = dict(color = [0, 1, 2, 3])\n)\n\ntrace4 = go.Scatter(\nx = data_glob[\"Date\"],\ny = data_glob[\"Active\"],\nmode = \"lines\",\nname = \"Active\",\nline = dict(width = 2.5),\nmarker = dict(color = [0, 1, 2, 3])\n)\n\ndata_plt = [trace1,trace2,trace3,trace4]\nlayout = go.Layout(title = \"Global Case States\",xaxis_title=\"Date\",yaxis_title=\"Number of Total Cases\",\n                   legend=dict(\n        x=0,\n        y=1,),hovermode='x')\nfig = go.Figure(data = data_plt,layout = layout)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2.3\"></a> <br>\n## Pie Chart of Global Distribution ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [\"Recovered\",\"Deaths\",\"Active\"]\nvalues = [data_glob.tail(1)[\"Recovered\"].iloc[0],data_glob.tail(1)[\"Deaths\"].iloc[0],data_glob.tail(1)[\"Active\"].iloc[0]]\n\nfig = go.Figure(data = [go.Pie(labels = labels, values = values,textinfo='label+percent',insidetextorientation='radial')],layout = go.Layout(title = \"Global Patient Percentage\"))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2.4\"></a> <br>\n## Comparison between SARS and Covid-19 dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid19_compare.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sars_compare.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"date_list_cov_compare = list(df_covid19_compare[\"ObservationDate\"].unique())\nconfirmed = []\ndeaths = []\nrecovered = []\nfor i in date_list_cov_compare:\n    x = df_covid19_compare[df_covid19_compare[\"ObservationDate\"] == i]\n    confirmed.append(sum(x[\"Confirmed\"]))\n    deaths.append(sum(x[\"Deaths\"]))\n    recovered.append(sum(x[\"Recovered\"]))\ndata_glob_cov = pd.DataFrame(list(zip(date_list_cov_compare,confirmed,deaths,recovered)),columns = [\"Date\",\"Confirmed\",\"Deaths\",\"Recovered\"])\ndata_glob_cov.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_list_sars_compare = list(df_sars_compare[\"ObservationDate\"].unique())\nconfirmed = []\ndeaths = []\nrecovered = []\nfor i in date_list_sars_compare:\n    x = df_sars_compare[df_sars_compare[\"ObservationDate\"] == i]\n    confirmed.append(sum(x[\"Confirmed\"]))\n    deaths.append(sum(x[\"Deaths\"]))\n    recovered.append(sum(x[\"Recovered\"]))\ndata_glob_sars = pd.DataFrame(list(zip(date_list_sars_compare,confirmed,deaths,recovered)),columns = [\"Date\",\"Confirmed\",\"Deaths\",\"Recovered\"])\ndata_glob_sars.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2.41\"></a> <br>\n## Confirmed Percentage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly import subplots\ndeath_percent_sars = ((data_glob_sars[\"Deaths\"]*100)/data_glob_sars[\"Confirmed\"])\ndeath_percent_cov = ((data_glob_cov[\"Deaths\"]*100)/data_glob_cov[\"Confirmed\"])\n\ntrace_death_sars = go.Scatter(x=data_glob_sars[\"Date\"],\n                                  y = death_percent_sars,\n                                  mode = \"lines\",\n                                  name = \"Death Percentage for SARS\",\n                                  marker = dict(color = [0, 1, 2, 3]))\n    \ntrace_death_cov = go.Scatter(x=data_glob_cov[\"Date\"],\n                                  y = death_percent_cov,\n                                  mode = \"lines\",\n                                  name = \"Death Percentage for Covid-19\",\n                                  marker = dict(color = [0, 1, 2, 3]))\n    \ndeath_plt = [trace_death_sars,trace_death_cov]\n\nfig = subplots.make_subplots(rows=1,cols=2)\nfig.append_trace(trace_death_sars,1,1)\nfig.append_trace(trace_death_cov,1,2)\n\nfig.layout.width = 1000\nfig.layout.height = 600\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2.42\"></a> <br>\n## Recovered Percentage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"recover_percent_sars = ((data_glob_sars[\"Recovered\"]*100)/data_glob_sars[\"Confirmed\"])\nrecover_percent_cov = ((data_glob_cov[\"Recovered\"]*100)/data_glob_cov[\"Confirmed\"])\n\ntrace_recover_sars = go.Scatter(x=data_glob_sars[\"Date\"],\n                                  y = recover_percent_sars,\n                                  mode = \"lines\",\n                                  name = \"Recover Percentage for SARS\",\n                                  marker = dict(color = [0, 1, 2, 3]))\n    \ntrace_recover_cov = go.Scatter(x=data_glob_cov[\"Date\"],\n                                  y = recover_percent_cov,\n                                  mode = \"lines\",\n                                  name = \"Recover Percentage for Covid-19\",\n                                  marker = dict(color = [0, 1, 2, 3]))\n    \nrecover_plt = [trace_recover_sars,trace_recover_cov]\n\nfig = subplots.make_subplots(rows=1,cols=2)\nfig.append_trace(trace_recover_sars,1,1)\nfig.append_trace(trace_recover_cov,1,2)\nfig.layout.width = 1000\nfig.layout.height = 600\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The recovered percentage explains the probability of using Sars dataset as a base model to analyze Covid-19, because the left graph could clearly show the Sars outbreak had been finished until July 2003 and the recovery rate was almost 90% back then. However, the Coronavirus is still spreading until now. As a result, we could learn some potential distribution from the closed Sars model to model for the new Covid-19.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n# Prediction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**The main procedures for the prediction part has been shown below,**\n\n* Input SARS_2003 data set to build our base model\n* Because the outbreak for SARS was mainly located in China, I built an RNN prediction model for SARS in China instead of Canada\n* Save the base model and load COVID-19 dataset\n* Load previous model I built, and its corresponding weights and weights\n* Fine tune that model to predict the cases for Coronavirus in Canada\n\n\n\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.1\"></a> <br>\n## Base Model","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# load Sars data set, and set the country as China\nSars_CA.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.11\"></a> <br>\n## Feature Extraction","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# data normalization\n\ntrain_num_sars = int(len(Sars_CA)*0.8)\n\nscaler_sars = MinMaxScaler()\n\ntrain_origin = pd.DataFrame(Sars_CA.iloc[:train_num_sars,1])\ntest_origin = pd.DataFrame(Sars_CA.iloc[train_num_sars:,1])\n\nscaler_sars.fit(train_origin)\nscaled_train_sars = scaler_sars.transform(train_origin)\nscaled_test_sars = scaler_sars.transform(test_origin)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# using 10 day lag to predict the model\nn_input = 15\nn_features = 1\ngenerator_sars = TimeseriesGenerator(scaled_train_sars, scaled_train_sars, length=n_input, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# show the format of our input data\nfor i in range(3):\n    x, y = generator_sars[i]\n    print('%s => %s' % (x, y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.12\"></a> <br>\n## Compile the SARS model","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# build 4-layer RNN model\n# define model\nmodel = Sequential([\n    layers.LSTM(256, activation='relu', input_shape=(n_input, n_features),return_sequences=True),\n    layers.LSTM(128, activation='relu', input_shape=(n_input, n_features),return_sequences=True),\n    layers.LSTM(64, activation='relu', input_shape=(n_input, n_features)),\n    layers.Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.14\"></a> <br>\n## Train the SARS model","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# train the model\nmodel.fit_generator(generator_sars,epochs=25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.14\"></a> <br>\n## Learning Curves","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the loss curve\nloss_per_epoch = model.history.history['loss']\nfig = plt.figure(dpi = 120,figsize = (6,4))\nax = plt.axes()\nax.set(xlabel = 'Number of Epochs',ylabel = 'MSE Loss',title = 'Sars Loss Curve')\nplt.plot(range(len(loss_per_epoch)),loss_per_epoch,lw = 1);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# test our model in test set\ntest_predictions = []\n\nfirst_eval_batch = scaled_train_sars[-n_input:]\ncurrent_batch = first_eval_batch.reshape((1, n_input, n_features))\n\nfor i in range(len(test_origin)):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    test_predictions.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# fill test table with prediction\ntrue_predictions = scaler_sars.inverse_transform(test_predictions)\ntest_origin['Predictions'] = true_predictions\nprint(test_origin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the comparison between actual value and predicted value\nfig = plt.figure(dpi = 120)\nax=plt.axes()\ntest_origin.plot(legend=True,figsize=(6,4),lw = 2,ax=ax)\nplt.xlabel('Date')\nplt.ylabel('Count of Cases')\nplt.title('Comparision Test and Prediction')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# build complete SARS model with the whole data set (train+test)\nscaler_sars = MinMaxScaler()\n\ntrain_origin = pd.DataFrame(Sars_CA.iloc[:,1])\n\n\nscaler_sars.fit(train_origin)\nscaled_train_sars = scaler_sars.transform(train_origin)\n\nn_input = 15\nn_features = 1\ngenerator_sars = TimeseriesGenerator(scaled_train_sars, scaled_train_sars, length=n_input, batch_size=1)\n\n# define model\nmodel_whole = Sequential([\n    layers.LSTM(256, activation='relu', input_shape=(n_input, n_features),return_sequences=True),\n    layers.LSTM(128, activation='relu', input_shape=(n_input, n_features),return_sequences=True),\n    layers.LSTM(64, activation='relu', input_shape=(n_input, n_features)),\n    layers.Dense(1)\n])\nmodel_whole.compile(optimizer='adam', loss='mse')\n\n# fit model\nmodel_whole.fit_generator(generator_sars,epochs=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot loss curve for complete model\nloss_per_epoch = model_whole.history.history['loss']\nfig = plt.figure(dpi = 120,figsize = (6,4))\nax = plt.axes()\nax.set(xlabel = 'Number of Epochs',ylabel = 'MSE Loss',title = 'Loss Curve of Base Model')\nplt.plot(range(len(loss_per_epoch)),loss_per_epoch,lw = 2)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# save SARS model and its weights\njson_config = model_whole.to_json()\nwith open('model_config.json', 'w') as json_file:\n    json_file.write(json_config)\nmodel_whole.save_weights('path_to_my_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.2\"></a> <br>\n## Fine Tuning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.21\"></a> <br>\n## Format the data","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# load COVID-19 dataset, and set country as Canada\nCovid_CA = df_covid19[df_covid19['Country/Region'] == 'Canada']\nCovid_CA.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# normalization\n\ntrain_num = int(len(Covid_CA)*0.8)\n\nscaler = MinMaxScaler()\n\ntrain = pd.DataFrame(Covid_CA.iloc[:train_num,1])\ntest = pd.DataFrame(Covid_CA.iloc[train_num:,1])\n\nscaler.fit(train)\nscaled_train = scaler.transform(train)\nscaled_test = scaler.transform(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# show confirmation cases after normalization \nprint(\"Scaled Train Set:\", scaled_train[:3],\"\\n\")\nprint(\"Scaled Test Set:\", scaled_test[:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# equally, we set 10 day lag for modelling\nn_input = 15\nn_features = 1\ngenerator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# show data format\nfor i in range(3):\n    x, y = generator[i]\n    print('%s => %s' % (x, y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.22\"></a> <br>\n## Load the Base Model","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# load SARS model\nmodel_cov = model_from_json(open('model_config.json').read())\nmodel_cov.load_weights('path_to_my_weights.h5')\nmodel_cov.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Take a look at the Trainable params.**\n\n**Up to now, we haven't freeze any layers of the model, so all of the params are trainable.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.23\"></a> <br>\n## Freeze the Bottom LSTM Layers","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Let's take a look to see how many layers are in the base model\nprint(\"Number of layers in the base model: \", len(model_cov.layers))\n\n# Fine-tune from this layer onwards\nfine_tune_at = 1\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in model_cov.layers[:fine_tune_at]:\n  layer.trainable =  False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now, we locked the first one layers of our network. The Non-trainable params is 66560+49408\nmodel_cov.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# compile the new model for training\nmodel_cov.compile(optimizer='adam', loss='mse')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3.24\"></a> <br>\n## Re-train the model with Covid-19 dataset","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# fit model\nmodel_cov.fit_generator(generator,epochs=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot loss curve for new model\nloss_per_epoch = model_cov.history.history['loss']\nfig = plt.figure(dpi = 120,figsize = (6,4))\nax = plt.axes()\nax.set(xlabel = 'Number of Epochs',ylabel = 'MSE Loss',title = 'Loss Curve - Fine Tuning')\nplt.plot(range(len(loss_per_epoch)),loss_per_epoch,lw = 2);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"test_predictions = []\n\nfirst_eval_batch = scaled_train[-n_input:]\ncurrent_batch = first_eval_batch.reshape((1, n_input, n_features))\n\nfor i in range(len(test)):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model_cov.predict(current_batch)[0]\n    \n    # store prediction\n    test_predictions.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"true_predictions = scaler.inverse_transform(test_predictions)\ntest['Predictions'] = true_predictions\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(dpi = 120)\nax=plt.axes()\ntest.plot(legend=True,figsize=(6,4),lw = 2,ax=ax)\nplt.xlabel('Date')\nplt.ylabel('Count of Cases')\nplt.show();","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}