{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Human Activity Recognition with Mobile Sensing"},{"metadata":{},"cell_type":"markdown","source":"In this lab, we will learn how to analyse mobile sensor data with the use of applied machine learning, in order to predict the user's activity in the following six classes:\n1. Walking\n2. Walking Upstairs\n3. Walking Downstairs\n4. Sitting\n5. Standing\n6. Laying"},{"metadata":{},"cell_type":"markdown","source":"## Dataset Information\n\nThe following two datasets will be used in this tutorial:\n\n**a.** the [MotionSense Data Set](https://github.com/mmalekzadeh/motion-sense), by Malekzadeh et al. from Queen Mary University of London.\n\nThis dataset includes sensor recordings (i.e. Accelerometer, Gyroscope and Device Motion in 100Hz sampling rate) from 24 participants performing the following activities: _walking_, _walking upstairs_, _walking downstairs_, _sitting_, _standing_ and _jogging_. This dataset provides the raw sensor data, collected from an iPhone 6S using [SensingKit mobile sensing framework](https://www.sensingkit.org).\n\n**b.** the [Human Activity Recognition Using Smartphones Data Set](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones), by Anguita et al. from University of Genova.\n\nThis dataset includes sensor recordings (i.e. Accelerometer and Gyroscope in 50Hz sampling rate) from 30 participants performing the following activities: _walking_, _walking upstairs_, _walking downstairs_, _sitting_, _standing_ and _laying_. Note that the dataset has been pre-processed with noise filters and a series of features have been extracted in fixed-width sliding windows of 2.56 second and 50% overlap (128 readings/window)."},{"metadata":{},"cell_type":"markdown","source":"## Import the required libraries\n\nFirst import some important libraries that will be use in the rest of this tutorial:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A. Examine the first dataset\n\nLet's first have a look at the MotionSense dataset. The aim of this task is to understand the structure of the dataset and visualise each motion activity.\n\n> The dataset consists of three directories, one for each type of sensor:\n- A_DeviceMotion_data\n- B_Accelerometer_data\n- C_Gyroscope_data\n\nIn this lab we will only work with the Device Motion sensor as it includes the calibrated data (using sensor fusion) from both Accelerometer and Gyroscope sensors. There is also one directory for each activity session:\n- **dws**: walking downstairs\n- **ups**: walking upstairs\n- **sit**: sitting\n- **std**: standing\n- **wlk**: walking\n- **jog**: jogging\n\nLoad the data for activity _walking downstairs_ (i.e. `dws_1`) of _Participant 1_ (i.e. `sub_1`) and print the head (i.e. the first 5 rows):"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data(path, filename):\n    return pd.read_csv(os.path.join(path, filename), index_col=0)\n\ndf = read_data('../input/motionsense-dataset/A_DeviceMotion_data/A_DeviceMotion_data/dws_1/', 'sub_1.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that since the data has been collected using 3-axis sensors, the dataframe includes three measurements (x, y, and z) for each of the following properties:\n- Attitude\n- Gravity\n- Rotation Rate\n- User Acceleration\n\nFor more information about these measurements, visit SensingKit documentation page for [SKDeviceMotionData](http://www.sensingkit.org/documentation/ios/Classes/SKDeviceMotionData.html).\n\nIn our case, we are interested in a classifier that accurately detects motion activity in any physical alignment that the user has placed the phone in his/her pocket. Thus, the magnitude (resultant vector) of each sensor should be computed since each user places the smartphone in a different physical alignment and individual axis reading will not provided useful information. You can compute the magnitude using the formula:\n\n$mag = \\sqrt{x^2 + y^2 + z^2}$\n\nWe will only focus on the properties User Acceleration and Rotation Rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"def produce_magnitude(df, column):\n    df[column+'.mag'] = np.sqrt(df[column+'.x']**2 + df[column+'.y']**2 + df[column+'.z']**2)\n\nproduce_magnitude(df, 'userAcceleration')\nproduce_magnitude(df, 'rotationRate')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that magnitude is computed, plot the first 5 seconds of the two signals (i.e. User Acceleration and Rotation Rate) for the following motion activities:\n- Walking\n- Jogging\n\nCompare the signals of the two different activities. Feel free to change the code and explore more activities of different users."},{"metadata":{},"cell_type":"markdown","source":"### Visualise the MotionSense dataset"},{"metadata":{},"cell_type":"markdown","source":"We will use `motionsense.py`, a module provided by the MotionSense dataset and facilitates the interaction with the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndef get_ds_infos():\n    \"\"\"\n    Read the file includes data subject information.\n    \n    Data Columns:\n    0: code [1-24]\n    1: weight [kg]\n    2: height [cm]\n    3: age [years]\n    4: gender [0:Female, 1:Male]\n    \n    Returns:\n        A pandas DataFrame that contains inforamtion about data subjects' attributes \n    \"\"\" \n\n    dss = pd.read_csv(\"../input/motionsense-dataset/data_subjects_info.csv\")\n    print(\"[INFO] -- Data subjects' information is imported.\")\n    \n    return dss\n\ndef set_data_types(data_types=[\"userAcceleration\"]):\n    \"\"\"\n    Select the sensors and the mode to shape the final dataset.\n    \n    Args:\n        data_types: A list of sensor data type from this list: [attitude, gravity, rotationRate, userAcceleration] \n\n    Returns:\n        It returns a list of columns to use for creating time-series from files.\n    \"\"\"\n    dt_list = []\n    for t in data_types:\n        if t != \"attitude\":\n            dt_list.append([t+\".x\",t+\".y\",t+\".z\"])\n        else:\n            dt_list.append([t+\".roll\", t+\".pitch\", t+\".yaw\"])\n    print(dt_list)\n    return dt_list\n\n\ndef creat_time_series(folder_name, dt_list, act_labels, trial_codes, mode=\"mag\", labeled=True):\n    \"\"\"\n    Args:\n        folder_name: one of 'A_DeviceMotion_data', 'B_Accelerometer_data', or C_Gyroscope_data\n        dt_list: A list of columns that shows the type of data we want.\n        act_labels: list of activites\n        trial_codes: list of trials\n        mode: It can be 'raw' which means you want raw data\n        for every dimention of each data type,\n        [attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)].\n        or it can be 'mag' which means you only want the magnitude for each data type: (x^2+y^2+z^2)^(1/2)\n        labeled: True, if we want a labeld dataset. False, if we only want sensor values.\n\n    Returns:\n        It returns a time-series of sensor data.\n    \n    \"\"\"\n    num_data_cols = len(dt_list) if mode == \"mag\" else len(dt_list*3)\n\n    if labeled:\n        dataset = np.zeros((0,num_data_cols+7)) # \"7\" --> [act, code, weight, height, age, gender, trial] \n    else:\n        dataset = np.zeros((0,num_data_cols))\n        \n    ds_list = get_ds_infos()\n    \n    print(\"[INFO] -- Creating Time-Series\")\n    for sub_id in ds_list[\"code\"]:\n        for act_id, act in enumerate(act_labels):\n            for trial in trial_codes[act_id]:\n                fname = folder_name+'/'+act+'_'+str(trial)+'/sub_'+str(int(sub_id))+'.csv'\n                raw_data = pd.read_csv(fname)\n                raw_data = raw_data.drop(['Unnamed: 0'], axis=1)\n                vals = np.zeros((len(raw_data), num_data_cols))\n                for x_id, axes in enumerate(dt_list):\n                    if mode == \"mag\":\n                        vals[:,x_id] = (raw_data[axes]**2).sum(axis=1)**0.5        \n                    else:\n                        vals[:,x_id*3:(x_id+1)*3] = raw_data[axes].values\n                    vals = vals[:,:num_data_cols]\n                if labeled:\n                    lbls = np.array([[act_id,\n                            sub_id-1,\n                            ds_list[\"weight\"][sub_id-1],\n                            ds_list[\"height\"][sub_id-1],\n                            ds_list[\"age\"][sub_id-1],\n                            ds_list[\"gender\"][sub_id-1],\n                            trial          \n                           ]]*len(raw_data), dtype=int)\n                    vals = np.concatenate((vals, lbls), axis=1)\n                dataset = np.append(dataset,vals, axis=0)\n    cols = []\n    for axes in dt_list:\n        if mode == \"raw\":\n            cols += axes\n        else:\n            cols += [str(axes[0][:-2])]\n            \n    if labeled:\n        cols += [\"act\", \"id\", \"weight\", \"height\", \"age\", \"gender\", \"trial\"]\n    \n    dataset = pd.DataFrame(data=dataset, columns=cols)\n    return dataset\n#________________________________\n\n\nACT_LABELS = [\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\nTRIAL_CODES = {\n    ACT_LABELS[0]:[1,2,11],\n    ACT_LABELS[1]:[3,4,12],\n    ACT_LABELS[2]:[7,8,15],\n    ACT_LABELS[3]:[9,16],\n    ACT_LABELS[4]:[6,14],\n    ACT_LABELS[5]:[5,13]\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we set parameter to build labeled time-series from dataset.\n\n**attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)**\n\n For example, here we choose `rotationRate`, `userAcceleration`. You can play with this and add other features such as `gravity` and `attitude` or remove an existing feature. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sdt = [\"rotationRate\", \"userAcceleration\"]\nprint(\"Selected sensor data types:\\n\" + str(sdt))\ndt_list = set_data_types(sdt)\nprint(\"\\nSelected columns from dataset:\\n\" + str(dt_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set the list of activities we will use. In our case, we will choose all the activities."},{"metadata":{"trusted":true},"cell_type":"code","source":"ACT_LABELS = [\"sit\", \"std\", \"dws\", \"ups\", \"wlk\", \"jog\"]\nact_labels = ACT_LABELS [0:6]  # all activities\nprint(\"Selected activites: \" + str(act_labels))    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the MotionSense dataset, several sessions exist for each activity. For instance, 3 sessions (code 7, 8, 15) have been recorded for the _walking_ activity (label 4). \n\nSo, you can choose which trials you want to be included in your data.\n\n```\nTRIAL_CODES = {\n    ACT_LABELS[0]:[5,13],\n    ACT_LABELS[1]:[6,14],\n    ACT_LABELS[2]:[1,2,11],\n    ACT_LABELS[3]:[3,4,12],\n    ACT_LABELS[4]:[7,8,15],\n    ACT_LABELS[5]:[9,16],\n}\n```\n\nIn our case we will only choose one session for each activity."},{"metadata":{"trusted":true},"cell_type":"code","source":"TRIAL_CODES = {\n    ACT_LABELS[0]:[5],\n    ACT_LABELS[1]:[6],\n    ACT_LABELS[2]:[1],\n    ACT_LABELS[3]:[3],\n    ACT_LABELS[4]:[7],\n    ACT_LABELS[5]:[9],\n}\ntrial_codes = [TRIAL_CODES[act] for act in act_labels]\nprint(\"[INFO] -- Selected trials: \" + str(trial_codes))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We set `mode=\"mag\"` to compute the magnitude of the three axes.\n* We set `labeled = True` to get a labeled time-series (here, the label is the type of activity).\n* We set `combine_grav_acc = False` to use the linear acceleration (total acceleration excluding the gravity)."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Loading...\")\ndataset = creat_time_series(\"../input/motionsense-dataset/A_DeviceMotion_data/A_DeviceMotion_data\", dt_list, act_labels, trial_codes, mode=\"mag\", labeled=True)\nprint(\"Finished!\")\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset is now loaded under the variable `dataset`.\n\nNext, we will visualize the dataset per activity:"},{"metadata":{},"cell_type":"markdown","source":"#### 1. Configure `matplotlib` module:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (30,8)\nplt.rcParams['font.size'] = 32\nplt.rcParams['image.cmap'] = 'plasma'\nplt.rcParams['axes.linewidth'] = 2\nclr1 = [\"rs-\",\"r*-\",\"ro-\",\"rv-\",\"rp-\",\"r^-\"]\nclr2 = [\"bs-\",\"b*-\",\"bo-\",\"bv-\",\"bp-\",\"b^-\"]\nact_lbl = [\"Sat\", \"Stand-Up\", \"Downstairs\", \"Upstairs\", \"Walking\", \"Jogging\"]\nlbl = [\"rotation\", \"acceleration\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Set the duration of each time-series plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"period = 2.5 # Seconds\nsample_rate = 50 # Hz\npoints = int(period*sample_rate)\nx_ticks = np.arange(0.,points/sample_rate,1./sample_rate)\nprint(\"Data points per time-series: \" + str(points))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Plot data per activity"},{"metadata":{"trusted":true},"cell_type":"code","source":"act_data = np.zeros((6,points))\nfig, ax = plt.subplots(1, 6, sharex='col', sharey='row')\nuid = 12 # We have 24 users in the dataset, uid can be selected from {0,1,...23}\nfor i in np.unique(dataset[\"act\"]):\n    i =int(i)\n    data = dataset[(dataset[\"id\"] == uid) & (dataset[\"act\"] == i)]\n    acc = data[\"userAcceleration\"].values\n    rot = data[\"rotationRate\"].values\n    acc = acc[:points]\n    rot = rot[:points]\n    \n    if i!=0:\n        ax[i].plot(x_ticks, rot, \"ro-\", linewidth=2, markersize=8)\n        ax[i].plot(x_ticks, acc, \"b^-\", linewidth=2, markersize=8)\n    else:\n        ax[i].plot(x_ticks, rot, \"ro-\", linewidth=2, markersize=12, label=lbl[0])\n        ax[i].plot(x_ticks, acc, \"b^-\", linewidth=2, markersize=12, label=lbl[1])\n\n    ax[i].set_title(act_lbl[i])\nplt.setp(ax, yticks=np.arange(0, 11, 2))\nfig.text(0.5, 0.004, 'second', ha='center')\nfig.text(0.075, 0.5, 'magnitude value', va='center', rotation='vertical', )\nax[0].legend(loc=\"upper center\", fontsize = 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## B. Examine the second dataset\n\nIn classical machine learning, a feature extraction process is required that converts the raw data into informative features that the model can understand. This process, also known as feature engineering is a time consuming and creative task. We will skip this part by using the [Human Activity Recognition Using Smartphones Data Set](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones) that includes pre-computed features in fixed-width sliding windows of 2.56 second and 50% overlap (128 readings/window).\n\nThe data directory includes a `train.csv` file that will be used for training the model, and a `test.csv` file that will be used for validating the model's performance.\nExamine the dataset to get an idea of its structure."},{"metadata":{},"cell_type":"markdown","source":"## Load the dataset\n\nLoad the features of the train set and print the first 5 rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data(path, filename):\n    return pd.read_csv(os.path.join(path, filename))\n\ndf = read_data('../input/human-activity-recognition-with-smartphones', 'train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now load the labels (ground truth) of the train set and print the first five rows. Remember, the labels (listed in file `activity_labels.txt`) are:\n\n1. Walking\n2. Walking Upstairs\n3. Walking Downstairs\n4. Sitting\n5. Standing\n6. Laying"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['Activity']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we know how to load individual files, it's time to load the complete dataset and save it under the following four variables:\n- `train_X`: features used to train the model\n- `train_y`: labels used to train the model\n- `test_X`: features used to validate the model\n- `test_y`: labels used to validate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_dataset(label_dict):\n    train_X = read_data('../input/human-activity-recognition-with-smartphones', 'train.csv').values[:,:-2]\n    train_y = read_data('../input/human-activity-recognition-with-smartphones', 'train.csv')['Activity']\n    train_y = train_y.map(label_dict).values\n    test_X = read_data('../input/human-activity-recognition-with-smartphones', 'test.csv').values[:,:-2]\n    test_y = read_data('../input/human-activity-recognition-with-smartphones', 'test.csv')\n    test_y = test_y['Activity'].map(label_dict).values\n    return(train_X, train_y, test_X, test_y)\nlabel_dict = {'WALKING':0, 'WALKING_UPSTAIRS':1, 'WALKING_DOWNSTAIRS':2, 'SITTING':3, 'STANDING':4, 'LAYING':5}\ntrain_X, train_y, test_X, test_y = load_dataset(label_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Choose a model\n\nWe will choose Random Forest as a model with default parameters and 100 number of estimators (`n_estimators` parameter)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the model\n\nTrain the model using the features from train set (`train_X`) and the labels as ground truth (`train_y`)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate the model\n\nUse the trained model to predict the motion activity using the features from test set (`test_X`). Predictions will be saved into `yhat` array."},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat = model.predict(test_X)\nyhat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Print a classification report by comparing the predictions (`yhat`) with the ground truth (`test_y`).\n\nWhat is the difference between precision and recall performances? What is F1 score?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ntarget_names = ['Walking', 'Walking Upstairs', 'Walking Downstairs', 'Sitting', 'Standing', 'Laying']\n\nprint(classification_report(test_y, yhat, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## More\n\nNote that this is just a simplified version and excludes other important steps such as experiment with other models, apply parameter tuning, evaluate using cross validation etc.\n\nFeel free to experiment with different models (e.g. Logistic Regression, SVM or XGBoost) and/or different model parameters (e.g. `n_estimators`, `max_depth` or `min_samples_split`). What is the impact on the model performance?"},{"metadata":{},"cell_type":"markdown","source":"## References"},{"metadata":{},"cell_type":"markdown","source":"- [MotionSense Data Set](https://github.com/mmalekzadeh/motion-sense)\n- [Human Activity Recognition Using Smartphones Data Set](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones)\n- [Scikit-Learn: Machine Learning in Python](http://scikit-learn.org/stable/)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}