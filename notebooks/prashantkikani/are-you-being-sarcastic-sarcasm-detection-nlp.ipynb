{"cells":[{"metadata":{},"cell_type":"markdown","source":"In the [previous lesson](https://www.kaggle.com/prashantkikani/is-it-a-cat-or-dog-convolutional-nn/), we learned basics of computer vision & CNN(convolutional neural network) by building a classifier with hands-on Python code experience.\n\nToday in this lesson, we will learn the basics of Natural Language Processing(NLP) by building a model on \"Sarcasm Detection\".<br><br>\n![sarcasm](https://memegenerator.net/img/instances/75322005/was-that-sarcasm.jpg)\n\nWe will use LSTM (Long Short Term Memory) layers to create our deep learning model who can predict whether a statement is sarcastic or not.<br><br>\n\nBut before going to start the coding part of the lesson, let's first understand basics of few things that we are going to use.<br>\n\n## Tokenize\nIn NLP, we work with text data.<br>\nBut, machines / computers can't read text. They just deal with numbers.<br>\nSo, to convert text data into numbers, we give a unique ID number to each word & replace word it's corresponding number.\n### That's what Tokenizer does.\nIt converts our text data to numeric format.<br>\nLike, sentence `\"machine learning is cool\"` will convert into something like `\"2315 834 12 4510\"`.<br><br>\n\nAfter converting our text data into numbers, now, we can use that data to train our ML models.<br>\nBut, there's one thing we are missing.<br><br>\n\nAfter converting, each word into numbers, `king` word will converted into something like 1678, `queen` to something like 56832, `man` to something like 4285, `woman` to something like 6387 etc. (These numbers I have choosen randomly. Point being, it'll converted to numbers).<br>\n\n* How, we will give our model information like, `king` & `queen` are related same as `man` & `woman`.<br>\n* Or, `italy`, `spain`, `brazil`, `india`, `japan` etc. words have one thing in common. (They all are name of countris)<br>\n* Or, `cricket`, `football`, `tennis` etc. words are also similar. (names of games)<br>\n* Or, the way words `china` & `beijing` are related, words `russia` & `moscow` are also related in the same way.<br><br>\n\nAll these are knowledge related things.<br>\nHow can we give this knowledge to our NLP models so that, it can use that knowledge to perform better at detecting sarcasm?<br>\n\nAnswer is `word embeddings`.\n\n## Word embeddings\n\nWord embeddings are nothing but 1D vector of length 200-500, containing floating point numbers.<br>\nEach word have corresponding 1D array for it.<br><br>\n\nFor e.g.<br>\n`king` : `[-0.34, 0.98, -0.04, 0.32]`<br>\n`queen` : `[0.25, -0.23, -0.10, -0.72]`<br>\n.<br>\n.<br>\n.<br>\n\nHere it's just 4 numbers in a 1D vector, but in actual there will be 200-500 numbers in a 1D vector for each word.<br>\n\n### Why we use word embeddings?\nThese word embeddings captures intrinsic meaning in some way for each words.<br>\nIt kind of stores “meaning” of words or “insights” of the words inside them.<br>\nFor e.g.<br>\n![gender](https://blog.acolyer.org/wp-content/uploads/2016/04/word2vec-gender-relation.png)\n<center>The way man & woman are related, king & queen, uncle & aunt are related the same way. </center><br>\n\nWe can even perform mathematical operations on word embeddings & get expected results.<br>\n![gender maths](http://jalammar.github.io/images/word2vec/king-analogy-viz.png)\n<center>If we subtract word embeddings of `man` from `king` & add word embedding of `woman`, we get word embedding of `queen`</center><br>\n\n![word embeddings country](https://blog.acolyer.org/wp-content/uploads/2016/04/word2vec-dr-fig-2.png)\n<center>Relation between country & capitals</center><br>\n\n### We use word embeddings to transfer the knowledge of words to model so that it can perform better.\n\n### How do we get these word embeddings?\n* There's something called [Language Models](https://en.wikipedia.org/wiki/Language_model).<br>\n* They basically learn to predict next word of the sentence given all the past words.<br>\n* We will not go there as of now. If you want to learn more about it, I recommend reading [this blog](https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/).\n\nSo, now we have understood tokenizer & word embeddings.<br>\nLet's also quickly understand LSTM.<br>\n\n## LSTM (Long Short Term Memory)\n\nLSTM is type of RNN (Recurrent Neural Network) which \"remembers\" text to perform better in the task we are doing.<br>\nFor e.g. <br>\nTo answer the question, `\"Bob is from France. Alice is from Japan. So, what's the mother-tongue of Bob?\"`<br>\nWe need to remember the first sentence, `\"Bob is from France\"`.<br>\nWithout knowing / remembering that, we can't answer the question `\"French\"`<br><br>\n\nSame way, LSTM remembers, past text to perform better at the task we are doing.<br>\n\nI highly recommend to check out [this blog](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714) to understand LSTM. <br>That blog contains a simple explanation of LSTM that almost everyone can understand.<br>\n\nHere's what a cell of LSTM looks like.<br>\n![LSTM](https://miro.medium.com/max/700/1*laH0_xXEkFE0lKJu54gkFQ.png)\n<center>An LSTM cell</center>\n<br>"},{"metadata":{},"cell_type":"markdown","source":"There are 3 valves in a single LSTM cell.<br>\n1. **Forget valve**\n    * This valve decides what should be forgetted from the past text. What things are irrelevant to remember.\n    * First part from left is forget valve.\n2. **Memory valve**\n    * This valve decides how much new memory should influence the old memory.\n    * New memory is generated by a single layer neural network with pentagon shaped activation function(tanh).\n    * Output will be element-wise multiple the new memory valve, and add to the old memory to form the new memory.\n    * Memory valve is in the middle of the above figure.\n3. **Output valve**\n    * This valve generates output of this perticular LSTM cell.\n    * It's in the right part of above LSTM cell figure.\n    * It's controlled by new memory & previous output.\n    * This valve controls how much new memory should output to the next LSTM unit.\n<br>\n\n### In one LSTM layer, there will be a chain of this type of LSTM cells. \n\nSo, that's enough of theoratical talking.<br>\nLet's do it by code."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# pandas to open data files & processing it.\nimport pandas as pd\n# to see all columns\npd.set_option('display.max_columns', None)\n# To see whole text\npd.set_option('max_colwidth', -1)\n\n# numpy for numeric data processing\nimport numpy as np\n\n# keras for deep learning model creation\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras.utils import plot_model\n\n# to fix random seeds\nimport random\nimport tensorflow as tf\nimport torch\nimport os\n\n# Regular Expression for text cleaning\nimport re\n\n# to track the progress - progress bar\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's open the data files now."},{"metadata":{"trusted":true},"cell_type":"code","source":"sarcasm_data = pd.read_csv(\"../input/sarcasm/train-balanced-sarcasm.csv\")\nprint(sarcasm_data.shape)\nsarcasm_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We just need `comment` & `label` column.<br>\nSo, let's remove others."},{"metadata":{"trusted":true},"cell_type":"code","source":"sarcasm_data.drop(['author', 'subreddit', 'score', 'ups', 'downs', 'date', 'created_utc', 'parent_comment'], axis=1, inplace=True)\n# remove empty rows\nsarcasm_data.dropna(inplace=True)\nsarcasm_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sarcasm_data['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, there are `505368` sentences with sarcastic news headlines !!<br>\n\nLet's do some pre-processing on our text data.<br>\nThese are the common practices which can improve performance in almost any NLP task.<br><br>\n\nOne common thing we can do is to remove `contractions`.<br>\n### Like, \"ain't\" to \"is not\", \"can't\" to \"can not\" etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"mispell_dict = {\"ain't\": \"is not\", \"cannot\": \"can not\", \"aren't\": \"are not\", \"can't\": \"can not\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n                \"doesn't\": \"does not\",\n                \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n                \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\",\n                \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n                \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n                \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n                \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n                \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\",\n                \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\", \"they'd\": \"they would\",\n                \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n                \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n                \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n                \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n                \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"wont\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n                \"wouldn't\": \"would not\",\n                \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n                \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color',\n                'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n                'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I',\n                'theBest': 'the best', 'howdoes': 'how does', 'Etherium': 'Ethereum',\n                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what',\n                'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n\nmispell_dict = {k.lower(): v.lower() for k, v in mispell_dict.items()}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make our preprocessing function."},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing_text(s):\n    # making our string lowercase & removing extra spaces\n    s = str(s).lower().strip()\n    \n    # remove contractions.\n    s = \" \".join([mispell_dict[word] if word in mispell_dict.keys() else word for word in s.split()])\n    \n    # removing \\n\n    s = re.sub('\\n', '', s)\n    \n    # put spaces before & after punctuations to make words seprate. Like \"king?\" to \"king\", \"?\".\n    s = re.sub(r\"([?!,+=—&%\\'\\\";:¿।।।|\\(\\){}\\[\\]//])\", r\" \\1 \", s)\n    \n    # Remove more than 2 continues spaces with 1 space.\n    s = re.sub('[ ]{2,}', ' ', s).strip()\n    \n    return s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply preprocessing_text function\nsarcasm_data['comment'] = sarcasm_data['comment'].apply(preprocessing_text)\nsarcasm_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# total unique words we are going to use.\nTOTAL_WORDS = 40000\n\n# max number of words one sentence can have\nMAX_LEN = 50\n\n# width of of 1D embedding vector\nEMBEDDING_SIZE = 300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntokenizer = Tokenizer(num_words=TOTAL_WORDS)\ntokenizer.fit_on_texts(list(sarcasm_data['comment']))\n\ntrain_data = tokenizer.texts_to_sequences(sarcasm_data['comment'])\ntrain_data = pad_sequences(train_data, maxlen = MAX_LEN)\ntarget = sarcasm_data['label']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's open embedding file now & store in a matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nEMBEDDING_FILE = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in tqdm(open(EMBEDDING_FILE)))\n\nword_index = tokenizer.word_index\nnb_words = min(TOTAL_WORDS, len(word_index))\nembedding_matrix = np.zeros((nb_words, EMBEDDING_SIZE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for word, i in tqdm(word_index.items()):\n    if i >= TOTAL_WORDS: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's build our NLP deep learning model now.."},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    tf.random.set_seed(seed)\n\n# We fix all the random seed so that, we can reproduce the results.\nseed_everything(2020)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = Input(shape=(MAX_LEN,))\n\nembedding_layer = Embedding(TOTAL_WORDS, EMBEDDING_SIZE, weights = [embedding_matrix])(input_layer)\n\nLSTM_layer = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)\nmaxpool_layer = GlobalMaxPool1D()(LSTM_layer)\n\ndense_layer_1 = Dense(64, activation=\"relu\")(maxpool_layer)\ndropout_1 = Dropout(0.5)(dense_layer_1)\n\ndense_layer_2 = Dense(32, activation=\"relu\")(dropout_1)\ndropout_2 = Dropout(0.5)(dense_layer_2)\n\noutput_layer = Dense(1, activation=\"sigmoid\")(dropout_2)\n\nmodel = Model(input=input_layer, output=output_layer)\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Before training the model first, let's understand our model first.\n\n`input_layer` : Input layer with which we will get text sentence.<br><br>\n`embedding_layer` : Embedding layer with which we will map each word with it's corresponding embedding vector.<br><br>\n`LSTM_layer` : LSTM layer with 128 LSTM cells.\n* We are using Bidirectional to run LSTM from both side of the text sentence.\n    1. Left to Right\n    2. Right to Left\n* Purpose of this is to give our model both side context.\n* It's also possible to not use this. But using this have provided good results.\n<br><br>\n`maxpool_layer` : Max pool layer is used to minimize the image size by pooling maximum number out of 2x2 grid.\n![maxpool](https://distilledai.com/wp-content/uploads/2020/04/2x2-max-pool-CNN.png)\n<br><br>\n`dense_layer_1` : Feed-forward dense layer to classify the features captured by LSTM layer.\n<br><br>\n`dropout_1` : Dropout is interesting trick. In Dropout, we randomly turn off some percentage of our neurons so that their's output can't go to next layer. Here we are turning off 20% of our total neurons.\n* Purpose of doing this is again to make our training robust.\n* Network should not depend some specific neurons to make predictions. And random turn will allow us to do that.\n* Picture below help us to understand it. \n![dropout](https://distilledai.com/wp-content/uploads/2020/04/dropout-in-deep-learning.png)\n<br><br>\n`dense_layer_2` & `dropout_2` are same as above.\n<br><br>\n`output_layer` : To get the output prediction from the neural network.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's start the training of our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 512\nEPOCHS = 2\n\nhistory = model.fit(\n    train_data, target,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    # We are using randomly selected 20% sentences as validation data.\n    validation_split=0.2\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Cool !!\n\nTraining is over.<br>\nWe can see validation accuracy above.<br>\n\n### Let's test our model on some random input now."},{"metadata":{"trusted":true},"cell_type":"code","source":"sarcasm_data[sarcasm_data['label']==1].sample(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = \"sun rises from the east\"\nsentence = preprocessing_text(sentence)\nprint(sentence)\n\nsentence = tokenizer.texts_to_sequences([sentence])\nsentence = pad_sequences(sentence, maxlen = MAX_LEN)\nsentence","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see, how our sentence got converted into numbers.<br>\n\n## We are doing padding to keep the final length same for every sentence no matter the sentence length.\n## Our Neural network have learned to ignore 0 in the training itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make the prediction.\nprediction = model.predict(sentence)\nprediction[0][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"So, it's saying sentence have probability of %.3f percent\"%(prediction[0][0]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = \"Isn't it great that, your girlfriend dumped you?\"\nsentence = preprocessing_text(sentence)\nprint(sentence)\n\nsentence = tokenizer.texts_to_sequences([sentence])\nsentence = pad_sequences(sentence, maxlen = MAX_LEN)\nsentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make the prediction.\nprediction = model.predict(sentence)\nprediction[0][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"So, it's saying sentence have probability of %.3f percent\"%(prediction[0][0]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sarcasm is a subjective matter.<br> One person can call a sentence sarcastic & another don't.\n### So, results may be subjective. But our goal was to understand some basic concepts of NLP.\n\n# Summary\n\n* We saw what Tokenization is.\n* We saw what word embedings are.\n* We understood how a LSTM cell works in a nutshell.\n* We cleaned the text data to use it in training\n* We trained a deep learning model\n\n## Upvote this kernel if you have learned something from it.<br>\n## Tell me if you have any kind of doubts / questions in comment section below.\n\n## In the next lesson, we will learn basics of \"Recommendation\" by recommending similar Movies to users.\n\n# See you in the [next lesson](https://www.kaggle.com/prashantkikani/can-you-recommend-me-a-movie-recommendation) !"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}