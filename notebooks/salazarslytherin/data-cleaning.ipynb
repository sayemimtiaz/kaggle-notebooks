{"cells":[{"metadata":{},"cell_type":"markdown","source":"# All codes of Data Cleaning mini course by kaggle","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"nfl_data  = pd.read_csv(\"../input/nflplaybyplay2009to2016/NFL Play by Play 2009-2017 (v4).csv\")\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to observe if there is any NaN / None in first few cols\nnfl_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if there is null how many of them at ever col only 10 seen at 2nd line\nmissing_vals = nfl_data.isnull().sum()\nmissing_vals[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_missing = missing_vals.sum()\ntotal_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_cells = np.product(nfl_data.shape)\ntotal_cells","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nfl_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"407688*102","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_missing = (total_missing / total_cells) * 100 \npercent_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nfl_data.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_with_NaN_dropped = nfl_data.dropna(axis=1)\ncols_with_NaN_dropped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_with_NaN_dropped.columns.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_with_NaN_dropped.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nfl_data.columns.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range (102):\n    print(nfl_data.columns[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"original dataset columns size %d\\n\" % nfl_data.shape[1])\nprint(\"after dropped cols with NaN columns size %d\\n\" % cols_with_NaN_dropped.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()\nsubset_nfl_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subset_nfl_data.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill with the data comes at next column\nsubset_nfl_data.fillna(method='bfill',axis=0).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"scaling and normalization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nfrom mlxtend.preprocessing import minmax_scaling\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* in scaling, you're changing the range of your data, while\n* in normalization, you're changing the shape of the distribution of your data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"original_data = np.random.exponential(size=1000)\nscaled_data = minmax_scaling(original_data, columns=[0])\n\nfig,ax = plt.subplots(1,2)\nsns.distplot(original_data, ax=ax[0])\nax[0].set_title(\"Original data\")\n\nsns.distplot(scaled_data, ax=ax[1])\nax[1].set_title(\"Scaled data\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Normalization**\nScaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n\n*Normal distribution:* Also known as the \"bell curve\", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n\nIn general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include **t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes****. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)\n\nThe method we're using to normalize here is called the **Box-Cox Transformation**. Let's take a quick peek at what normalizing some data looks like:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_data = stats.boxcox(original_data)\nprint(type(normalized_data))\n\n\nfig,ax = plt.subplots(1,2)\nsns.distplot(original_data, ax=ax[0])\nax[0].set_title(\"Original data\")\nsns.distplot(normalized_data[0], ax=ax[1])\nax[1].set_title(\"Normalized data\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Parse Dates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nlandslides = pd.read_csv(\"../input/landslide-events/catalog.csv\")\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"landslides.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"landslides.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"landslides['date'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"landslides['date'].dtype # it will print dtype('O') where O = Object {O is a keyword}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"landslides['date_parsed'] = pd.to_datetime(landslides['date'], infer_datetime_format=True) \n# infer_datetime_format=True or use the format specified in dataset format = format=\"%m/%d/%y\" \nlandslides['date_parsed'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"landslides['date_parsed'].dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the day of the month from the date_parsed column\nday_of_month_landslides = landslides['date_parsed'].dt.day\nday_of_month_landslides.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day_of_month_landslides = day_of_month_landslides.dropna()\nsns.distplot(day_of_month_landslides, kde=False, bins=31)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earthquakes = pd.read_csv(\"../input/earthquake-database/database.csv\")\ndate_lengths = earthquakes.Date.str.len()\ndate_lengths.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here 3 rows got different/corrupted format","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = np.where([date_lengths == 24])[1]\nprint('Indices with corrupted data:', indices)\nearthquakes.loc[indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earthquakes.loc[3378, \"Date\"] = \"02/23/1975\"\nearthquakes.loc[7512, \"Date\"] = \"04/28/1985\"\nearthquakes.loc[20650, \"Date\"] = \"03/13/2011\"\nearthquakes['date_parsed'] = pd.to_datetime(earthquakes['Date'], format=\"%m/%d/%Y\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earthquakes['date_parsed'] = pd.to_datetime(earthquakes['Date'], infer_datetime_format=True) # it didnt work as the data were corrupted so hardcoded to correct it.  \n# infer_datetime_format=True or use the format specified in dataset format = format=\"%m/%d/%y\" \nfor i in indices:\n    print(earthquakes['date_parsed'][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earthquakes.loc[indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earthquakes['date_parsed'].dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"volcanos = pd.read_csv(\"../input/volcanic-eruptions/database.csv\")\nvolcanos['Last Known Eruption'].sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/residentmario/time-series-plotting-optional","execution_count":null},{"metadata":{},"cell_type":"raw","source":"character enconding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import chardet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"before = \"This is the euro symbol: €\"\ntype(before)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"after = before.encode(\"utf-8\", errors=\"replace\")\ntype(after)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"after","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\\xe2\\x82\\xac' -> mojibake","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(after.decode(\"utf-8\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(after.decode(\"ascii\")) # as it was utf-8 encoded so ascii decode wont work","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"before = \"This is the euro symbol: €\"\nafter = before.encode(\"ascii\", errors = \"replace\")\nprint(after.decode(\"ascii\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# it was not encoded in utf-8 and we dont know what it is, so got error\n# also default for python/pandas to decode is utf-8 so utf-8 enocoded reading/decoding works without any issue\nkickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at the first ten thousand bytes to guess the character encoding\nwith open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(10000))\n\n# check what the character encoding might be\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So chardet is 73% confidence that the right encoding is \"Windows-1252\". Let's see if that's correct:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# read in the file with the encoding detected by chardet\nkickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')\n\n# look at the first few lines\nkickstarter_2016.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now saving in utf-8\n# save our file (will be saved as UTF-8 by default!)\nkickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_entry = b'\\xa7A\\xa6n'\nprint(sample_entry)\nprint('data type:', type(sample_entry))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = chardet.detect(sample_entry)\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"before = sample_entry.decode('big5-tw')\nnew_entry = before.encode(\"utf-8\", errors=\"replace\")\nprint(new_entry)\nprint('data type:', type(new_entry))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = chardet.detect(new_entry)\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv', 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(100000))\n\nprint(result)\npolice_killings = pd.read_csv('../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv', encoding='Windows-1252')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"police_killings.to_csv('PoliceKillingsUS_utf.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inconsistent data entry","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import fuzzywuzzy\nfrom fuzzywuzzy import process","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read in all our data\nsuicide_attacks = pd.read_csv(\"../input/data-cleaning-challenge-inconsistent-data-entry/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", encoding='Windows-1252')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"suicide_attacks.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we will clean city column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cities = suicide_attacks['City'].unique()\ncities.sort()\ncities","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"there are inconsistency e.g. 'Lahore' & 'Lahore ',  'Lakki Marwat' and 'Lakki marwat' etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to lower case (solved issues like 'Lakki Marwat' and 'Lakki marwat')\nsuicide_attacks['City'] = suicide_attacks['City'].str.lower()\n# remove trailing white spaces (solved issues like 'Lahore' & 'Lahore ')\nsuicide_attacks['City'] = suicide_attacks['City'].str.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get all the unique values in the 'City' column\ncities = suicide_attacks['City'].unique()\ncities.sort()\ncities","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fuzzy matching:**** The process of automatically finding text strings that are very similar to the target string. In general, a string is considered \"closer\" to another one the fewer characters you'd need to change if you were transforming one string into another. So \"apple\" and \"snapple\" are two changes away from each other (add \"s\" and \"n\") while \"in\" and \"on\" and one change away (rplace \"i\" with \"o\"). You won't always be able to rely on fuzzy matching 100%, but it will usually end up saving you at least a little time.\n\nFuzzywuzzy returns a ratio given two strings. The closer the ratio is to 100, the smaller the edit distance between the two strings. Here, we're going to get the ten strings from our list of cities that have the closest distance to \"d.i khan\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the top 10 closest matches to \"d.i khan\"\nmatches = fuzzywuzzy.process.extract(\"d.i khan\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n# take a look at them\nmatches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n    strings = df[column].unique()  # get a list of unique strings\n    matches = fuzzywuzzy.process.extract(string_to_match, strings, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio] # only get matches with a ratio > 90\n    rows_with_matches = df[column].isin(close_matches) # get the rows of all the close matches in our dataframe\n    df.loc[rows_with_matches, column] = string_to_match # replace all rows with close matches with the input matches \n    print(\"All done!\") # let us know the function's done","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"d.i khan\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cities = suicide_attacks['City'].unique()\ncities.sort()\ncities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}