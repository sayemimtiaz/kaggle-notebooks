{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Case Study -- US Election Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This is a case study notebook. It includes processing/cleaning the data and visualisation. Mostly, it is a case study of approaching data analytics from an information theoretical point of view. We will build an ID3 decision tree in the later part of the notebook.\n\n## 4/Sep/2020 <span style=\"color:red\">Caveat</span>\n\nThe notebook is under construction. The algorithm-check (prediction on data and compare with target labels) is ONLY A SANITY CHECK, NOT a standard evaluation procedure.\n\nThe notebook has an accompanying video series:\nhttps://www.youtube.com/playlist?list=PLuXKrCpJ4KeZ1jB3_8EjtC9r4pSUs1rvB\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets \n# preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved \n# outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"usa_2016_presidential_election_by_county = pd.read_csv('/kaggle/input/us-elections-dataset/usa-2016-presidential-election-by-county.csv', sep=';')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Totally, there are {len(usa_2016_presidential_election_by_county)} records\")\n\nusa_2016_presidential_election_by_county.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in usa_2016_presidential_election_by_county.keys():\n    print(k)\n# State\n# County","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = usa_2016_presidential_election_by_county.dropna(subset=[\n    \"Votes16 Clintonh\", \"Votes16 Trumpd\", \n    \"Republicans 2016\", \"Democrats 2016\",\n    \"Republicans 2012\", \"Republicans 2008\", \n    \"Democrats 2012\", \"Democrats 2008\", \"Votes\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Entropy of the vote distributions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's start by asking a random American would vote which candidates in 2016. And let us simplify the query by just considering Trump, D vs Clinton, H.\n\nIf there is no futher information of \"the American\" of interest, the chance is approximately half/half. In terms of entropy, the decision of an average American needs about 1-bit to transmit.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_dem = df[\"Votes16 Clintonh\"].sum() \nn_rep = df[\"Votes16 Trumpd\"].sum()\np_dem = n_dem / (n_dem + n_rep)\np_rep = n_rep / (n_dem + n_rep)\nprint(f\"Votes for DEM {n_dem}, probability {p_dem:.4f}\")\nprint(f\"Votes for REP {n_rep}, probability {p_rep:.4f}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us recall the entropy\n\n$- (p_1 \\log p_1 + p_2 \\log p_2 + ...)$\n\nHere in this example, we only have two classes, and $p_1$ and $p_2$.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ent = - (p_dem * np.log2(p_dem) + p_rep * np.log2(p_rep)).sum()\nprint(f\"Entropy: {ent:.4f}\")\nprint(f\"\"\"This means if you store all the election ballots in 2016, the MINIMUM file size\ncannot be less than {ent * (n_dem+n_rep):.2f} bits\"\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see some places where the people had more obvious preference.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# check out all California \ndf[df[\"State\"] == \"California\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us summarise the entropy computation and report in a function\ndef exam_votes(df_i):\n    n_dem = df_i[\"Votes16 Clintonh\"].sum() \n    n_rep = df_i[\"Votes16 Trumpd\"].sum()\n    p_dem = n_dem / (n_dem + n_rep)\n    p_rep = n_rep / (n_dem + n_rep)\n    print(f\"2016 Vote Statistics {n_dem + n_rep} votes in {len(df_i)} counties\")\n    print(f\"Votes for DEM {n_dem}, probability {p_dem:.4f}\")\n    print(f\"Votes for REP {n_rep}, probability {p_rep:.4f}\")\n    ent = - (p_dem * np.log2(p_dem) + p_rep * np.log2(p_rep)).sum()\n    print(f\"Entropy: {ent:.4f}\")\n    print(f\"\"\"This means if you store all the election ballots in 2016, the MINIMUM file size\ncannot be less than {ent * (n_dem+n_rep):.2f} bits\"\"\")\n    return ent, p_dem, p_rep, n_dem, n_rep\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ent, p_dem, p_rep, n_dem, n_rep = exam_votes(df[df[\"State\"] == \"California\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before we move forward, find a really partisan place to check.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.scatter_geo(df, lat=\"lat\", lon=\"lon\", color=\"Republicans 2016\", hover_name=\"County\", size=\"Votes\")#, \n# you can try to remove \"size\" to get quicker rendering and get smaller counties more visible\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.scatter(df, x=\"Republicans 2016\", y=\"Democrats 2016\", hover_name=\"County\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the record of the county \"District of Columbia, District of Columbia\"\ndf[df[\"County\"] == \"District of Columbia, District of Columbia\"] # only one record","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's to the entropy computation\n_ = exam_votes(df[df[\"County\"] == \"District of Columbia, District of Columbia\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictive task -- setting up and primitive attempt","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we have a measurement of how uncertain / predictable the people in a place is when it comes to voting. The next question is how to apply this analysis to perform some analytics. Say, try to say something about a future election (or an election that the result is not given to the data model).\n\nFirst, let us change our view point from individual ballots to counties (**Why?**). We introduce the targets of interest -- one party won the election in a county, and split the data in two parts. (We will use \"Democrats 2016\" and \"Republicans 2016\" instead of Clinton and Trump's votes where they represent roughly the same kind of information with the former normalised.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Republicans Won 2016\"] = df[\"Democrats 2016\"] < df[\"Republicans 2016\"]\ndf[\"Republicans Won 2012\"] = df[\"Democrats 2012\"] < df[\"Republicans 2012\"]\ndf[\"Republicans Won 2008\"] = df[\"Democrats 2008\"] < df[\"Republicans 2008\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the 2016 results\ndf[\"Republicans Won 2016\"].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob = df[\"Republicans Won 2016\"].value_counts(normalize=True)\nprob = np.array(prob)\nprint(f\"Distribution of *repub won* w.r.t. county is [True (Rep Won), False (Dem Won)]={prob}\")\nent = - (prob * np.log2(prob)).sum()\nprint(f\"Entropy is {ent:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Q: why the prob/ent changed from the previous investigation?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### State","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Following the idea of the previous attempt, we want to consider sub-groups of the population, so that the vote results are more certain. \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# summerise the previous analysis into a county based function\n\ndef exam_counties(df, verbose=True):\n    prob = df[\"Republicans Won 2016\"].value_counts(normalize=True)\n    prob = np.array(prob)\n    ent = - (prob * np.log2(np.maximum(prob, 1e-6))).sum()\n    if verbose:\n        print(f\"Distribution of *repub won* w.r.t. county is [True (Rep Won), False (Dem Won)]={prob}\")\n        print(f\"Entropy is {ent:.4f}\")\n    return ent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try states ...\nstates = df[\"State\"].value_counts()\nstates","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nexam_counties(df[df[\"State\"]==\"Georgia\"], verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_ent = 0\nnum_counties = 0\nfor k, v in states.iteritems():\n    ent = exam_counties(df[df[\"State\"]==k], verbose=False) # in this particular state\n    print(f\"State {k} has {v} counties, result entropy {ent:.3f}\")\n    total_ent += v * ent\n    num_counties += v\n    \nprint(f\"Weighted sum of entropies {total_ent/num_counties :.3f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Knowing the states provides information about the outcome of the counties. Overall, the county-wise results become more predictable given the knowledge of the states. \n\nðŸ‘‰ The statement is **OVERALL**, it does not apply to individual sub-population. See the example below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ent = exam_counties(df[df[\"ST\"]==\"CA\"]) # in this particular state the result is more unpredicable (in terms of counties)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ðŸ‘‰ Note also now the \"population\" consists of individual counties, and narrowing down to a single county won't be of much use.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The difference between the overall uncertainty is the _information gain_ of knowning the \"State\". Let us see another example of using some other information.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Education","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examine the education information.\ndf[[\"Less Than High School Diploma\", \"At Least High School Diploma\",\n    \"At Least Bachelors's Degree\",\"Graduate Degree\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(df, x=\"At Least Bachelors's Degree\", y=\"Democrats 2016\", \n                 color=\"Republicans Won 2016\", color_discrete_sequence=['red','blue'])\nfig.show()\n# fig = px.scatter(df, x=\"At Least Bachelors's Degree\", y=\"Democrats 2016\", color=\"Republicans Won 2016\",\n#                  color_discrete_sequence=['red','blue'], size=\"Votes\")\n# fig.show()\n# You can check the size=\"Votes\" to see how significant the individual counties are\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So it seems that whether there are more than ? percent of population has \"at least bachelors's degree\" can be an indicator of the vote outcome. Let us split the data and do some statistics.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df[\"More Than 30p Bachelors\"] = df[\"At Least Bachelors's Degree\"] > 30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We do the same calculation as above\ntotal_ent = 0\nnum_counties = 0\nattr = \"More Than 30p Bachelors\"\nfor k, v in df[attr].value_counts().iteritems():\n    ent = exam_counties(df[df[attr]==k], verbose=False) # in this particular state\n    print(f\"there are {v} counties where {attr} is {k}, result entropy {ent:.3f}\")\n    total_ent += v * ent\n    num_counties += v\n    \nprint(f\"Weighted sum of entropies {total_ent/num_counties :.3f}\")\n\n# recall that the original entropy is ... (copied from above)\nprob = df[\"Republicans Won 2016\"].value_counts(normalize=True)\nprob = np.array(prob)\nprint(f\"Distribution of *repub won* w.r.t. county is [True (Rep Won), False (Dem Won)]={prob}\")\nent0 = - (prob * np.log2(prob)).sum()\nprint(f\"Entropy is {ent0:.4f}\")\nprint(f\"Info Gain: {ent0 - total_ent/num_counties:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Population\n\nLet's check the following attributes\n\n```\nWhite (Not Latino) Population\nAfrican American Population\nNative American Population\nAsian American Population\nOther Race or Races\nLatino Population\nChildren Under 6 Living in Poverty\nAdults 65 and Older Living in Poverty\nTotal Population\n```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(df, x=\"White (Not Latino) Population\", y=\"Democrats 2016\", color=\"Republicans Won 2016\",\n                color_discrete_sequence=['red','blue'])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"White (Not Latino) Population Is Greater Than 60p\"] = df[\"White (Not Latino) Population\"] > 60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# By now we have a pattern of measuring the information gain\n# 1. get the unique values:\n#    True/False, \n#    Texas, Georgia, Virginia ...\n#    and get the sub-populations (of counties)\n# 2. compute the entropy of the sub-populations\n# 3. get the weighted sum of entropy\n# 4. compare\n# \n\ndef compute_weighted_sub_entropy(df, attr, verbose=True):\n    total_ent = 0\n    num_counties = 0\n    for k, v in df[attr].value_counts().iteritems():\n        ent = exam_counties(df[df[attr]==k], verbose=False) # in this particular sub-population\n        if verbose:\n            print(f\"there are {v} counties where {attr} is {k}, result entropy {ent:.3f}\")\n        total_ent += v * ent\n        num_counties += v\n    \n    weighted_ent = total_ent/num_counties\n    if verbose:\n        print(f\"Weighted sum of entropies {weighted_ent:.3f}\")\n    return weighted_ent\n\nweighted_ent = compute_weighted_sub_entropy(df, \"White (Not Latino) Population Is Greater Than 60p\")\nprint(f\"Info Gain: {ent0 - weighted_ent:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Select which attribute to examine","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It becomes natural / obvious that how to find the most efficient (in terms of getting more information) attribute to examine. \n\nðŸ‘‰ Of course, an immediate question would be \"how to figure out a candidate set of promising attributes\", such as how do you know to check the education level, how do you know to cut at 30p. The answer is in most cases, we don't have a certain strategy, and rely on experience and playing with data (EDA). Machine learning research is heading for the direction where less human experience is needed. For our study of decision trees, let us use the following ones.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes = [\"White (Not Latino) Population\", \n    \"African American Population\",\n    \"Native American Population\",\n    \"Asian American Population\", \n    \"Latino Population\",\n    \"Less Than High School Diploma\",\n    \"At Least High School Diploma\",\n    \"At Least Bachelors's Degree\",\n    \"Graduate Degree\",\n    \"School Enrollment\",\n    \"Median Earnings 2010\",\n    \"Children Under 6 Living in Poverty\",\n    \"Adults 65 and Older Living in Poverty\",\n    \"Preschool.Enrollment.Ratio.enrolled.ages.3.and.4\",\n    \"Poverty.Rate.below.federal.poverty.threshold\",\n    \"Gini.Coefficient\",\n    \"Child.Poverty.living.in.families.below.the.poverty.line\",\n    \"Management.professional.and.related.occupations\",\n    \"Service.occupations\",\n    \"Sales.and.office.occupations\",\n    \"Farming.fishing.and.forestry.occupations\",\n    \"Construction.extraction.maintenance.and.repair.occupations\",\n    \"Production.transportation.and.material.moving.occupations\",\n    \"Median Age\",\n    \"Poor.physical.health.days\",\n    \"Poor.mental.health.days\",\n    \"Low.birthweight\",\n    \"Teen.births\",\n    \"Children.in.single.parent.households\",\n    \"Adult.smoking\",\n    \"Adult.obesity\",\n    \"Diabetes\",\n    \"Sexually.transmitted.infections\",\n    \"HIV.prevalence.rate\",\n    \"Uninsured\",\n    \"Unemployment\",\n    \"Violent.crime\",\n    \"Homicide.rate\",\n    \"Injury.deaths\",\n    \"Infant.mortality\"]\nnew_attributes = []\nfor a in attributes:\n    new_a = \"Quant4.\" + a\n    df[new_a] = pd.qcut(df[a], q=4, labels=[\"q1\", \"q2\", \"q3\", \"q4\"])\n    new_attributes.append(new_a)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vote_info = [\n    \"Votes16 Trumpd\",\n    \"Votes16 Clintonh\",\n    \"State\",\n    \"ST\",\n    \"Fips\",\n    \"County\",\n    \"Precincts\",\n    \"Votes\",\n    \"Democrats 08 (Votes)\",\n    \"Democrats 12 (Votes)\",\n    \"Republicans 08 (Votes)\",\n    \"Republicans 12 (Votes)\",\n    \"Republicans 2016\",\n    \"Democrats 2016\",\n    \"Green 2016\",\n    \"Libertarians 2016\",\n    \"Republicans 2012\",\n    \"Republicans 2008\",\n    \"Democrats 2012\",\n    \"Democrats 2008\"]\ndf_new = df[new_attributes]\ndf_new.dropna('columns', 'any')\ndf_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The compute entropy and info_gain are copied from our exercise notebook,\n# the procedure is as explained in the analysis steps above. \ndef compute_entropy(y):\n    \"\"\"\n    :param y: The data samples of a discrete distribution\n    \"\"\"\n    if len(y) < 2: #  a trivial case\n        return 0\n    freq = np.array( y.value_counts(normalize=True) )\n    return -(freq * np.log2(freq + 1e-6)).sum() # the small eps for \n    # safe numerical computation \n    \ndef compute_info_gain(samples, attr, target):\n    values = samples[attr].value_counts(normalize=True)\n    split_ent = 0\n    for v, fr in values.iteritems():\n        index = samples[attr]==v\n        sub_ent = compute_entropy(target[index])\n        split_ent += fr * sub_ent\n    \n    ent = compute_entropy(target)\n    return ent - split_ent\n\nclass TreeNode:\n    \"\"\"\n    A recursively defined data structure to store a tree.\n    Each node can contain other nodes as its children\n    \"\"\"\n    def __init__(self, node_name=\"\", min_sample_num=10, default_decision=None):\n        self.children = {} # Sub nodes --\n        # recursive, those elements of the same type (TreeNode)\n        self.decision = None # Undecided\n        self.split_feat_name = None # Splitting feature\n        self.name = node_name\n        self.default_decision = default_decision\n        self.min_sample_num = min_sample_num\n\n    def pretty_print(self, prefix=''):\n        if self.split_feat_name is not None:\n            for k, v in self.children.items():\n                v.pretty_print(f\"{prefix}:When {self.split_feat_name} is {k}\")\n                #v.pretty_print(f\"{prefix}:{k}:\")\n        else:\n            print(f\"{prefix}:{self.decision}\")\n\n    def predict(self, sample):\n        if self.decision is not None:\n            # uncomment to get log information of code execution\n            print(\"Decision:\", self.decision)\n            return self.decision\n        else: \n            # this node is an internal one, further queries about an attribute \n            # of the data is needed.\n            attr_val = sample[self.split_feat_name]\n            child = self.children[attr_val]\n            # uncomment to get log information of code execution\n            print(\"Testing \", self.split_feat_name, \"->\", attr_val)\n\n            # [Exercise]\n            # Insert your code here\n            return child.predict(sample)\n\n    def fit(self, X, y):\n        \"\"\"\n        The function accepts a training dataset, from which it builds the tree \n        structure to make decisions or to make children nodes (tree branches) \n        to do further inquiries\n        :param X: [n * p] n observed data samples of p attributes\n        :param y: [n] target values\n        \"\"\"\n        if self.default_decision is None:\n            self.default_decision = y.mode()[0]\n            \n            \n        print(self.name, \"received\", len(X), \"samples\")\n        if len(X) < self.min_sample_num:\n            # If the data is empty when this node is arrived, \n            # we just make an arbitrary decision\n            if len(X) == 0:\n                self.decision = self.default_decision\n                print(\"DECESION\", self.decision)\n            else:\n                self.decision = y.mode()[0]\n                print(\"DECESION\", self.decision)\n            return\n        else: \n            unique_values = y.unique()\n            if len(unique_values) == 1:\n                self.decision = unique_values[0]\n                print(\"DECESION\", self.decision)\n                return\n            else:\n                info_gain_max = 0\n                for a in X.keys(): # Examine each attribute\n                    aig = compute_info_gain(X, a, y)\n                    if aig > info_gain_max:\n                        # [Exercise]\n                        # Insert your code here\n                        info_gain_max = aig\n                        self.split_feat_name = a\n                print(f\"Split by {self.split_feat_name}, IG: {info_gain_max:.2f}\")\n                self.children = {}\n                for v in X[self.split_feat_name].unique():\n                    index = X[self.split_feat_name] == v\n                    self.children[v] = TreeNode(\n                        node_name=self.name + \":\" + self.split_feat_name + \"==\" + str(v),\n                        min_sample_num=self.min_sample_num,\n                        default_decision=self.default_decision)\n                    self.children[v].fit(X[index], y[index])\n\n# Test tree building\ndata = df[new_attributes].dropna('columns', 'any')\ntarget = df[\"Republicans Won 2016\"]\n\nt = TreeNode(min_sample_num=50)\nt.fit(data, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = 0\nerr_fp = 0\nerr_fn = 0\nfor (i, ct), tgt in zip(data.iterrows(), target):\n    a = t.predict(ct)\n    if a and not tgt:\n        err_fp += 1\n    elif not a and tgt:\n        err_fn += 1\n    else:\n        corr += 1\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr, err_fp, err_fn","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}