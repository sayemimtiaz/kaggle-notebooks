{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Image Captioner\n\nResearched and adapted from Hvass_labs code on github and accompanying youtube video\n\n* https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/22_Image_Captioning.ipynb\n* https://youtu.be/uCSTpOLMC48\n\n### Basic Outline\n\n* Encode images into a dense 4096 size vector using vgg16 pretrained model. This theoretcally contains a compressed representation of what is in the image.\n* Tokenize the captions into sequences of numbers which can be used to train a neural network.\n* Build a recurrent neural network which will take as input the image vector. The network will be able to iteratively generate a sequence of words which hopefully will be a well constructed english sentence and relatable to the image. \n* Split the data into training and validating sets, train the network on the training set and if the model is good, the validation images will produce good captions. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#IMPORTS \n\nimport numpy as np\nimport pandas as pd\n\nimport keras\nfrom keras.applications import vgg16\nfrom keras.preprocessing import image, text, sequence\n\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, GRU, Embedding\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport keras.backend as K\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm.notebook import tqdm\n\nimport matplotlib.pyplot as plt\n\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"results_df = pd.read_csv('/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv', delimiter='|')\nresults_df.head()#Extract list of all of the image names\nimage_names = results_df['image_name'][::5].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract list of all of the image names\nimage_names = results_df['image_name'][::5].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Image processing\n* load and resize images to 224x244 pixels\n* process images ready to pass through the vgg16 pre_trained model.\n* Each image is passed through the vgg16 pretrained model to get a compressed representation of itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"#load the vgg16 model and create the encoder which encludes all bar the final dense and softmax layers\n#resulting in a 4096 length vector representation of each image\nvgg = vgg16.VGG16(weights='imagenet', include_top=True)\nencoder = Model(vgg.input, vgg.layers[-2].output)\n\n#encoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preload and process each image with the encoder model. so that we don't have ot repeat the process for each epoch\n#in this implementation we will not be training the encoder so makes sense to precompute vectors\nroot_path = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/'\n\ndef load_process_image(root_path, image_name):\n    \"\"\"\n    load and process an image ready to be fed into the pre_build vgg16 encoder.\n    \"\"\"\n    img = image.load_img(root_path + image_name)\n    img = image.img_to_array(img)\n    img = cv2.resize(img, (224,224))\n    img = vgg16.preprocess_input(img)\n    return img\n\ndef vectorize_images(root_path, image_names):\n    image_vectors = []\n    for image_name in tqdm(image_names):\n        img = load_process_image(root_path, image_name)\n        image_vectors.append(encoder.predict(np.expand_dims(img, axis=0)))\n\n    image_vectors = np.array(image_vectors)\n    image_vectors = image_vectors.squeeze()    \n    return image_vectors\n    \nimage_vectors = vectorize_images(root_path, image_names)                         ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text processing\n* Simple text processing using keras built-in text tokeniser class\n* Remove punctuation\n* Tokenize and transform into number sequences\n* Could pad sequences to equal lengths now. but will do that per batch, when training.\n* Select a maximum vocabulary size. (arbitarily 10000, experiment with other sizes).\n* More advanced tokenizing could be done with nltk or other NLP libraries but this seems to work quite well."},{"metadata":{"trusted":true},"cell_type":"code","source":"#process all of the text\n\nVOCAB_SIZE = 10000\n\ntokenizer = text.Tokenizer(num_words=VOCAB_SIZE)\n\nsequenced_comments = ['ssss ' + str(t) + ' eeee' for t in results_df[' comment']]  # add start and end markers to the sentences\ntokenizer.fit_on_texts(sequenced_comments)\nsequenced_comments = tokenizer.texts_to_sequences(sequenced_comments)\nsequenced_comments = np.array(sequenced_comments)\n\n# reshape into an array of the same length of images but with 5 comments per image. \nsequenced_comments = sequenced_comments.reshape(-1,5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sanity check, make sure the sequences have been encoded and decoded as expected, and reshaped into a matrix which is easy or us to use\ncomment_index = 124\nprint(results_df[' comment'][comment_index])\n' '.join([tokenizer.index_word[i] for i in sequenced_comments[divmod(comment_index, 5)] if i != 0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build decoder model\n* The architecture of this model was copied from Hvass-labs\n* Recommended to use RMSprop as the optimizer by Hvass_labs\n* Hvass_labs found a problem with the builtin sparse_categorical_crossentropy loss and built his own loss function but it seems to work ok here?"},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()\nnp.random.seed(20)\n\nHIDDEN_LAYER_SIZE = 512   # experiment with the number of nodes in the hidden layer\n\ntext_input = Input(shape=(None,))\ntext_embedding = Embedding(VOCAB_SIZE, 64)(text_input)\n\nimage_vector_input = Input(shape=(4096,))\ninital_state = Dense(HIDDEN_LAYER_SIZE, activation='tanh')(image_vector_input)\n\nrecurrent_layer_1 = GRU(HIDDEN_LAYER_SIZE, return_sequences=True)(text_embedding, initial_state=inital_state)\nrecurrent_layer_2 = GRU(HIDDEN_LAYER_SIZE, return_sequences=True)(recurrent_layer_1, initial_state=inital_state)\nrecurrent_layer_3 = GRU(HIDDEN_LAYER_SIZE, return_sequences=True)(recurrent_layer_2, initial_state=inital_state)\n\ntext_output = Dense(VOCAB_SIZE, activation='softmax')(recurrent_layer_3)\n\ndecoder = Model([text_input, image_vector_input], text_output)\n\ndecoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder.compile(optimizer='RMSprop', loss='sparse_categorical_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Text_Generator(keras.utils.Sequence):\n    \n    def __init__(self, image_vectors, sequenced_comments, batch_size=128, shuffle=True):\n        self.image_vectors = image_vectors\n        self.sequenced_comments = sequenced_comments\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.indexes = np.arange(image_vectors.shape[0])\n        self.on_epoch_end()\n        \n        \n    def __len__(self):\n        \"\"\"\n        total number of batches per epoch\n        \"\"\"\n        return self.image_vectors.shape[0]//self.batch_size\n    \n    \n    def __getitem__(self, index):\n        \"\"\"\n        generate a batch of inputs and outputs\n        \"\"\"\n        batch_indexes = self.indexes[(index*self.batch_size): (index+1)*self.batch_size]\n        \n        batch_comments  = [self.sequenced_comments[i, np.random.randint(5)] for i in batch_indexes]\n        batch_comments = sequence.pad_sequences(batch_comments, padding='post', truncating='post')\n        batch_comments = np.array(batch_comments)\n        \n        batch_image_vectors = self.image_vectors[batch_indexes]\n        \n        \n        text_input = batch_comments[:,:-1]\n        text_output = batch_comments[:,1:].reshape(self.batch_size,-1,1)\n        \n        X = [text_input, batch_image_vectors]\n        y = text_output\n    \n        return X, y\n\n    \n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)    \n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split the image_names, image_vectors, sequenced_comments and original comments for training and validation\nimage_names_train, image_names_val, image_vectors_train, image_vectors_val, sequenced_comments_train, sequenced_comments_val, original_comments_train, original_comments_val = train_test_split(image_names, image_vectors, sequenced_comments, results_df[' comment'].values.reshape(-1,5), test_size=0.05, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#generators\ntrain_generator = Text_Generator(image_vectors_train, sequenced_comments_train)\nval_generator = Text_Generator(image_vectors_val, sequenced_comments_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n# model_checkpoint = ModelCheckpoint('model_weights.h5',monitor='val_loss', save_best_only=True)\n# call_backs_list = [early_stopping, model_checkpoint]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model \n\n#decided not to use the call backs as the validation data is too small. the validation data is purely so I have some unseen data to demonstate on.\n# and I do not think the model is overfitting after only 20 epochs. \n\ndecoder.fit_generator(generator=train_generator, epochs=20, validation_data=val_generator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#decoder.load_weights('model_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The results\n* Helper functions to generate the text given an image_vector\n* To generate the caption for the image, we feed the decoder with the image_vector and the input text. Begin with the input text as the start token 'ssss' the decoder will predict the next_word append the next_word to the input text eg ('ssss', 'a'), The decoder will predict a sequence of words, we only need the last word, append this to the input text iterate until the decoder predicts the end token 'eeee'\n* Some predicted captions from the training data (expecting resonable captions as the network has already seen this data)\n* Some predicted captions from the validation data Probably not as good but hopefully still relevant)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_caption(image_vector):\n    \"\"\"\n    Generate an english sentence given an image_vector\n    \"\"\"\n    word = 'ssss'\n    token = tokenizer.word_index[word]\n    sentence = [word]\n    sequence = [token]\n    \n    while word != 'eeee':\n        pred = decoder.predict([[sequence], [image_vector]]).reshape(-1,VOCAB_SIZE)[-1]\n        token = np.argmax(pred)\n        word = tokenizer.index_word[token]\n        sentence.append(word)\n        sequence.append(token)\n        \n    print('generated: ', ' '.join(sentence[1:-1]))\n\ndef get_original_captions(original_captions):\n    for i in range(5):\n        print('original: ', original_captions[i])\n        \ndef get_image(image_name):\n    img = plt.imread(f\"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/{image_name}\")\n    plt.imshow(img)\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions from training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(50):\n    generate_caption(image_vectors_train[i])\n    print()\n    get_original_captions(original_comments_train[i])\n    get_image(image_names_train[i])\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions from validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(50):\n    generate_caption(image_vectors_val[i])\n    print()\n    get_original_captions(original_comments_val[i])\n    get_image(image_names_val[i])\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusions\n* The results from the validation set were  promising. Some of the captions are quite accurate, and even in some of the bad prediction you can see why the model predicted what it did.\n\n* Some of the predictions on the training data were off, suggesting the model is not overfitting therefore could train for longer. \n\n* The vocabulary might not be large enough so the model can't articulate what it wants to say.\n\n* VGG16 output image_vectors of size 4096. In this model we compressed that down to 512 to feed into the decoder, possibly losing a lot of the information? Could experiment with different down sampling, 1024, 2048...\n\n* The Domain space of all images and captions is enormous. this data set of 31000 images barely scratches the surface. I image that to get anywhere near human level of performance in this kind of task might need millions of images.\n\n* With that in mind I am happy with the results of this model."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}