{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kaggle Challenge - COVID-19 Open Research Dataset Challenge (CORD-19)\n\n> https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\n\nThe Team working on this task consists of 5 People:\n* Anna (https://www.kaggle.com/annavoigt)\n* Quirin (https://www.kaggle.com/quirins)\n* George (https://www.kaggle.com/giorgosspanos)\n* Christian (https://www.kaggle.com/crisoss)\n* Ramón (https://www.kaggle.com/ramonrw)\n\nWe focus on the task: **What do we know about virus genetics, origin, and evolution?**\n    \nThe 6 questions with additional 3 sub-questions in this task are diverse and we therefor apply a modular approach to derive actionable insights for each of these questions.\n\n**Agenda**\n\n* Method 1: Pattern Search - Accession Number Search for Genomes and Animal Entities\n   * Introduction\n   * Codebase\n   * Exemplary Answer of a Subquestion\n   * Outlook on this approach\n* Method 2: TF-IDF, Cosine Similarity & RAKE (Keyphrase Extraction)\n   * Introduction\n   * Codebase\n   * Exemplary Answer of a Subquestion\n   * Outlook on this approach   \n* Method 3: Keyword-based Scientific Paper Recommendation Engine\n   * Introduction\n   * Codebase\n   * Exemplary Answer of a Subquestion\n   * Outlook on this approach\n* Creative Outlook of our Findings / Concluding Remarks on the Methods\n   * Geographical (and temporal) Visualization about ...:\n      * Mentioned Animals/Hosts\n      * Accession number\n      * Number Paper found based on unviversity, named areas ...\n   * Time-control about your findings  (in a tool integrated / dynamically?)\n   * Combination of Tools\n      * Pattern Search on a subset\n      * Create Subsets based on Pattern Search\n      * Create tables based on multiple criterias (task relevant subsets via keyword search, add column(s) with key phrases)\n      * Keyword search + Rake (Approach for answering questions directly)\n   * Clustering\n      * Based on key phrases (Rake) (Outlier detection maybe interesting)\n      \n      \n**Exemplary answers to subquestions**\n\n\n1. Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time\n\n   * Genome pattern search on subset of paper + geographic constraint\n\n2. Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged\n\n   * Genome pattern search on subset of paper + geographic constraint + comparing differences\n\n\n3. Animal host(s) and any evidence of continued spill-over to humans\n\n   * Creating subsets of paper and find all mentions about animals"},{"metadata":{},"cell_type":"markdown","source":"# Method 1: Accession Number Search for Genomes (Pattern Search)\n\n### Introduction\n\nWhat is the host range for the corona virus and are there known cases of spill over from animals to humans? To answer such questions differentiated a basic precondition is to distinguish between known corona-related viral strains described in papers which may also enable to scale the potential danger certain strains exhibit and allow for the precise search of differences between strains. An efficient way of discriminating between corona strains and to search for them in papers is using the accession numbers of published viral genomes. These identifiers for each genome are found in a databank accessible via the National Center of Biotechnology Information (NCBI). In this data the accession numbers linked to all known viral genomes are collected. Furthermore, potentially helpful details such as the known host groups e.g. humans and vertebrates as well as the publishing and latest editing date are noted. To exploit these information three submodules have been developed. Submodule 1 creates a copy of the NCBI table providing the data of interest in a usable format for further algorithms such as submodule 2. This module links the data from NCBI with the given papers creating a new table containing all accession numbers of corona-related virus strains of every paper. This allows a directed search for papers including information about specific viral strains. Entries of this table could be used as a basis to identify animals probably serving as corona-hosts by applying a Named Entity Recognition approach on papers containing accession numbers of viral strains known to be found in both, humans and animals (submodule 3). Following, these submodules with a first attempt in identifying corona hosts as well as an outlook how to further make use of this accession number approach will be described below.\n\n**List of content for this method:**\n* Codebase\n   * Initialize Notebook\n   * Submodule 1: retrieve_listofgenomes\n   * Submodule 2: process article - Function 1,2 & 3\n   * Submodule 3: textscore_animal\n* Exemplary Answer of a Subquestion - Livestock exemplary with Cattle, Sheep and Pig\n   * Identify Assession Number associated with Host (Vertebrate and Human) which has the most mentions in papers and the one most recently updated.\n   * Use Submodule 3 on all paper associated with this Assession Number\n   * Discuss results in identified animals\n* Concluding remarks on method and outlook for next steps\n"},{"metadata":{},"cell_type":"markdown","source":"### Codebase\n\n#### Initialize Notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install wikipedia\n!pip install rake-nltk\n!pip install scispacy\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz\n!pip install pytextrank\n!pip install nltk\n!pip install requests\n!pip install spacy\n!python -m spacy download en_core_web_lg\n\n#Libraries\nimport os\nimport io\nimport re\nimport requests\nimport pandas as pd\n\n#Read all paths, create a list and store it as csv.\npaths = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #if os.path.splitext(filename)[-1] == \".json\" and filename.endswith('.xml.json') is not True:\n        if filename.endswith(\".json\") is True and filename.endswith('.xml.json') is not True:\n            paths.append(os.path.join(dirname, filename))\n        else:\n            continue\n        \npd.DataFrame(paths).to_csv(\"/kaggle/working/paths.csv\")\npath_list = pd.read_csv(\"/kaggle/working/paths.csv\").iloc[:,1].to_list()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submodule 1: retrieve_listofgenomes\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def retrieve_listofgenomes(url='https://www.ncbi.nlm.nih.gov/genomes/GenomesGroup.cgi?taxid=10239&cmd=download',\n                           viruspattern = 'corona'):\n    \n    '''\n    This code extracts all information from an ncbi genome sample table. Especially virusname, \n    dates of creation & last update and NCBI Accession Code to be referred to in research.\n    \n    @param url is predefined for ncbi webpage - table with virus genomes. Another link will not work, \n            because the code is highly customized to this specific table.\n    @param viruspattern is predefined to search for corona viruses. This can be adjusted to any other keyword.\n    @output pandas table with the 11 attributes defined on the ncbi webpage filtered for the given viruspattern.\n    '''\n    \n    import pandas as pd\n    import re\n    import requests\n    import io\n    \n    s=requests.get(url).content\n    column_names = []\n    counter = 0\n        \n    for n, line in enumerate(io.StringIO(s.decode('utf-8'))):\n        \n        if n == 1:\n            for chunknumber, chunk in enumerate(line.split('\\t')):\n                tmp = re.sub(r'[\\\")(,;\\r\\n\\[\\]]','',chunk)\n                column_names.append(tmp)\n        \n        genome_information = dict.fromkeys(column_names, [])\n    \n    listofcoronaviruses = pd.DataFrame(genome_information)\n    \n\n    for n, line in enumerate(io.StringIO(s.decode('utf-8'))):\n                \n        if viruspattern in line:\n            if len(line.split('\\t')) < 11:\n                pass\n            else:\n                for chunknumber, chunk in enumerate(line.split('\\t')):\n                    if chunknumber == 11:\n                        pass\n                    else:\n                        listofcoronaviruses.loc[counter, list(genome_information.keys())[chunknumber]] = ' '.join(chunk.split())\n                        \n                counter += 1\n\n    return listofcoronaviruses","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Execute the function and store the results in a csv file with the name **finallistofcoronaviruses.csv**."},{"metadata":{"trusted":true},"cell_type":"code","source":"listofcoronaviruses = retrieve_listofgenomes()\nlistofcoronaviruses.to_csv(\"finallistofcoronaviruses.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submodule 2: process article - Function 1,2 & 3\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function 1 - Extract Dict from Path\n\ndef preprocess_article(path):\n    '''\n    @param path from json document in kaggle challenge (as prepared in for statement * path_list = pd.read_csv(\"/kaggle/working/paths.csv\").iloc[:,1].to_list() * )\n    @output article_dict is a dictionary with paper_id, title, authors, abstract and textbody in one row to be further processed in pandas\n    '''\n    \n    import json\n    \n    with open(path) as file:\n        article_dict_load = json.load(file)\n        \n        #Add PaperID and Title to dict\n        article_dict = {'paper_id': [article_dict_load['paper_id']],\n                        'title': [article_dict_load['metadata']['title']]\n                       }\n        \n        #Add Authors to dict\n        authors_list = []\n        for i in range(len(article_dict_load['metadata']['authors'])):\n            try:\n                authors_list.append(article_dict_load['metadata']['authors'][i]['first'][0] + '. ' + article_dict_load['metadata']['authors'][i]['last'])            \n            except:\n                authors_list.append(article_dict_load['metadata']['authors'][i]['last'])\n        article_dict['authors'] = [', '.join(authors_list)]\n        \n        #Add Abstract to dict\n        if len(article_dict_load['abstract']) == 1:\n            article_dict['abstract'] = [article_dict_load['abstract'][0]['text'].replace('\\\"', '\\'')]\n        else:\n            abstract_list = []\n            for i in range(len(article_dict_load['abstract'])):\n                abstract_list.append(article_dict_load['abstract'][i]['text'].replace('\\\"', '\\''))\n            article_dict['abstract'] = [' '.join(abstract_list)]\n                    \n        #Add textbody to dict\n        if len(article_dict_load['body_text']) == 1:\n            article_dict['textbody'] = [article_dict_load['body_text'][0]['text'].replace('\\\"', '\\'')]\n        else:\n            textbody_list = []\n            for i in range(len(article_dict_load['body_text'])):\n                textbody_list.append(article_dict_load['body_text'][i]['text'].replace('\\\"', '\\''))\n            article_dict['textbody'] = [' \\n '.join(textbody_list)]\n    \n    return article_dict\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function 2 - Evaluate for Accession Number Appearance\n\ndef enhance_articledict(article_dict, finallistofcoronaviruses):\n    '''\n    @param article_dict is python dictionary resulting from a file processed with the function \"preprocess_article\"\n    @param finallistofcoronaviruses is a list of NCBI Accession numbers related to a Coronavirus\n    @output the dict is added with additional keys with the respective viruses referred to in the article based on the accession number. The value is 1.\n    '''\n    \n    import re\n    \n    for corona in finallistofcoronaviruses:\n        for chunk in article_dict['abstract'][0].split(' '): #Probably is splitting not necessary for this stage. Maybe potential optimization.\n            if re.findall(corona, chunk):\n                article_dict[corona] = [1]\n                #print(corona)\n        \n        for chunk in article_dict['textbody'][0].split(' '): #Probably is splitting not necessary for this stage. Maybe potential optimization.\n            if re.findall(corona, chunk):\n                article_dict[corona] = [1]\n                #print(corona)\n      \n    return article_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function 3 - Create enhanced Pandas Entry for Path\n\ndef retrieve_relevant_paper(path, path_finallistofcoronaviruses = \"/kaggle/working/finallistofcoronaviruses.csv\"):\n    '''\n    @param path takes an os-path referring to an json file containing a scientific article.\n    @output pandas table to be appended \n    '''\n    \n    import pandas as pd\n    \n    finallistofcoronaviruses = pd.read_csv(path_finallistofcoronaviruses).iloc[:,2].to_list()\n    \n    article_dict = preprocess_article(path)\n    \n    article_dict_enhanced = enhance_articledict(article_dict, finallistofcoronaviruses)\n    \n    keys = list(article_dict.keys())\n    keys.extend(finallistofcoronaviruses)\n    \n    df = pd.DataFrame({key: [] for key in keys})\n    df = df.append(pd.DataFrame(article_dict_enhanced))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let these function run on all documents and store them in a csv file."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport time\n\npath_list = pd.read_csv(\"/kaggle/working/paths.csv\").iloc[:,1].to_list()\n\nstart_time = time.time()\n\nfor n, path in enumerate(path_list):\n    if n == 0:\n        df = retrieve_relevant_paper(path, path_finallistofcoronaviruses = \"/kaggle/working/finallistofcoronaviruses.csv\")\n    else:\n        df = df.append(retrieve_relevant_paper(path, path_finallistofcoronaviruses = \"/kaggle/working/finallistofcoronaviruses.csv\"))\n        \nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\ndf.to_csv('finaldataframe.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submodule 3: textscore_animal"},{"metadata":{"trusted":true},"cell_type":"code","source":"def textscore_animal(spacy_nlp, inputtext, animal = 'animal', modus = 'default', max_length = 50000):\n    '''\n    The best way to utilize this module efficiently is to load the spacy model of your choice, preferrable 'en_core_web_lg' before and input it to the function as spacy_nlp. \n    This will decrease a single computation by over 90% as the loading of the model is the bottleneck and should be done outside your iteration.\n    \n    @param spacy_nlp accepts a string referring to a spacy model or an english spacy model with type() spacy.lang.en.Englisch\n    @param inputtext accepts a string in whatever length\n    @param animal accepts a string of length 1 word\n    @param modus has 3 attributes\n        'default' - for maximal value of score\n        'list' - list of all word scores\n    @param max_length the spacy model has a maximum character length it can sufficiently deal with (this is 100000). We set a max_length to deal with long text input by splitting it into chunks.\n    @output animal_score according to modus \n    '''\n    \n    import spacy\n    \n    #Test Spacy model and load if necessary\n    if isinstance(spacy_nlp, str):\n        nlp = spacy.load(spacy_nlp)\n    elif type(spacy_nlp) == spacy.lang.en.English:\n        nlp = spacy_nlp\n    else:\n        print(\"Variable spacy_nlp has the wrong format. It is neither a string or a spacy.lang.en.English model.\")\n    \n    #Initialize Scores List\n    scores = []\n     \n    #Create Scores comparing vocab lists.\n    if isinstance(inputtext, str) and isinstance(animal, str) and len(animal.split()) == 1:\n        animal_vocab = nlp.vocab[animal]\n        if len(inputtext) > max_length:\n            for split in inputtext.split('.'):\n                for token in nlp(split):\n                    if nlp.vocab[token.text].vector[0] != 0.0 and (token.tag_ == \"NN\" or token.tag_ == \"NNP\"):\n                        scores.append(nlp.vocab[token.text].similarity(animal_vocab))\n        else:\n            for token in nlp(inputtext):\n                if nlp.vocab[token.text].vector[0] != 0.0 and (token.tag_ == \"NN\" or token.tag_ == \"NNP\"):\n                    scores.append(nlp.vocab[token.text].similarity(animal_vocab))\n    else:\n        print(\"Variable inputtext or animal has the wrong format. It is not a string.\")\n    \n    #Create output according to modus.\n    if scores == []:\n        print('No Scores...!')\n        animal_score = 0\n    elif modus == 'default':\n        animal_score = max(scores)\n    elif modus == 'list':\n        animal_score = scores\n    #'average' could also be a viable modus with giving the average of the highest 10 values. Variations of it are also possible\n    else:\n        print('Variable modus has the wrong format. It only accepts a string with default or list.')\n        \n    return animal_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exemplary Answer of a Subquestion - Livestock exemplary with Cattle, Sheep and Pig\n\n\n#### Identify Assession Number associated with Host (Vertebrate and Human) which has the most mentions in papers and the one most recently updated."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ncoronaviruses = pd.read_csv(\"finallistofcoronaviruses.csv\")\ncoronaviruses = pd.DataFrame(coronaviruses.loc[coronaviruses['Host'].str.contains('human, vertebrates')]['Accession'].reset_index(drop = True))\ncoronaviruses['scores'] = None\ndocument_df = pd.read_csv('finaldataframe.csv')\nscores = []\nfor i, corona in enumerate(coronaviruses['Accession']):\n    coronaviruses.loc[i, 'scores'] = sum(document_df[corona] == 1)\n\ncovid_candidate = coronaviruses.loc[coronaviruses['scores'].values.argmax(), 'Accession']\nprint(covid_candidate) #Most mentioned\n\ncoronaviruses = pd.read_csv(\"finallistofcoronaviruses.csv\")\ncovid_candidate_no2 = coronaviruses.loc[pd.to_datetime(coronaviruses['Date updated']).values.argmax(),'Accession']\nprint(covid_candidate_no2) #Most recent updated\n\ncolumn_names1 = ['title', 'textbody', 'abstract', covid_candidate]\ndocument_df1 = document_df[column_names1]\ndocument_df1 = document_df1[document_df1[covid_candidate] == 1].reset_index(drop=True)\n\ncolumn_names2 = ['title', 'textbody', 'abstract', covid_candidate_no2]\ndocument_df2 = document_df[column_names2]\ndocument_df2 = document_df2[document_df2[covid_candidate_no2] == 1].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Use Submodule 3 on all paper associated with this Assession Number"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display\nimport spacy\n\nlivestock = ['cattle', 'sheep', 'pig']\nspacy_nlp = spacy.load('en_core_web_lg')\n\nfor animal in livestock:\n    document_df1[animal] = None\n    for n, document in enumerate(document_df1['textbody']):\n        document_df1.loc[n, animal] = textscore_animal(spacy_nlp, document, animal = animal, modus = 'default')\n\nfor animal in livestock:\n    document_df2[animal] = None\n    for n, document in enumerate(document_df1['textbody']):\n        document_df2.loc[n, animal] = textscore_animal(spacy_nlp, document, animal = animal, modus = 'default')\n\ndisplay(document_df1)\ndisplay(document_df2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Discuss results in identified animals"},{"metadata":{"trusted":true},"cell_type":"code","source":"coronaviruses = pd.read_csv(\"finallistofcoronaviruses.csv\")\ncoronaviruses1 = coronaviruses.loc[coronaviruses['Accession'] == covid_candidate].loc[:, 'Genome'].reset_index(drop=True)\ncoronaviruses2 = coronaviruses.loc[coronaviruses['Accession'] == covid_candidate_no2].loc[:, 'Genome'].reset_index(drop=True)\nlivestock = ['cattle', 'sheep', 'pig']\nfor stock in livestock:\n    print('''\n    {}\\n\n    A Preliminary Analysis on {} as a representant of Livestock and Coronaviruses yielded the following result:\n    The most recent updated virus is the \\\"{}\\\", we identified {} references in the scientific literature.\n    On a threshold of 66% of similarity to {}, we counted {} articles mentioning {}.\n    This is in about {:2f}% of all articles referencing this coronavirus.\\n\n    The most refferenced virus is the \\\"{}\\\", we identified {} references in the scientific literature.\n    On a threshold of 66% of similarity to {}, we counted {} articles mentioning {}.\n    This is in about {:2f}% of all articles referencing this coronavirus.\\n\\n\n    '''.format(stock, \n                stock, \n                coronaviruses2[0], len(document_df2[stock]),\n                stock, sum(document_df2[stock] > 0.66), stock,\n                sum(document_df2[stock] > 0.66) / len(document_df2[stock]),\n                coronaviruses1[0], len(document_df1[stock]),\n                stock, sum(document_df1[stock] > 0.66), stock,\n                sum(document_df1[stock] > 0.66) / len(document_df1[stock])\n              ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Concluding remarks on method and outlook for next steps\n\n#### Strengths\n* NER (Named Entity Recognition) Approach can also be used to identify other relevant adjectives or nounes in a paper. The method is currently only used for animal recognition with regard to potential livestock. Interesting approaches might be to identify relevant and important keywords with regard to the subquestions, such as Behavioral Risk or Spillover.\n* The approach can be easily adapted to other Virus Databases and Identificationnumbers.\n* It's explainable and not \"blackbox\".\n\n#### Challenges\n* The identification of an animal is not very precise and not yet optimized for a recognition of livestock in a paper. There are two options to improve on this:\n   1. Improve on the NER Approach, which is not very efficient.\n   2. Identify another method to identify relevant mentionings of livestock in a paper.\n* The mentioning of Livestock with a vertebrate hosted coronavirus does not allow for deductions of context. EG the sentences: a) Pig is affected by Coronavirus, or b) Pic is not affected by Coronavirus, does currently lead to the same results. You may deduct that a positive relation is mentioned more often, as a negative is just expected and \"known\". There is no evidence allowing for such a deduction.\n* Are these useful keywords (kattle, sheep, pig) for livestock? There is no test to evaluate the performance of the specific question/approach.\n\n#### Outlook\n* The referenced NCBI Accession numbers is an easy to implement and strong Indicator for Relevance with regard to a specific virus type. As they are 44 Coronaviruses reported, it is important to differentiate between them. This approach gives you a fast but efficient filter either before your selecting algorithms (such as TF-IDF) or afterwards, when you are tracking specific insights (with eg RAKE). In combination with our presented methods, two combinations seems obvious to be tested:\n   1. Utilization in Combination with Method 2 as Prefilter before TF-IDF or as Prefilter before RAKE to identify important / relevant Paragraphs ('body_text').\n   2. Utilization in Combination with Method 3 as Scoring Criteria with Method 3.\n* The Approach to identify animals will be improved as described above.\n* This method will be applied to every subquestion to deduce specific insights based on the Genome Accession Numbers and the method to identify specific entities in a text.\n* Enhance Entity Module to identify relevant Database and Research Organisations, to give hints on the utilization of specific genomes. Could be deducted from Metadata, but also from the texts itself.\n* Create a submodule able to identify geographic information. This can be used to identify the distribution of a sample, the spread of the virus or to identify pattern with regard to animals. (A reference to an animal could be strongly biased by the country a research is conducted in.)"},{"metadata":{},"cell_type":"markdown","source":"# **Method 2: TF-IDF, Cosine Similarity & RAKE (Keyphrase Extraction)**"},{"metadata":{},"cell_type":"markdown","source":"**Introduction**"},{"metadata":{},"cell_type":"markdown","source":"In this approach we will try to rank each document against a tes word list, wiith TF-ODF and Cosiine Similairty. Afte the ranking we will test the top ranked documents with RAKE for keyphrase extraction that will hlp the research.\n\nThe code base that we will se in the following chapter, will be splited into smaller tasks and then the main loop will follow, as for optimization reasons, all the file ranking will be done sequencially (one file at a time will get ranked and appended to a output file)."},{"metadata":{},"cell_type":"markdown","source":"**Codebase**"},{"metadata":{},"cell_type":"markdown","source":"    The following chapter will highlight the code that needs to be run, for the document ranking."},{"metadata":{},"cell_type":"markdown","source":"    Import and initial definitions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add all import and preprocessor definitions\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nfrom nltk.stem import WordNetLemmatizer # used for preprocessing\n\n#!pip install num2words\n#from num2words import num2words\n\nimport nltk\nimport os\nimport string\nimport numpy as np\nimport copy\nimport pandas as pd\nimport pickle\nimport re\nimport math\nimport time\nimport datetime\n\nfrom csv import writer\nimport json\nfrom collections import OrderedDict\n\nfrom multiprocessing import Process, Value, Lock, Manager, Pool\nfrom joblib import Parallel, delayed, parallel_backend\nfrom math import modf\n\nfrom functools import lru_cache\nfrom sklearn.metrics.pairwise import cosine_similarity as cosine_similarity_new\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")                     #Ignoring unnecessory warnings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the folowing definition, we see the preprocessor for the texts. We choose stemming istaid of leminiization, as it's faster and we are quering a single language, english to engish rarnking."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Initializing definitions\", end=\"\", flush=True)\ndef convert_lower_case(data):\n    return np.char.lower(data)\n\n\n# remove urls, handles, and the hashtag from hashtags (taken from https://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression)\ndef remove_urls(text):\n    new_text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n    return new_text\n# make all text lowercase\ndef text_lowercase(text):\n    return text.lower()\n# remove numbers\ndef remove_numbers(text):\n    result = re.sub(r'\\d+', '', text)\n    return result\n# remove punctuation\ndef remove_punctuation(text):\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n# tokenize\ndef tokenize(text):\n    text = word_tokenize(text)\n    return text\n# remove stopwords\nstop_words = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    text = [i for i in text if not i in stop_words]\n    return text\n# lemmatize\nlemmatizer = WordNetLemmatizer()\ndef lemmatize(text):\n    text = [lemmatizer.lemmatize(token) for token in text]\n    return text\n\n#instantiate stemmer\nstemmer = PorterStemmer()\ndef stemming(text):\n    text = [stemmer.stem(token) for token in text]\n    return text\n\ndef preprocess(text):\n    text = text_lowercase(text)\n    text = remove_urls(text)\n    text = remove_numbers(text)\n    text = remove_punctuation(text)\n    text = tokenize(text)\n    text = remove_stopwords(text)\n    text = stemming(text)\n    text = remove_stopwords(text)\n    #text = lemmatize(text)\n    return text\n\n\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\n\nprint(\"...[DONE]\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Next we need to read the dataset of documents that we have available"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Reading all file names\", end=\"\", flush=True)\nalpha = 0.3\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nN = 0\nfiles = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename.lower().endswith((\".json\")):\n            files.append(os.path.join(dirname, filename))\n            N = N + 1\n#for x in folder:\n#    print(x)\n# Any results you write to the current directory are saved as output.\n\nprint(\"...[DONE]\")\n\nprint(\"Number of files to calculate: \", end=\"\", flush=True)\nprint(N)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will define some extra function that will help the matching score , and some helper function that may be required like print_doc , that will help debugging"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"extra definitions\", end=\"\", flush=True)\ndef extract_values(obj, key):\n    \"\"\"Pull all values of specified key from nested JSON.\"\"\"\n    arr = []\n\n    def extract(obj, arr, key):\n        \"\"\"Recursively search for values of key in JSON tree.\"\"\"\n        if isinstance(obj, dict):\n            for k, v in obj.items():\n                if isinstance(v, (dict, list)):\n                    extract(v, arr, key)\n                elif k == key:\n                    arr.append(v)\n        elif isinstance(obj, list):\n            for item in obj:\n                extract(item, arr, key)\n        return arr\n\n    results = extract(obj, arr, key)\n    return results\n\n##DEFINES\ndef print_doc(id):\n    print(dataset[id])\n    file = open(dataset[id][0], 'r', encoding='cp1250')\n    text = file.read().strip()\n    file.close()\n    print(text)\n\ndef doc_freq(word):\n    c = 0\n    try:\n        c = DF[word]\n    except:\n        pass\n    return c\n\ndef matching_score(k, tokens, tf_idf):\n    query_weights = {}\n\n    for key in tf_idf:\n        \n        if key[1] in tokens:\n            try:\n                query_weights[key[0]] += tf_idf[key]\n            except:\n                query_weights[key[0]] = tf_idf[key]\n    \n    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n    \n    l = []\n    \n    for i in query_weights[:k]:\n        l.append(i[1])\n    \n    if not l:\n        l.append(0)\n        \n    return l\n\ndef cosine_sim(a, b):\n    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n    return cos_sim\n    \ndef gen_vector(tokens,total_vocab):\n\n    Q = np.zeros((len(total_vocab)))\n    \n    counter = Counter(tokens)\n    words_count = len(tokens)\n\n    query_weights = {}\n    \n    for token in np.unique(tokens):\n        \n        tf = counter[token]/words_count\n        df = doc_freq(token)\n        idf = math.log((N+1)/(df+1))\n\n        try:\n            ind = total_vocab.index(token)\n            Q[ind] = tf*idf\n        except:\n            pass\n    return Q\n\ndef cosine_similarity(k, tokens,total_vocab, D, dataset):\n    d_cosines = []\n    \n    query_vector = gen_vector(tokens,total_vocab)\n    \n    for d in D:\n        score = cosine_sim(query_vector, d)\n        d_cosines.append(cosine_sim(query_vector, d))\n        \n    return d_cosines\n\ndef printProgressBar(iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█'):\n    \"\"\"\n    Call in a loop to create terminal progress bar\n    @params:\n        iteration   - Required  : current iteration (Int)\n        total       - Required  : total iterations (Int)\n        prefix      - Optional  : prefix string (Str)\n        suffix      - Optional  : suffix string (Str)\n        decimals    - Optional  : positive number of decimals in percent complete (Int)\n        length      - Optional  : character length of bar (Int)\n        fill        - Optional  : bar fill character (Str)\n    \"\"\"\n    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n    filledLength = int(length * iteration // total)\n    bar = fill * filledLength + '-' * (length - filledLength)\n    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n    # Print New Line on Complete\n    if iteration == total:\n        print()\n\n# function to add to JSON \ndef write_json(data, filename='data.json'): \n    with open(filename,'w') as f: \n        json.dump(data, f, indent=4) \n#END OF NEW DEFINES\n\ndef write_csv(list_of_elem, filename='data.csv'):\n    # Open file in append mode\n    with open(filename, 'a+', newline='') as write_obj:\n        # Create a writer object from csv module\n        csv_writer = writer(write_obj)\n        # Add contents of list as last row in the csv file\n        csv_writer.writerow(list_of_elem)\n\n\nprint(\"...[DONE]\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Next is esenssial to define the testwords in a testword vector, that will be used to rank each file for each task that we have"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Testwords initializing\", end=\"\", flush=True)\n\n# Corona\nlst_corona = [\"Corona\", \"corona\", \"corona virus\", \"coronavirus\", \"corona viruses\", \"coronaviruses\", \"Coronaviridae\", \"coronaviridae\", \"COVID-19\", \"Covid-19\", \"covid-19\", \"COVID\", \"COV\", \"SARS\"]\n\n# Main Task\nlst_genetics = [\"genetics\"]\nlst_origin = [\"origin\", \"member\", \"family\"]\nlst_evolution = [\"evolution\", \"development\", \"develops\", \"developed\"]\nlst_task = [lst_genetics, lst_origin, lst_evolution]\n\n# Sub task 1 - Real-time tracking ...\nlst_subtask_1_genome = [\"Genome\", \"genome\"]\nlst_subtask_1_dissemination = [\"dissemination\", \"Dissemination\", \"propagation\", \"Propagation\", \"spread\", \"Spread\", \"spreading\", \"Spreading\"]\nlst_subtask_1_treatment = [\"treatment\", \"Treatment\", \"diagnostic\", \"Diagnostic\", \"diagnostics\", \"Diagnostics\", \"therapeutics\", \"Therapeutics\"]\nlst_subtask_1_variation = [\"Difference\" , \"in contrast\", \"variation\", \"deviation\", \"shows mutations\", \"enrichment\", \"similarities\"]\nlst_subtask_1_reference = [\"Accession number\", \"reference\", \"sample\", \"identification of\"]\nlst_subtask_1_known = [\"Known\", \"already published\", \"already reported\"]\nlst_subtask_1 = [lst_subtask_1_genome, lst_subtask_1_dissemination, lst_subtask_1_treatment, lst_subtask_1_variation, lst_subtask_1_reference, lst_subtask_1_known, lst_corona]\n\n# Sub task 2 - Access to geographic ...\nlst_subtask_2_1 = []\nlst_subtask_2_2 = []\nlst_subtask_2_3 = []\nlst_subtask_2_4 = []\nlst_subtask_2_5 = []\nlst_subtask_2 = [lst_subtask_2_1, lst_subtask_2_2, lst_subtask_2_3, lst_subtask_2_4, lst_subtask_2_5]\n# Sub task 3 - Evidence that livestock ...\n\n# Sub sub task 3-1\nlst_subtask_3_1_livestock = [\"farm\" , \"wildlife\", \"wild animal\", \"undomesticated\", \"livestock\"]\nlst_subtask_3_1_area = [\"Southeast-Asia\"]\nlst_subtask_3_1_control = [\"surveil\", \"control\", \"screen\", \"check\", \"monitor\", \"examine\"]\nlst_subtask_3_1 = [lst_subtask_3_1_livestock, lst_subtask_3_1_area, lst_subtask_3_1_control, lst_corona]\n\n# Sub sub task 3-2\nlst_subtask_3_2_livestock = [\"farm\" , \"wildlife\", \"wild animal\", \"undomesticated\", \"livestock\"]\nlst_subtask_3_2_area = [\"Southeast-Asia\"]\nlst_subtask_3_2_control = [\"surveil\", \"control\", \"screen\", \"check\", \"monitor\", \"examine\"]\nlst_subtask_3_2 = [lst_subtask_3_2_livestock, lst_subtask_3_2_area, lst_subtask_3_2_control, lst_corona]\n\n# Sub sub task 3-3\nlst_subtask_3_3_1_host = [\"host\" , \"organism\", \"human\"]\nlst_subtask_3_3_2_infection = [\"infection\", \"disease\", \"respiratory syndrom\"]\nlst_subtask_3_3_3_lab = [\"experimental\", \"laboratory\", \"under conditions\"]\nlst_subtask_3 = [lst_subtask_3_3_1_host, lst_subtask_3_3_2_infection, lst_subtask_3_3_3_lab, lst_corona]\n\n# Sub task 4\nlst_subtask_4_host = [\"animal\", \"host\", \"hosts\", \"Host\", \"Hosts\", \"human\", \"Human\", \"Humans\", \"humans\", \"CoV-Host\", \"organism\"]\nlst_subtask_4_transmission = [\"pathogen\", \"spill-over\", \"intraspecies\", \"interaction\", \"host-shift\", \"spread\", \"evolution\", \"transmission\", \"infection\"]\nlst_subtask_4_evidence = [\"evidence\", \"proof\", \"association\", \"connection\", \"associated\"]\nlst_subtask_4 = [lst_subtask_4_host, lst_subtask_4_transmission, lst_subtask_4_evidence, lst_corona]\n\n# Sub task 5\nlst_subtask_5_1 = []\nlst_subtask_5_2 = []\nlst_subtask_5_3 = []\nlst_subtask_5_4 = []\nlst_subtask_5 = []\n\n# Sub task 6\nlst_subtask_6_1 = []\nlst_subtask_6_2 = []\nlst_subtask_6_3 = []\nlst_subtask_6_4 = []\nlst_subtask_6_5 = []\nlst_subtask_6 = []\n\ntestwords = []\ntestwords.append([\"lst_corona\",preprocess(\" \".join(lst_corona))])\ntestwords.append([\"lst_genetics\",preprocess(\" \".join(lst_genetics))])\ntestwords.append([\"lst_origin\",preprocess(\" \".join(lst_origin))])\ntestwords.append([\"lst_evolution\",preprocess(\" \".join(lst_evolution))])\ntestwords.append([\"lst_subtask_1_genome\",preprocess(\" \".join(lst_subtask_1_genome))])\ntestwords.append([\"lst_subtask_1_dissemination\",preprocess(\" \".join(lst_subtask_1_dissemination))])\ntestwords.append([\"lst_subtask_1_treatment\",preprocess(\" \".join(lst_subtask_1_treatment))])\ntestwords.append([\"lst_subtask_1_variation\",preprocess(\" \".join(lst_subtask_1_variation))])\ntestwords.append([\"lst_subtask_1_reference\",preprocess(\" \".join(lst_subtask_1_reference))])\ntestwords.append([\"lst_subtask_1_known\",preprocess(\" \".join(lst_subtask_1_known))])\ntestwords.append([\"lst_subtask_2_1\",preprocess(\" \".join(lst_subtask_2_1))])\ntestwords.append([\"lst_subtask_2_2\",preprocess(\" \".join(lst_subtask_2_2))])\ntestwords.append([\"lst_subtask_2_3\",preprocess(\" \".join(lst_subtask_2_3))])\ntestwords.append([\"lst_subtask_2_4\",preprocess(\" \".join(lst_subtask_2_4))])\ntestwords.append([\"lst_subtask_2_5\",preprocess(\" \".join(lst_subtask_2_5))])\ntestwords.append([\"lst_subtask_3_1_livestock\",preprocess(\" \".join(lst_subtask_3_1_livestock))])\ntestwords.append([\"lst_subtask_3_1_area\",preprocess(\" \".join(lst_subtask_3_1_area))])\ntestwords.append([\"lst_subtask_3_1_control\",preprocess(\" \".join(lst_subtask_3_1_control))])\ntestwords.append([\"lst_subtask_3_2_livestock\",preprocess(\" \".join(lst_subtask_3_2_livestock))])\ntestwords.append([\"lst_subtask_3_2_area\",preprocess(\" \".join(lst_subtask_3_2_area))])\ntestwords.append([\"lst_subtask_3_2_control\",preprocess(\" \".join(lst_subtask_3_2_control))])\ntestwords.append([\"lst_subtask_3_3_1_host\",preprocess(\" \".join(lst_subtask_3_3_1_host))])\ntestwords.append([\"lst_subtask_3_3_2_infection\",preprocess(\" \".join(lst_subtask_3_3_2_infection))])\ntestwords.append([\"lst_subtask_3_3_3_lab\",preprocess(\" \".join(lst_subtask_3_3_3_lab))])\ntestwords.append([\"lst_subtask_4_host\",preprocess(\" \".join(lst_subtask_4_host))])\ntestwords.append([\"lst_subtask_4_transmission\",preprocess(\" \".join(lst_subtask_4_transmission))])\ntestwords.append([\"lst_subtask_4_evidence\",preprocess(\" \".join(lst_subtask_4_evidence))])\ntestwords.append([\"lst_subtask_5_1\",preprocess(\" \".join(lst_subtask_5_1))])\ntestwords.append([\"lst_subtask_5_2\",preprocess(\" \".join(lst_subtask_5_2))])\ntestwords.append([\"lst_subtask_5_3\",preprocess(\" \".join(lst_subtask_5_3))])\ntestwords.append([\"lst_subtask_5_4\",preprocess(\" \".join(lst_subtask_5_4))])\ntestwords.append([\"lst_subtask_6_1\",preprocess(\" \".join(lst_subtask_6_1))])\ntestwords.append([\"lst_subtask_6_2\",preprocess(\" \".join(lst_subtask_6_2))])\ntestwords.append([\"lst_subtask_6_3\",preprocess(\" \".join(lst_subtask_6_3))])\ntestwords.append([\"lst_subtask_6_4\",preprocess(\" \".join(lst_subtask_6_4))])\ntestwords.append([\"lst_subtask_6_5\",preprocess(\" \".join(lst_subtask_6_5))])\n\ntestwords_string = []\nfor i in range(len(testwords)):\n    try:\n        testwords_string.append([testwords[i][0],\" \".join(testwords[i][1])])\n    except:\n        testwords_string.append([testwords[i][0],\" \"])\n        \nprint(\"...[DONE]\")\n## END OF TESTWORDS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before the start of the main loop, it is required that we initialize the output directory.\n\nWe can choose four output methods, CSV , JSON , scrreen , none\n* CSV will create csv output file for every sub task, and will append the results line by line\n* JSON will crerate a json output file for every sub task, and will each time, load the json to an object, append the result , and rewrite the file (This method is slower)\n* screen will output everything to the screen instead of the file.\n* none will append nothing (for code execution time measuring and optimization)\n\nOur selection is csv, to achieve smaller execution time"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Cleaning and initializing output directory\", end=\"\", flush=True)\n\nmethod = \"CSV\"\n\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        if filename.lower().endswith((\".json\")):\n            os.remove(filename)\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        if filename.lower().endswith((\".csv\")):\n            os.remove(filename)\n            \nif method == \"JSON\":          \n    for x in range(len(testwords)):\n        json_name = testwords[x][0]+\".json\"\n        with open(json_name, 'w+') as json_file: \n            data = []\n            data.append([\"\",\"\"])\n        write_json(data,json_name)\n    for x in range(len(testwords)):\n        json_name = testwords[x][0]+\"_cosine.json\"\n        with open(json_name, 'w+') as json_file: \n            data = []\n            data.append([\"\",\"\"])\n        write_json(data,json_name)\nelif method == \"CSV\":\n    for x in range(len(testwords)):\n        csv_name = testwords[x][0]+\".csv\"\n        data = [\"File Title\",\"Score\"]\n        write_csv(data,csv_name)\n    for x in range(len(testwords)):\n        csv_name = testwords[x][0]+\"_cosine.csv\"\n        data = [\"File Title\",\"Score\"]\n        write_csv(data,csv_name)\n    \nprint(\"...[DONE]\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also will define two classes for paraller and not counting of progress and execution time.\nThese are required as in parallel programming lock and sharedmemory sync should always be implemented."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CountProgress(object):\n    def __init__(self, manager, initval=0, time = 0):\n        self.val = manager.Value('i', initval)\n        self.lock = manager.Lock()\n\n    def increment(self):\n        with self.lock:\n            self.val.value += 1\n\n    def value(self):\n        with self.lock:\n            return self.val.value\n\nclass CountTime(object):\n    def __init__(self, manager, initval=0):\n        self.val = manager.Value('d', initval)\n        self.lock = manager.Lock()\n        \n    def totaltime(self,t):\n        with self.lock:\n            self.val.value += t\n            \n    def value(self):\n        with self.lock:\n            return self.val.value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will define the last procedure. This will be used for parallel and non-parallel solutions. It takes as arguments the file to be ranked, the progress, time and parallel jobs that are curently in use.\nThis will ensure that the metrics will progerss as the files are being processed. and will handle the increment of time and progress.\n\nIt will first open the file and load the contents of the file.\nThen it will extract all the values in any field with key text.\nWill call the preprocess methods for the text and then will stat rankind DF , TF-IDF, Cosine similarity.\nAfter the ranking of TF-IDS and Cosine similarity it will output the file path and the score to the applicable csv or json file, or at screen.\nFinally, it will calculate the progression and the Estimated Time To Finish (ETTF) and post it on output for the user to see how the progression of this task is going.\n\nFrom our runs, we saw that each file is finished in less than 0.5 seconds.\n\nYou can also set the debug value\n* DEBUG = -1 is for linear ranking , one file at a time and will just output the progress in the screen\n* DEBUG = 0 is for parallel ranking, default 4 thread and will just output the progress in the screen, you can change the threads in the next part\n* DEBUG = 1 is for linear ranking, one file at a time and will output many execution time diagnostic messages for each file\n* DEBUG = 2 is for linear ranking, one file at a time and will output a single execution time diagnostic for each file"},{"metadata":{"trusted":true},"cell_type":"code","source":"DEBUG = -1\n\ndef process_file(file,countprogress,countime,paralleljobs):\n    if DEBUG == 1:\n        start_time = time.time()\n    elif DEBUG == 2:\n        start_time = time.time()\n    with open(file) as data_file:\n        file_start_time = time.time()\n        dataset = []\n        data = json.load(data_file, object_pairs_hook=OrderedDict)\n        dataset.append([file,extract_values(data, 'text')])\n        if DEBUG == 1:\n            print(\"--- %s seconds to open file---\" % (time.time() - start_time))\n            \n        processed_text = []\n        processed_text_string = []\n\n        try:\n            text = \" \".join(dataset[0][1])\n            processed_text.append([dataset[0][0],preprocess(text)])\n        except:\n            processed_text.append([dataset[0][0],[\" \"]])\n        \n        if DEBUG == 1:\n            print(\"--- %s seconds to process---\" % (time.time() - start_time))\n            \n        try:\n            processed_text_string.append(\" \".join(processed_text[0][1]))\n        except:\n            processed_text_string.append(\" \")\n        #end\n\n        if DEBUG == 1:\n            print(\"--- %s seconds to process text string---\" % (time.time() - start_time))\n        DF = []\n\n        i=0\n        DF.append({})\n        tokens = processed_text[i][1]\n        for w in tokens:\n            try:\n                DF[i][w].add(i)\n            except:\n                DF[i][w] = {i}\n\n        for w in DF[i]:\n            DF[i][w] = len(DF[i][w])\n\n        ##END OF DF\n        if DEBUG == 1:\n            print(\"--- %s seconds to END OF DF---\" % (time.time() - start_time))\n        total_vocab_size = []\n        total_vocab = []\n\n        total_vocab_size.append(len(DF[i]))\n        #print(total_vocab_size[i])\n        total_vocab.append([x for x in DF[i]])\n\n        ##END OF VOCAB SIZE\n        if DEBUG == 1:\n            print(\"--- %s seconds to END OF VOCAB---\" % (time.time() - start_time))\n        doc = 0\n\n        tf_idf = []\n\n        tf_idf.append({})\n        tokens = processed_text[i][1]\n\n        counter = Counter(tokens)\n        words_count = len(tokens)\n\n        for token in np.unique(tokens):\n\n            tf = counter[token]/words_count\n            df = doc_freq(token)\n            idf = np.log((N+1)/(df+1))\n\n            tf_idf[i][doc, token] = tf*idf\n\n        doc += 1\n\n        ## END OF TF\n        if DEBUG == 1:\n            print(\"--- %s seconds to END OF TF/IDF---\" % (time.time() - start_time))\n        for j in tf_idf[i]:\n            tf_idf[i][j] *= alpha\n\n        ## END OF MERGE\n        if DEBUG == 1:\n            print(\"--- %s seconds to END OF MERGE---\" % (time.time() - start_time))\n        D = []\n        #     for i in out:\n        #         print(i, dataset[i][0])\n        D.append(np.zeros((1, total_vocab_size[i])))\n        for j in tf_idf[i]:\n            try:\n                ind = total_vocab[i].index(j[1])\n                D[i][j[0]][ind] = tf_idf[i][j]\n            except:\n                pass\n        if DEBUG == 1:\n            print(\"--- %s seconds to END OF D---\" % (time.time() - start_time))\n            \n        for x in range(len(testwords)):\n            results = []\n            score = matching_score(1, testwords[x][1], tf_idf[i])\n            results.append([dataset[i][0],score[0]])\n            if method == \"CSV\":\n                csv_name = testwords[x][0]+\".csv\"\n                data = [dataset[i][0],score[0]]\n                write_csv(data,csv_name)\n            elif method == \"JSON\":  \n                json_name = testwords[x][0]+\".json\"\n                with open(json_name) as json_file: \n                    data = json.load(json_file) \n                    temp = data[0] \n                    temp.append(results) \n                write_json(data,json_name)\n            elif method == \"screen\":\n                data = [dataset[i][0],score[0]]\n                print(data)\n            elif method == \"none\":\n                data = [dataset[i][0],score[0]]\n                \n            if DEBUG == 1:\n                print(\"--- %s seconds to END OF TF/IDF SCORING---\" % (time.time() - start_time))\n\n            Q = []\n            Q.append(cosine_similarity(1, testwords[x][1],total_vocab[i], D[i], dataset[i]))\n\n            if DEBUG == 1:\n                print(\"--- %s seconds to END OF Q---\" % (time.time() - start_time))\n                \n            results = []\n            score = Q[i][i]\n            if math.isnan(score):\n                score = 0\n            results.append([dataset[i][0],score])\n            if method == \"CSV\":\n                csv_name = testwords[x][0]+\"_cosine.csv\"\n                data = [dataset[i][0],score]\n                write_csv(data,csv_name)\n            elif method == \"JSON\":\n                with open(testwords[x][0]+\"_cosine.json\", 'w') as json_file:\n                    data = json.load(json_file) \n                    temp = data[0] \n                    temp.append(results) \n                    json.dump(results, json_file)\n            elif method == \"screen\":\n                data = [dataset[i][0],score]\n                print(data)\n            elif method == \"none\":\n                data = [dataset[i][0],score]\n\n            if DEBUG == 1:\n                print(\"--- %s seconds to END OF COSINE SIMMILARITY---\" % (time.time() - start_time))\n        if DEBUG == 1:\n            print(\"--- %s seconds to END OF FILE---\" % (time.time() - start_time))\n        elif DEBUG == 2:\n            print(\"--- %s seconds to END OF FILE---\" % (time.time() - start_time))\n            \n            \n    countprogress.increment()\n    elapsed_time = time.time() - file_start_time\n    countime.totaltime(elapsed_time)\n    \n    totaltime = countime.value()\n    cprogress = countprogress.value()\n    remaining = ((totaltime/cprogress) * (N - cprogress))\n    \n    if DEBUG == 0:\n        totaltime = totaltime/paralleljobs\n        remaining = remaining/paralleljobs\n    \n    totaltime = str(datetime.timedelta(seconds=totaltime))\n    remaining = str(datetime.timedelta(seconds=remaining))\n    \n    suf = 'Complete ['+str(cprogress)+'/'+str(N)+'] File processed in: ' + str(round(elapsed_time,3)) \n    suf += 's ETTF -> ' + remaining + ' elapsed -> ' + totaltime\n    printProgressBar(cprogress, N, prefix = 'Progress:', suffix = suf, length = 50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Main part of the script that uses the above\n\nAfter we init the managers and the counter classes, We set the parallel threads/cpu to be used Default is 4 (paralleljobs = 4)\n* NOTE that in kaggle, paraller programming is slower than linear for our current task, but on pypy ultitasking processors, runs faster.\n\n* We recoment to use single thread linear rannking"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Starting main loop...\")\nmanager = Manager()\nmanager2 = Manager()\n\nresult = manager.dict()\nresult2 = manager.dict()\n\ncountprogress = CountProgress(manager, 0)\ncountime = CountTime(manager2, 0)\n\nparalleljobs = 4\n\nif DEBUG == 1:\n    for file in files[:1]:\n        process_file(file,countprogress,countime,paralleljobs)\nelif DEBUG == 0:\n    Parallel(n_jobs=paralleljobs, prefer=\"threads\")(delayed(process_file)(file,countprogress,countime,paralleljobs) for file in files)\nelse:\n    for file in files:\n        process_file(file,countprogress,countime,paralleljobs)\n        \n        \n        ## END OF COSINE SIMILARITY\nprint()\nprint(\"...[DONE]\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exemplary Answer of a Subquestion**"},{"metadata":{},"cell_type":"markdown","source":"**Outlook on this approach**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Method 3: Keyword-based Scientific Paper Recommendation Engine\n\n### Introduction\n\nFunction:\n\n* Question characterized by keyword lists (Examples for keywords list in Code)\n* Identify important paper by keyword extraction search (One example)\n\nOutput\n\n* 1. Based on keyword lists. Performance depends on richness of those lists.\n* 2. Ranking based on frequency of keywords and matching to different keyword groups"},{"metadata":{},"cell_type":"markdown","source":"### Codebase"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_number_of_elements_nested_list(list_of_keyword_lists):\n    counter = 0\n    for lst in list_of_keyword_lists:\n        counter += len(lst)\n    return counter\n\ndef evaluate_text_via_list_of_list_of_keywords(text, list_of_keyword_lists):\n    num_keyword_lists = len(list_of_keyword_lists)\n    len_text = len(text)\n    #num_total_keyword_hits = 0\n    arr_keyword_list_hits = np.zeros(num_keyword_lists)\n    for num_word in range(len_text):\n        for num_keyword_list in range(num_keyword_lists):\n            if text[num_word] in list_of_keyword_lists[num_keyword_list]:\n                #num_total_keyword_hits += 1\n                arr_keyword_list_hits[num_keyword_list] += 1\n    return arr_keyword_list_hits\n\ndef evaluate_journal_by_keywords(df_paper, list_of_keyword_lists):\n    \"\"\"\n    df_paper                 pd.DataFrame with Covid 19 papers\n    list_of_keyword_lists    Nested list. Contains multiple lists with keywords. Each list is a subgroup/clustering of keywords.\n    \"\"\"\n    num_entries = df_paper.shape[0]\n    dct_abstracts = {}\n    num_keywords = get_number_of_elements_nested_list(list_of_keyword_lists)\n    # For every document\n    for num_paper in range(num_entries):\n        # Continue with paper if abstract is not empty\n        if type(df_paper.iloc[num_paper,8]) != float:\n            \n            # Get abstract\n            txt_abstract = df_paper.iloc[num_paper,8].split()\n            idx = num_paper\n            \n            # Evaluate abstract\n            arr_keyword_list_hits = evaluate_text_via_list_of_list_of_keywords(txt_abstract, list_of_keyword_lists)\n\n            # Scoring abstract\n            int_keyword_frequency = sum(arr_keyword_list_hits)\n            num_keyword_sources = len(arr_keyword_list_hits[arr_keyword_list_hits > 0])\n            num_keyword_lists = len(list_of_keyword_lists)\n            numerator = num_keyword_sources + int_keyword_frequency\n            denominator = num_keyword_lists + num_keywords\n\n            abstract_score = numerator/denominator\n\n            dct_abstracts.update({num_paper: {\"Abstract Score\": abstract_score}})\n\n    return dct_abstracts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"../input/CORD-19-research-challenge/\"\ndata_file = \"metadata.csv\"\ndata = pd.read_csv(data_dir+data_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Corona\nlst_corona = [\"Corona\", \"corona\", \"corona virus\", \"coronavirus\", \"corona viruses\", \"coronaviruses\", \"Coronaviridae\", \"coronaviridae\", \"COVID-19\", \"Covid-19\", \"covid-19\", \"COVID\", \"COV\", \"SARS\"]\n\n# Main Task\nlst_genetics = [\"genetics\"]\nlst_origin = [\"origin\", \"member\", \"family\"]\nlst_evolution = [\"evolution\", \"development\", \"develops\", \"developed\"]\nlst_task = [lst_genetics, lst_origin, lst_evolution]\n\n# Sub task 1 - Real-time tracking ...\nlst_subtask_1_genome = [\"Genome\", \"genome\"]\nlst_subtask_1_dissemination = [\"dissemination\", \"Dissemination\", \"propagation\", \"Propagation\", \"spread\", \"Spread\", \"spreading\", \"Spreading\"]\nlst_subtask_1_treatment = [\"treatment\", \"Treatment\", \"diagnostic\", \"Diagnostic\", \"diagnostics\", \"Diagnostics\", \"therapeutics\", \"Therapeutics\"]\nlst_subtask_1_variation = [\"Difference\" , \"in contrast\", \"variation\", \"deviation\", \"shows mutations\", \"enrichment\", \"similarities\"]\nlst_subtask_1_reference = [\"Accession number\", \"reference\", \"sample\", \"identification of\"]\nlst_subtask_1_known = [\"Known\", \"already published\", \"already reported\"]\nlst_subtask_1 = [lst_subtask_1_genome, lst_subtask_1_dissemination, lst_subtask_1_treatment, lst_subtask_1_variation, lst_subtask_1_reference, lst_subtask_1_known, lst_corona]\n\n# Sub task 2 - Access to geographic ...\nlst_subtask_2_ = []\nlst_subtask_2_ = []\nlst_subtask_2_ = []\nlst_subtask_2_ = []\nlst_subtask_2_ = []\nlst_subtask_2 = []\n\n# Sub task 3 - Evidence that livestock ...\n\n# Sub sub task 3-1\n#lst_subtask_3_1_Test = []\n#lst_subtask_3_1 = []\n\n# Sub sub task 3-2\nlst_subtask_3_2_livestock = [\"farm\" , \"wildlife\", \"wild animal\", \"undomesticated\", \"livestock\"]\nlst_subtask_3_2_area = [\"Southeast-Asia\"]\nlst_subtask_3_2_control = [\"surveil\", \"control\", \"screen\", \"check\", \"monitor\", \"examine\"]\nlst_subtask_3_2 = [lst_subtask_3_2_livestock, lst_subtask_3_2_area, lst_subtask_3_2_control, lst_corona]\n\n# Sub sub task 3-3\nlst_subtask_3_1_host = [\"host\" , \"organism\", \"human\"]\nlst_subtask_3_2_infection = [\"infection\", \"disease\", \"respiratory syndrom\"]\nlst_subtask_3_3_lab = [\"experimental\", \"laboratory\", \"under conditions\"]\nlst_subtask_3 = [lst_subtask_3_1_host, lst_subtask_3_2_infection, lst_subtask_3_3_lab, lst_corona]\n\n# Sub task 4\nlst_subtask_4_host = [\"animal\", \"host\", \"hosts\", \"Host\", \"Hosts\", \"human\", \"Human\", \"Humans\", \"humans\", \"CoV-Host\", \"organism\"]\nlst_subtask_4_transmission = [\"pathogen\", \"spill-over\", \"intraspecies\", \"interaction\", \"host-shift\", \"spread\", \"evolution\", \"transmission\", \"infection\"]\nlst_subtask_4_evidence = [\"evidence\", \"proof\", \"association\", \"connection\", \"associated\"]\nlst_subtask_4 = [lst_subtask_4_host, lst_subtask_4_transmission, lst_subtask_4_evidence, lst_corona]\n\n# Sub task 5\nlst_subtask_5_ = []\nlst_subtask_5_ = []\nlst_subtask_5_ = []\nlst_subtask_5_ = []\nlst_subtask_5 = []\n\n# Sub task 6\nlst_subtask_6_ = []\nlst_subtask_6_ = []\nlst_subtask_6_ = []\nlst_subtask_6_ = []\nlst_subtask_6_ = []\nlst_subtask_6 = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exemplary Answer of a Subquestion"},{"metadata":{"trusted":true},"cell_type":"code","source":"dct_res = evaluate_journal_by_keywords(data, lst_subtask_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lst_ranked = sorted(dct_res, key = lambda x: (dct_res[x][\"Abstract Score\"]), reverse = True)\nlst_ranked[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.iloc[23643, 8]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outlook on this approach\n\n##### Rake on top of keyword search\n* Question characterized by keyword lists\n* Identify important paper by keyword extraction search\n* Rake summarizes paper, giving answers on the questions\n\n##### Evaluation for the method\n* A measurement to evaluate the quality over all the output\n* A measurement to identify the most important paper"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}