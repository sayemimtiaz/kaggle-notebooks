{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1> <center> Heart Failure Prediction</center> </h1>"},{"metadata":{},"cell_type":"markdown","source":"<center><img width=\"800\" height=\"600\" src=\"https://image.freepik.com/vetores-gratis/infografico-de-informacoes-de-sintomas-de-insuficiencia-cardiaca_1308-53507.jpg\"></center>\n<center> Fonte: <a href='https://br.freepik.com/vetores/fundo'>Fundo vetor criado por brgfx - br.freepik.com</a> </center>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:gray; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick navigation</center></h2>\n\n* [1. Introduction](#1)\n* [2. Data analysis and Visualization](#2)\n* [3. Data transformations](#3)\n* [4. Modeling](#4)\n* [5. Conclusion](#5)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style='background:gray; border:0; color:white'><center>1. Introduction<center><h24>"},{"metadata":{},"cell_type":"markdown","source":"The present work was intended to analyze some data related to people's health. So known that cardiovascular diseases(CVDs) are the number 1 cause of death globally, and heart failure is a common event caused by CVDs, the  <a href=\"https://www.kaggle.com/andrewmvd/heart-failure-clinical-data\"><strong>Heart Failure Prediction</strong></a> dataset was chosen.\n\nAssuming most cardiovascular diseases preventeble by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity using population-wide strategies. People with cardiovascular disease or who are at high cardiovascular risk need early detection and management.\n\nIn that way, machine learning models can help to the early risk identifcation. So, in the flolowing sections we will take a look to the data, making a initial analysis and quick visualization. Futher some transformation was made to make the data more presentable to the modeling process. Finaly, a model was created to detect the heart failure. In the end a short conclusion was made, where can be helpful for the strategies taken to prevent the heart failure.\n\nIt is important to remember this is a Notebook to study purposes only.\n\nThis notebook was created by [Matheus Gama Costa](https://www.linkedin.com/in/matheus-gama-costa-903456a6/). I hope you find this Notebook useful. Feel free to contact me if you have any questions!"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h2 style='background:gray; border:0; color:white'><center>2. Data analysis and Visualization<center><h24>"},{"metadata":{},"cell_type":"markdown","source":"### First of all the libraries will be imported. Here we have all the libraries used in the code. They were included as they were needed during the work progress"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the libraries\n\n# data handling\nimport numpy as np \nimport pandas as pd\n\n# data visualization\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we are going to import the dataset and do a quick overview in the available variables."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Importing the dataset\ndataset = pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\n\n# shown the columns names\nprint(dataset.columns)\n\n# shown the first 5 rows\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick statistical description of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing the data\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Till here we were able to see that the dataset has only numerical data. This is good for mathematical approaches or statistical usage of the data. To our aim, this is good because will facilitate the data usage in the machine learning approach. For other data analyzes approaches and visualization this is also can be a good thing.\n### Another good thing to be analyzed is that we do not have missing values."},{"metadata":{},"cell_type":"markdown","source":"### Now we will take a look at some distributions of the data."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# histogram plot for all dataset\ndataset.hist(figsize=(20,20));","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# counter plot for all data concerning death events\nfig, ((axis1, axis2), (axis3, axis4), \n      (axis5, axis6), (axis7, axis8), \n      (axis9, axis10), (axis11, axis12)) = plt.subplots(6,2, figsize=(20,50))\n\nsns.countplot(x='age', hue='DEATH_EVENT', data=dataset, ax=axis1)\nsns.countplot(x='anaemia', hue='DEATH_EVENT', data=dataset, ax=axis2)\nsns.countplot(x='creatinine_phosphokinase', hue='DEATH_EVENT', data=dataset, ax=axis3);\nsns.countplot(x='diabetes', hue='DEATH_EVENT', data=dataset, ax=axis4)\nsns.countplot(x='ejection_fraction', hue='DEATH_EVENT', data=dataset, ax=axis5)\nsns.countplot(x='high_blood_pressure', hue='DEATH_EVENT', data=dataset, ax=axis6);\nsns.countplot(x='platelets', hue='DEATH_EVENT', data=dataset, ax=axis7)\nsns.countplot(x='serum_creatinine', hue='DEATH_EVENT', data=dataset, ax=axis8)\nsns.countplot(x='serum_sodium', hue='DEATH_EVENT', data=dataset, ax=axis9);\nsns.countplot(x='sex', hue='DEATH_EVENT', data=dataset, ax=axis10)\nsns.countplot(x='smoking', hue='DEATH_EVENT', data=dataset, ax=axis11)\nsns.countplot(x='time', hue='DEATH_EVENT', data=dataset, ax=axis12);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Here we will make some interactive plots just to have a better data handling\n#### This part is focused on the data visualization"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig_1 = px.histogram(\n    dataset,\n    'age',\n    color='DEATH_EVENT',\n    nbins=50,\n    title='Data distribution per age')\nfig_1.show()\n\nfig_2 = px.box(\n    dataset, \n    x=\"DEATH_EVENT\", \n    y=\"age\", \n    title='Box Plot (DEATH_EVENT x Age)')\nfig_2.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_an = dataset['anaemia'].value_counts().reset_index()\nds_an.columns = ['anaemia', 'count']\n\nfig_3 = px.pie(\n    ds_an, \n    values='count', \n    names=\"anaemia\", \n    title='Anaemia Pie chart')\nfig_3.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_4 = px.histogram(\n    dataset,\n    'creatinine_phosphokinase',\n    color='DEATH_EVENT',\n    nbins=50,\n    title='Data distribution per creatinine_phosphokinase')\nfig_4.show()\n\nfig_5 = px.box(\n    dataset, \n    x=\"DEATH_EVENT\", \n    y=\"creatinine_phosphokinase\", \n    title='Box Plot (DEATH_EVENT x creatinine_phosphokinase)')\nfig_5.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_diab = dataset['diabetes'].value_counts().reset_index()\nds_diab.columns = ['diabetes', 'count']\n\nfig_6 = px.pie(\n    ds_diab, \n    values='count', \n    names=\"diabetes\", \n    title='Diabetes Pie chart')\nfig_6.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_7 = px.histogram(\n    dataset,\n    'ejection_fraction',\n    color='DEATH_EVENT',\n    nbins=50,\n    title='Data distribution per ejection_fraction')\nfig_7.show()\n\nfig_8 = px.box(\n    dataset, \n    x=\"DEATH_EVENT\", \n    y=\"ejection_fraction\", \n    title='Box Plot (DEATH_EVENT x ejection_fraction)')\nfig_8.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"ds_hbp = dataset['high_blood_pressure'].value_counts().reset_index()\nds_hbp.columns = ['high_blood_pressure', 'count']\n\nfig_9 = px.pie(\n    ds_hbp, \n    values='count', \n    names=\"high_blood_pressure\", \n    title='High blood pressure Pie chart')\nfig_9.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_10 = px.histogram(\n    dataset,\n    'platelets',\n    color='DEATH_EVENT',\n    nbins=50,\n    title='Data distribution per platelets')\nfig_10.show()\n\nfig_11 = px.box(\n    dataset, \n    x=\"DEATH_EVENT\", \n    y=\"platelets\", \n    title='Box Plot (DEATH_EVENT x platelets)')\nfig_11.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_12 = px.histogram(\n    dataset,\n    'serum_creatinine',\n    color='DEATH_EVENT',\n    nbins=50,\n    title='Data distribution per serum_creatinine')\nfig_12.show()\n\nfig_13 = px.box(\n    dataset, \n    x=\"DEATH_EVENT\", \n    y=\"serum_creatinine\", \n    title='Box Plot (DEATH_EVENT x serum_creatinine)')\nfig_13.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_14 = px.histogram(\n    dataset,\n    'serum_sodium',\n    color='DEATH_EVENT',\n    nbins=50,\n    title='Data distribution per serum_sodium')\nfig_14.show()\n\nfig_15 = px.box(\n    dataset, \n    x=\"DEATH_EVENT\", \n    y=\"serum_sodium\", \n    title='Box Plot (DEATH_EVENT x serum_sodium)')\nfig_15.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_sex = dataset['sex'].value_counts().reset_index()\nds_sex.columns = ['sex', 'count']\n\nfig_16 = px.pie(\n    ds_sex, \n    values='count', \n    names=\"sex\", \n    title='Gender Pie chart')\nfig_16.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"ds_smk = dataset['smoking'].value_counts().reset_index()\nds_smk.columns = ['smoking', 'count']\n\nfig_17 = px.pie(\n    ds_smk, \n    values='count', \n    names=\"smoking\", \n    title='Smoking Pie chart')\nfig_17.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_18 = px.histogram(\n    dataset,\n    'time',\n    color='DEATH_EVENT',\n    nbins=50,\n    title='Data distribution per time')\nfig_18.show()\n\nfig_19 = px.box(\n    dataset, \n    x=\"DEATH_EVENT\", \n    y=\"time\", \n    title='Box Plot (DEATH_EVENT x time)')\nfig_19.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 15))\nsns.heatmap(dataset.corr(), cmap='coolwarm', fmt='.2f', linewidths=0.1,\n            vmax=1.0, square=True, linecolor='white', annot=True);\n\nplt.xticks(range(dataset.shape[1]), dataset.columns, rotation=90)\nplt.yticks(range(dataset.shape[1]), dataset.columns)\n\ndataset.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loking to the plots made some insigths can be made. \n\n### The age is directly proportional to heart failure, and the cases observed shown that between 55 and 75 years most of the deaths registered took place. Looking to the statistical correlation betwen the death and the age, around 0.25, confirms the histogram with a positive correlation.\n\n### Have or don't have anemia does not show any good correlation (around 0.066) to indicate if a patient will or will not have heart failure. The creatinine phosphokinase and diabetes, also do not show a good correlation (around 0.062 and -0.001 respectively). This information is very visual in the histogram and box plots, and is coroborated with de statistical correlation.\n\n### The ejection fraction, in other hand have a negative correlation (around -0.268), so, as it increase the chance of have a heart failure decrease. \n\n### We can see another couple of factors that do not have a good correlation, high blood pressure, and platelets. We have to reinforce that what we are talking about here is about the data, motherless clinically a high blood pressure is an indicator of heart problem, in this dataset it appears to not have a good correlation with the chance of having a death per heart failure.\n\n### The serum creatinine and serum sodium levels are good indicators of the occurrence of death per heart failure. The first has a positive correlation and the second a negative correlation.\n\n### The sex does not appear to have a low correlation with the deaths registered. \n\n### The time shows the best correlation to the death event. But, taking into consideration that in this dataset it was a factor indicating the number of days that a patient was being monitored, this is not a good indicator to take into consideration in a machine learning model. We say that because using this data can induce a trend in the model, so, in further data, the model probably will not work well.\n\n### Assuming what was considered in this initial analysis we will use to create the model only the data with more than 0.1 of correlation. In that case the next section we will handle the data to get only the age, ejection fraction, serum creatinine, serum sodium columns."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h2 style='background:gray; border:0; color:white'><center>3. Data transformations<center><h24>"},{"metadata":{},"cell_type":"markdown","source":"### In this part, we will not have much work to do. First, we already know that our dataset only has numerical data and does  not have missing values. So, we will not have to trouble ourselves to take care of categorical data or missing values.\n\n### Our work here will be:\n### <blockquote> 1. Separate the dataset into de dependents variables Y (that will be the Death events) and independent variables X (all other variables);</blockquote> \n### <blockquote> 2. Drop the columns that have less than 0.1 of correlation ('anaemia', 'creatinine_phosphokinase',  'diabetes', 'high_blood_pressure', 'platelets', 'sex', 'smoking', 'time', 'DEATH_EVENT') in the independent variables;</blockquote> \n### <blockquote> 3. Separate the data into the training and test set;</blockquote> \n### <blockquote> 4. Make a feature scaling. This will be important because our data have many differences between the values like some columns have a maximum value of 148 and others of 9.59. It makes the underlying assumption that higher ranging numbers have superiority of some sort. So these more significant number starts playing a more decisive role while training the model.</blockquote> "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking the dependent and independet variables\nX = dataset\ny = dataset['DEATH_EVENT']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# excluding the columns that will not be used\nX = X.drop(['anaemia', 'creatinine_phosphokinase', \n            'diabetes', 'high_blood_pressure', \n            'platelets', 'sex', 'smoking', \n            'time', 'DEATH_EVENT'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h2 style='background:gray; border:0; color:white'><center>4. Modeling<center><h24>"},{"metadata":{},"cell_type":"markdown","source":"### Now we will make some classification models. We will use classification algorithms because our problem addresses the occurrence or not of a death event (0 or 1). For that purpose, we will evaluate three different models using the K Neighbors classifier, support vector machine classifier, and logistic regression classifier. After that, we will evaluate the peforma√ße of each one."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the k Neighbors model on the Training set\nclassifier_KN = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier_KN.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Support vector machine model on the Training set\nclassifier_SVM = SVC(kernel = 'linear', random_state = 0)\nclassifier_SVM.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Logistic Regression model on the Training set\nclassifier_LR = LogisticRegression(random_state = 0)\nclassifier_LR.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Test set results\ny_pred_KN = classifier_KN.predict(X_test)\naccuracies_KN = cross_val_score(estimator = classifier_KN, X = X_test, y = y_test, cv = 10)\nprint(\"Kmeans Accuracy: {:.2f} %\".format(accuracies_KN.mean()*100))\nprint(\"Kmeans Standard Deviation: {:.2f} %\\n\".format(accuracies_KN.std()*100))\n\ny_pred_SVM = classifier_SVM.predict(X_test)\naccuracies_SVM = cross_val_score(estimator = classifier_SVM, X = X_test, y = y_test, cv = 10)\nprint(\"Support vector machine Accuracy: {:.2f} %\".format(accuracies_SVM.mean()*100))\nprint(\"Support vector machine Standard Deviation: {:.2f} %\\n\".format(accuracies_SVM.std()*100))\n\ny_pred_LR = classifier_LR.predict(X_test)\naccuracies_LR = cross_val_score(estimator = classifier_LR, X = X_test, y = y_test, cv = 10)\nprint(\"Logistic Regression Accuracy: {:.2f} %\".format(accuracies_LR.mean()*100))\nprint(\"Logistic Regression Standard Deviation: {:.2f} %\".format(accuracies_LR.std()*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the Confusion Matrix plot function\ndef plot_confusion_matrix(y_pred, y_true, title):\n    cm = confusion_matrix(y_true, y_pred)\n    ax= plt.subplot()\n    sns.heatmap(\n        cm, \n        annot=True, \n        ax=ax, \n        fmt='g'\n    )\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels')\n    ax.set_title(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ploting the Confusion Matrix\nplot_confusion_matrix(y_pred_KN, y_test, 'kNeighbors')\nplot_confusion_matrix(y_pred_SVM, y_test, 'SVM')\nplot_confusion_matrix(y_pred_LR, y_test, 'Logistic Regression')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking at the models trained we can see that both support vector machine and logistic regression had the same accuracy (72.34%), just a little bit greater than the k Neighbors model (69.29). Remember this accuracy was obtained in the test set, which is good because this data wasn't used in the model training process.\n### Assuming our problem we can choose the support vector machine model to further use because that model had the lowest false-negative off all three. This is something very important when we are talking about health because is very dangerous to let someone who needs special care without it."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n<h2 style='background:gray; border:0; color:white'><center>5. Conclusion<center><h24>"},{"metadata":{},"cell_type":"markdown","source":"### In the end we have a model that can be really used in further data. That model can be improved with new data to training it and maybe taking another factor, like high blood pressure into account, the optimization will not be done here. \n### In the data analysis, we could see that the age, ejection fraction, serum creatinine, serum sodium column were the pieces of information with more correlation due to the death event. So, it is a good thing to do pay attention to that indicators in new patients admitted to the facility, also a regular evaluation in that indicators on the patients already in observation maybe is a good approach to reduce de risk of death.\n### Looking at what we analyzed here, it is very helpful to use computing tools to analyze data in health care especially machine learning tools. So, some approaches like that could be useful in hospitals, nursing homes, life insurance companies, and other health facilities. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}