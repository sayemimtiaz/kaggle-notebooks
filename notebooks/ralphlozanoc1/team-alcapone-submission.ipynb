{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Overview: Description of Project and Methods Used\n\nIn this notebook, we aim to explore the 51,045 Covid-19 articles and provide a more efficient way to look for answers. We chose to use the Specter embeddings of the tiles and abstracts as given in the most recent version of the COVID-19 project file. For all dataset records, results have been run and displayed in the order below: \n* Reading and indexing specter embeddings \n* PCA and TSNE plots displaying results for three clustering methods (Fuzzy C-means clustering, K-means clustering and Hierarchical clustering)\n* The titles of the top 5 closest points to the centroids for two clustering methods (Fuzzy C-means clustering and K-means clustering) to show similarity within clusters and dissimilarity among clusters.\n\nTo efficiently run the Hierarchical clustering on the Kaggle kernel, we narrowed down the dataset to 1,000 records, and provided the code for this task. We also provided our resulting hierarchical clusters and plots for the full dataset, which we processed in the same way as demonstrated on the sample records, just at full-scale. \n\nWe have also provided some evaluation in the form of comparisons between each of the different clustering methods used, as well as some observations and next steps. As a preview, Kmeans clustering seems to be the winner in terms of clear separation between clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n# create conda environment with recquired packages\n# this takes ~10-15 mins\n!conda create -n alcapone_rapids -c rapidsai -c nvidia -c conda-forge rapids scikit-fuzzy python=3.6 cudatoolkit=10.1 -y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is to make the conda packages accessible\nimport sys\nsys.path = [\"/opt/conda/envs/alcapone_rapids/lib/python3.6/site-packages\"]+ sys.path\nsys.path = [\"/opt/conda/envs/alcapone_rapids/lib/python3.6\"] + sys.path\nsys.path = [\"/opt/conda/envs/alcapone_rapids/lib\"] + sys.path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nimport skfuzzy as fuzz\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import AgglomerativeClustering, KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.spatial import distance as eudist\nimport glob\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport csv\nfrom cuml.manifold import TSNE as cTSNE\nfrom cuml import KMeans as cKMeans\nfrom IPython.display import Image, display","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define global variables\nROOT_PATH = '/kaggle/input/CORD-19-research-challenge/'\nMETADATA_PATH = f'{ROOT_PATH}/metadata.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load metadata into a df and look at the contents\nmeta_df = pd.read_csv(METADATA_PATH, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Processing SPECTER Embeddings\n\nIn this section, we use the full data set to compare different methods of clustering (K-Means, Fuzzy C-Means, Heirarchical). \n\nFor this task, we used the Specter Embeddings as described in the upcoming ACL paper found [here](https://github.com/allenai/paper-embedding-public-apis#specter). \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_embedding_dict(filepath, sample_size=None):\n    \"\"\"create embedding dictionary from file at given filepath\"\"\"\n\n    embedding_dict = {}\n    with open(filepath) as csvfile:\n        reader = csv.reader(csvfile, delimiter=',')\n        for i, row in enumerate(reader):\n            # exit the loop if the desired sample size is reached\n            if sample_size and i == sample_size:\n                break\n            embed = np.zeros((768,))\n            for idx, val in enumerate(row):\n                if idx > 0:\n                    embed[idx-1] = float(val)\n            embedding_dict[row[0]] = embed\n    return embedding_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict = create_embedding_dict(f'{ROOT_PATH}/cord19_specter_embeddings_2020-04-10/cord19_specter_embeddings_2020-04-10.csv',\n                                       sample_size=None\n                                      )\nembedding_mat = np.array(list(embedding_dict.values()))\nembedding_mat.shape, len(embedding_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering \n\n## Fuzzy C-Means \nWe first chose to use Fuzzy C-means as it is an unsupervised method of dealing with a dataset that contains similarities-- it is probable that the semantics of COVID-19 paper titles and subject matter are similar enough to warrant membership in multiple clusters. (Note that we then counteract this assumption below in K-means). The evaluation of Fuzzy C-means on the full set of records shows that Fuzzy C-means can be used with some degree of success to cluster this data. See below for the set up of our clusters, their centroids, and the results for each. "},{"metadata":{"trusted":true},"cell_type":"code","source":"n_clusters = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fuzzy_clustering(all_embedding, n_clusters):\n    \"\"\"returns clusters and centroids as results of fuzzy c-means clustering\"\"\"\n    \n    centroids, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(data=all_embedding.T, \n                                                          c=n_clusters, \n                                                          m=2, \n                                                          error=0.5, \n                                                          maxiter=1000, \n                                                          init=None)\n    clusters = np.argmax(u, axis=0)\n    return clusters, centroids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_clusters(embedding_dict, n_clusters, clusters, centroids = None, k = 5):\n    \"\"\"returns dictionary for clusters\"\"\"\n    \n    cluster_dict = {}\n    distance_dict = {}\n    for i in range(n_clusters):\n        cluster_dict[i] = []\n        distance_dict[i] = []\n        for j in np.where(clusters == i)[0]:\n            paper_id = list(embedding_dict.keys())[j]\n            cluster_dict[i].append(paper_id)\n            if centroids is not None:\n                distance = eudist.euclidean(embedding_mat[j], centroids[i])\n                distance_dict[i].append(distance)\n    \n    if centroids is not None:\n        closest_dict = {}\n        for i in range(n_clusters):\n            closest_idx = np.argsort(distance_dict[i])[0:k]\n            closest_dict[i] = []\n            for j in range(min(k, len(closest_idx))):\n                closest_dict[i].append(cluster_dict[i][j])\n        return cluster_dict, closest_dict\n    else:\n        return cluster_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzzy_clusters, fuzzy_centroids = fuzzy_clustering(embedding_mat, n_clusters)\nfuzzy_clusters_dict, fuzzy_closest_dict = get_clusters(embedding_dict, n_clusters, fuzzy_clusters, fuzzy_centroids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fuzzy C-Means Evaluation\n\n### 1. PCA and t-SNE\nThe selection of both of these forms of analysis dealt with both narrowing the set of variables (dimensions) and labeling the clusters, as well as the ability to provide a simple visualization tool - greater distance apart means more dissimilarity between paper topic and similar papers will be in closer proximity to each other. \n\nOverall, the PCA plot looks acceptable - papers from the same cluster are close to each other, forming groups. However, for Cluster 6, Cluster 8 and Cluster 9, there are overlaps and the topics seem convoluted-- some overlap in cluster subject.\n\nIn addition, from the t-SNE plot,  we observe similar patterns to the PCA plot - groups formed with some overlaps."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pca(all_embedding):\n    \"\"\"returns result of pca given an embedding matrix\"\"\"\n    \n    pca = PCA()\n    pca_result = pca.fit_transform(all_embedding)\n    return pca_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_pca(pca_result, clusters, title):\n    \"\"\"plots and saves pca result image\"\"\"\n    \n    sns.set(rc={'figure.figsize':(10, 10)})\n    palette = sns.color_palette(\"bright\", len(set(clusters)))\n    sns.scatterplot(pca_result[:,0], pca_result[:,1], hue=clusters, legend='full', palette=palette)\n    \n    plt.title(title)\n    plt.savefig(f\"/kaggle/working/{title}.png\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzzy_pca = get_pca(embedding_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use PCA to plot embeddings v. fuzzy output clusters\nplot_pca(fuzzy_pca, fuzzy_clusters, \"PCA Covid-19 Articles - Clustered(Fuzzy C-Means)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tsne(all_embedding):\n    \"\"\"returns result of TNSE given an embedding matrix\"\"\"\n    \n    tsne = cTSNE(verbose=1)\n    tsne_result = tsne.fit_transform(all_embedding)\n    return tsne_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_tsne(tsne_result, clusters, title):\n    \"\"\"plots and saves tsne result image \"\"\"\n    \n    sns.set(rc={'figure.figsize':(10, 10)})\n    palette = sns.color_palette(\"bright\", len(set(clusters)))\n    sns.scatterplot(tsne_result[:,0], tsne_result[:,1], hue=clusters, legend='full', palette=palette)\n    \n    plt.title(title)\n    plt.savefig(f\"/kaggle/working/{title}.png\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzzy_tsne = get_tsne(embedding_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use tSNE to plot embeddings v. fuzzy output clusters \nplot_tsne(fuzzy_tsne, fuzzy_clusters, \"t-SNE Covid-19 Articles - Clustered(Fuzzy C-Means)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. The Titles of the Top 5 Closest Points to Centroids\nTo evaluate how Fuzzy C-means clustering performs, we find the top 5 closest points to the centroids of each cluster and present their corresponding paper titles. In theory, the papers within each cluster should have similar topics, and the papers from different clusters should have different topics. For example, topics in Cluster 2 seems to be about treatment methods of the virus. Cluster 6 seems to be discussing the geographical features of the virus, which is a different topic than Cluster 8. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for cluster, paper_id in fuzzy_closest_dict.items():\n    print(f\"Cluster {cluster} - Titles\")\n    for idx in paper_id:\n        print(f\"{meta_df['title'].loc[meta_df['cord_uid'] == idx].values[0]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-Means Clustering\n\nSince we first used Fuzzy C-means, we wanted to follow that up with K-means to make the partition stricter, and see if this change in cluster overlap showed significant difference in producing clusters and subsequent visualizations of these results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def kmeans_clustering(all_embedding, n_clusters):\n    \"\"\"returns result of k-means clustering\"\"\"\n    \n    kmeans = cKMeans(n_clusters=n_clusters, random_state=0).fit(all_embedding)\n    clusters = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n    return clusters, centroids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_clusters, kmeans_centroids = kmeans_clustering(embedding_mat, n_clusters)\nkmeans_clusters_dict, kmeans_closest_dict = get_clusters(embedding_dict, n_clusters, kmeans_clusters, kmeans_centroids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-Means Evaluation\n### 1. PCA and t-SNE\n\nFrom the PCA plot and the t-SNE plot labeled by K-means clusters, we observe more clearly defined cluster clouds, and the number of data points in each cluster is more evenly spread out."},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_pca = get_pca(embedding_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pca(kmeans_pca, kmeans_clusters, \"PCA Covid-19 Articles - Clustered(kmeans)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_tsne = get_tsne(embedding_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tsne(kmeans_tsne, kmeans_clusters, \"t-SNE Covid-19 Articles - Clustered(kmeans)\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. The Titles of the top 5 Closest Points to Centroids\nSimilarly, we find the top 5 closest points to the centroids of each cluster and check their paper titles for K-means clustering results. Cluster 6 seems to discuss the viruses geographically, and Cluster 3 seems to discuss how health regulation plays a part in this pandemic. The results are promising."},{"metadata":{"trusted":true},"cell_type":"code","source":"for cluster, paper_id in kmeans_closest_dict.items():\n    print(f\"Cluster {cluster} - Titles\")\n    for idx in paper_id:\n        print(f\"{meta_df['title'].loc[meta_df['cord_uid'] == idx].values[0]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hierarchical Clustering\n\nThe last form of clustering that we chose to use on this dataset was hierarchical clustering, used to more closely reproduce the target relationships between the different clusters that may have formed in both methods above. We also thought that hierarchical clustering could give more insight into the overlap shown in the analysis of the Fuzzy C-means clusters. In running all three of these algorithms on this particular unlabeled dataset, we hoped to reinforce similarities."},{"metadata":{"trusted":true},"cell_type":"code","source":"# this section contains code we used to generate the clusters using hierarchical clustering\n# to run it on kaggle kernel, we suggest limiting the sample size to 1000\n# to do so, regenerate the embedding matrix by running the following:\n\n# embedding_dict = create_embedding_dict(f'{ROOT_PATH}/cord19_specter_embeddings_2020-04-10/cord19_specter_embeddings_2020-04-10.csv',\n#                                        sample_size=1000,\n#                                       )\n# embedding_mat = np.array(list(embedding_dict.values()))\n# embedding_mat.shape, len(embedding_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hierarchical_clustering(all_embedding, n_clusters):\n    hierarchical = AgglomerativeClustering(n_clusters=n_clusters).fit(all_embedding)\n    clusters = hierarchical.labels_\n    return clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hierarchical_clusters = hierarchical_clustering(embedding_mat, n_clusters)\nhierarchical_clusters_dict = get_clusters(embedding_dict, n_clusters, hierarchical_clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hierarchical_pca = get_pca(embedding_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pca(hierarchical_pca, hierarchical_clusters, \"PCA Covid-19 Articles - Clustered(Hierarchical)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hierarchical_tsne = get_tsne(embedding_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tsne(hierarchical_tsne, hierarchical_clusters, \"t-SNE Covid-19 Articles - Clustered(Hierarchical)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hierarchical Clustering Evaluation \nHere we have attached the results from Hierarchical Clustering, obtained by running the code above.\n\nFrom the PCA plot and the t-SNE plot labeled by Hierarchical clusters, we observe more clearly defined cluster clouds, as well. However, Cluster 6 and Cluster 3 are more convoluted in the t-SNE."},{"metadata":{"trusted":true},"cell_type":"code","source":"hierarchical_pca = Image(\"/kaggle/input/results/PCA Covid-19 Articles - Clustered(Hierarchical).png\")\nhierarchical_tsne = Image(\"/kaggle/input/results/t-SNE Covid-19 Articles - Clustered(Hierarchical).png\")\ndisplay(hierarchical_pca, hierarchical_tsne)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Future Steps\nFor phase II, we aim to move towards a more general purpose Information Retrieval System by implementing a Neural Information Retrieval (Neural IR) system.  While traditional Neural IR systems focus heavily on word-level interactions, we aim to incorporate entity-oriented search into the system.  By integrating textual base IR and entity search IR, we hope that the augmented system will be able to parse harder queries with enriched data and interact with the documents in a more dynamic way.\n\nExtraction of information from a knowledge graph will have three parts.  First, by performing entity linking on the titles we aim to extract a set of important entities.  Each entity is associated with a description and type.  By creating an embedding of entities, embedding of types with attention, and embedding the description, we have a system that incorporates entity semantic relationship with augmented text data not found in the document text.  \n\nThese three embeddings are combined by a linear layer and fed to a downstream interaction matrix.  The final component of the interaction matrix will be the text data from the documents.  From there, weâ€™ll apply an RBF kernel on the interaction matrix to generate translation scores.  \n\nThe translation scores from the query-word, document-word, query-entity and document-entity pairs will form the basis for the neural ranking model.  The neural ranking aspect can be trained by another standard ranking loss function, but for the sake of simplicity will be a pairwise loss function.\n\nThe system enables an end-to-end optimization problem.  By integrating entities, entity description, types, and text data, we will create a system that is able to learn embeddings and ranking in one go.  "},{"metadata":{},"cell_type":"markdown","source":"# References\n\n```\n@inproceedings{specter_cohan_2020,\n    title = \"SPECTER: Scientific Paper Embeddings using Citation-informed TransformERs\",\n    author = \"Cohan, Arman and\n      Feldman, Sergey and\n      Beltagy, Iz  and\n      Downey, Doug and\n      Weld, Daniel\",\n    booktitle = \"ACL\",\n    year = \"2020\",\n}\n```"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}