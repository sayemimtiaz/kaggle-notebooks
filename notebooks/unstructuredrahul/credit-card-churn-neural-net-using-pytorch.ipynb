{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" * This Notebook is intended to showcase a Multilayer Perceptron **(MLP) implementation in Pytorch using structured dataset** such as this - Credit Card data.\n * EDA etc of the data is not carried out , there are plenty of good notebooks for this dataset depicting  the same , you can refer them over : [here](https://www.kaggle.com/thomaskonstantin/bank-churn-data-exploration-and-churn-prediction)\n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.metrics import f1_score as f1\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import *\nfrom imblearn.over_sampling import SMOTE\n\n#-- Pytorch specific libraries import -----#\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data=pd.read_csv('../input/credit-card-customers/BankChurners.csv')\ndf_data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing \n\n* OHE of Categorical features\n* Up-sampling using SMOTE\n* Dropping redundant and unwanted fields","metadata":{}},{"cell_type":"code","source":"#OHE of Categorical features\ndf_data.Attrition_Flag = df_data.Attrition_Flag.replace({'Attrited Customer':1,'Existing Customer':0})\ndf_data.Gender = df_data.Gender.replace({'F':1,'M':0})\ndf_data = pd.concat([df_data,pd.get_dummies(df_data['Education_Level']).drop(columns=['Unknown'])],axis=1)\ndf_data = pd.concat([df_data,pd.get_dummies(df_data['Income_Category']).drop(columns=['Unknown'])],axis=1)\ndf_data = pd.concat([df_data,pd.get_dummies(df_data['Marital_Status']).drop(columns=['Unknown'])],axis=1)\ndf_data = pd.concat([df_data,pd.get_dummies(df_data['Card_Category']).drop(columns=['Platinum'])],axis=1)\ndf_data.drop(columns = ['Education_Level','Income_Category','Marital_Status','Card_Category','CLIENTNUM'],inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SMOTE upsampling\noversample = SMOTE()\nX, y = oversample.fit_resample(df_data[df_data.columns[1:]], df_data[df_data.columns[0]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data.columns[1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data.columns[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"upsampled_df = pd.DataFrame(data=X,columns=df_data.columns[1:])\nupsampled_df = upsampled_df.assign(Churn = y)\nohe_data =upsampled_df[upsampled_df.columns[15:-1]].copy()\nupsampled_df = upsampled_df.drop(columns=upsampled_df.columns[15:-1])\nupsampled_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"upsampled_df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train & Test Set\nX= upsampled_df.loc[: , upsampled_df.columns != 'Churn']\n#y = upsampled_df['Churn']\ny = pd.DataFrame(upsampled_df['Churn'])\n\ntrain_x,test_x,train_y,test_y = train_test_split(X,y,random_state=42)\nprint(test_x.shape)\nprint(test_y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pytorch - MLP implementation","metadata":{}},{"cell_type":"markdown","source":"### Converting Data into Pytorch Tensors","metadata":{}},{"cell_type":"code","source":"###First use a MinMaxscaler to scale all the features of Train & Test dataframes\n\nscaler = preprocessing.MinMaxScaler()\nx_train = scaler.fit_transform(train_x.values)\nx_test =  scaler.fit_transform(test_x.values)\n\nprint(\"Scaled values of Train set \\n\")\nprint(x_train)\nprint(\"\\nScaled values of Test set \\n\")\nprint(x_test)\n\n\n###Then convert the Train and Test sets into Tensors\n\nx_tensor =  torch.from_numpy(x_train).float()\ny_tensor =  torch.from_numpy(train_y.values.ravel()).float()\nxtest_tensor =  torch.from_numpy(x_test).float()\nytest_tensor =  torch.from_numpy(test_y.values.ravel()).float()\n\nprint(\"\\nTrain set Tensors \\n\")\nprint(x_tensor)\nprint(y_tensor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataloader to pass data in batches","metadata":{}},{"cell_type":"code","source":"#Define a batch size , hyperparameter can be further tuned\nbs = 64\n#Both x_train and y_train can be combined in a single TensorDataset, which will be easier to iterate over and slice\ny_tensor = y_tensor.unsqueeze(1)\ntrain_ds = TensorDataset(x_tensor, y_tensor)\n#Pytorchâ€™s DataLoader is responsible for managing batches. \n#You can create a DataLoader from any Dataset. DataLoader makes it easier to iterate over batches\ntrain_dl = DataLoader(train_ds, batch_size=bs)\n\n\n#For the validation/test dataset\nytest_tensor = ytest_tensor.unsqueeze(1)\ntest_ds = TensorDataset(xtest_tensor, ytest_tensor)\ntest_loader = DataLoader(test_ds, batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MLP (Model) \n\nDefine the Layers , Activation function , Number of nodes for the MultiLayerPerceptron\n\nStructure of MLP\n\n* 2 Hidden Layers\n* Normalizing the batch data usign batchnorm in between each layer\n* Using ReLU Activation function between the layers\n* Using dropout before sending to output\n* Sigmoid to make probabilities between 0 to 1","metadata":{}},{"cell_type":"code","source":"n_input_dim = train_x.shape[1]\n\n#Layer size\nn_hidden1 = 120  # Number of hidden nodes\nn_hidden2 = 100\nn_output =  1   # Number of output nodes = for binary classifier\n\n\nclass ChurnModel(nn.Module):\n    def __init__(self):\n        super(ChurnModel, self).__init__()\n        self.layer_1 = nn.Linear(n_input_dim, n_hidden1) \n        self.layer_2 = nn.Linear(n_hidden1, n_hidden2)\n        self.layer_out = nn.Linear(n_hidden2, n_output) \n        \n        \n        self.relu = nn.ReLU()\n        self.sigmoid =  nn.Sigmoid()\n        self.dropout = nn.Dropout(p=0.1)\n        self.batchnorm1 = nn.BatchNorm1d(n_hidden1)\n        self.batchnorm2 = nn.BatchNorm1d(n_hidden2)\n        \n        \n    def forward(self, inputs):\n        x = self.relu(self.layer_1(inputs))\n        x = self.batchnorm1(x)\n        x = self.relu(self.layer_2(x))\n        x = self.batchnorm2(x)\n        x = self.dropout(x)\n        x = self.sigmoid(self.layer_out(x))\n        \n        return x\n    \n\nmodel = ChurnModel()\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining \n\n* Loss computation function : Here using Binary Cross Entropy (BCE) which is defacto for Binary class problems\n* Learning rate : Setting as 0.001 (can be optimized further)\n* Optimizer : Using Adam and\n* Epochs of Training : setting as 50 ","metadata":{}},{"cell_type":"code","source":"#Loss Computation\nloss_func = nn.BCELoss()\n#Optimizer\nlearning_rate = 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nepochs = 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the MLP Model\n\nNN Steps\n1. Forward Propagation\n2. Loss computation\n3. Backpropagation\n4. Updating the parameters","metadata":{}},{"cell_type":"code","source":"model.train()\ntrain_loss = []\nfor epoch in range(epochs):\n    #Within each epoch run the subsets of data = batch sizes.\n    for xb, yb in train_dl:\n        y_pred = model(xb)            # Forward Propagation\n        loss = loss_func(y_pred, yb)  # Loss Computation\n        optimizer.zero_grad()         # Clearing all previous gradients, setting to zero \n        loss.backward()               # Back Propagation\n        optimizer.step()              # Updating the parameters\n        #optimizer.zero_grad() \n    #print(\"Loss in iteration :\"+str(epoch)+\" is: \"+str(loss.item()))\n    train_loss.append(loss.item())\nprint('Last iteration loss value: '+str(loss.item()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting the loss function shows it stabilized after 20th epoch itself","metadata":{}},{"cell_type":"code","source":"plt.plot(train_loss)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test Dataset prediction on trained NN","metadata":{}},{"cell_type":"code","source":"import itertools\n\ny_pred_list = []\nmodel.eval()\n#Since we don't need model to back propagate the gradients in test set we use torch.no_grad()\n# reduces memory usage and speeds up computation\nwith torch.no_grad():\n    for xb_test,yb_test  in test_loader:\n        y_test_pred = model(xb_test)\n        y_pred_tag = torch.round(y_test_pred)\n        y_pred_list.append(y_pred_tag.detach().numpy())\n\n#Takes arrays and makes them list of list for each batch        \ny_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n#flattens the lists in sequence\nytest_pred = list(itertools.chain.from_iterable(y_pred_list))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true_test = test_y.values.ravel()\nconf_matrix = confusion_matrix(y_true_test ,ytest_pred)\nprint(\"Confusion Matrix of the Test Set\")\nprint(\"-----------\")\nprint(conf_matrix)\nprint(\"Precision of the MLP :\\t\"+str(precision_score(y_true_test,ytest_pred)))\nprint(\"Recall of the MLP    :\\t\"+str(recall_score(y_true_test,ytest_pred)))\nprint(\"F1 Score of the Model :\\t\"+str(f1_score(y_true_test,ytest_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}