{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP for stock market prediction"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"import time\nstart = time.time()\nimport os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\n\n#plotting\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#statistics & econometrics\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\n\n#model fiiting and selection\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# 1. Load the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/stocknews/Combined_News_DJIA.csv\",low_memory=False,\n                    parse_dates=[0])\n\nfull_stock = pd.read_csv(\"../input/stocknews/DJIA_table.csv\",low_memory=False,\n                    parse_dates=[0])\n\n#add the closing stock value to the df - this will be the y variable\ndf[\"Close\"]=full_stock.Close\n\n#show how the dataset looks like\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop the label column\ndf = df.drop([\"Label\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data cleaning"},{"metadata":{},"cell_type":"markdown","source":"### NA treatment\nWe'll simply fill the NAs in the numerical features (Date, Close). \nIn the text features we'll fill the missing values with ''."},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for NAN\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a few headlines missing. Let's fill them in with a whitespace."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace(np.nan, ' ', regex=True)\n\n#sanity check\ndf.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove the HTML tags\nThere are several non-word tags in the headlines that would just bias the sentiment analysis so we need to remove them and replace with ''. This can be done with regex."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace('b\\\"|b\\'|\\\\\\\\|\\\\\\\"', '', regex=True)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sentiment and subjectivity score extraction\nNow I run the sentiment analysis extracting the compound score that goes from -0.5 (most negative) to 0.5 (most positive). I'm going to use the \"dirty\" texts in this part because VADER can utilize the information such as ALL CAPS, punctuation, etc. I'll also calculate the subjectivity of each headline using the TextBlob package.\n\nInitialise the VADER analyzer."},{"metadata":{"trusted":true},"cell_type":"code","source":"Anakin = SentimentIntensityAnalyzer()\n\nAnakin.polarity_scores(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Write a function to save the subjectivity score directly from TextBlob function's output. Subjectivity score might detect direct quotes in the headlines and positive stuff is rarely quoted in the headline."},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_subjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\ndetect_subjectivity(\" \") #should return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the headline columns' names\ncols = []\nfor i in range(1,26):\n    col = (\"Top{}\".format(i))\n    cols.append(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_vect=time.time()\nprint(\"ANAKIN: 'Intializing the process..'\")\n\n#get the name of the headline columns\ncols = []\nfor i in range(1,26):\n    col = (\"Top{}\".format(i))\n    cols.append(col)\n\n\nfor col in cols:\n    df[col] = df[col].astype(str) # Make sure data is treated as a string\n    df[col+'_comp']= df[col].apply(lambda x:Anakin.polarity_scores(x)['compound'])\n    df[col+'_sub'] = df[col].apply(detect_subjectivity)\n    print(\"{} Done\".format(col))\n    \nprint(\"VADER: Vaderization completed after %0.2f Minutes\"%((time.time() - start_vect)/60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the text isn't required anymore\ndf = df.drop(cols,axis=1)\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summarise the compound and subjectivity scores weighted by rating of the headline (top1 has the most weight)"},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_cols = []\nfor col in cols:\n    comp_col = col + \"_comp\"\n    comp_cols.append(comp_col)\n\nw = np.arange(1,26,1).tolist()\nw.reverse()\n\nweighted_comp = []\nmax_comp = []\nmin_comp = []\nfor i in range(0,len(df)):\n    a = df.loc[i,comp_cols].tolist()\n    weighted_comp.append(np.average(a, weights=w))\n    max_comp.append(max(a))\n    min_comp.append(min(a))\n\ndf['compound_mean'] = weighted_comp\ndf['compound_max'] = max_comp\ndf['compound_min'] = min_comp\n\n\nsub_cols = []\nfor col in cols:\n    sub_col = col + \"_sub\"\n    sub_cols.append(sub_col)\n\n\nweighted_sub = []\nmax_sub = []\nmin_sub = []\nfor i in range(0,len(df)):\n    a = df.loc[i,sub_cols].tolist()\n    weighted_sub.append(np.average(a, weights=w))\n    max_sub.append(max(a))\n    min_sub.append(min(a))\n\ndf['subjectivity_mean'] = weighted_sub\ndf['subjectivity_max'] = max_sub\ndf['subjectivity_min'] = min_sub\n\nto_drop = sub_cols+comp_cols\ndf = df.drop(to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Explorative Data Analysis\n\nFirst, the timeseries of the y (to be predicted) variable will be explored. It's likely the the timeseries isn't stationary which however doesn't worry us in this case as the models won't be of the classical timeseries methods family.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = go.Figure()\nfig1.add_trace(go.Scatter(x=df.Date, y=df.Close,\n                    mode='lines'))\ntitle = []\ntitle.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='Development of stock values from Aug, 2008 to Jun, 2016',\n                              font=dict(family='Arial',\n                                        size=30,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\nfig1.update_layout(xaxis_title='Date',\n                   yaxis_title='Closing stock value (in $)',\n                  annotations=title)\nfig1.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is quite obvious that the timeseries isn't stationary at all. There just seems to be a downwards trend over the time."},{"metadata":{},"cell_type":"markdown","source":"So let's look at how much unstationary the timeseries actually is ;-)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#function for quick plotting and testing of stationarity\ndef stationary_plot(y, lags=None, figsize=(12, 7), style='bmh'):\n    \"\"\"\n        Plot time series, its ACF and PACF, calculate Dickey–Fuller test\n        \n        y - timeseries\n        lags - how many lags to include in ACF, PACF calculation\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n        \n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (2, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        \n        y.plot(ax=ts_ax)\n        p_value = sm.tsa.stattools.adfuller(y)[1]\n        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n        plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stationary_plot(df.Close)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So that's a very unstationary timeseries. Although we won't need it for our models it might still be interesting to try to make the data stationary. Challenge accepted."},{"metadata":{"trusted":true},"cell_type":"code","source":"diff = df.Close - df.Close.shift(7)\nstationary_plot(diff[7:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that removing weekly seasonality helped a bit but the autocorrelation plot still shows many significant lags."},{"metadata":{"trusted":true},"cell_type":"code","source":"diff2 = diff - diff.shift(1)\nstationary_plot(diff2[7+1:], lags=60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now this is a relatively stationary...well it's something oscilating around zero. From these plots the parameters for a SARIMA model can be inferred. However, this is for now beond the scope of this kernel."},{"metadata":{},"cell_type":"markdown","source":"Next we look at the compound sentiment scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig2 = go.Figure()\nfig2.add_trace(go.Scatter(x=df.Date, y=df.compound_mean,\n                    mode='lines',\n                    name='Mean'))\nfig2.add_trace(go.Scatter(x=df.Date, y=df.compound_max,\n                    mode='lines',\n                    name='Maximum'))\nfig2.add_trace(go.Scatter(x=df.Date, y=df.compound_min,\n                    mode='lines',\n                    name='Minimum'))\ntitle = []\ntitle.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='Development of sentiment compound score',\n                               font=dict(family='Arial',\n                                       size=30,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\nfig2.update_layout(xaxis_title='Date',\n                   yaxis_title='Compound score',\n                  annotations=title)\nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also plot the distribution of the compound score."},{"metadata":{"trusted":true},"cell_type":"code","source":"compm_hist = px.histogram(df, x=\"compound_mean\")\ncompm_hist.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the subjectivity scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig3 = go.Figure()\nfig3.add_trace(go.Scatter(x=df.Date, y=df.subjectivity_mean,\n                    mode='lines',\n                    name='Mean'))\nfig3.add_trace(go.Scatter(x=df.Date, y=df.subjectivity_min,\n                    mode='lines',\n                    name='Min'))\nfig3.add_trace(go.Scatter(x=df.Date, y=df.subjectivity_max,\n                    mode='lines',\n                    name='Max'))\ntitle = []\ntitle.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='Development of subjectivity score',\n                              font=dict(family='Arial',\n                                        size=30,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\nfig3.update_layout(xaxis_title='Date',\n                   yaxis_title='Subjectivity score',\n                  annotations=title)\nfig3.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we plot distribution of the subjectivity scores as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"subm_hist = px.histogram(df, x=\"subjectivity_mean\")\nsubm_hist.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we'll look at some descriptive statistics about the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature selection\nI'm not going to use many FS methods since the features were mostly handcrafted. So we'll simply look at their variance and proportion of unique values."},{"metadata":{"trusted":true},"cell_type":"code","source":"def unique_ratio (col):\n    return len(np.unique(col))/len(col)\n\ncols = ['Close', 'compound_mean', 'compound_max', 'compound_min', 'subjectivity_mean', 'subjectivity_max', 'subjectivity_min']\n\nur = []\nvar = []\nfor col in cols:\n    ur.append(unique_ratio(df[col]))\n    var.append(np.var(df[col]))\n    \nfeature_sel = pd.DataFrame({'Column': cols, \n              'Unique': ur,\n              'Variance': var})\nfeature_sel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_fig = go.Figure(data=go.Scatter(\n    x=feature_sel.Column,\n    y=feature_sel.Unique,\n    mode='markers',\n    marker=dict(size=(feature_sel.Unique*100)),\n))\nsel_fig.update_layout(title='Ratio of unique values', \n                      yaxis_title='Unique ratio')\nsel_fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compound maximum and minimum are potentially less interesting as only ~18% of their values are unique. Also maximum of subjectivity has very low values. Minimum subjectivity has contains almost only 0. We'll drop the subjectivity min and max."},{"metadata":{"trusted":true},"cell_type":"code","source":"drop = ['subjectivity_min', 'subjectivity_max']\nclean_df = df.drop(drop,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Lag the extracted features\nTo allow the models to look into the past, we'll add features which are essentially just copies of rows from n-steps back. In order to not create too many new features we'll add only features from 1 week prior to the current datapoint.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_df = clean_df.copy()\nlag_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_lag = list(lag_df.columns)\nto_lag_4 = to_lag[1]\nto_lag_1 = to_lag[2:len(to_lag)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lagging text features two days back\nfor col in to_lag_1:\n    for i in range(1,3):\n        new_name = col + ('_lag_{}'.format(i))\n        lag_df[new_name] = lag_df[col].shift(i)\n    \n#lagging closing values 4 days back\nfor i in range(1, 5):\n    new_name = to_lag_4 + ('_lag_{}'.format(i))\n    lag_df[new_name] = lag_df[to_lag_4].shift(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this process, rows with NAs were created. Unfortunately these rows will have to be removed since we simply don't have the data from the future."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Show many rows need to be removed\nlag_df.head(10) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above we can see that the first 7 rows now have missing values. Let's delete them and reset index."},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_df = lag_df.drop(lag_df.index[[np.arange(0,4)]])\nlag_df = lag_df.reset_index(drop=True)\n\n#sanity check for NaNs\nlag_df.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Model training\nLet's train 3 ML models. We'll do this in 2 rounds. First, using the econometric features alone (7 lags of y). Second, including the information extracted from the headlines (compound, subjectivity and their lags)\n\n**Models**\n- Ridge regression - punish model for using too many features but doesn't allow the coeficients drop to zero completely\n- Random forest\n- XGBoost\n\nWe'll score all models by mean squared error as it gives higher penalty to larger mistakes.\nAnd before each model training we'll standardize the training data.\n"},{"metadata":{},"cell_type":"markdown","source":"The first step will be creating folds for cross-validation. We'll use the same folds for all models in order to allow for creating a meta-model. Since we're working with timeseries the folds cannot be randomly selected. Instead a fold will be a sequence of data so that we don't lose the time information."},{"metadata":{"trusted":true},"cell_type":"code","source":"# for time-series cross-validation set 10 folds \ntscv = TimeSeriesSplit(n_splits=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cost function to minimize is mean squared error because this function assigns cost proportionally to the error size. The mean absolute percentage error will be used for plotting and easier interpretation. It's much easier to understand the errors of a model in terms of percentage.\nEach training set is scaled (normalized) independently to minimize data leakage."},{"metadata":{"trusted":true},"cell_type":"code","source":"def mape(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\nscorer = make_scorer(mean_squared_error)\nscaler = StandardScaler()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we split the dataset into training and testing. 20% of the data will be used for testing."},{"metadata":{"trusted":true},"cell_type":"code","source":"def ts_train_test_split(X, y, test_size):\n    \"\"\"\n        Perform train-test split with respect to time series structure\n    \"\"\"\n    \n    # get the index after which test set starts\n    test_index = int(len(X)*(1-test_size))\n    \n    X_train = X.iloc[:test_index]\n    y_train = y.iloc[:test_index]\n    X_test = X.iloc[test_index:]\n    y_test = y.iloc[test_index:]\n    \n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = lag_df.drop(['Close'],axis=1)\nX.index = X[\"Date\"]\nX = X.drop(['Date'],axis=1)\ny = lag_df.Close\n\nX_train, X_test, y_train, y_test = ts_train_test_split(X, y, test_size = 0.2)\n\n#sanity check\n(len(X_train)+len(X_test))==len(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function for plotting coeficients of models (lasso and XGBoost)\ndef plotCoef(model,train_x):\n    \"\"\"\n        Plots sorted coefficient values of the model\n    \"\"\"\n    \n    coefs = pd.DataFrame(model.coef_, train_x.columns)\n    coefs.columns = [\"coef\"]\n    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n    \n    plt.figure(figsize=(15, 7))\n    coefs.coef.plot(kind='bar')\n    plt.grid(True, axis='y')\n    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8.1 Econometric models\nFirst let's train models using only the lags of the y variable (i.e. Close)."},{"metadata":{"trusted":true},"cell_type":"code","source":"econ_cols = list(X_train.columns)\necon_cols = econ_cols[12:17]\nX_train_e = X_train[econ_cols]\nX_test_e = X_test[econ_cols]\ny_train_e = y_train\ny_test_e = y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"econ_perf = pd.DataFrame(columns=['Model','MSE', 'SD'])\necon_perf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_param = {'model__alpha': list(np.arange(0.001,1,0.001))}\nridge = Ridge(max_iter=5000)\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', ridge)])\nsearch_ridge = GridSearchCV(estimator=pipe,\n                          param_grid = ridge_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4\n                         )\nsearch_ridge.fit(X_train_e, y_train_e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_e = search_ridge.best_estimator_\n\n#get cv results of the best model + confidence intervals\nfrom sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(ridge_e, X_train_e, y_train_e, cv=tscv, scoring=scorer)\necon_perf = econ_perf.append({'Model':'Ridge', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nridge_e","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotCoef(ridge_e['model'], X_train_e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = ridge_e['model'].coef_\nridge_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train_e.columns)})\nridge_coefs[\"abs\"] = ridge_coefs.Coef.apply(np.abs)\nridge_coefs = ridge_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nridge_coefs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"econ_perf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_param = {'model__n_estimators': [10, 100, 300],\n            'model__max_depth': [10, 20, 30, 40],\n            'model__min_samples_split': [2, 5, 10],\n            'model__min_samples_leaf': [1, 2, 3],\n            'model__max_features': [\"auto\", 'sqrt']}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor()\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', rf)])\ngridsearch_rf = GridSearchCV(estimator=pipe,\n                          param_grid = rf_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridsearch_rf.fit(X_train_e, y_train_e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_e = gridsearch_rf.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(rf_e, X_train_e, y_train_e, cv=tscv, scoring=scorer)\necon_perf = econ_perf.append({'Model':'RF', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost\nUsing linear booster because tree methods don't work with timeseries very well"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_param = {'model__lambda': list(np.arange(0.1,3, 0.1)), #L2 regularisation\n             'model__alpha': list(np.arange(0.1,3, 0.1)),  #L1 regularisation\n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBRegressor(booster='gblinear', feature_selector='shuffle', objective='reg:squarederror')\n\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', xgb)])\ngridsearch_xgb = GridSearchCV(estimator=pipe,\n                          param_grid = xgb_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gridsearch_xgb.fit(X_train_e, y_train_e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_e = gridsearch_xgb.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(xgb_e, X_train_e, y_train_e, cv=tscv, scoring=scorer)\necon_perf = econ_perf.append({'Model':'XGB', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nxgb_e","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(econ_perf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot performance of econ models including error bars"},{"metadata":{"trusted":true},"cell_type":"code","source":"econ_fig = px.scatter(econ_perf, x=\"Model\", y='MSE', color='Model', error_y=\"SD\")\necon_fig.update_layout(title_text=\"Performance of models trained on lags of y\")\necon_fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NLP models\nLet's try now predict the stock value using only information from the news headlines."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_n = X_train.drop(econ_cols, axis=1)\nX_test_n = X_test.drop(econ_cols, axis=1)\ny_train_n = y_train\ny_test_n = y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_perf = pd.DataFrame(columns=['Model','MSE', 'SD'])\nnlp_perf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ridge regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_param = {'model__alpha': list(np.arange(1,10,0.1))}\nridge = Ridge(max_iter=5000)\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', ridge)\n])\nsearch_ridge = GridSearchCV(estimator=pipe,\n                          param_grid = ridge_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4\n                         )\nsearch_ridge.fit(X_train_n, y_train_n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_n = search_ridge.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(ridge_n, X_train_n, y_train_n, cv=tscv, scoring=scorer)\nnlp_perf = nlp_perf.append({'Model':'Ridge', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nridge_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotCoef(ridge_n['model'], X_train_n)\n\ncoefs = ridge_n['model'].coef_\nridge_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train_n.columns)})\nridge_coefs[\"abs\"] = ridge_coefs.Coef.apply(np.abs)\nridge_coefs = ridge_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nridge_coefs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mape(y_test, ridge_n.predict(X_test_n))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_param = {'model__n_estimators': [10, 100, 300],\n            'model__max_depth': [10, 20, 30, 40],\n            'model__min_samples_split': [2, 5, 10],\n            'model__min_samples_leaf': [1, 2, 3],\n            'model__max_features': [\"auto\", 'sqrt']}\nrf = RandomForestRegressor()\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', rf)])\ngridsearch_rf = GridSearchCV(estimator=pipe,\n                          param_grid = rf_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_rf.fit(X_train_n, y_train_n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_n = gridsearch_rf.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(rf_n, X_train_n, y_train_n, cv=tscv, scoring=scorer)\nnlp_perf = nlp_perf.append({'Model':'RF', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_param = {'model__lambda': list(np.arange(1,10, 1)), #L2 regularisation\n             'model__alpha': list(np.arange(1,10, 1)),  #L1 regularisation\n            }\nxgb = XGBRegressor(booster='gblinear', feature_selector='shuffle', objective='reg:squarederror')\n\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', xgb)])\ngridsearch_xgb = GridSearchCV(estimator=pipe,\n                          param_grid = xgb_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_xgb.fit(X_train_n, y_train_n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_n = gridsearch_xgb.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(xgb_n, X_train_n, y_train_n, cv=tscv, scoring=scorer)\nnlp_perf = nlp_perf.append({'Model':'XGB', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nxgb_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(nlp_perf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_fig = px.scatter(nlp_perf, x=\"Model\", y='MSE', color='Model', error_y=\"SD\")\n#nlp_fig.update_layout(title_text=\"Performance of models trained on NLP features\",\nnlp_fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 3 models are performing quite similary. They might be useful candidates for stacking."},{"metadata":{},"cell_type":"markdown","source":"# Econ+NLP models\nLet's use all features now\n\n## Ridge regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"en_perf = pd.DataFrame(columns=['Model','MSE', 'SD'])\nen_perf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_param = {'model__alpha': list(np.arange(0.1,1,0.01))}\nridge = Ridge(max_iter=5000)\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', ridge)])\nsearch_ridge = GridSearchCV(estimator=pipe,\n                          param_grid = ridge_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\nsearch_ridge.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_en = search_ridge.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(ridge_en, X_train, y_train, cv=tscv, scoring=scorer)\nen_perf = en_perf.append({'Model':'Ridge', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nridge_en","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = ridge_en['model'].coef_\nridge_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train.columns)})\nridge_coefs[\"abs\"] = ridge_coefs.Coef.apply(np.abs)\nridge_coefs = ridge_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nridge_coefs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotCoef(ridge_en['model'], X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_param = {'model__n_estimators': [10, 100, 300],\n            'model__max_depth': [10, 20, 30, 40],\n            'model__min_samples_split': [2, 5, 10],\n            'model__min_samples_leaf': [1, 2, 3],\n            'model__max_features': [\"auto\", 'sqrt']}\nrf = RandomForestRegressor()\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', rf)])\ngridsearch_rf = GridSearchCV(estimator=pipe,\n                          param_grid = rf_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_en = gridsearch_rf.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(rf_en, X_train, y_train, cv=tscv, scoring=scorer)\nen_perf = en_perf.append({'Model':'RF', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nrf_en","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_param = {'model__lambda': list(np.arange(1,10, 1)), #L2 regularisation\n             'model__alpha': list(np.arange(1,10, 1)),  #L1 regularisation\n            }\nxgb = XGBRegressor(booster='gblinear', feature_selector='shuffle', objective='reg:squarederror')\n\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', xgb)])\ngridsearch_xgb = GridSearchCV(estimator=pipe,\n                          param_grid = xgb_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_xgb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_en = gridsearch_xgb.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(xgb_en, X_train, y_train, cv=tscv, scoring=scorer)\nen_perf = en_perf.append({'Model':'XGB', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nxgb_en","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Try stacking econometric and NLP models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\n\nX_train_stack = pd.DataFrame(pd.DataFrame(columns=['econ_r', 'nlp_r']))\nX_train_stack['econ_r'] = cross_val_predict(ridge_e, X_train_e, y_train, cv=10)\nX_train_stack['nlp_r'] = cross_val_predict(ridge_n, X_train_n, y_train, cv=10)\n\nX_test_stack = pd.DataFrame(pd.DataFrame(columns=['econ_r', 'nlp_r']))\nX_test_stack['econ_r'] = ridge_e.predict(X_test_e)\nX_test_stack['nlp_r'] = ridge_n.predict(X_test_n)\n\nX_train_stack.to_csv(\"Stack_train.csv\")\nX_test_stack.to_csv(\"Stack_test.csv\")\n\nfrom sklearn.linear_model import ElasticNetCV\nstack = ElasticNetCV(cv=tscv)\nstack.fit(X_train_stack, y_train)\ncv_score = cross_val_score(stack, X_train_stack, y_train, cv=tscv, scoring=scorer)\nstack_performance = {'Model':'XGB', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}\nstack_performance\n\nmape(y_test, stack.predict(X_test_stack))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = stack.coef_\nridge_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train_stack.columns)})\nridge_coefs[\"abs\"] = ridge_coefs.Coef.apply(np.abs)\nridge_coefs = ridge_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nprint(ridge_coefs)\nplotCoef(stack, X_train_stack)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_compare = pd.DataFrame(pd.DataFrame(columns=['y_true', 'econ_r', 'econ_rf', 'econ_x', 'nlp_r', 'nlp_rf', 'nlp_x', 'comb_r', 'comb_rf', 'comb_x', 'stack']))\nprediction_compare['y_true'] = y_test\nprediction_compare['econ_r'] = ridge_e.predict(X_test_e)\nprediction_compare['econ_rf'] = rf_e.predict(X_test_e)\nprediction_compare['econ_x'] = xgb_e.predict(X_test_e)\nprediction_compare['nlp_r'] = ridge_n.predict(X_test_n)\nprediction_compare['nlp_rf'] = rf_n.predict(X_test_n)\nprediction_compare['nlp_x'] = xgb_n.predict(X_test_n)\nprediction_compare['comb_r'] = ridge_en.predict(X_test)\nprediction_compare['comb_rf'] = rf_en.predict(X_test)\nprediction_compare['comb_x'] = xgb_en.predict(X_test)\nprediction_compare['stack'] = stack.predict(X_test_stack)\n\nprediction_compare.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"econ_perf.to_csv(\"econ_perf.csv\")\nnlp_perf.to_csv(\"nlp_perf.csv\")\nen_perf.to_csv(\"en_perf.csv\")\nprediction_compare.to_csv('compare_predictions.csv')\nX_test.to_csv('X_test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking classification and regression\nUsing NLP features in the above models turned out to by highly ineffective. There are, however some problems of the econometric models that NLP features might be able to solve. From other kernels you can see that they can be used for predicting whether the stock value will go up or down. In following section, we'll train direction classifier with NLP features and use the output of this model to improve the econometric models."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}