{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n#load the data\ndf=pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EXPLORE AND MANIPULATE THE DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#first five rows\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataframe info\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataframe describe\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"#Create and explore a new dataframe with less columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a new dataframe with most important columns for us\ndf=df[['publish_time','authors','title','abstract']]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the total null cell for the column of abstract\ndf['abstract'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#delete rows where  abstract are null\ndf.dropna(subset=['abstract'], inplace=True)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explore the text in the colunm abstract"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fetch word count for each abstract\ndf['word_count'] = df['abstract'].apply(lambda x: len(str(x).split(\" \")))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Descriptive statistics of word counts\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identify common words (20 top words)\nfreq = pd.Series(' '.join(df['abstract']).split()).value_counts()[:20]\nfreq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the most 20 common words\nfreq.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identify uncommon words (top 20)\nfreq1 =  pd.Series(' '.join(df['abstract']).split()).value_counts()[-20:]\nfreq1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TEXT PRE-PROCESSING\n     # Steps:\n- Text clean up\n- Shrinking the vocabulary to retain only the relevant/important words\n- Reduce sparsity  "},{"metadata":{},"cell_type":"markdown","source":"**Normalize the data**: stemming and lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import the required libraries for the text processing\nimport re\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nnltk.download('wordnet') \nfrom nltk.stem.wordnet import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing stopwords\n    ##Creating a list of stop words and adding custom stopwords\nstop_words = set(stopwords.words(\"english\"))\n\n    ##Creating a list of custom stopwords (all other words you want to remove from the text)\nnew_words = [\"using\", \"show\", \"result\", \"also\", \"iv\", \"one\", 'however',\"two\", \"new\", \"previously\", \"shown\"]\nstop_words = stop_words.union(new_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#carry out the pre-processing tasks step-by-step to get a cleaned and normalised text corpus:\ncorpus = []\nfor i in list(df.index.values): # list of index of the dataframe [0,1,2......]'\n    #Remove punctuations\n    text = re.sub('[^a-zA-Z]', ' ', df['abstract'][i])\n    #Convert to lowercase\n    text = text.lower()\n    #remove tags\n    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n    #remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    #Convert to list from string\n    text = text.split()\n    #Stemming\n    ps=PorterStemmer()\n    #Lemmatisation\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if not word in  \n            stop_words] \n    text = \" \".join(text)\n    corpus.append(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#View corpus item\ncorpus[1000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explore and visualize the corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word cloud: Vizualize the corpus (frequency or the importance of each word)\n#from os import path\n#from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%matplotlib inline\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stop_words,\n                          max_words=100,\n                          max_font_size=70, \n                          random_state=42\n                         ).generate(str(corpus))\nprint(wordcloud)\nfig = plt.figure(1,figsize=(20,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identify common words in the corpus  (20 top words)\nfreq = pd.Series(' '.join(corpus).split()).value_counts()[:20]\nfreq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the result (top 20 words in the corpus)\n#Convert most freq words to dataframe for plotting bar plot\ntop_words = pd.Series(' '.join(corpus).split()).value_counts()[:20]\ntop_df = pd.DataFrame(top_words).reset_index()\ntop_df.columns=[\"Word\", \"Freq\"]\n\n#Barplot of most freq words\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\ng = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\ng.set_title('Top 20 words in the corpus')\ng.set_xticklabels(g.get_xticklabels(), rotation=30)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TEXT PREPARATION"},{"metadata":{},"cell_type":"markdown","source":"Text in the corpus needs to be converted to a format that can be interpreted by the machine learning algorithms. There are 2 parts of this conversion — Tokenisation and Vectorisation.\n\nTokenisation is the process of converting the continuous text into a list of words. The list of words is then converted to a matrix of integers by the process of vectorisation. Vectorisation is also called feature extraction.\n\nFor text preparation we use the bag of words model which ignores the sequence of the words and only considers word frequencies."},{"metadata":{},"cell_type":"markdown","source":"    -Vectorization\nAs the first step of conversion, we will use the CountVectoriser to tokenise the text and build a vocabulary of known words. We first create a variable “cv” of the CountVectoriser class, and then evoke the fit_transform function to learn and build the vocabulary.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a vector of word counts\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\ncv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\nX=cv.fit_transform(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shape of X\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print a list of 10 vocabulary from the list of vocabulary\nlist(cv.vocabulary_.keys())[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize top N uni-grams, bi-grams, tri-grams and 4-grams"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Uni-grams\n    #Most frequently occuring words\ndef get_top_unigram_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                       reverse=True)\n    return words_freq[:n]\n    #Convert most freq words to dataframe for plotting bar plot\ntop_words = get_top_unigram_words(corpus, n=20)\ntop_df = pd.DataFrame(top_words)\ntop_df.columns=[\"Word\", \"Freq\"]\n    #Barplot of most freq words\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\ng = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\ng.set_title('Top 20 Uni_grams')\ng.set_xticklabels(g.get_xticklabels(), rotation=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bi_grams\n    #Most frequently occuring Bi-grams\ndef get_top_bi_grams_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(2,2), max_features=4000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop2_words = get_top_bi_grams_words(corpus, n=20)\ntop2_df = pd.DataFrame(top2_words)\ntop2_df.columns=[\"Bi-gram\", \"Freq\"]\n\n    #Barplot of most freq Bi-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nh=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\nh.set_title('Top 20 Bi_grams')\nh.set_xticklabels(h.get_xticklabels(), rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tri_Grams\n    #Most frequently occuring Tri-grams\ndef get_top_n3_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(3,3), max_features=4000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop3_words = get_top_n3_words(corpus, n=20)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"Tri-gram\", \"Freq\"]\nprint(top3_df)\n    #Barplot of most freq Tri-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nj=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\nj.set_title('Top 20 Tri_grams')\nj.set_xticklabels(j.get_xticklabels(), rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4_Grams\n    #Most frequently occuring 4-grams\ndef get_top_n4_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(3,3), max_features=4000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop3_words = get_top_n4_words(corpus, n=20)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"4-gram\", \"Freq\"]\nprint(top3_df)\n    #Barplot of most freq Tri-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nl=sns.barplot(x=\"4-gram\", y=\"Freq\", data=top3_df)\nl.set_title('Top 20 4_grams')\nl.set_xticklabels(j.get_xticklabels(), rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting to a matrix of integers\nfrom sklearn.feature_extraction.text import TfidfTransformer\n \ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(X)\n\n# get feature names\nfeature_names=cv.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **KEYWORDS EXTRACTION FOR EACH ABSTRACT OF THE CORPUS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Function for sorting tf_idf in descending order\n\nfrom scipy.sparse import coo_matrix\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n \ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n \n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n \n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Example****: Extract the keywords for the abstract number 304 in the corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract the keywords for the abstract number 304 (1)\nabstract_335=corpus[335]\n    #generate tf-idf for the given document\ntf_idf_vector_abstract_335=tfidf_transformer.transform(cv.transform([abstract_335]))\n#sort the tf-idf vectors by descending order of scores\n\nsorted_items=sort_coo(tf_idf_vector_abstract_335.tocoo())\n\n#extract only the top n; n here is 5\nkeywords=extract_topn_from_vector(feature_names,sorted_items,5)\n    \n \n# now print the results\nprint(\"\\nAbstract 335:\")\nprint(abstract_335)\nprint(\"\\nKeywords:\")\nfor k in keywords:\n    print(k,keywords[k])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Extract keywords for the corpus (each abstract) and add them to the dataframe (colunm=keywords)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sort the tf-idf vectors by descending order of scores\ntf_idf_vector_corpus=tfidf_transformer.transform(cv.transform(corpus))\nkeywords=[]\nfor b in tf_idf_vector_corpus:\n    sorted_items=sort_coo(b.tocoo())\n    keywords.append(extract_topn_from_vector(feature_names,sorted_items,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add the keywords for each abstract in the Dataframe\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_columns', None)\ndf['keywords']=keywords\ndf1=df.drop(columns='word_count', axis=1)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **CORPUS CLUSTERING**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use the the algorith MinisBatch as a Classifier\n    #Import the required libraries\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import MiniBatchKMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict the cluster\nX1=tf_idf_vector_corpus\n\n#Make the prediction for 10 clusters\nk = 10\n\nkmeans = MiniBatchKMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(X1)\ny=y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Visualize the clusters\nUse the Principal component analysis (PCA) to decompoze the data in project it to a lower dimensional space."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\npca_result = pca.fit_transform(X1.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Vizualize the clusters\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n# colors\npalette = sns.color_palette(\"bright\", len(set(y)))\n# plot\nsns.scatterplot(pca_result[:,0], pca_result[:,1], hue=y, legend='full', palette=palette)\nplt.title(\"Covid-19 Abstracts - Clustered (K-Means)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vizualize in 3D\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nax = plt.figure(figsize=(16,10)).gca(projection='3d')\nax.scatter(\n    xs=pca_result[:,0], \n    ys=pca_result[:,1], \n    zs=pca_result[:,2], \n    c=y, \n    cmap='tab10'\n)\nax.set_xlabel('PCA_1')\nax.set_ylabel('PCA_2')\nax.set_zlabel('PCA_3')\nplt.title(\"Covid-19 Abstracts - Clustered (K-Means)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got a pretty good results!!"},{"metadata":{},"cell_type":"markdown","source":"Generate the cluster of each abstract in the the DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['cluster']=y\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generate the size of each cluster\ndf1.groupby('cluster').apply(len)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}