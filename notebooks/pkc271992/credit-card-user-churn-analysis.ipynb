{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading dataframe"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/credit-card-customers/BankChurners.csv')\ncolumn_list = df.columns\ndf = df.loc[:,column_list[:21]]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic data analysis"},{"metadata":{},"cell_type":"markdown","source":"**Plotting the target variable**"},{"metadata":{"trusted":true},"cell_type":"code","source":"flag =  df['Attrition_Flag'].unique()\nflag_count = []\nfor f in flag:\n    filt = (df['Attrition_Flag'] == f)\n    flag_count.append(df.loc[filt,'Attrition_Flag'].count())    \nfig,ax = plt.subplots(2,figsize=(10,10))\nax[0].pie(flag_count,labels=flag,autopct='%1.1f%%',shadow=True)\nax[0].set_title('Current Situation')\nsns.countplot(df['Attrition_Flag'],ax=ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a highly unbalanced dataset."},{"metadata":{},"cell_type":"markdown","source":"Variation of age groups in dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Customer_Age'].max()\nage_bin = list(range(20,100,5))\nfig,ax = plt.subplots(2,figsize=(10,10))\nsns.countplot(df['Customer_Age'],ax=ax[0])\nax[0].set_title('Actual count of age groups')\nsns.distplot(df['Customer_Age'],bins=age_bin,ax=ax[1])\nax[1].set_title('Variation of age group')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age looks to be uniformly distributed"},{"metadata":{},"cell_type":"markdown","source":"1- Gender plot.\n2- Dependent Distribution\n3- Education Level Plot\n4- Income Category Plot\n5- Card Category"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(5,figsize=(20,20))\nsns.countplot(df['Gender'],ax=ax[0])\nax[0].set_title('Gender count plot')\nsns.countplot(df['Dependent_count'],ax=ax[1])\nax[1].set_title('Dependent count distribution')\nsns.countplot(df['Education_Level'],ax=ax[2])\nax[2].set_title('Education_Level level')\nsns.countplot(df['Income_Category'],ax=ax[3])\nax[3].set_title('Income_Category plot')\ncard_list = df['Card_Category'].unique()\ncard_count = []\nfor card in card_list:\n    filt = (df['Card_Category'] == card)\n    card_count.append(df.loc[filt,'Card_Category'].count())\nax[4].pie(card_count,labels = card_list,autopct='%1.1f%%',shadow=True)    \nax[4].set_title('Percentge of various cards')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's visualize trend among people churned**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#['Existing Customer', 'Attrited Customer']\nfig, ax = plt.subplots(4,figsize=(30,30))\ndf_EC = df[df['Attrition_Flag'] == 'Existing Customer']\ndf_AC = df[df['Attrition_Flag'] == 'Attrited Customer']\nfor i,card in enumerate(card_list):\n    filt_EC = (df_EC['Card_Category'] == card)\n    filt_AC = (df_AC['Card_Category'] == card)\n    ax[i].scatter(df_EC.loc[filt_EC,'Months_on_book'],df_EC.loc[filt_EC,'Credit_Limit'],color='r',marker='.',label='Existing Customer')\n    ax[i].scatter(df_AC.loc[filt_AC,'Months_on_book'],df_AC.loc[filt_AC,'Credit_Limit'],color='g',marker='x',label='Churned Customer')\n    ax[i].set_title('for {} card'.format(card))\n    ax[i].set_xlabel('Duration in months with cutomer')\n    ax[i].set_ylabel('Credit Limit')\nplt.legend()\nplt.show()    \n#df.head()\n#sns.countplot('Card_Category',hue='Customer_Age',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we know majority of cutomers hold Blue card and looking at the plot for blue card it seems that people with medium to low credit limit are most likely to churn, alleast for the blue card, not much of a noticeble pattern in other products**"},{"metadata":{},"cell_type":"markdown","source":"**Let's see if contacting customer from the bank has any pattern with churning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['Contacts_Count_12_mon'],hue=df['Attrition_Flag'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**customers contacted arund 2-4 times churned most, may be more follow ups were required**"},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering."},{"metadata":{},"cell_type":"markdown","source":"Dropping 'CLIENTNUM'column, Checking for Null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['CLIENTNUM'],inplace=True)\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we need to OneHotEncode our categorical features except 'Income_Category' as this is a ordinal feature and we need to map it to ordinal values.**\n\n'Income_Category' column has some unknown values, we need to figure out a strategy to fill in suitable value there.\n"},{"metadata":{},"cell_type":"markdown","source":"Lets see if any column correlates with income catagory, so that we can use that to replace unknown.\nWe average out the range for Income Category and draw heatmap, from heatmap we can see that income catagory is highly negative correlated to Avg_Utilization_Ratio, we'll have to create a linear regression model to predict unknown values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#['Attrition_Flag', 'Gender', 'Education_Level', 'Marital_Status','Income_Category', 'Card_Category']\nincome_map = {'Less than $40K':20,\n              '$40K - $60K':50,\n              '$60K - $80K':70,\n              '$80K - $120K':100,\n              '$120K +':140,\n              'Unknown':0}\ndf['Income_Category'] = df['Income_Category'].map(income_map)\ndf.head()\nfilt = (df['Income_Category'] != 0)\nnum_cols = df.select_dtypes(include=['int32','int64','float64']).columns\ndf_num = df.loc[filt,num_cols]\ncorr = df_num.corr()\nsns.heatmap(corr)\n#income_array = np.array(df.loc[filt,'Income_Category'])\n#limit_array = np.array(df.loc[filt,'Avg_Utilization_Ratio'])\n#plt.plot(limit_array,income_array)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Building linear regression model to predict unknown values in income category**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nnum_feat = ['Customer_Age', 'Dependent_count', 'Months_on_book','Total_Relationship_Count', 'Months_Inactive_12_mon','Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal','Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt','Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']\ny = df_num['Income_Category']\nX = df_num.drop(columns = 'Income_Category')\nlr = LinearRegression()\nlr.fit(X,y)\nfilt2 = (df['Income_Category'] == 0)\nfor i in range(len(df)):\n    if df.loc[i,'Income_Category'] == 0:\n               feature = np.array(df.loc[i,num_feat]).reshape(1,-1)\n               temp_y = lr.predict(feature)\n               df.loc[i,'Income_Category'] = temp_y ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we have populated the Unknown values in 'Income_Category' column with the help of linear regression."},{"metadata":{},"cell_type":"markdown","source":"**Encoding the Catagorical Variables**\n**We need to encode our target column 'Attrition_Flag' as well into numeric(0,1)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Attrition_Flag'] = df['Attrition_Flag'].map({'Existing Customer':0,'Attrited Customer':1})\ny = df['Attrition_Flag']\nX = df.drop(columns='Attrition_Flag')\ncat_cols = X.select_dtypes(include=['object']).columns\nX_cat = pd.get_dummies(X[cat_cols],drop_first=True)\nX.drop(columns=cat_cols,inplace=True)\nX = pd.concat([X,X_cat],axis=1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # Balancing our dataset\n**As we know there is an imbalance of classes in our dataset, we need to balance it before moving to build our model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above result shows the high degree of imbalance, next we balance it"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfig,ax = plt.subplots(2,figsize=(15,15))\nprint('class counts before balancing dataset')\nprint(y.value_counts())\nsns.countplot(y,ax=ax[0])\nax[0].set_title('Before balancing')\n\nsmote = SMOTE(sampling_strategy='minority')\nX_bal, y_bal = smote.fit_sample(X,y)\nprint('class count after balancing the dataset')\nprint(y_bal.value_counts())\nsns.countplot(y_bal,ax=ax[1])\nax[1].set_title('After balancing')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN model\nclf = KNeighborsClassifier(n_neighbors = 9)\nscoring = {'accuracy': 'accuracy',\n           'precision': 'precision',\n           'recall': 'recall',\n           'f1': 'f1'\n           }\nModel = ''\ntest_accuracy = 0\ntest_precision = 0\ntest_recall = 0\ntest_f1 = 0\nperformance_metrics = pd.DataFrame(columns=['Model','test_accuracy','test_precision','test_recall','test_f1'])\nmetrics_dict = {'Model':Model,'test_accuracy':test_accuracy,'test_precision':test_precision,'test_recall':test_recall,'test_f1':test_f1}\nscores_dict = cross_validate(clf, X_bal, y_bal, scoring=scoring, n_jobs=-1)\nfit_time = scores_dict['fit_time'].mean()\nscore_time = scores_dict['score_time'].mean()\nmetrics_dict['Model'] = 'KNN'\nmetrics_dict['test_accuracy'] = scores_dict['test_accuracy'].mean()\nmetrics_dict['test_precision'] = scores_dict['test_precision'].mean()\nmetrics_dict['test_recall'] = scores_dict['test_recall'].mean()\nmetrics_dict['test_f1'] = scores_dict['test_f1'].mean()\nperformance_metrics = performance_metrics.append(metrics_dict, ignore_index=True)\n\n#Logistic regression\nclf = LogisticRegression()\nscores_dict = cross_validate(clf, X_bal, y_bal, scoring=scoring, n_jobs=-1)\nfit_time = scores_dict['fit_time'].mean()\nscore_time = scores_dict['score_time'].mean()\nmetrics_dict['Model'] = 'LogisticRegression'\nmetrics_dict['test_accuracy'] = scores_dict['test_accuracy'].mean()\nmetrics_dict['test_precision'] = scores_dict['test_precision'].mean()\nmetrics_dict['test_recall'] = scores_dict['test_recall'].mean()\nmetrics_dict['test_f1'] = scores_dict['test_f1'].mean()\nperformance_metrics = performance_metrics.append(metrics_dict, ignore_index=True)\n\n#Support Vector\nclf = SVC()\nscores_dict = cross_validate(clf, X_bal, y_bal, scoring=scoring, n_jobs=-1)\nfit_time = scores_dict['fit_time'].mean()\nscore_time = scores_dict['score_time'].mean()\nmetrics_dict['Model'] = 'SVM'\nmetrics_dict['test_accuracy'] = scores_dict['test_accuracy'].mean()\nmetrics_dict['test_precision'] = scores_dict['test_precision'].mean()\nmetrics_dict['test_recall'] = scores_dict['test_recall'].mean()\nmetrics_dict['test_f1'] = scores_dict['test_f1'].mean()\nperformance_metrics = performance_metrics.append(metrics_dict, ignore_index=True)\n\n#Naive bayes\n\nclf = GaussianNB()\nscores_dict = cross_validate(clf, X_bal, y_bal, scoring=scoring, n_jobs=-1)\nfit_time = scores_dict['fit_time'].mean()\nscore_time = scores_dict['score_time'].mean()\nmetrics_dict['Model'] = 'Naive bayes'\nmetrics_dict['test_accuracy'] = scores_dict['test_accuracy'].mean()\nmetrics_dict['test_precision'] = scores_dict['test_precision'].mean()\nmetrics_dict['test_recall'] = scores_dict['test_recall'].mean()\nmetrics_dict['test_f1'] = scores_dict['test_f1'].mean()\nperformance_metrics = performance_metrics.append(metrics_dict, ignore_index=True)\n\n#Random forest \nclf = RandomForestClassifier(n_estimators = 100)\nscores_dict = cross_validate(clf, X_bal, y_bal, scoring=scoring, n_jobs=-1)\nfit_time = scores_dict['fit_time'].mean()\nscore_time = scores_dict['score_time'].mean()\nmetrics_dict['Model'] = 'Randon Forest'\nmetrics_dict['test_accuracy'] = scores_dict['test_accuracy'].mean()\nmetrics_dict['test_precision'] = scores_dict['test_precision'].mean()\nmetrics_dict['test_recall'] = scores_dict['test_recall'].mean()\nmetrics_dict['test_f1'] = scores_dict['test_f1'].mean()\nperformance_metrics = performance_metrics.append(metrics_dict, ignore_index=True)\n\n#\n\n\nperformance_metrics\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}