{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport plotly.express as px\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","collapsed":true,"jupyter":{"outputs_hidden":true},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://www.stressmarq.com/wp-content/uploads/SOD1-Aggregation-in-ALS-1024x512.png)https://www.stressmarq.com/sod1-aggregation-in-als/?v=3e8d115eb4b3","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:#DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% / 10% 40%\">SOD1 (Superoxide Dismutase 1)</h1>\n\n\"SOD1 (Superoxide Dismutase 1) is a Protein Coding gene. Diseases associated with SOD1 include Amyotrophic Lateral Sclerosis (ALS) and Spastic Tetraplegia And Axial Hypotonia, Progressive.\" \n\n\"Among its related pathways are Association Between Physico-Chemical Features and Toxicity Associated Pathways and Response to elevated platelet cytosolic Ca2+. Gene Ontology (GO) annotations related to this gene include protein homodimerization activity and enzyme binding. An important paralog of this gene is CCS.\"\n\nhttps://www.genecards.org/cgi-bin/carddisp.pl?gene=SOD1","metadata":{}},{"cell_type":"markdown","source":"# **<span style=\"color:#DC143C;\">Data from Mutant SOD1</span>**\n\n\"Expression profiling of spinal cord from SOD1(G93A) mice and age matched controls at ages 28, 42, 56, 70, 98, 112, and 126 days of age. The authors used microarrays to determine differential gene expression throughout disease progression in the spinal cord of mutant SOD1(G93A) model of ALS.\"\n\n\"Samples were collected from male B6SJL SOD1(G93A) and age matched controls. 3 samples were collected representing each genotype and age group for RNA extraction and hybridization on Affymetrix microarrays.\"\n\nhttp://biogps.org/#goto=genereport&id=12566&show_dataset=E-GEOD-18597","metadata":{}},{"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf = pd.read_csv('../input/cusersmarildownloadsmutantcsv/mutant.csv', delimiter=';', encoding = \"ISO-8859-2\", nrows = nRowsRead)\ndf.dataframeName = 'mutant.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head().style.set_properties(**{'background-color':'red',\n                                     'color': 'black'})","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Many forms of ALS, regardless of the offending primary gene mutation, show TDP-43 pathology; exceptions are cases associated with SOD1 and FUS gene mutations.\n\nhttps://www.kaggle.com/mpwolke/als-dna-genetics (ALS DNA & Genetics).","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color:#DC143C;\">Protein attributes for SOD1 Gene</span>**\n\nSize: 154 amino acids\n\nMolecular mass: 15936 Da\n\nCofactor: Name=Cu cation; Xref=ChEBI:CHEBI:23378;\n\nCofactor: Name=Zn(2+); Xref=ChEBI:CHEBI:29105;\n\nQuaternary structure: Homodimer; non-disulfide linked. Homodimerization may take place via the ditryptophan cross-link at Trp-33. The pathogenic variants ALS1 Arg-38, Arg-47, Arg-86 and Ala-94 interact with RNF19A, whereas wild-type protein does not. The pathogenic variants ALS1 Arg-86 and Ala-94 interact with MARCH5, whereas wild-type protein does not.\n\nMiscellaneous: The protein (both wild-type and ALS1 variants) has a tendency to form fibrillar aggregates in the absence of the intramolecular disulfide bond or of bound zinc ions. These aggregates may have cytotoxic effects. Zinc binding promotes dimerization and stabilizes the native form.\n\nhttps://www.genecards.org/cgi-bin/carddisp.pl?gene=SOD1","metadata":{}},{"cell_type":"code","source":"# donut plot\nfeature_names = \"Samples\",\"1416873_a_at\",\"1447617_at\"\nfeature_size = [len('Samples'),len('1416873_a_at'),len('1447617_at')]\n# create a circle for the center of plot\ncircle = plt.Circle((0,0),0.2,color = \"white\")\nplt.pie(feature_size, labels = feature_names, colors = [\"red\",\"green\",\"blue\"] )\np = plt.gcf()\np.gca().add_artist(circle)\nplt.title(\"Number of Each Feature\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#DC143C;\">SOD1 Aggregation in ALS</span>**\n\nAuthor: Patricia Thomson - JUNE 6TH, 2019 \n\n**<span style=\"color:#DC143C;\">Linking SOD1 and ALS</span>**\n\n\"Approximately 20% of fALS (family history ALS) cases are associated with SOD1 mutations. Over 150 mutations have been implicated. Most are point mutations; it is unclear whether all of these are sufficient to cause ALS as single mutations or if several could be needed in conjunction. SOD1 mutations can also occur in sALS, and have been associated with variations in fALS survival times.\"\n\n\"It has been suggested that mutations cause SOD1 to lose its ability to scavenge superoxide radicals, leading to increased oxidative stress, but it now seems more likely that SOD1 contributes to ALS by gaining a toxic function rather than losing its regular function. This toxic gain-of function is thought to be SOD1 misfolding and aggregation into oligomers and ultimately larger aggregates. This idea is supported by the presence of mutant SOD1–containing aggregates that increase in abundance as disease progresses.\"\n\nhttps://www.stressmarq.com/sod1-aggregation-in-als/?v=3e8d115eb4b3","metadata":{}},{"cell_type":"code","source":"fig = px.bar(df, x= \"Samples\", y= \"1416873_a_at\", color_discrete_sequence=['crimson'], title= 'Mutant SOD1 Samples')\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#DC143C;\">How SOD1 Mutations Lead to Aggregation</span>**\n\nSOD1 usually undergoes the following post-translational modifications:\n\nCopper Insertion, Zinc Insertion, Dimerization, Disulfide Bond Formatio.\n\n\"Many fALS-associated mutations disrupt these post-translational modifications, preventing the proper structure and folding of the SOD1 protein. Misfolded SOD1 is prone to aggregation into soluble oligomers, via the formation of intermolecular disulfide bonds between the free cysteine residues of different SOD1 molecules and non-covalent interactions (hydrogen bonds) between beta strands of SOD1 subunits.\"\n\n**<span style=\"color:#DC143C;\">Demetallation</span>**\n\n\"Apo, or completely demetallated SOD1 is susceptible to oligomerization, as is zinc-deficient SOD1. FALS-associated mutations are thought to lower the zinc binding affinity of SOD1 by altering the zinc binding geometry. One possibility is that mutations that perturb the electrostatic loop allow solvent to access the metal sites, preventing metallation with Cu and Zn.\"\n\n**<span style=\"color:#DC143C;\">Dimerization</span>**\n\n\"Dimerization of SOD1 reduces the surface area that is accessible to solvent, which increases its stability. When dimerization is disrupted, unstable monomers are prone to aggregation. fALS-associated mutations occur on the dimer interphase and can cause dimers to dissociate into monomers which act as aggregation templates.\"\n\n**<span style=\"color:#DC143C;\">Disulfide Bonding</span>**\n\n\"Each SOD1 subunit contains a disulfide bond between two of cysteine residues (Cys 57 and Cys 146). Intrasubunit disulfide bonding increases SOD1 stability, so its interruption can result in aggregation of unstable species. Intrasubunit disulfide bond reduction is necessary for fibril initiation and promotes faster seeding of aggregates.\"\n\nhttps://www.stressmarq.com/sod1-aggregation-in-als/?v=3e8d115eb4b3","metadata":{}},{"cell_type":"code","source":"fig = px.bar(df, x= \"1447617_at\", y= \"Samples\", color_discrete_sequence=['#2B3A67'], title= 'Mutant SOD1 Samples')\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#DC143C;\">Mechanisms of SOD1 Toxicity</span>**\n\n\"As in other neurodegenerative diseases, the oligomeric form of SOD1 is likely more toxic than large SOD1 aggregates. Some hypothesize that aggregation is a protective mechanism against these toxic oligomers.These oligomers are thought to contain antiparallel, out-of-register β-sheet structures involving segment.\"\n\nThere are several proposed mechanisms of toxicity for SOD1 oligomers:\n\n**<span style=\"color:#DC143C;\">Excitotoxicity</span>**\n\n\"An increase in the extracellular glutamate concentration causes an influx of calcium into the postsynaptic neuron, which can result in mitochondrial damage and ultimately apoptosis. Riluzole, a drug that extends survival for ALS patients by 2-3 months, is thought to negate excitotoxicity.\"\n\n**<span style=\"color:#DC143C;\">Endoplasmic Reticulum Stress</span>**\n\n\"Mutant SOD1 interacts with ER-associated degradation machinery (ERAD) and causes it to malfunction.  This can also lead to apoptosis.\"\n\n**<span style=\"color:#DC143C;\">Axonal Transport Disruption</span>**\n\n\"Mutant SOD1 can induce axonal transport defects by reducing microtubule stability and disrupting mitochondrial transport.\"\n\n**<span style=\"color:#DC143C;\">Oxidative Stress and Mitochondrial Damage</span>**\n\n\"Oxidative stress is associated with a variety of disease states, and oxidative stress is thought to  contribute to the pathogenesis of sporadic ALS. The responsible ROS may be a product of mitochondrial dysfunction due to the accumulation of aggregated SOD1. The release of mitochondrial Ca2+ can also contribute to apoptosis.\"\n\n**<span style=\"color:#DC143C;\">Non-Cell Autonomous Toxicity</span>**\n\n\"ALS is considered to be non-cell autonomous, meaning that other non-neuronal cells, such as astrocytes and microglia also contribute to pathogenesis and disease progression.\"\n\nhttps://www.stressmarq.com/sod1-aggregation-in-als/?v=3e8d115eb4b3","metadata":{}},{"cell_type":"code","source":"# 3D Scatter Plot\nfig = px.scatter_3d(df, x='1416873_a_at', y='1447617_at', z='Samples')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Not much \"scatters\" in this 3D. What is weird is that the number of these Probes (e.g.1447617_at) are the same of the \"Cytokine inhibition, production and autoimmunity\" Dataset and cytokine file.","metadata":{}},{"cell_type":"code","source":"#Code by Salman Ibne Eunus  https://www.kaggle.com/salmaneunus/linear-models-and-regularization-for-regresssion\n\n!pip install mglearn==0.1.9","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport sys\nfrom scipy import sparse\nprint(\"Python version: {}\".format(sys.version))\nprint(\"pandas version: {}\".format(pd.__version__))\nimport matplotlib\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\nprint(\"NumPy version: {}\".format(np.__version__))\nimport scipy as sp\nprint(\"SciPy version: {}\".format(sp.__version__))\nimport IPython\nprint(\"IPython version: {}\".format(IPython.__version__))\nimport sklearn\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\nimport mglearn\nimport matplotlib.pyplot as plt","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mglearn.plots.plot_linear_regression_wave()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nX, y = mglearn.datasets.make_wave(n_samples=60)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.get_dummies(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LinearRegression().fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Salman Ibne Eunus  https://www.kaggle.com/salmaneunus/linear-models-and-regularization-for-regresssion\n\nprint(\"lr.coef_: {}\".format(lr.coef_))\nprint(\"lr.intercept_: {}\".format(lr.intercept_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The “slope” parameters (w), also called weights or coefficients, are stored in the coef attribute, while the offset or intercept (b) is stored in the intercept attribute","metadata":{}},{"cell_type":"code","source":"#Code by Salman Ibne Eunus  https://www.kaggle.com/salmaneunus/linear-models-and-regularization-for-regresssion\n\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"The test score of around 0.66 is not very impressive, but we can see training and test score are very close to each other. This implies that we are likely underfitting and not overfitting. Although for this one dimensional dataset there is a little danger of overfitting as the model is very simple. Moreover, with higher dimensional datasets which has large number of features, linear models become quite powerful and thus more chance of overfitting.\"\n\nMy numbers are exactly Salman numbers in another Dataset???","metadata":{}},{"cell_type":"markdown","source":"#Ridge Regression","metadata":{}},{"cell_type":"code","source":"#Code by Salman Ibne Eunus  https://www.kaggle.com/salmaneunus/linear-models-and-regularization-for-regresssion\n\nfrom sklearn.linear_model import Ridge\nridge = Ridge().fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Salman Ibne Eunus  https://www.kaggle.com/salmaneunus/linear-models-and-regularization-for-regresssion\n\nridge10 = Ridge(alpha=10).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#setting alpha=0.1 will give better score\n\nridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\")\nplt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\nplt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\nplt.plot(lr.coef_, 'o', label=\"LinearRegression\")\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")\nplt.hlines(0, 0, len(lr.coef_))\nplt.ylim(-25, 25)\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mglearn.plots.plot_ridge_n_samples()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Salman Ibne Eunus  https://www.kaggle.com/salmaneunus/linear-models-and-regularization-for-regresssion\n\nfrom sklearn.linear_model import Lasso\nlasso = Lasso().fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we increase the default setting of \"max_iter\",\n# otherwise the model would warn us that we should increase max_iter.\nlasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Salman Ibne Eunus  https://www.kaggle.com/salmaneunus/linear-models-and-regularization-for-regresssion\n\nlasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Salman Ibne Eunus  https://www.kaggle.com/salmaneunus/linear-models-and-regularization-for-regresssion\n\nplt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\")\nplt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\nplt.plot(lasso00001.coef_, 'v', label=\"Lasso alpha=0.0001\")\nplt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\nplt.legend(ncol=2, loc=(0, 1.05))\nplt.ylim(-25, 25)\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#All script above is by Salman Ibne Eunus  https://www.kaggle.com/salmaneunus/linear-models-and-regularization-for-regresssion \n\nThe results are IDENTICAL from Salman's Boston Housing Dataset, however I'm working with Mutant Genes SOD1. Therefore, the results above are wrong, except the Mutant SOD1 file.","metadata":{}}]}