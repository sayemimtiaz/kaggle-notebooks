{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **IMDB Films Review Sentiment Text Classification using Keras**\nThe text data, used in this notebook, is the film reviews data from IMDB resource. In this python notebook i would like to create a mechanism for **text classification** into two groups (positive, negative), based on the sentiment.  We are going to accomplish this task by building neural networks using the Keras framework."},{"metadata":{},"cell_type":"markdown","source":"## **Import packages**\nWe start by importing the packages. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom collections import Counter\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nimport re\nfrom nltk.stem import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Load and inspect data** \nLet's load the data and inspect its structure.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\",\n                 sep=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us find the factors for sentiment values and convert them to integer values (0 and 1) for the algorithm to be able to process."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----- Get labels -----\ny = np.int32(df.sentiment.astype('category').cat.codes.to_numpy())\n# ----- Get number of classes -----\nnum_classes = np.unique(y).shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocess text"},{"metadata":{},"cell_type":"markdown","source":"Before we apply any algorithm, we need to have a cleaned data set. In our case with text data, it is essential to perform preprocessing steps in order to increase the performance of the algorithms. The steps we perform are: \n* remove br tags\n* remove all single characters\n* substituting multiple spaces with single space\n* removing prefixed 'b'\n* converting to lowercase"},{"metadata":{},"cell_type":"markdown","source":"The final preprocessing step is the lemmatization. In lemmatization, we reduce the word into dictionary root form. For instance \"cats\" is converted into \"cat\". Lemmatization is done in order to avoid creating features that are semantically similar but syntactically different. For instance, we don't want two different features named \"cats\" and \"cat\", which are semantically similar, therefore we perform lemmatization."},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = WordNetLemmatizer()\ndef custom_standardization(text):\n    \n    text = re.sub('<br />', ' ', str(text))\n    \n    text = re.sub(r'\\W', ' ', str(text))\n    \n    # remove all single characters\n    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n    \n    # substituting multiple spaces with single space\n    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n    \n    # removing prefixed 'b'\n    text = re.sub(r'^b\\s+', '', text)\n    \n    # converting to Lowercase\n    text = text.lower()\n    \n    # lemmatization\n    text = text.split()\n\n    text = [stemmer.lemmatize(word) for word in text]\n    text = ' '.join(text)\n    \n    return text\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Cleaned_Text'] = df.review.apply(custom_standardization)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Cleaned_Text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further we are going to use a word embedding representation for our text data reviews. A word embedding is a class of approaches for representing words and documents using a dense vector representation, where a vector represents the projection of the word into a continuous vector space. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. The position of a word in the learned vector space is referred to as its embedding. More information about word embeddings can be found here: \n* https://www.tensorflow.org/tutorials/text/word_embeddings"},{"metadata":{},"cell_type":"markdown","source":"Before we use word embeddings, we need to convert our words into integers first. To do this we define the size of our vocabulary (max_features) and find the most occuring words in our data, to be used in the dictionary. After that, we use a tokenizer to represent each review as an integer vector corresponding to the dictionary.\nTokenizer vectorize a text corpus into a list of integers. Each integer maps to a value in a dictionary that encodes the entire corpus, with the keys in the dictionary being the vocabulary terms themselves. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----- Prepare text for embedding -----\nmax_features = 10000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----- Get top 10000 most occuring words in list-----\nresults = Counter()\ndf['Cleaned_Text'].str.split().apply(results.update)\nvocabulary = [key[0] for key in results.most_common(max_features)]\n\n# ----- Create tokenizer based on your top 10000 words -----\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(vocabulary)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Cleaned_Text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# ----- Convert words to ints and pad -----\nX = tokenizer.texts_to_sequences(df['Cleaned_Text'].values)\nX = pad_sequences(X)\n\n\n# ----- Split into Train, Test, Validation sets -----\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Define and train a model**"},{"metadata":{},"cell_type":"markdown","source":"To crete an embedding layer in Keras we need to give as input 3 parameters. \n* input_dim: This is the size of the vocabulary in the text data\n* output_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger\n* input_length: This is the length of input sequences, as you would define for any input layer of a Keras model"},{"metadata":{"trusted":true},"cell_type":"code","source":"output_dim = 16\nmax_input_lenght = X.shape[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We define a sequential keras model, starting with an embedding layer with output dimension 16. After that we use GlobalAveragePooling1D layer and one Dense layer. \nA GlobalAveragePooling1D layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\nThis fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units. Finally, the dense layer with two classes is used for classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----- Define model -----\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Embedding(input_dim=max_features, output_dim=output_dim, input_length=max_input_lenght))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.GlobalAveragePooling1D())\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\n# ----- Compile model -----\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(1e-4), metrics=[\"accuracy\"])\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----- Train model -----\nhistory_1 = model.fit(X_train, y_train, batch_size=8,epochs=20, validation_data=(X_val, y_val))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Evaluate the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----- Evaluate model -----\nprobabilities = model.predict(X_test)\npred = np.argmax(probabilities, axis=1)\n\nprint(\" \")\nprint(\"Results\")\n\naccuracy = accuracy_score(y_test, pred)\n\nprint('Accuracy: {:.4f}'.format(accuracy))\nprint(\" \")\nprint(classification_report(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We obtained quite good results of 89% accuracy on a test data set with a simple NN design. With the plots above we also see that we are not doing too much overfitting."},{"metadata":{},"cell_type":"markdown","source":"As a next step the parameters of the neural network can be optimized by performing a grid search. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}