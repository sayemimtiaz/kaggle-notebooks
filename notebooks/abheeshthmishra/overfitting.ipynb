{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn import datasets\nfrom sklearn import manifold\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overfitting"},{"metadata":{},"cell_type":"markdown","source":"To explain overfitting, I think itâ€™s best if we look at a dataset. There is a red winequality dataset2 which is quite famous. This dataset has 11 different attributes that\ndecide the quality of red wine.\nThese attributes include:\n* fixed acidity\n* volatile acidity\n* citric acid\n* residual sugar\n* chlorides\n* free sulfur dioxide\n* total sulfur dioxide\n* density\n* pH\n* sulphates\n* alcohol\n\nBased on these different attributes, we are required to predict the quality of red wine\nwhich is a value between 0 and 10."},{"metadata":{"trusted":true},"cell_type":"code","source":"Data = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = Data['quality'].unique()\nprint(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset, however, consists of only six\ntypes of quality values. We will thus map all quality values from 0 to 5."},{"metadata":{"trusted":true},"cell_type":"code","source":"quality_mapping = {\n    3:0,\n    4:1,\n    5:2,\n    6:3,\n    7:4,\n    8:5\n}\n\nData.loc[:,\"quality\"] = Data.quality.map(quality_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use sample with frac=1 to shuffle the dataframe\n# we reset the indices since they change after\n# shuffling the dataframe\nData = Data.sample(frac = 1).reset_index(drop=True)\nData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top 1000 rows are selected\n# for training\ndata_train = Data.head(1000)\n# bottom 599 values are selected\n# for testing/validation\ndata_test = Data.tail(599)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now train a decision tree model on the training set. For the decision tree\nmodel, I am going to use scikit-learn"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import from scikit-learn\nfrom sklearn import tree\nfrom sklearn import metrics\n# initialize decision tree classifier class\n# with a max_depth of 3\nclf = tree.DecisionTreeClassifier(max_depth=3)\n# choose the columns you want to train on\n# these are the features for the model\ncols = ['fixed acidity',\n 'volatile acidity',\n 'citric acid',\n 'residual sugar',\n 'chlorides',\n 'free sulfur dioxide',\n 'total sulfur dioxide',\n 'density',\n 'pH',\n 'sulphates',\n 'alcohol']\n  \n# train the model on the provided features\n# and mapped quality from before\nclf.fit(data_train[cols],data_train.quality)\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" I have used a max_depth of 3 for the decision tree classifier. \n \n I have left all other parameters of this model to its default value.\n Now, we test the accuracy of this model on the training set and the test set:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate predictions on the training set\ntrain_predictions = clf.predict(data_train[cols])\n\n# generate predictions on the test set\ntest_predictions = clf.predict(data_test[cols])\n\n# calculate the accuracy of predictions on\n# training data set\n\ntraining_accuracy = metrics.accuracy_score(\ndata_train.quality,train_predictions\n)\n\n# calculate the accuracy of predictions on\n# test data set\ntest_accuracy = metrics.accuracy_score(\ndata_test.quality, test_predictions\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_accuracy,training_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training and test accuracies are found to be 58.9% and 54.25%. Now we\nincrease the max_depth to 7 and repeat the process. This gives training accuracy of\n76.6% and test accuracy of 57.3%"},{"metadata":{},"cell_type":"markdown","source":"we calculate these accuracies for different values of max_depth and\nmake a plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import scikit-learn tree and metrics\nfrom sklearn import tree\nfrom sklearn import metrics\n# import matplotlib and seaborn\n# for plotting\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# this is our global size of label text\n# on the plots\nmatplotlib.rc('xtick', labelsize=20)\nmatplotlib.rc('ytick', labelsize=20)\n# This line ensures that the plot is displayed\n# inside the notebook\n%matplotlib inline\n# initialize lists to store accuracies\n# for training and test data\n# we start with 50% accuracy\ntrain_accuracies = [0.5]\ntest_accuracies = [0.5]\n# iterate over a few depth values\nfor depth in range(1, 25):\n # init the model\n clf = tree.DecisionTreeClassifier(max_depth=depth)\n # columns/features for training\n # note that, this can be done outside\n # the loop\n cols = [\n 'fixed acidity',\n 'volatile acidity',\n 'citric acid',\n 'residual sugar',\n 'chlorides',\n 'free sulfur dioxide',\n 'total sulfur dioxide',\n 'density',\n 'pH',\n 'sulphates',\n 'alcohol'\n ]\n # fit the model on given features\n clf.fit(data_train[cols], data_train.quality)\n # create training & test predictions\n train_predictions = clf.predict(data_train[cols])\n test_predictions = clf.predict(data_test[cols])\n # calculate training & test accuracies\n train_accuracy = metrics.accuracy_score(\n data_train.quality, train_predictions\n )\n test_accuracy = metrics.accuracy_score(\n data_test.quality, test_predictions\n )\n\n # append accuracies\n train_accuracies.append(train_accuracy)\n test_accuracies.append(test_accuracy)\n# create two plots using matplotlib\n# and seaborn\nplt.figure(figsize=(10, 5))\nsns.set_style(\"whitegrid\")\nplt.plot(train_accuracies, label=\"train accuracy\")\nplt.plot(test_accuracies, label=\"test accuracy\")\nplt.legend(loc=\"upper left\", prop={'size': 15})\nplt.xticks(range(0, 26, 5))\nplt.xlabel(\"max_depth\", size=20)\nplt.ylabel(\"accuracy\", size=20)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the best score for test data is obtained when max_depth has a value of 14. As we keep increasing the value of this parameter, test accuracy remains the\nsame or gets worse, but the training accuracy keeps increasing. It means that our\nsimple decision tree model keeps learning about the training data better and better\nwith an increase in max_depth, but the performance on test data does not improve\nat all. \n\n\nThis is called overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}