{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"bbb53629-d2a7-ffc6-8e0c-9f3cf2f9c497"},"source":" 1. In this problem we have to use 30 different columns and we have to predict the Stage of Breast Cancer M (Malignant)  and B (Bengin)\n 2. This analysis has been done using Basic Machine Learning Algorithm with detailed explanation\n 3. This is good for beginners like as me Lets start.\n \n4.Attribute Information:\n\n1) ID number\n\n2) Diagnosis (M = malignant, B = benign)\n\n-3-32.Ten real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\n\nb) texture (standard deviation of gray-scale values)\n\nc) perimeter\n\nd) area\n\ne) smoothness (local variation in radius lengths)\n\nf) compactness (perimeter^2 / area - 1.0)\n\ng). concavity (severity of concave portions of the contour)\n\nh). concave points (number of concave portions of the contour)\n\ni). symmetry\n\nj). fractal dimension (\"coastline approximation\" - 1)\n\n5  here 3- 32 are divided into three parts first is Mean (3-13),  Stranded Error(13-23) and  Worst(23-32) and each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension) \n\n 6. Here Mean means the means of the all cells,  standard Error of all cell and worst means the worst  cell "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e3200e6e-62d4-1d74-496b-c5197a574dcb"},"outputs":[],"source":"# here we will import the libraries used for machine learning\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph. I like it most for plot\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression # to apply the Logistic regression\nfrom sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.cross_validation import KFold # use for cross validation\nfrom sklearn.model_selection import GridSearchCV# for tuning parameter\nfrom sklearn.ensemble import RandomForestClassifier # for random forest classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm # for Support Vector Machine\nfrom sklearn import metrics # for the check the error and accuracy of the model\n# Any results you write to the current directory are saved as output.\n# dont worry about the error if its not working then insteda of model_selection we can use cross_validation"},{"cell_type":"markdown","metadata":{"_cell_guid":"fa78428b-ecbb-56a4-2904-f90d725281ec"},"source":"**Import data **"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7036d299-0057-db0a-d892-fe212c0d612c"},"outputs":[],"source":"data = pd.read_csv(\"../input/data.csv\",header=0)# here header 0 means the 0 th row is our coloumn \n                                                # header in data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3521dc85-be7e-a735-0cc1-b3437c16ab22"},"outputs":[],"source":"# have a look at the data\nprint(data.head(2))# as u can see our data have imported and having 33 columns\n# head is used for to see top 5 by default I used 2 so it will print 2 rows\n# If we will use print(data.tail(2))# it will print last 2 rows in data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1109ea03-5b73-25fa-72ca-6695de4b0c5b"},"outputs":[],"source":"# now lets look at the type of data we have. We can use \ndata.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"9041739c-1d90-970b-55ec-224ed821898a"},"source":"*As I said I m beginner, so here I am explaining every thing in detail.\n\n 1.So lets describe what these data type means, e.g 5 radius_mean 569 non-null float64 that means the radius_mean have 569 float type value.\n\n2. Now we can see Unnamed:32 have 0 non null object it means the all values are null in this column so we cannot use this column for our analysis*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ec57be7-c20b-5895-2f20-0f32651b3f79"},"outputs":[],"source":"# now we can drop this column Unnamed: 32\ndata.drop(\"Unnamed: 32\",axis=1,inplace=True) # in this process this will change in our data itself \n# if you want to save your old data then you can use below code\n# data1=data.drop(\"Unnamed:32\",axis=1)\n# here axis 1 means we are droping the column"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a93a7754-8b38-6ca4-cc7d-e31cfd58afa6"},"outputs":[],"source":"# here you can check the column has been droped\ndata.columns # this gives the column name which are persent in our data no Unnamed: 32 is not now there"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0da592b-04ff-9c69-b5b0-46e87ebc391d"},"outputs":[],"source":"# like this we also don't want the Id column for our analysis\ndata.drop(\"id\",axis=1,inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"68f07b98-c05a-f3e6-7dbe-8e39ab7d13f6"},"outputs":[],"source":"# As I said above the data can be divided into three parts.lets divied the features according to their category\nfeatures_mean= list(data.columns[1:11])\nfeatures_se= list(data.columns[11:20])\nfeatures_worst=list(data.columns[21:31])\nprint(features_mean)\nprint(\"-----------------------------------\")\nprint(features_se)\nprint(\"------------------------------------\")\nprint(features_worst)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dbdd0bf6-da03-c390-0e7a-f6b181f8e127"},"outputs":[],"source":"# lets now start with features_mean \n# now as ou know our diagnosis column is a object type so we can map it to integer value\ndata['diagnosis']=data['diagnosis'].map({'M':1,'B':0})"},{"cell_type":"markdown","metadata":{"_cell_guid":"19034b94-8ddf-ef3f-675d-c87b6a558c1b"},"source":"## Explore the Data now"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"09c22a48-dbc2-f1c3-517a-491564ab161e"},"outputs":[],"source":"data.describe() # this will describe the all statistical function of our data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"598dce8d-1245-14e5-0e1c-ab609e51fec6"},"outputs":[],"source":"# lets get the frequency of cancer stages\nsns.countplot(data['diagnosis'],label=\"Count\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"38041fef-a314-d3b5-8594-3234b492d8d1"},"outputs":[],"source":"# from this graph we can see that there is a more number of bengin stage of cancer which can be cure"},{"cell_type":"markdown","metadata":{"_cell_guid":"5b9a0c99-86ca-9b5e-687d-8da91bcf9ff5"},"source":"## Data Analysis a little feature selection"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"592f4ece-d0b1-cccf-972e-3b7feace219b"},"outputs":[],"source":"# now lets draw a correlation graph so that we can remove multi colinearity it means the columns are\n# dependenig on each other so we should avoid it because what is the use of using same column twice\n# lets check the correlation between features\n# now we will do this analysis only for features_mean then we will do for others and will see who is doing best\ncorr = data[features_mean].corr() # .corr is used for find corelation\nplt.figure(figsize=(14,14))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n           xticklabels= features_mean, yticklabels= features_mean,\n           cmap= 'coolwarm') # for more on heatmap you can visit Link(http://seaborn.pydata.org/generated/seaborn.heatmap.html)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b79fcdb1-8104-291f-64a1-2b5faa145f0d"},"source":"*observation*\n\n - the radius, parameter and area  are highly correlated as expected from their relation*\n    so from these we will use anyone of them *\n - *compactness_mean, concavity_mean and concavepoint_mean are highly correlated so we will use compactness_mean from here *\n - so selected Parameter for use is perimeter_mean, texture_mean, compactness_mean, symmetry_mean*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9ee0ccd-4364-6186-e844-87ed319c1519"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fcd92fe8-fb89-d377-f135-c053e562e820"},"outputs":[],"source":"prediction_var = ['texture_mean','perimeter_mean','smoothness_mean','compactness_mean','symmetry_mean']\n# now these are the variables which will use for prediction"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da2eaf5c-6f86-b320-a591-fb7b7e0fb12c"},"outputs":[],"source":"#now split our data into train and test\ntrain, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and test\n# we can check their dimension\nprint(train.shape)\nprint(test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27cb4563-3191-8ab9-8ce2-224c2db223d0"},"outputs":[],"source":"train_X = train[prediction_var]# taking the training data input \ntrain_y=train.diagnosis# This is output of our training data\n# same we have to do for test\ntest_X= test[prediction_var] # taking test data inputs\ntest_y =test.diagnosis   #output value of test dat"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4936a3a9-e7fd-81a1-1aec-a267224569d5"},"outputs":[],"source":"model=RandomForestClassifier(n_estimators=100)# a simple random forest model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c20ee887-14c4-fd6c-98fe-d54c06d6d4e4"},"outputs":[],"source":"model.fit(train_X,train_y)# now fit our model for traiing data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"abb767bd-ed3e-6af0-9aa5-55ccbf50f9c7"},"outputs":[],"source":"prediction=model.predict(test_X)# predict for the test data\n# prediction will contain the predicted value by our model predicted values of dignosis column for test inputs"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f37e4a4e-214c-9f56-391b-19a9a6b141ec"},"outputs":[],"source":"metrics.accuracy_score(prediction,test_y) # to check the accuracy\n# here we will use accuracy measurement between our predicted value and our test output values"},{"cell_type":"markdown","metadata":{"_cell_guid":"b8ae477d-f309-03b1-e224-f993ee9fed5b"},"source":"* Here the Accuracy for our model is 91 % which seems good*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1340630-a7e2-2f63-5b53-0fb407f5e177"},"outputs":[],"source":"# lets now try with SVM"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74acfdf3-a6c1-e262-eaf5-742c62bf1382"},"outputs":[],"source":"model = svm.SVC()\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nmetrics.accuracy_score(prediction,test_y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3a2a515c-5b1f-4086-48cb-96dbaa67815c"},"source":"**SVM is giving only 0.85 which we can improve by using different techniques** \n**i will improve it till then beginners can understand how to model a data and they can have a overview of ML**"},{"cell_type":"markdown","metadata":{"_cell_guid":"87950903-844c-058d-23d5-2d55ae58de75"},"source":"*Now lets do this for all feature_mean so that from Random forest we can get the feature which are important**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4dd23ad4-a0e5-09e3-4319-187051f24199"},"outputs":[],"source":"prediction_var = features_mean # taking all features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ffa199c-9f4e-8574-430d-d3133ae0d8c6"},"outputs":[],"source":"train_X= train[prediction_var]\ntrain_y= train.diagnosis\ntest_X = test[prediction_var]\ntest_y = test.diagnosis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20b8aaf5-677c-a2af-c998-dfd5fa713115"},"outputs":[],"source":"model=RandomForestClassifier(n_estimators=100)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b55cdfda-aab0-4130-990f-506754f16734"},"outputs":[],"source":"model.fit(train_X,train_y)\nprediction = model.predict(test_X)\nmetrics.accuracy_score(prediction,test_y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8d0c1af5-f907-00c5-e189-3dc1e78c032b"},"source":" - by taking all features accuracy increased but not so much so according to Razor's rule simpler method is better\n - by the way now lets check the importan features in the prediction"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de3e85a8-20c3-8ebf-57f0-68dd360e0816"},"outputs":[],"source":"featimp = pd.Series(model.feature_importances_, index=prediction_var).sort_values(ascending=False)\nprint(featimp) # this is the property of Random Forest classifier that it provide us the importance \n# of the features used"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8a4c22d-757e-2768-b975-88656f0a9ad4"},"outputs":[],"source":"# first lets do with SVM also using all features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0ae8f9c-05a7-e4b4-c7ac-4ca46c69c66e"},"outputs":[],"source":"model = svm.SVC()\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nmetrics.accuracy_score(prediction,test_y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6b8ba417-d74b-67fe-f507-6c648de683ed"},"outputs":[],"source":"# as you can see the accuracy of SVM decrease very much\n# now lets take only top 5 important features given by RandomForest classifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36a11667-fce7-4fa0-8b84-7780230ef339"},"outputs":[],"source":"prediction_var=['concave points_mean','perimeter_mean' , 'concavity_mean' , 'radius_mean','area_mean']      "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9aafdb3-1ee7-56d7-57d0-ac009f9a9522"},"outputs":[],"source":"train_X= train[prediction_var]\ntrain_y= train.diagnosis\ntest_X = test[prediction_var]\ntest_y = test.diagnosis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"37caf223-82ef-0579-32c8-de2a530faa3a"},"outputs":[],"source":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_y)\nprediction = model.predict(test_X)\nmetrics.accuracy_score(prediction,test_y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"113982ab-8065-3e27-92b8-0716924138e7"},"outputs":[],"source":"model = svm.SVC()\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nmetrics.accuracy_score(prediction,test_y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"85893f2f-c831-e819-6724-f107ebf2833d"},"outputs":[],"source":"# so from this discussion we got multi colinearty effecting our SVM part a lot \n# but its not affecting so much randomforest because for random forest we dont need to make so much effort for our analysis part\n# now lets do with the 3rd part of data which is worst\n# first start with all features_worst"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"33c5c58a-31af-6309-1c6c-dd2280c0b112"},"outputs":[],"source":"prediction_var = features_worst"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5a92132-7ec3-8901-e52e-b42973ae34f6"},"outputs":[],"source":"train_X= train[prediction_var]\ntrain_y= train.diagnosis\ntest_X = test[prediction_var]\ntest_y = test.diagnosis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f2fb8a5-176f-588f-86b9-676b4d94493c"},"outputs":[],"source":"model = svm.SVC()\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nmetrics.accuracy_score(prediction,test_y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e0aa84e-24ea-496a-81ac-ddc145e2c3cd"},"outputs":[],"source":"# but same problem With SVM, very much less accuray I think we have to tune its parameter\n# that i will do later in intermidate part\n#now we can get the important features from random forest now run Random Forest for it "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"942c21fa-86f8-eb02-3dcc-e71c3bdaa1c6"},"outputs":[],"source":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_y)\nprediction = model.predict(test_X)\nmetrics.accuracy_score(prediction,test_y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6fdbaa7b-2d17-d7df-91ac-d0b2d40ee29d"},"outputs":[],"source":"# the accuracy for RandomForest invcrease it means the value are more catogrical in Worst part\n#lets get the important features\nfeatimp = pd.Series(model.feature_importances_, index=prediction_var).sort_values(ascending=False)\nprint(featimp) # this is the property of Random Forest classifier that it provide us the importance \n# of the features used"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b123094f-0ec6-a780-d247-e455e814b475"},"outputs":[],"source":"# same parameter but with great importance and here it seamed the only conacve points_worst is making \n# very important so it may be bias lets check only for top 5 important features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"152b0a6f-b34b-adc4-8c71-f9e46143e635"},"outputs":[],"source":"prediction_var = ['concave points_worst','radius_worst','area_worst','perimeter_worst','concavity_worst'] "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0157475-6e2b-0462-9898-5a443afb9d8c"},"outputs":[],"source":"train_X= train[prediction_var]\ntrain_y= train.diagnosis\ntest_X = test[prediction_var]\ntest_y = test.diagnosis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e9d0dcd-f06d-c281-1152-691a99754bc1"},"outputs":[],"source":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_y)\nprediction = model.predict(test_X)\nmetrics.accuracy_score(prediction,test_y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b06553a2-25f9-ece8-00ef-cf139d887d04"},"outputs":[],"source":"#check for SVM\nmodel = svm.SVC()\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nmetrics.accuracy_score(prediction,test_y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df8ac8b7-5ff4-ab3e-f1c2-f678af857730"},"outputs":[],"source":"# now I think for simplicity the Randomforest will be better for prediction"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a701594e-ec58-29f2-079f-285a01c686b2"},"outputs":[],"source":"# Now explore a little bit more\n# now from features_mean i will try to find the variable which can be use for classify\n# so lets plot a scatter plot for identify those variable who have a separable boundary between two class\n#of cancer"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a10b6b41-e528-2eb0-5f22-086af611e4fe"},"outputs":[],"source":"# Lets start with the data analysis for features_mean\n# Just try to understand which features can be used for prediction\n# I will plot scatter plot for the all features_mean for both of diagnosis Category\n# and from it we will find which are easily can used for differenciate between two category"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"834d5543-115d-55f9-0289-5709ed068e7d"},"outputs":[],"source":"color_function = {0: \"blue\", 1: \"red\"} # Here Red color will be 1 which means M and blue foo 0 means B\ncolors = data[\"diagnosis\"].map(lambda x: color_function.get(x))# mapping the color fuction with diagnosis column\npd.scatter_matrix(data[features_mean], c=colors, alpha = 0.5, figsize = (15, 15)); # plotting scatter plot matrix"},{"cell_type":"markdown","metadata":{"_cell_guid":"78883101-b683-4a38-efdf-961db36f673a"},"source":"** Observation**\n\n** 1. Radius, area and perimeter have a strong linear relationship as expected\n     2 As graph shows the features like as texture_mean, smoothness_mean, symmetry_mean and fractal_dimension_mean can t be used for classify two category because both category are mixed there is no separable plane\n3. So we can remove them from our prediction_var**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7fb4459c-b7c2-8dda-ff20-29757ca229fe"},"outputs":[],"source":"# So predicton features will be \nfeatures_mean"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a799df8-71c3-590a-f05e-2bbb4e8afa99"},"outputs":[],"source":"# So predicton features will be \npredictor_var = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concave points_mean']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"128449c4-c45b-edd5-1978-452f0577477d"},"outputs":[],"source":"# Now with these variable we will try to explore a liitle bit we will move to how to use cross validiation\n# for a detail on cross validation use this link https://www.analyticsvidhya.com/blog/2015/11/improve-model-performance-cross-validation-in-python-r/"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ab6c00c-e045-3ed7-d8fa-cb59629ed18a"},"outputs":[],"source":"def model(model,data,prediction,outcome):\n    # This function will be used for to check accuracy of different model\n    # model is the m\n    kf = KFold(data.shape[0], n_folds=10) # if you have refer the link then you must understand what is n_folds\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa211445-a1b2-9521-3502-24d39562ddda"},"outputs":[],"source":"prediction_var = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concave points_mean']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58c24a07-d0c2-005d-2049-3b07af37d36b"},"outputs":[],"source":"# so those features who are capable of classify classe will be more useful"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44635f70-d0a3-c8b5-4b5e-6ad8be28a6b3"},"outputs":[],"source":"# so in this part i am going to explain about only some concept of machine learnig \n# here I will also compare the accuracy of different models\n# I will First use cross validation with different model\n# then I will explain about how to to tune the parameter of models using gridSearchCV "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8b5ac97-0cfc-32fb-9fda-81e6b01bac57"},"outputs":[],"source":"# As we are going to use many models lets make a function\n# Which we can use with different models\ndef classification_model(model,data,prediction_input,output):\n    # here the model means the model \n    # data is used for the data \n    #prediction_input means the inputs used for prediction\n    # output mean the value which are to be predicted\n    # here we will try to find out the Accuarcy of model by using same data for fiiting and \n    #comparison for same data\n    #Fit the model:\n    model.fit(data[prediction_input],data[output]) #Here we fit the model using training set\n  \n    #Make predictions on training set:\n    predictions = model.predict(data[prediction_input])\n  \n    #Print accuracy\n    # now checkin accuracy for same data\n    accuracy = metrics.accuracy_score(predictions,data[output])\n    print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n \n    \n    kf = KFold(data.shape[0], n_folds=5)\n    # About cross validitaion please follow this link\n    #https://www.analyticsvidhya.com/blog/2015/11/improve-model-performance-cross-validation-in-python-r/\n    #let me explain a little bit data.shape[0] means number of rows in data\n    #n_folds is for number of folds\n    error = []\n    for train, test in kf:\n        # as the data is divided into train and test using KFold\n        # now as explained above we have fit many models \n        # so here also we are going to fit model\n        #in the cross validation the data in train and test will change for evry iteration\n        train_X = (data[prediction_input].iloc[train,:])# in this iloc is used for index of trainig data\n        # here iloc[train,:] means all row in train in kf amd the all columns\n        train_y = data[output].iloc[train]# here is only column so it repersenting only row in train\n        # Training the algorithm using the predictors and target.\n        model.fit(train_X, train_y)\n    \n        # now do this for test data also\n        test_X=data[prediction_input].iloc[test,:]\n        test_y=data[output].iloc[test]\n        error.append(model.score(test_X,test_y))\n        # printing the score \n        print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))\n    \n    \n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b9362b9-2514-685f-6830-d390cf729ac8"},"outputs":[],"source":"# Now from Here start using different model"},{"cell_type":"markdown","metadata":{"_cell_guid":"a989f695-6939-743c-c3dd-6f6203ad7f02"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8038059-da27-77a5-a67a-2a3936149919"},"outputs":[],"source":"model = DecisionTreeClassifier()\nprediction_var = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concave points_mean']\noutcome_var= \"diagnosis\"\nclassification_model(model,data,prediction_var,outcome_var)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"2e6a3fcb-7001-a7f3-c662-325706f59db6"},"source":"**observation\n\n 1. Accuracy is 100 % means over fitting \n 2. but cross validation scores are not good\n 3 so accuracy cant be considered only factor here\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff54139d-7f93-6180-d3cf-ca8f6a9c1c2f"},"outputs":[],"source":"# now move to svm"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"95e14b94-d970-318a-c94f-89bb8d253d78"},"outputs":[],"source":"model = svm.SVC()\n\nclassification_model(model,data,prediction_var,outcome_var)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61fb1252-dfd0-6cc5-7085-af14ea1acb58"},"outputs":[],"source":"# I am facing problem with SVM dont know why?\n#lets leave that we will try to do it later "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"593073d9-7f25-06f4-2887-c937e0212da8"},"outputs":[],"source":"model = KNeighborsClassifier()\nclassification_model(model,data,prediction_var,outcome_var)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a0bc621f-8cd3-62aa-0058-79cad0313e94"},"outputs":[],"source":"# same here cross validation scores are not good\n# now move to RandomForestclassifier\nmodel = RandomForestClassifier(n_estimators=100)\nclassification_model(model,data,prediction_var,outcome_var)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be1ef210-09d6-1e8e-1972-4b7b7efda883"},"outputs":[],"source":"# cross validation score are also not bed\n# so Random forest is good\n# lets try with logistic regression\nmodel=LogisticRegression()\nclassification_model(model,data,prediction_var,outcome_var)"},{"cell_type":"markdown","metadata":{"_cell_guid":"42918503-55a6-b2ac-256a-81b8adfd67ad"},"source":"** It was a detailed comparison of machine learning models \n\n 1. In next segment I will try to  explain the tuning of parameter for different models\n 2. Then using those parameter we will try to forecast**"},{"cell_type":"markdown","metadata":{"_cell_guid":"a67b38c7-d64d-04e1-7d29-ec2476102e41"},"source":"** Tuning Parameters  using grid search CV**"},{"cell_type":"markdown","metadata":{"_cell_guid":"ed127fda-4f0b-5e94-9a9e-c5e56701050b"},"source":" *Lets Start with decision tree classifier\nTuning the parameters means using the best parameter for predict \n there are many parameters need to model a Machine learning Algorithm\n for decision tree classifier refer this link [Link](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c9a0f2cf-8ac9-e434-d300-54f898f74d04"},"outputs":[],"source":"data_X= data[prediction_var]\ndata_y= data[\"diagnosis\"]\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e2f69f37-7dcd-7afe-1307-b0b16af59c2a"},"outputs":[],"source":"# lets Make a function for Grid Search CV\ndef Classification_model_gridsearchCV(model,param_grid,data_X,data_y):\n    clf = GridSearchCV(model,param_grid,cv=10,scoring=\"accuracy\")\n    # this is how we use grid serch CV we are giving our model\n    # the we gave parameters those we want to tune\n    # Cv is for cross validation\n    # scoring means to score the classifier\n    \n    clf.fit(train_X,train_y)\n    print(\"The best parameter found on development set is :\")\n    # this will gie us our best parameter to use\n    print(clf.best_params_)\n    print(\"the bset estimator is \")\n    print(clf.best_estimator_)\n    print(\"The best score is \")\n    # this is the best score that we can achieve using these parameters#\n    print(clf.best_score_)\n    \n    \n    \n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fff56e83-608f-bee1-8cdf-d75b07cab36a"},"outputs":[],"source":"# Here we have to take parameters that are used for Decison tree Classifier\n# you will understand these terms once you follow the link above\nparam_grid = {'max_features': ['auto', 'sqrt', 'log2'],\n              'min_samples_split': [2,3,4,5,6,7,8,9,10], \n              'min_samples_leaf':[2,3,4,5,6,7,8,9,10] }\n# here our gridasearchCV will take all combinations of these parameter and apply it to model \n# and then it will find the best parameter for model\nmodel= DecisionTreeClassifier()\nClassification_model_gridsearchCV(model,param_grid,data_X,data_y)\n# call our function"},{"cell_type":"markdown","metadata":{"_cell_guid":"67fb10fa-b386-2c7c-6c3c-d23119ab0501"},"source":"*observation*\n\n 1. the score increase to 95 % \n 2. Seems to be good\n 3. Lets do with KNN\n 4. link for KNN  [Link](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n 5. if you are a beginner please follow the link it will be very much useful"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f5238051-7292-5a2a-f255-cd68c9797125"},"outputs":[],"source":"model = KNeighborsClassifier()\n\nk_range = list(range(1, 30))\nleaf_size = list(range(1,30))\nweight_options = ['uniform', 'distance']\nparam_grid = {'n_neighbors': k_range, 'leaf_size': leaf_size, 'weights': weight_options}\nClassification_model_gridsearchCV(model,param_grid,data_X,data_y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d7918a00-0d11-f883-98a5-cab998584c03"},"source":" 1. Try with SVM\n 2. [link](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0c2e19e8-bd5f-45d3-3134-845c0c4ad11f"},"outputs":[],"source":"model=svm.SVC()\nparam_grid = [\n              {'C': [1, 10, 100, 1000], \n               'kernel': ['linear']\n              },\n              {'C': [1, 10, 100, 1000], \n               'gamma': [0.001, 0.0001], \n               'kernel': ['rbf']\n              },\n ]\nClassification_model_gridsearchCV(model,param_grid,data_X,data_y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a3e8c0c9-1a3a-047e-0fa3-bdfff182caaa"},"source":"*observation*\n\n 1. The SVM is working fine with good parameter it shows us what is the use of running of parameters\n 2. In the first by using default  I was getting only 70 % accuracy\n 3. But with tuned parameter it is 95 %"},{"cell_type":"markdown","metadata":{"_cell_guid":"e1b37871-b764-9553-b059-108aa553d43b"},"source":" 1. Same we can do for Random Forest classifier\n 2. I will not do that \n 3. if someone is using this as reference please do for Random Forest Classifier also"},{"cell_type":"markdown","metadata":{"_cell_guid":"bc71536e-4f9f-0f5c-9363-3f27b643235f"},"source":" 1. The main objective of this notebook is to provide a hang on the the Machine learning methods\n 2. I think it will be very useful for beginner because in this I have provided every thing that a beginner needs  most\n 3. When I  was a beginner I face many problems finding these all so i tried to make everything available here \n 4. Thanks "}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}