{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Gender Classification\n\n> Gender Classification by Voice and Speech Analysis"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# importing necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/voicegender/voice.csv\")\ndf = pd.DataFrame(data)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# showing column wise %ge of NaN values they contains \n\nfor i in df.columns:\n  print(i,\"\\t-\\t\", df[i].isna().mean()*100)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Since data does'nt contain any null values, we can move further"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='label', data = df) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Here graph sows that data is equally balanced between both the classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since our last column(label: male/female) is categorical let's first convert it into numerical\n\nfrom sklearn.preprocessing import LabelEncoder\n\nenc = LabelEncoder()\ndf['label'] = enc.fit_transform(df['label'].astype('str'))\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Now for visualising each class, here is plot of any any 1 random sample of voices of each class."},{"metadata":{"trusted":true},"cell_type":"code","source":"index = 0\nlabel = ['Female', 'Male']\n\nfig, ax = plt.subplots(nrows = 2, ncols = 1, figsize=(20,7))\n\nfor i in range(2):\n  ax[i].plot(df[df['label'] == i].sample(1).iloc[0,:20])\n  ax[i].set_title(label[i],)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This graphs clearly shows how __Kurt__ is genrally high for males as compared to females whereas __Maxdom__ and __Dfrange__ are quite low. These attributes are essential for classfication of data\n\n\n\n\n> Let's further see how other attributes are related to each other using correlation matrix and pairplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"cormap = df.corr()\nfig, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(cormap, annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple Function to get the name of top most corelated attributes\n\ndef get_corelated_col(cor_dat, threshold): \n  # Cor_data to be column along which corelation to be measured \n  #Threshold be the value above wich of corelation to considered\n  feature=[]\n  value=[]\n\n  for i ,index in enumerate(cor_dat.index):\n    if abs(cor_dat[index]) > threshold:\n      feature.append(index)\n      value.append(cor_dat[index])\n\n  df = pd.DataFrame(data = value, index = feature, columns=['corr value'])\n  return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_corelated_values = get_corelated_col(cormap['label'], 0.30)\ntop_corelated_values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Here I'm plotting pairplot of attributes shown in heatmap most related to attribute __label__."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df[top_corelated_values.index], hue='label')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Here diagonal of pairplot depicts how both the classes are correlated with these attributes so nicely , thus they are perfect for using in classification.\n\n> Other plots also shows nice correlation with each other and how both classes are easily saparable over these columns\n\n> So for classification I'm going to use there attributes plus  Kurt, Maxdom and Dfrange since they are also seems essential flrom the line plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saparating features and labels \n\nX = df[list(top_corelated_values.index[:-1]) + [ 'kurt', 'maxdom', 'dfrange']]\nY = df['label']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Standardisation\n\n> We arehere standardising data so in order to make the mean of data . Doing standaradisation is appreciated before applying SVM (classifier used in this notebook) as Standardization gives all features the same influence on the distance metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the data to be between -1 and 1\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now lets split data in test train pairs\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Support Vector Machine\n\n> Here I'm using SVM classifier on this dataset. I'm also gonna tune it's hyperparameters in order to increase the accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialising the SVM classifier \n\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\n\nSVC().get_params()   # Hyperparamters of SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's make grid for tunning the hyperparametes\n\nfrom sklearn.model_selection import GridSearchCV\n\nC = np.arange(0.1, 2, 0.1)\nkernel = ['linear', 'rbf', 'poly']\ngamma = [0.01,0.02,0.03,0.04,0.05]\n\n\ngrid = {'C': C,\n        'kernel': kernel,\n        'gamma': gamma }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting SVM classifiers with hyperparameter tunned using grid search(cross validation with 10 folds)\n\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\n\nsvc_grid = GridSearchCV(estimator = SVC(), param_grid = grid, cv = 10)\nsvc_grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_grid.best_params_  # Best pairs of hyperparameters provided by grid search","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction\n\ny_pred = svc_grid.best_estimator_.predict(X_test)\n\npred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\npred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nmat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(mat, annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\n# Generate the roc curve using scikit-learn.\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\nplt.plot(fpr, tpr)\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.show()\n\n# Measure the area under the curve.  The closer to 1, the \"better\" the predictions.\nprint(\"AUC of the predictions: {0}\".format(metrics.auc(fpr, tpr)))\n\n# Measure the Accuracy Score\nprint(\"Accuracy score of the predictions: {0}\".format(metrics.accuracy_score(y_pred, y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting Decision surface of Radial basis function(rbf) SVC\n\n> Here is a plot of dicision boundary of of SVM usinf __rbf__ kernel. Here two attributes of our dataset __Meanfun__ and __Centroid__ is used for plotting an 2D surface. "},{"metadata":{"trusted":true},"cell_type":"code","source":"x_min, x_max = X['meanfun'].min() - 1, X['meanfun'].max() + 1\ny_min, y_max = X['centroid'].min() - 1, X['centroid'].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                     np.arange(y_min, y_max, 0.02))\n\nsvc_plot = SVC(C = 1.8000000000000003, gamma = 0.05, kernel = 'rbf')\nsvc_plot.fit(X[['meanfun', 'centroid']], Y)\nZ = svc_plot.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, cmap = plt.cm.coolwarm, alpha=0.8)\nplt.scatter(X['meanfun'], X['centroid'], c = Y, cmap = plt.cm.coolwarm)\nplt.xlabel('Meanfun')\nplt.ylabel('Centroid')\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks(())\nplt.yticks(())\nplt.title(\"Support Vector Machine (Classifier)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}