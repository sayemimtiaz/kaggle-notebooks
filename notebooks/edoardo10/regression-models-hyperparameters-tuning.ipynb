{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Housing Dataset: regression models"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import KFold, GridSearchCV, StratifiedKFold\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## About the data\n\nThe data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data.\n\nWe can see that all variables are numerical, continuous and on different scales, except for the ocean_proximity variable which is a string and so catergorical."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/california-housing-prices/housing.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data types and null values"},{"metadata":{},"cell_type":"markdown","source":"Only total_bedrooms has null values, we could simply drop these rows or assign them mean or median value, but in order to be more precise, I will fill these values using a regression model."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the ocean_proximity variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.ocean_proximity.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's transform this column into five dummy variables, then we will drop the ISLAND column, because it will always be 0, except for 5 cases on 20640."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df.drop('ocean_proximity', axis=1), pd.get_dummies(df.ocean_proximity)], axis=1) #getting columns for dummy variables\ndf = df.drop('ISLAND', axis=1) #dropping the column having five times 1 and otherwise 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I decided to handle null values in total_bedroom column by predicting them using the other features. We can take the rows where total_bedrooms is null, in order to predict this value."},{"metadata":{"trusted":true},"cell_type":"code","source":"to_predict = df[df.total_bedrooms.isnull()].drop(['total_bedrooms'], axis=1) #all rows where total_bedroom is null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df.dropna() #original dataset without null values\n\nscaler = StandardScaler() #scaler\nX = train.drop('total_bedrooms', axis=1) #dropping target column\nX = scaler.fit_transform(X) #scaling X\ny = train.total_bedrooms #target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=44) #splitting in training and test set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trying three algorithms, with default parameters, Linear Regression, Gradient Boosting Regression and Random Forest Regression, we can see that Gradient Boosting has a lower Root Mean Squared Error and a better R^2 score, so we will predict the missing total_bedrooms values through this model."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [('Linear Regresion', LinearRegression()),\n          ('Gradient Boosting', GradientBoostingRegressor()),\n          ('Random Forest', RandomForestRegressor())]\n\n\n\nfor model in models: #for loop through the three models\n    reg = model[1]  #initialize the model object\n    reg.fit(X_train,y_train)  #fitting the training data\n    pred = reg.predict(X_test)  #predict target\n    print(model[0])\n    print('R2: ',r2_score(y_test, pred))  #check r2 score\n    print('RMSE: ', np.sqrt(mean_squared_error(y_test, pred)))  #check root mean squared error\n    print('-'*30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can impute the predicted values to fill the null ones"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = GradientBoostingRegressor() #best model\nreg.fit(X, y) #fitting to the data\nfilled_values = reg.predict(to_predict) #getting predicted data to fill null values\n\nto_predict['total_bedrooms'] = filled_values  #filling the null values \ndf = pd.concat([df.dropna(),to_predict]).reindex(df.index)  #adding the rows in which total_bedrooms was null and reset indexes\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can go ahead in order to find the best model for predicting median_house_value, so we will try: Linear Regression, KNN Regressor, Support Vector Regressor, Decision Tree and Random Forest.\n\nI will evaluate the model using these metrics:\n- R2 score\n- Root Mean Squared Error\n- Time for computation"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('median_house_value', axis=1)\ny = df.median_house_value\n\nscaler = StandardScaler()  #scaler object\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=22)  #splitting into training and test\nkfold = KFold(n_splits=10, random_state=99, shuffle=True)   #kfold cross validation object with 10 splits\n\nX_train = scaler.fit_transform(X_train)  #scaling training set\nX_test = scaler.transform(X_test)  #scaling test set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = LinearRegression() \nstart = datetime.now()\nreg.fit(X_train, y_train)\npred = reg.predict(X_test)\nstop = datetime.now()\ndelta = stop - start\n\nprint('Linear Regression\\n')\nr2 = r2_score(y_test, pred)\nprint('R2: ', r2)\nerr = np.sqrt(mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error: ', err)\nseconds = delta.seconds + delta.microseconds/1E6\nprint('Time to compute: ', seconds, 'seconds')\n\nlinear_reg = ('Linear Regression', r2, err, seconds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-Nearest Neighbors Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'n_neighbors': [9],  #'n_neighbors': [3,4,5,6,7,8,9,10,11,12], Number of neighbor points to consider for prediction\n    'weights': ['distance'],  #'weights': ['uniform', 'distance'], weight function used in prediction\n    'p': [1]  #'p': [1,2] # p=1 compute manhattan distance, p=2 compute euclidean distance\n    }\n\nknn = KNeighborsRegressor()\nrs = GridSearchCV(estimator=knn, param_grid=params, cv=10, n_jobs=-1, scoring='neg_mean_squared_error')\nrs.fit(X_train, y_train)\nprint(rs.best_estimator_)\nknn = rs.best_estimator_\nstart = datetime.now()\nknn.fit(X_train, y_train)\npred = knn.predict(X_test)\nstop = datetime.now()\ndelta = stop - start\n\nprint('-'*30)\nr2 = r2_score(y_test, pred)\nprint('R2: ', r2)\nerr = np.sqrt(mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error: ', err)\nseconds = delta.seconds + delta.microseconds/1E6\nprint('Time to compute: ', seconds, 'seconds')\n\nknn_reg = ('KNN', r2, err, seconds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Regressor\n\nDue to high computational time, I decided to run the model on a balanced subsample of the dataset in order to find the best hyperparameters. I just commented this for running the notebook fastly. The chosen hyperparameters are the ones obtaining with the commented lines of code."},{"metadata":{"trusted":true},"cell_type":"code","source":"#skf = StratifiedKFold(n_splits=3,shuffle=True, random_state=22)  #Stratified k-fold object\n#\n#train_index, test_index = next(skf.split(X, y))  #obtaining indexes of 1/3 of original dataset rows##\n#\n#X_fold = X.iloc[test_index]\n#y_fold = y.iloc[test_index]\n#\n#scaler_train_fold = StandardScaler()\n#scaler_test_fold = StandardScaler()\n#\n#X_train_fold, X_test_fold, y_train_fold, y_test_fold = train_test_split(X_fold, y_fold, test_size=0.3, random_state=22)\n#\n#X_train_fold = scaler_train_fold.fit_transform(X_train_fold)\n#X_train_fold = scaler_test_fold.fit_transform(X_test_fold)\n#\n#params = {'C': [100],         #'C': [0.1,1,10,100]\n#         'gamma': [1],     #'gamma': [0.01,0.1,1,10]\n#         'kernel': ['linear'],   \n#        }\n#\n#svr = SVR()\n#rs = GridSearchCV(estimator=svr, param_grid=params, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n#rs.fit(X_train_fold, y_train_fold)\n#print(rs.best_estimator_)\n#svr = rs.best_estimator_\n\nsvr = SVR(C=100, gamma=1, kernel='linear')\nstart = datetime.now()\nsvr.fit(X_train, y_train)\npred = svr.predict(X_test)\nstop = datetime.now()\ndelta = stop - start\n\nprint('-'*30)\nr2 = r2_score(y_test, pred)\nprint('R2: ', r2)\nerr = np.sqrt(mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error: ', err)\nseconds = delta.seconds + delta.microseconds/1E6\nprint('Time to compute: ', seconds, 'seconds')\n\nsupport_vector_reg = ('SVR', r2, err, seconds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Regressor\n\nTree based algorithms don't need scaling, so we will get training e test set without scaling them"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=22) #getting not scaled data\n\nparams = {'max_depth': [7], #np.linspace(1, 10, 10) The maximum depth of the tree\n          'max_features': ['auto', 'sqrt'], #The number of features to consider when looking for the best split\n          'min_samples_leaf': [7], #[3,4,5,6,7,8] The minimum number of samples required to be at a leaf node\n          'min_samples_split': [0.1], #np.linspace(0.1, 1.0, 10) The minimum number of samples required to split an internal node\n          'criterion': ['mse'] #The function to measure the quality of a split\n         }\n\ntree = DecisionTreeRegressor()\nrs = GridSearchCV(estimator=tree, param_grid=params, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\nrs.fit(X_train, y_train)\nprint(rs.best_estimator_)\n\ntree = rs.best_estimator_\nstart = datetime.now()\ntree.fit(X_train, y_train)\npred = tree.predict(X_test)\nstop = datetime.now()\ndelta = stop - start\n\nprint('-'*30)\nr2 = r2_score(y_test, pred)\nprint('R2: ', r2)\nerr = np.sqrt(mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error: ', err)\nseconds = delta.seconds + delta.microseconds/1E6\nprint('Time to compute: ', seconds, 'seconds')\n\ndecision_tree = ('Tree', r2, err, seconds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Regressor\n\nJust as done for SVR, due to high computation time I decided to run the model on a balanced subsample of the dataset in order to find the best hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=3,shuffle=True, random_state=22)\ntrain_index, test_index = next(skf.split(X, y))  #obtaining indexes of 1/3 of original dataset rows##\n\nX_fold = X.iloc[test_index]\ny_fold = y.iloc[test_index]\n\nX_train_fold, X_test_fold, y_train_fold, y_test_fold = train_test_split(X_fold, y_fold, test_size=0.3, random_state=22)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n\nparams = { \n          'n_estimators': [1000],\n          'max_depth': [8],                  #'max_depth': [4,5,6,7,8,9]\n          'max_features': ['auto', 'sqrt'],  #'max_features': ['auto', 'sqrt']\n          'min_samples_leaf': [4],           #'min_samples_leaf': [2,3,4,5,6,7]\n          'min_samples_split' : [0.01],      #'min_samples_split' : [0.01]\n         }\n\nrf = RandomForestRegressor()\nrs = GridSearchCV(estimator=rf, param_grid=params, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\nrs.fit(X_train_fold, y_train_fold)\nprint(rs.best_estimator_)\n\nrf = rs.best_estimator_\nstart = datetime.now()\nrf.fit(X_train, y_train)\npred = rf.predict(X_test)\nstop = datetime.now()\ndelta = stop - start\n\nprint('-'*30)\nr2 = r2_score(y_test, pred)\nprint('R2: ', r2)\nerr = np.sqrt(mean_squared_error(y_test, pred))\nprint('Root Mean Squared Error: ', err)\nseconds = delta.seconds + delta.microseconds/1E6\nprint('Time to compute: ', seconds, 'seconds')\n\nrandom_forest = ('Random Forest', r2, err, seconds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results: which is the best model?"},{"metadata":{},"cell_type":"markdown","source":"As can be seen from the table below, K-nearest neighbors resulted to be the best model for this dataset because of:\n- highest R^2 score\n- lowest root mean squared error\n\nConsidering computation time, we can see that K-nearest neighbors algorithm is 15 times slower than Decision tree and 50 times slower than a Linear regression, but the performances in terms of R2 and error are considerably better than these two models, maybe in situations with really high dimensional data, simpler models can be preferred."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_results = pd.DataFrame([linear_reg, knn_reg, support_vector_reg, decision_tree, random_forest], columns=['model', 'R2','RMSE','comp_time'])\ndf_results.sort_values('R2',ascending=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}