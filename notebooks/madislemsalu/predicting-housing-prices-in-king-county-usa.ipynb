{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","pygments_lexer":"ipython3","file_extension":".py","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":2,"cells":[{"metadata":{"_uuid":"fff956d585839103c50d9f51271c847f82996c15","_cell_guid":"b84ebaa7-5b4a-4735-ac03-343a6cc5a1ea"},"cell_type":"markdown","source":"### Hello , this is my first kernel. \n### I will be exploring the housing sale prices in King County, USA between the time period May 2014 - May 2015. \n#### Firstly, I will go through a thorough data exploration to identify most important features and to explore the intercorrelation between features. After that I apply data normalization between varialbes and conduct feature engineering, Finally, I will be applying different machine learning algorithms and evaluating their respective success to a cross-validated splitted train-test set."},{"metadata":{"_uuid":"a62146aa79239c742ebbb31f6dc6fc7f2c97f42c","_cell_guid":"eed81550-0f76-4f53-bca7-81f81fb1e815","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#Importing fundamental data exploration libaries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\n\n%config InlineBackend.figure_format = 'png' #set 'png' here when working on notebook\n%matplotlib inline\n\ndf_train = pd.read_csv(\"../input/kc_house_data.csv\")\ndf_train.rename(columns ={'price': 'SalePrice'}, inplace =True)\n\ndf_train.head()"},{"metadata":{"_uuid":"f3a0becb9ce7bbd5413f104d57cd89c0794284be","_cell_guid":"bda9670c-c484-4e80-81fd-a50637441371","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#checking the columns in the dataset\ndf_train.columns"},{"metadata":{"_uuid":"15435527e41bb8bce4e33caf0c7fafd64e065e18","_cell_guid":"2959f535-b060-4b10-97ac-950eafc4134d"},"cell_type":"markdown","source":"#### At a first glance, the column  date can be removed as it serves unsignificant value (timeframe is one year).  The features seem to be pre-selected as  important influencers for a sale price of a house. "},{"metadata":{"_uuid":"cdbd98386708e55f8e20137a584ad8b9aca64121","_cell_guid":"5da4012d-620d-4f81-8aac-ca9ae27759cb","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"df_train['SalePrice'].describe()\ndf_train['SalePrice']=df_train['SalePrice']"},{"metadata":{"_uuid":"21ff7fd7077067208537f717a062a5da374101f6","_cell_guid":"aa9d109b-64ce-40c9-ae5f-ecd3b8a719b8","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"\n#histogram\nsns.distplot(df_train['SalePrice'], bins=50, kde=False);\n\n\n"},{"metadata":{"_uuid":"09f4f1c0ce5a94fe3f77521276ce93a854fd7aab","_cell_guid":"a89932d1-a344-4249-af4d-702b77ff8747"},"cell_type":"markdown","source":"Given from the histogram: The saleprice has appreciable positive skewness,\ndeviates from the normal distribution and\nshows peakedness.\nLet's take a look at the skewness and kurtosis in numbers:"},{"metadata":{"_uuid":"137b51cafc92215342035857c80a73ef8432be65","_cell_guid":"dae9cfcc-2abd-499e-8d90-10d34ce874e5","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())"},{"metadata":{"_uuid":"3e962893faf8a69e74dbc7376a7de350d1fe8823","_cell_guid":"5d21a465-912a-4cbf-a6a7-074dbcf7c8b3"},"cell_type":"markdown","source":"This is quite significant. At the data standardisation section, we will fix this. "},{"metadata":{"_uuid":"3b6fd366702d3fdec7cc59907498598013796b8c","_cell_guid":"4456da0b-598d-46a2-9338-653e23960acf"},"cell_type":"markdown","source":"# 2. Feature exploration \n\n##### In this section I will be investigating different features by plotting them to determine the relationship to SalePrice. "},{"metadata":{"_uuid":"6ce8c98065287d625c47e46ddf709463c360910f","_cell_guid":"22ded60a-255f-444d-bc05-e70acbbcafcd","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"var = 'sqft_living15'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(3,8000000));"},{"metadata":{"_uuid":"bd06114efb018cc05e86e8d054bd23cebde7bcd6","_cell_guid":"642acb2f-75c1-41d8-858e-8f785c64ddee"},"cell_type":"markdown","source":"There's clearly a linear relationship with a significant portion of outliers. "},{"metadata":{"_uuid":"3cf63d4596a0e8a777fc2d529528d51a5a0a953d","_cell_guid":"694c8656-551c-4e37-834d-9a8bdedc778d","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"var = 'bedrooms'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(14, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=3500000);"},{"metadata":{"_uuid":"e3072f20295b099ca05ce05f2f14a9c03bfbf9c7","_cell_guid":"c2eb4877-6232-43e5-ada2-7d4ce5b6eb55","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"var = 'grade'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=3500000);"},{"metadata":{"_uuid":"aec17e9aa744a02cfa943cbd9dd9441b6e7724c1","_cell_guid":"572bd301-12e2-4456-9100-8da35c82bd9d","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"var = 'bathrooms'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(20, 20))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=3500000);"},{"metadata":{"_uuid":"97360aa92061b01e190a43a40340f80efb2b438c","_cell_guid":"0011962d-9895-4a45-b149-18e3301ee0ba"},"cell_type":"markdown","source":"#### Clearly, as the features  \"bathrooms\", \"grade\", \"bedrooms\" increase, so does the SalePrice. This is most evident in case of the features bathrooms and grade. "},{"metadata":{"_uuid":"cb4ea82dc1c12cf97f08f97d0b0fee69520a3251","_cell_guid":"8e8c1ac2-1bb0-43fa-a453-7bca993eda11","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"var = 'yr_built'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);"},{"metadata":{"_uuid":"fffc362eca4b57fc5a58958565ba0ae40e3eaa4f","_cell_guid":"f4b4c889-c1ef-405f-b182-7b72ae5a0a25"},"cell_type":"markdown","source":"#### Interestingly enough, one would expect a linear relationship with newer houses being significantly more expensive. However, this is not the case, as seen by the graph. Next let's explore intercolleration between features."},{"metadata":{"_uuid":"f4ab237094397f5cd5ddf730be36aecccdf5d59c","_cell_guid":"cc00ac89-795b-488a-8297-aefbc6f15d51","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);"},{"metadata":{"_uuid":"d83753b1a17864189450ad5fca777f506ccd681a","_cell_guid":"68feec81-fe49-4d8b-a77a-ae19c0351a2d"},"cell_type":"markdown","source":"#### There are some interesting correlation between variables - let's take a closer look:"},{"metadata":{"_uuid":"3714756571b4a3839e5a01d9408673ee2b99372d","_cell_guid":"31876d21-6146-477a-979d-a44f4bfc1862","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()"},{"metadata":{"_uuid":"e3fd1241e0f5edbd9c177a05c423b96a96f8cabd","_cell_guid":"dd005954-aadb-436a-bf14-9ac46d791dbd"},"cell_type":"markdown","source":"#### Okay, so the features: square foot living area, grade(amount of floors), square feet abouve the ground level and sqft_15 features displayed the highest correlation wih the price of the house. \n#### Moreover, there is a high correlation of sqft_living with e.g. number of bathrooms and grade. This is common sense, as the square feet increase, so does the number of floors and bathrooms. There is potential to implement clever feature engineering here. "},{"metadata":{"_uuid":"3c0bce657bc70555dc34e23634dbfa250b144440","_cell_guid":"8e4b7667-3b1b-4641-9cd1-ee5befe296b2","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'sqft_living', 'grade', 'sqft_above', 'view', 'bathrooms']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();\n"},{"metadata":{"_uuid":"90bd82b81afb0f35560969b33fde1b4580d702e0","_cell_guid":"8ffe2be4-4e8f-48f8-98ab-f53c009bb6d5"},"cell_type":"markdown","source":"#### These overall scatterplots confirm the findings of the heatmap.  There is myriad of linear correlation between sqft_living, sqft_above, bathrooms and grade. This yields an opportunity for to combine features. Moreover, what we learned that the above mentioned features have the biggest impact on sale price. One would also expect location to play a role, but as they are in latitude/longitude coordinates, it requires advanced data manipulation to take it into account. Finally, due to many linear relationships we can apply regression models. "},{"metadata":{"_uuid":"dd82d13432ec3980e1d9ed24d9959790c573f197","_cell_guid":"9e7f30b3-e52d-4f52-85e7-04fdb97d7d7e"},"cell_type":"markdown","source":"Let's check for missing data before we proceed any further:"},{"metadata":{"_uuid":"679e3bd088e50857d55e30dc6fb0d76e23e0bb84","_cell_guid":"bebb3c87-a6c8-400b-9fcd-3c7a123ac9c8","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)"},{"metadata":{"_uuid":"35c2cc782213ca5e1f7f523c7ec806a70edc3706","_cell_guid":"99b28e52-e24c-4170-9ade-540ce8e551dd"},"cell_type":"markdown","source":"#### This dataset is clean of missing data. What a miracle. "},{"metadata":{"_uuid":"5635dd2b3408f39da22d93f3c19e4b6d276c044c","_cell_guid":"26b535f0-81a8-41c8-8360-ba024773578c"},"cell_type":"markdown","source":"# Standardization of data"},{"metadata":{"_uuid":"2534164c448d86938e63d8f048bf973630cc0036","_cell_guid":"42b1a9f0-815e-4fc3-9728-f25f0e124dc4","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#standardizing data to mitigate skewdness and kurtosis\nfrom sklearn.preprocessing import StandardScaler\nsaleprice_scaled = StandardScaler().fit_transform(df_train['SalePrice'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)"},{"metadata":{"_uuid":"b5c23fbdf6d31480e7ed184cd4d45b915d3ba284","_cell_guid":"f913f5e6-d910-40b8-896a-bfacfc46f46c"},"cell_type":"markdown","source":"\n\n### Let's normalise the Saleprice and sqft_living feature"},{"metadata":{"_uuid":"238378095702f7647f8d218f68e6b7b6b7474b95","_cell_guid":"5d83df7a-dede-4362-8144-734866f2e43a","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"from scipy.stats import norm\nfrom scipy import stats\n#histogram and normal probability plot\nsns.distplot(df_train['SalePrice'], fit=norm, bins=50, kde=False);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)"},{"metadata":{"_uuid":"cdcd024247b804afbfd7e74f4dd391c590f1ce04","_cell_guid":"958f161c-7a47-48ac-b88c-6b86e78fd379"},"cell_type":"markdown","source":"#### We'll fix it by taking the log and fitting the plot to a normal distribution curve"},{"metadata":{"_uuid":"c8a9a3693b90e612f33608be04e355ca4d934001","_cell_guid":"2b917f42-9543-4b80-8103-d255a74f6ace","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#applying log transformation\ndf_train['SalePrice'] = np.log(df_train['SalePrice'])"},{"metadata":{"_uuid":"ada614a7e0dea5af891afc4a35ecd8bfdb33bc1b","_cell_guid":"a5a8b441-86f5-4b4d-9ffe-6f51ed0c3e10","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#transformed histogram and normal probability plot\nsns.distplot(df_train['SalePrice'], fit=norm,  bins=50, kde=False);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)"},{"metadata":{"_uuid":"5cf49bf001c71f0adb173e8f6c357e4c8c90b38a","_cell_guid":"ce7f15b9-0e86-49ee-85e8-b17c02a4827f"},"cell_type":"markdown","source":"Done! Now for sqft_living"},{"metadata":{"_uuid":"170c3141d83129203475ff7cf1ee2f793a20cae1","_cell_guid":"b90f6ddf-fe5f-49bb-8841-05878477d4da","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#histogram and normal probability plot\nsns.distplot(df_train['sqft_living'], fit=norm, bins=50, kde=False);\nfig = plt.figure()\nres = stats.probplot(df_train['sqft_living'], plot=plt)"},{"metadata":{"_uuid":"a01d31393a192ac043df7809adc59794faab58bd","_cell_guid":"37b69a36-2dfa-4943-89f6-bf3f9cf90447","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#data transformation\ndf_train['sqft_living'] = np.log(df_train['sqft_living'])"},{"metadata":{"_uuid":"d35a6538fac643c048c64ac7690e9b78e859d31c","_cell_guid":"345490d2-fea9-49c7-bbd0-5f2577befd20","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#transformed histogram and normal probability plot\nsns.distplot(df_train['sqft_living'], fit=norm, bins=50, kde=False);\nfig = plt.figure()\nres = stats.probplot(df_train['sqft_living'], plot=plt)"},{"metadata":{"_uuid":"ff0f1feb675a43c1b5725294c85315021ca1bdaa","_cell_guid":"394e88a7-c73f-4c20-bcf4-cc7f23e91c83","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"#scatter plot\nplt.scatter(df_train['sqft_living'], df_train['SalePrice']);\n"},{"metadata":{"_uuid":"e0be8978c178cc9f123202db59bc0ae638453456","_cell_guid":"1b60936c-7eca-437a-9bd0-03b751be58fe","collapsed":true},"cell_type":"markdown","source":"#### Now there is a nice linear relationship between the features "},{"metadata":{"_uuid":"4f4ad8ce4b13a9e325468209e6e0b3d30932f101","_cell_guid":"1877a830-2dfe-49b9-8505-d7387616337b"},"cell_type":"markdown","source":"## 4. Fitting Machine Learning Models"},{"metadata":{"_uuid":"d25184260910df3fd2bbcbfd03099c0e03eae2f6","_cell_guid":"abf9e32c-e20b-436b-91aa-b777f9e2e0b7","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"df_train.head()"},{"metadata":{"_uuid":"455af5133ad99d14d5711769986405c46bce7a60","_cell_guid":"6a7ed490-3f9c-4734-946f-c179350ba6b0","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"Y = df_train.SalePrice.values\n"},{"metadata":{"_uuid":"ac15baa5904c8751c4372f6fddf82f26a178720d","_cell_guid":"fd2c2a80-fa03-4a2e-8abf-ec5713b81e18","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"feature_cols = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n       'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n       'sqft_living15', 'sqft_lot15']\nX=df_train[feature_cols]"},{"metadata":{"_uuid":"ccf7ca4cf55578541ef7865c7a44555c1aecb479","_cell_guid":"cc462cb3-4ce0-48e9-bfe8-2febfae98b80","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X, Y, random_state=3)"},{"metadata":{"_uuid":"ab5540179a58b6907b448a08494970180df1f3bd","_cell_guid":"75292b50-d77b-4b29-8bbd-d1dcc961fd29","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(x_train, y_train)"},{"metadata":{"_uuid":"a602aab352d62979c8e43ddff04956a80fe485cb","_cell_guid":"e93b178a-5ae0-4aa5-8b8b-de8018f1c1bd","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"accuracy = regressor.score(x_test, y_test)\n\"Accuracy: {}%\".format(int(round(accuracy * 100)))"},{"metadata":{"_uuid":"a89eaefc3b2dabfe84f2ece8384dfbc4673c5423","_cell_guid":"bed261e6-7029-4aa0-bf97-7222f2299221"},"cell_type":"markdown","source":"#### So 77% accuracy with simple linear regression. Let's try more advanced algorithms. \n## Elastic Net"},{"metadata":{"_uuid":"baf0c04ae9239091ce6a1f118483395b7540e867","_cell_guid":"053e3d77-325c-454d-a5ed-0cd47063d8ab","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"from sklearn import ensemble, tree, linear_model\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.utils import shuffle\n"},{"metadata":{"_uuid":"74cc3da8eec0c2b54228696d5926e977918e2842","_cell_guid":"64be7c06-8f61-4138-9f69-ae25494c5661","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# For accurate scoring\ndef get_score(prediction, lables):    \n    print('R2: {}'.format(r2_score(prediction, lables)))\n    print('RMSE: {}'.format(np.sqrt(mean_squared_error(prediction, lables))))"},{"metadata":{"_uuid":"e92533be7ad5c2f6612f48d75259221505ece3d1","_cell_guid":"cf818299-943f-4411-98e0-c5d61bccdfb6","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"def train_test(estimator, x_trn, x_tst, y_trn, y_tst):\n    prediction_train = estimator.predict(x_trn)\n    # Printing estimator\n    print(estimator)\n    # Printing train scores\n    get_score(prediction_train, y_trn)\n    prediction_test = estimator.predict(x_tst)\n    # Printing test scores\n    print(\"Test\")\n    get_score(prediction_test, y_tst)"},{"metadata":{"_uuid":"8f17ebaccb0767cd6d1bdbb12a1b4d88f42448bf","_cell_guid":"62dec580-499a-4008-b856-0c81ab4cd3a3","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"from sklearn import ensemble, tree, linear_model\nENSTest = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000).fit(x_train, y_train)"},{"metadata":{"_uuid":"fad33b748038ca0c5fd220ec67af394dd18f231c","_cell_guid":"7543eb2e-0ce7-4908-828b-cf24833a6964","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"train_test(ENSTest, x_train, x_test, y_train, y_test)"},{"metadata":{"_uuid":"381e8d32dc9803f9fd969e00c29e0202d34cded3","_cell_guid":"0ef376e5-43f8-4c24-9448-c2350c8ff20a","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Average R2 score and standart deviation of 5-fold cross-validation\nscores = cross_val_score(ENSTest, x_test, y_test, cv=5)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"},{"metadata":{"_uuid":"e1376769769078de8b2e84bf63c96ce772c198ee","_cell_guid":"0995c610-64bf-43d7-8b67-4b73b632fb0b"},"cell_type":"markdown","source":"#### Interesting that the accuracy is the same. RSME, which is a better measure of performance, is 0.25"},{"metadata":{"_uuid":"014673d21ff8b4eef2c30bb5b12f2374bee8f7b9","_cell_guid":"2f4d857e-e73e-4a78-827f-f65522e92322"},"cell_type":"markdown","source":"## Gradient Boosting"},{"metadata":{"_uuid":"9fc447746122cc5ae9c0f56f29d0855369769cf8","_cell_guid":"3ca28b0a-0c8b-423e-a3a1-55bfe4132c13"},"cell_type":"markdown","source":"#### As previously seen, we have many outliers. So I'm using max_features='sqrt' to reduce overfitting of my model. I also use loss='huber' because it more tolerant to outliers. All other hyper-parameters was chosen using GridSearchCV."},{"metadata":{"_uuid":"6085d5d8a6d2e0951fb2c01676b3b4617aab4ce6","_cell_guid":"1bf19c0f-8882-48f2-97c8-b04c620552d7","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"GBest = ensemble.GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=3, max_features='sqrt',\n                                               min_samples_leaf=15, min_samples_split=10, loss='huber').fit(x_train, y_train)\ntrain_test(GBest, x_train, x_test, y_train, y_test)"},{"metadata":{"_uuid":"d60a605ea46ff8afcbc78ce905434129f70753ea","_cell_guid":"0ac76ef9-608d-436b-8029-7e6062a95148","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Average R2 score and standart deviation of 5-fold cross-validation\nscores = cross_val_score(GBest, x_test, y_test, cv=5)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"},{"metadata":{"_uuid":"c1cc7e9131c5a63a9c7fee8f9d32e77f0b568d57","_cell_guid":"ade8bc50-4f65-4948-826d-cf9e318cf4f7"},"cell_type":"markdown","source":"Gradient boosting seems to work well for this data set"},{"metadata":{"_uuid":"bb4b1bc4b2baa1007468c39683984e017925caa9","_cell_guid":"85ef82eb-4beb-473e-8406-cb46e55ee3a1","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom scipy.stats import skew\nfrom IPython.display import display\nimport matplotlib.pyplot as plt"},{"metadata":{"_uuid":"70a3478846ed94330f12fd42e4f35e4c6ed32ca1","_cell_guid":"f536ff48-a046-4341-8be7-57c49d55e100"},"cell_type":"markdown","source":"## Linear Regression and Lasso"},{"metadata":{"_uuid":"ec7925ab99ff2a30d39ef9791ddfa7a371c5a56c","_cell_guid":"1e5452cf-dcc9-4067-a151-6491fc262451","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Defining two functions for error measuring: RMSE\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\ndef rmse_cv_train(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv = 10))\n    return(rmse)\n\ndef rmse_cv_test(model):\n    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring = scorer, cv = 10))\n    return(rmse)\n\nX_train= x_train\nX_test= x_test"},{"metadata":{"_uuid":"ac3433134de44c24603190b953f5ee27888f4d16","_cell_guid":"aa1b773c-2c6e-454b-b7f0-f076b68cd25b","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Linear Regression\nlr = LinearRegression()\nlr.fit(x_train, y_train)\n\n# Look at predictions on training and validation set\nprint(\"RMSE on Training set :\", rmse_cv_train(lr).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(lr).mean())\ny_train_pred = lr.predict(x_train)\ny_test_pred = lr.predict(x_test)\n\n# Plot residuals\nplt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test_pred - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 11.5, xmax = 15.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([11.5, 15.5], [11.5, 15.5], c = \"red\")\nplt.show()"},{"metadata":{"_uuid":"629cc4f287325fee867cca65d423db68cc42c203","_cell_guid":"e0121339-f7fc-47ca-b9cd-7532cee679dc"},"cell_type":"markdown","source":"#### Fundamentally same result as ElasticNet and simple linear regression"},{"metadata":{"_uuid":"b73e8e641ab8541bf67b79503925782832dd92a7","_cell_guid":"bdfe23ad-3b51-4c1a-ae93-e30409f31283","_execution_state":"busy","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# 2* Ridge\nridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                cv = 10)\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())\nprint(\"Ridge RMSE on Test set :\", rmse_cv_test(ridge).mean())\ny_train_rdg = ridge.predict(X_train)\ny_test_rdg = ridge.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_rdg, y_train_rdg - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test_rdg - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 11.5, xmax = 15.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_rdg, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([11.5, 15.5], [11.5, 15.5], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(ridge.coef_, index = X_train.columns)\nprint(\"Ridge picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Ridge Model\")\nplt.show()"},{"metadata":{"_uuid":"c0666891f7c3d744f91e8631b487546ab103685a","_cell_guid":"9689c4d7-820b-468f-a89c-ca2b4c9e6325","_execution_state":"busy","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# 3* Lasso\nlasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Lasso RMSE on Training set :\", rmse_cv_train(lasso).mean())\nprint(\"Lasso RMSE on Test set :\", rmse_cv_test(lasso).mean())\ny_train_las = lasso.predict(X_train)\ny_test_las = lasso.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_las, y_train_las - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_las, y_test_las - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 11.5, xmax = 15.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_las, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_las, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([11.5, 15.5], [11.5, 15.5], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(lasso.coef_, index = X_train.columns)\nprint(\"Lasso picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")\nplt.show()"},{"metadata":{"_uuid":"6d258546bc3479985e150094bdb3a9ef52b5e978","_cell_guid":"e1655948-be1c-4ef2-91d4-f5438b1b32f3","_execution_state":"busy","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# 4* ElasticNet\nelasticNet = ElasticNetCV(l1_ratio = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n                                    0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Try again for more precision with l1_ratio centered around \" + str(ratio))\nelasticNet = ElasticNetCV(l1_ratio = [ratio * .85, ratio * .9, ratio * .95, ratio, ratio * 1.05, ratio * 1.1, ratio * 1.15],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Now try again for more precision on alpha, with l1_ratio fixed at \" + str(ratio) + \n      \" and alpha centered around \" + str(alpha))\nelasticNet = ElasticNetCV(l1_ratio = ratio,\n                          alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n                                    alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, \n                                    alpha * 1.35, alpha * 1.4], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"ElasticNet RMSE on Training set :\", rmse_cv_train(elasticNet).mean())\nprint(\"ElasticNet RMSE on Test set :\", rmse_cv_test(elasticNet).mean())\ny_train_ela = elasticNet.predict(X_train)\ny_test_ela = elasticNet.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_ela, y_train_ela - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_ela, y_test_ela - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with ElasticNet regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 11.5, xmax = 15.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train, y_train_ela, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test, y_test_ela, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with ElasticNet regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([11.5, 15.5], [11.5, 15.5], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(elasticNet.coef_, index = X_train.columns)\nprint(\"ElasticNet picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the ElasticNet Model\")\nplt.show()"},{"metadata":{"_uuid":"7cee5e7084d73bcb84f11c861214939d90cb4307","_cell_guid":"90d2ace2-55df-4209-89b3-066c47391ad9","collapsed":true},"cell_type":"markdown","source":"### The previous linear regressions with different regulations yielded almost identical results. Conclusively, Gradient boosting seemed to work best for this dataset at accurately predicting the sale prices. With R2: 0.89, RMSE 0.164 and accuracy of 0.87\n\n\n#### Any feedback or comments are wholeheartedly welcome, as this is my first kernel "},{"metadata":{"_uuid":"d38d2f20c7d6b77620fb281e22b01e1474586c4d","_cell_guid":"61e54605-3f76-4b05-8041-400ae5909e0b","_execution_state":"busy","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":""}]}