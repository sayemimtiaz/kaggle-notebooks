{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, we will try to focus in the **Colombian** response to the virus based in different factors. <br> <br>\nAmong this factors we will look at **Mobility** , **HealthCare Coverage** , **COVID Evoultion**. \n<br> <br>\nThis with the hope to be able to discern some conclusion based in the models we are going to train. \n<br>\nUsing different **ANN** we shall train one for **Italy** and another for **Japan** targeting the available indicators in the data. These countries were chosen for their very different approach on the situation.  <br> <br>\nOnce is trained we shall the run it with the Colombian data so it would allow us to compare the response of the different countries.\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#The classics\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd # para leer datos\nimport sklearn.manifold\nimport os\nimport random\nimport glob\n\nfrom sklearn.preprocessing import StandardScaler\nimport sklearn.metrics\nimport sklearn.cluster\n\nimport torch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of the biggest problems is to organize our data. The Uncover library has a **TON** of data. Let's try to make out some cateogries. ","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"folderset = [folder for folder in glob.glob(\"/kaggle/input/uncover/UNCOVER/\" + \"**/\")]\nmobility = [\"/kaggle/input/uncover/UNCOVER/geotab/\",\"/kaggle/input/uncover/UNCOVER/google_mobility/\", \"/kaggle/input/uncover/UNCOVER/un_world_food_programme/\"]\nressources = [\"/kaggle/input/uncover/UNCOVER/hifld/hifld/\"]\nhealthcareData = [\"/kaggle/input/uncover/UNCOVER/oecd/\",\"/kaggle/input/uncover/UNCOVER/us_cdc/us_cdc/\"]\ncovidData = [\"/kaggle/input/uncover/UNCOVER/worldometer/\",\"/kaggle/input/uncover/UNCOVER/HDE/\",\"/kaggle/input/uncover/UNCOVER/johns_hopkins_csse/\",\"/kaggle/input/uncover/UNCOVER/world_bank/\",\"/kaggle/input/uncover/UNCOVER/ECDC/\",\"/kaggle/input/uncover/UNCOVER/WHO/\",\"/kaggle/input/uncover/UNCOVER/our_world_in_data/\"]\nOtherFacilities = [\"/kaggle/input/uncover/UNCOVER/OpenTable/\"]\n\ncsv_Mobility = []\ncsv_ressources = []\ncsv_health = []\ncsv_covid = []\ncsv_otherFacilities = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mobility\nprint(\"\\n Mobility \\n\")\nnum_carpetas = len(mobility)\nfor i in range(num_carpetas):\n    folder_here = mobility[i] + '*'\n    for name in glob.glob(folder_here):\n        print(name)\n        csv_Mobility.append(name)\n#ressources\nprint(\"\\n Ressources \\n\")\nnum_carpetas = len(ressources)\nfor i in range(num_carpetas):\n    folder_here = ressources[i] + '*'\n    for name in glob.glob(folder_here):\n        print(name)\n        csv_ressources.append(name)\n#Healthcare\nprint(\"\\n Healthcare Data \\n\")\nnum_carpetas = len(healthcareData)\nfor i in range(num_carpetas):\n    folder_here = healthcareData[i] + '*'\n    for name in glob.glob(folder_here):\n        print(name)\n        csv_health.append(name)\n#Covid \nprint(\"\\n Covid \\n\")\nnum_carpetas = len(covidData)\nfor i in range(num_carpetas):\n    folder_here = covidData[i] + '*'\n    for name in glob.glob(folder_here):\n        print(name)\n        csv_covid.append(name)\n#OtherFacilities\nprint(\"\\n OtherFacilities \\n\")\nnum_carpetas = len(OtherFacilities)\nfor i in range(num_carpetas):\n    folder_here = OtherFacilities[i] + '*'\n    for name in glob.glob(folder_here):\n        print(name)\n        csv_otherFacilities.append(name)\n                \n#We realize that the the school file should be in OtherFacilities\n\ncsv_otherFacilities.append(csv_covid.pop(3))\n\nprint(csv_otherFacilities)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it is possible to attack one problem at the time. Let's start by finding and visualization the Colombian Data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#HealthCare Utilization\nWorldHC = pd.read_csv(\"/kaggle/input/uncover/UNCOVER/oecd/health-care-utilization.csv\")\ncolombianHC = WorldHC[WorldHC['country'] == 'Colombia']\nitalianHC = WorldHC[WorldHC['country'] == 'Italy']\njapanHC = WorldHC[WorldHC['country'] == 'Japan']\nvariables=WorldHC['var'].unique()\nMundo = pd.DataFrame(columns= variables)\nColombiaHC = pd.DataFrame(columns= variables)\nItaliaHC = pd.DataFrame(columns= variables)\nJapanHC = pd.DataFrame(columns= variables)\n#WorldAveg, Colombian and Italian\nfor i in range(9):\n    anio = 2010+i\n    year = WorldHC[WorldHC['year'] == anio]\n    by_var = year.groupby('var')\n    new_row = pd.Series(data=by_var.median()['value'], name=anio)\n    Mundo = Mundo.append(new_row)\n    \n    yearC = colombianHC[colombianHC['year'] == anio]\n    by_varC = yearC.groupby('var')\n    new_rowC = pd.Series(data=by_varC.median()['value'], name=anio)\n    ColombiaHC = ColombiaHC.append(new_rowC)\n    \n    yearI = italianHC[italianHC['year'] == anio]\n    by_varI = yearI.groupby('var')\n    new_rowI = pd.Series(data=by_varI.median()['value'], name=anio)\n    ItaliaHC = ItaliaHC.append(new_rowI)\n    \n    yearJ = japanHC[japanHC['year'] == anio]\n    by_varJ = yearJ.groupby('var')\n    new_rowJ = pd.Series(data=by_varJ.median()['value'], name=anio)\n    JapanHC = JapanHC.append(new_rowJ)\nprint(np.shape(Mundo),np.shape(ColombiaHC),np.shape(ItaliaHC ), np.shape(JapanHC))\n\nplt.figure(figsize=(18, 8))\nanios = list(Mundo.index) \nfor i in range(3):\n    graficaremos =['ACATHEPB', 'ACATIMMU', 'CONSCOVI']\n    plt.subplot(1,3,i+1)\n    plt.plot(anios,ColombiaHC[graficaremos[i]], label='Colombia')\n    plt.plot(anios,ItaliaHC[graficaremos[i]], label='Italia')\n    plt.plot(anios,JapanHC[graficaremos[i]], label='Japan')\n    plt.legend()\n    plt.xlabel('Year')\n    plt.ylabel('Coverage')\n    plt.title('Evolution of the coverage on {}'.format(graficaremos[i]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Mobility\nWorldM = pd.read_csv('/kaggle/input/uncover/UNCOVER/google_mobility/regional-mobility.csv')\nmovs = list(WorldM.keys())[3::]\ncolombianM = WorldM[WorldM['country'] == 'Colombia']\ncolombianM = colombianM[colombianM['region'] == 'Total']\nitalianM = WorldM[WorldM['country'] == 'Italy']\nitalianM = italianM[italianM['region'] == 'Total']\njapanM = WorldM[WorldM['country'] == 'Japan']\njapanM = japanM[japanM['region'] == 'Total']\ncolombianM = colombianM.set_index(\"date\", drop = False)\nitalianM = italianM.set_index(\"date\", drop = False)\njapanM = japanM.set_index(\"date\", drop = False)\ncolombianM = colombianM[movs]\nitalianM = italianM[movs]\njapanM = japanM[movs]\nprint(np.shape(colombianM),np.shape(italianM),np.shape(japanM))\n\nplt.figure(figsize=(18, 15))\nFechas = list(colombianM.index) \nfor i in range(3):\n    graficaremos = random.choice(movs)\n    plt.subplot(3,1,i+1)\n    plt.plot(Fechas,colombianM[graficaremos], label='Colombia')\n    plt.plot(Fechas,italianM[graficaremos], label='Italia')\n    plt.plot(Fechas,japanM[graficaremos], label='Japan')\n    plt.legend()\n    plt.xlabel('Day')\n    plt.xticks(Fechas[::14])\n    plt.ylabel('Mobility (%)')\n    plt.title('Evolution of the mobility to {}'.format(graficaremos))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't have so much data, but it is enough to try. We shall try to evaluate two types of results. One will be the number of contamination by test taken, the other will be the date rate. Colombia should be more thorough in the colected data. <br>\nFor this part we have to understand that we are handeling a lot of datasets that report the same. We need to have a data sets on the tests made, on the cases reported and the deaths. <br>\n\nTests data source <br>\nUncover/our_world_in_data/covid-19-testing-all-observations.csv <br>\n<br>\nCovid cases, deaths, etc <br>\nUncover/our_world_in_data/coronavirus-disease-covid-19-statistics-and-research.csv <br>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#COVID-19 \nWorldTests = pd.read_csv(\"/kaggle/input/uncover/UNCOVER/our_world_in_data/covid-19-testing-all-observations.csv\")\nWorldCases2 = pd.read_csv(\"/kaggle/input/uncover/UNCOVER/our_world_in_data/coronavirus-disease-covid-19-statistics-and-research.csv\")\n#As we have different starting dates for each country we have to merge the resulting dataframes to be able to obtain a correct display of the datas. \n#Tests\ndats = ['cumulative_total','cumulative_total_per_thousand', 'daily_change_in_cumulative_total_per_thousand',]\ncolombianT = WorldTests[WorldTests['entity'] == 'Colombia - samples processed']\nitalianT = WorldTests[WorldTests['entity'] == 'Italy - tests performed']\njapanT = WorldTests[WorldTests['entity'] == 'Japan - tests performed']\ncolombianT = colombianT.set_index(\"date\", drop = False)\nitalianT = italianT.set_index(\"date\", drop = False)\njapanT = japanT.set_index(\"date\", drop = False)\ncolombianT = colombianT[dats]\nitalianT = italianT[dats]\njapanT = japanT[dats]\n\nTests = japanT.join(italianT.join(colombianT, lsuffix='_ITA', rsuffix='_COL'), lsuffix='_JPN',  rsuffix='')\ndats1 = list(map(lambda x: str(x)+'_COL', dats))\ndats2 = list(map(lambda x: str(x)+'_ITA', dats))\n#dats3 = list(map(lambda x: str(x)+'_JPN', dats))\ncolombianTest = Tests[dats1]\nitalianTest = Tests[dats2]\njapanTest = Tests[dats]\nprint(np.shape(colombianTest),np.shape(italianTest),np.shape(japanTest) )\n\n#Cases and Deaths\n\ndat = ['total_cases','total_deaths', 'total_cases_per_million', 'total_deaths_per_million','new_cases_per_million','new_deaths_per_million']\ncolombianCD2 = WorldCases2[WorldCases2['iso_code'] == 'COL']\nitalianCD2 = WorldCases2[WorldCases2['iso_code'] == 'ITA']\njapanCD2 = WorldCases2[WorldCases2['iso_code'] == 'JPN']\ncolombianCD2 = colombianCD2.set_index(\"date\", drop = False)\nitalianCD2 = italianCD2.set_index(\"date\", drop = False)\njapanCD2 = japanCD2.set_index(\"date\", drop = False)\n\ncolombianCD2 = colombianCD2[dat]\nitalianCD2 = italianCD2[dat]\njapanCD2 = japanCD2[dat]\n\nCOVID = japanCD2.join(italianCD2.join(colombianCD2, lsuffix='_ITA', rsuffix='_COL'),lsuffix='_JPN', rsuffix = '')\ndat1 = list(map(lambda x: str(x)+'_COL', dat))\ndat2 = list(map(lambda x: str(x)+'_ITA', dat))\n#dat3 = list(map(lambda x: str(x)+'_JPN', dat))\ncolombianCOVID = COVID[dat1]\nitalianCOVID = COVID[dat2]\njapanCOVID = COVID[dat]\nprint(np.shape(colombianCOVID),np.shape(italianCOVID), np.shape(japanCOVID) )\n\nplt.figure(figsize=(18, 20))\nplt.subplot(4,1,1)\nFechasT = list(Tests.index)\nplt.plot(FechasT,Tests['cumulative_total_per_thousand_COL'], label='Colombia')\nplt.plot(FechasT,Tests['cumulative_total_per_thousand_ITA'], label='Italia')\nplt.plot(FechasT,Tests['cumulative_total_per_thousand'], label='Japan')\nplt.legend()\nplt.xlabel('Day')\nplt.xticks(FechasT[::14])\nplt.ylabel('Tests per Thousand')\nplt.title('Evolution of the COVID tests performed')\n          \nplt.subplot(4,1,2)\nplt.plot(FechasT,Tests['cumulative_total_per_thousand_COL'], label='Colombia')\nplt.plot(FechasT,Tests['cumulative_total_per_thousand_ITA'], label='Italia')\nplt.plot(FechasT,Tests['cumulative_total_per_thousand'], label='Japan')\nplt.legend()\nplt.xlabel('Day')\nplt.xticks(FechasT[::14])\nplt.ylabel('Tests per Thousand')\nplt.title('Evolution of the COVID tests performed')\n       \nplt.subplot(4,1,3)\nFechas = list(COVID.index)\nplt.plot(Fechas,COVID['total_cases_COL'], label='Colombia')\nplt.plot(Fechas,COVID['total_cases_ITA'], label='Italia')\nplt.plot(Fechas,COVID['total_cases'], label='Japan')\nplt.legend()\nplt.xlabel('Day')\nplt.xticks(Fechas[::14])\nplt.ylabel('Cases')\nplt.title('Evolution of the COVID cases')     \n    \nplt.subplot(4,1,4)\nplt.plot(Fechas,COVID['total_deaths_COL'], label='Colombia')\nplt.plot(Fechas,COVID['total_deaths_ITA'], label='Italia')\nplt.plot(Fechas,COVID['total_deaths'], label='Japan')\nplt.legend()\nplt.xlabel('Day')\nplt.xticks(Fechas[::14])\nplt.ylabel('Deaths')\nplt.title('Evolution of the COVID deaths')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's reunite all the different matrixes to one big matrix for each country. For the health care we will take the 2017 indicator constant on each day. All nan will be replace for 0, this is correct as mobility is a porcentage change (it is then correct to assume that before the pandemic there was bearly a change) and for cases and deaths it is obvious.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#This allows to select the health indicators we have for all three countries. \nColombiaHC1 = ColombiaHC.drop(2018, axis=0)\nColombiaHC1 = ColombiaHC1.dropna(axis = 1)\nColombs = list(ColombiaHC1.keys())\nItaliaHC1 = ItaliaHC[Colombs]\nItaliaHC1 = ItaliaHC1.drop(2018, axis=0)\nItaliaHC1 = ItaliaHC1.dropna(axis = 1)\nJapanHC1 = JapanHC[Colombs]\nJapanHC1 = JapanHC1.drop(2018, axis=0)\nJapanHC1 = JapanHC1.dropna(axis = 1)\nJapans = list(JapanHC1.keys())\nColombiaHC1 = ColombiaHC1[Japans]\nItaliaHC1 = ItaliaHC1[Japans]\n#Now we unite the COVID and Mobility matrixes\nColombia = colombianCOVID.join(colombianM)\nItaly = italianCOVID.join(italianM)\nJapan = japanCOVID.join(japanM)\n#We create the health care columns.  \njaponsito = pd.DataFrame(columns= Japans)\ncolombito = pd.DataFrame(columns= Japans)\nitalianito = pd.DataFrame(columns= Japans)\ndiasAna = list(Colombia.index)\nfor i in range(len(diasAna)):\n    new_rowJ = pd.Series(data=JapanHC1.loc[2017], name=diasAna[i])\n    japonsito = japonsito.append(new_rowJ)\n    new_rowC =pd.Series(data=ColombiaHC1.loc[2017], name=diasAna[i])\n    colombito = colombito.append(new_rowC)\n    new_rowI =pd.Series(data=ItaliaHC1.loc[2017], name=diasAna[i])\n    italianito= italianito.append(new_rowI)\n#We add the health care columns.\nColombia = Colombia.join(colombito)\nItaly = Italy.join(italianito)\nJapan = Japan.join(japonsito)\n#We replace the nans.\nColombia = Colombia.replace(np.nan,0)\nItaly = Italy.replace(np.nan,0)\nJapan = Japan.replace(np.nan,0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have all the data organized we can work on defining the objetive indicator. <br>\nWe tried with NCPM (New Cases Per Million) and NDPM (Total Deaths per Million) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#We take what is the NCPM\nY_1_CO = np.asarray(Colombia.pop('new_cases_per_million_COL'))\nY_1_IT = np.asarray(Italy.pop('new_cases_per_million_ITA'))\nY_1_JP = np.asarray(Japan.pop('new_cases_per_million'))\n#We take what is TDPM\nY_2_CO = np.asarray(Colombia.pop('new_deaths_per_million_COL'))\nY_2_IT = np.asarray(Italy.pop('new_deaths_per_million_ITA'))\nY_2_JP = np.asarray(Japan.pop('new_deaths_per_million'))\n#We noramalize our data.\nscaler = sklearn.preprocessing.StandardScaler()\nColombia = scaler.fit_transform(Colombia)\nItaly = scaler.fit_transform(Italy)\nJapan = scaler.fit_transform(Japan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = torch.nn.Sequential(\n    torch.nn.Linear(14, 25),\n    torch.nn.Linear(25, 35),\n    torch.nn.Linear(35, 10),\n    torch.nn.Linear(10, 1)\n)\n\n#model1 = torch.nn.Sequential(\n#    torch.nn.Conv1d(1, 10, kernel_size=8, stride=1),\n#    torch.nn.Conv1d(10, 6, kernel_size=4, stride=1),\n#    torch.nn.Conv1d(6, 3, kernel_size=1, stride=1),\n#    torch.nn.Conv1d(3, 1, kernel_size=2, stride=3)\n#)\n\nmodel2 = torch.nn.Sequential(\n    torch.nn.Conv1d(1, 10, kernel_size=8, stride=1),\n    torch.nn.Conv1d(10, 6, kernel_size=4, stride=1),\n    torch.nn.Conv1d(6, 3, kernel_size=1, stride=1),\n    torch.nn.Conv1d(3, 1, kernel_size=2, stride=3)\n)\n\n\ndistance1 = torch.nn.KLDivLoss()\noptimizer1 = torch.optim.Adam(model1.parameters(), lr=9E-5, weight_decay=1E-3)\noptimizer1.zero_grad()\n\ndistance2 = torch.nn.KLDivLoss()\noptimizer2 = torch.optim.Adam(model2.parameters(), lr=1E-4, weight_decay=1E-3)\noptimizer2.zero_grad()\nepochs = 650\nfor epoch in range(epochs):\n    #Training Italy\n    X_new1 = np.expand_dims(Italy, 1) \n    inputs1 = torch.autograd.Variable(torch.Tensor(X_new1).float())\n    targets1 = torch.autograd.Variable(torch.Tensor(Y_1_IT).float())\n    \n    \n    out1 = model1(inputs1)\n    out1 = out1.squeeze(dim=1) # necesario para quitar la dimension intermedia de channel\n    loss1 = distance1(out1, targets1)\n    loss1.backward()\n    optimizer1.step()\n    \n    #Training Japan\n    X_new2 = np.expand_dims(Japan, 1) \n    inputs2 = torch.autograd.Variable(torch.Tensor(X_new2).float())\n    targets2 = torch.autograd.Variable(torch.Tensor(Y_1_JP).float())\n    \n    \n    out2 = model2(inputs2)\n    out2 = out2.squeeze(dim=1) # necesario para quitar la dimension intermedia de channel\n    loss2 = distance2(out2, targets2)\n    loss2.backward()\n    optimizer2.step()\n    if epoch>(epochs-5):\n        print('epoch [{}/{}], loss1:{:.4f} , loss2:{:.4f}  '.format(epoch+1, epochs, loss1.item(),loss2.item()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model3 = torch.nn.Sequential(\n#    torch.nn.Linear(14, 25),\n#    torch.nn.Linear(25, 35),\n#    torch.nn.Linear(35, 10),\n#    torch.nn.Linear(10, 1)\n#)\n\nmodel3 = torch.nn.Sequential(\n    torch.nn.Conv1d(1, 10, kernel_size=8, stride=1),\n    torch.nn.Conv1d(10, 6, kernel_size=4, stride=1),\n    torch.nn.Conv1d(6, 3, kernel_size=1, stride=1),\n    torch.nn.Conv1d(3, 1, kernel_size=2, stride=3)\n)\n\nmodel4 = torch.nn.Sequential(\n    torch.nn.Conv1d(1, 10, kernel_size=8, stride=1),\n    torch.nn.Conv1d(10, 6, kernel_size=4, stride=1),\n    torch.nn.Conv1d(6, 3, kernel_size=1, stride=1),\n    torch.nn.Conv1d(3, 1, kernel_size=2, stride=3)\n)\n\n\ndistance3 = torch.nn.KLDivLoss()\noptimizer3 = torch.optim.Adam(model3.parameters(), lr=1E-4, weight_decay=1E-3)\n\n\ndistance4 = torch.nn.KLDivLoss()\noptimizer4 = torch.optim.Adam(model4.parameters(), lr=5E-5, weight_decay=1E-3)\noptimizer4.zero_grad()\nepochs = 650\nfor epoch in range(epochs):\n    #Training Italy\n    X_new3 = np.expand_dims(Italy, 1) \n    inputs3 = torch.autograd.Variable(torch.Tensor(X_new3).float())\n    targets3 = torch.autograd.Variable(torch.Tensor(Y_2_IT).float())\n    \n    optimizer3.zero_grad()\n    out3 = model3(inputs3)\n    out3 = out3.squeeze(dim=1) # necesario para quitar la dimension intermedia de channel\n    loss3 = distance3(out3, targets3)\n    loss3.backward()\n    optimizer3.step()\n    \n    #Training Japan\n    X_new4 = np.expand_dims(Japan, 1) \n    inputs4 = torch.autograd.Variable(torch.Tensor(X_new4).float())\n    targets4 = torch.autograd.Variable(torch.Tensor(Y_2_JP).float())\n    \n    optimizer4.zero_grad()\n    out4 = model4(inputs4)\n    out4 = out4.squeeze(dim=1) # necesario para quitar la dimension intermedia de channel\n    loss4 = distance4(out4, targets4)\n    loss4.backward()\n    optimizer4.step()\n    if epoch>(epochs-5):\n        print('epoch [{}/{}], loss3:{:.4f} , loss4:{:.4f}  '.format(epoch+1, epochs, loss3.item(),loss4.item()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nLet's compare the model obtained from the ANN with the actual evolution of the indicators.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#NCPM\nvalues1, Y_predicted1 = torch.max(out1.data, 1)\nvalues2, Y_predicted2 = torch.max(out2.data, 1)\n\nplt.figure(figsize=(18, 14))\nplt.subplot(2,2,1)\nplt.plot(Fechas,values1, label='Model')\nplt.plot(Fechas,Y_1_IT, label='Real')\nplt.legend()\nplt.xlabel('Day')\nplt.xticks(Fechas[::30])\nplt.ylabel('New Cases per Million')\nplt.title('Italian Evolution of the COVID')\n          \nplt.subplot(2,2,2)\nplt.plot(Fechas,values2, label='Model')\nplt.plot(Fechas,Y_1_JP, label='Real')\nplt.legend()\nplt.xlabel('Day')\nplt.xticks(Fechas[::30])\nplt.ylabel('New Cases per Million')\nplt.title('Japanese Evolution of the COVID')\n\n#TDPM\nvalues3, Y_predicted3 = torch.max(out3.data, 1)\nvalues4, Y_predicted4 = torch.max(out4.data, 1)\n\nplt.subplot(2,2,3)\nplt.plot(Fechas,values3, label='Model')\nplt.plot(Fechas,Y_2_IT, label='Real')\nplt.legend()\nplt.xlabel('Day')\nplt.xticks(Fechas[::30])\nplt.ylabel('New Deaths per Million')\nplt.title('Italian Evolution of the COVID')\n          \nplt.subplot(2,2,4)\nplt.plot(Fechas,values4, label='Model')\nplt.plot(Fechas,Y_2_JP, label='Real')\nplt.legend()\nplt.xlabel('Day')\nplt.xticks(Fechas[::30])\nplt.ylabel('New Deathsper Million')\nplt.title('Japanese Evolution of the COVID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An now, let's model the Colombian evolution with each of the models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#datos\nX_new = np.expand_dims(Colombia, 1) \ninputs = torch.autograd.Variable(torch.Tensor(X_new).float())\n#Modelo 1 Italia\nx_transform_I = model1(inputs)\nvalues_I, Y_predicted_I = torch.max(x_transform_I.data, 1)\n\n#Modelo 2 Japon\nx_transform_J = model2(inputs)\nvalues_J, Y_predicted_J = torch.max(x_transform_J.data, 1)\n#Modelo 3 Italia\nx_transform_I2 = model3(inputs)\nvalues_I2, Y_predicted_I2 = torch.max(x_transform_I2.data, 1)\n\n#Modelo 4 Japon\nx_transform_J2 = model4(inputs)\nvalues_J2, Y_predicted_J2 = torch.max(x_transform_J2.data, 1)\n\n\n\nplt.figure(figsize=(15, 20))\nplt.subplot(2,1,1)\nplt.plot(Fechas,values_I, label='Modelo_Italia')\nplt.plot(Fechas,values_J, label='Modelo_Japan')\nplt.plot(Fechas,Y_1_CO, label='True')\nplt.legend()\nplt.xlabel('Day')\nplt.xticks(Fechas[::14])\nplt.ylabel('New Cases per Million')\nplt.title('Evolution of the COVID')\n\nplt.subplot(2,1,2)\nplt.plot(Fechas,values_I2, label='Modelo_Italia')\nplt.plot(Fechas,values_J2, label='Modelo_Japan')\nplt.plot(Fechas,Y_2_CO, label='True')\nplt.legend()\nplt.xlabel('Day')\nplt.xticks(Fechas[::14])\nplt.ylabel('New Deaths per Million')\nplt.title('Evolution of the COVID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's discuss these results. There is no need to underline the volatility of the model. Nonetheless, there are some clear tendencies that we can identify. <br> <br>\nFirst,the clear drop (at around 2020-04-18) in the projected number of cases is important. I link this to the strong decreased mobility. This, because the italian model drop is more sudden (as this event was in it's training) and even if the japanese model didn't train with it, it was affected by it, showing the effect of the confinment. <br><br>\nSecond, is the existence of other factors entering in the definition of the NCPM indicator. Trying to tackle it I used Convd1d for the Japanese model, as there are other behaviour patterns entering besides movment (Mask wearing, etc) and they didn't show the confinament restrictions. <br><br>\nFinally, we can conclude that the behaviour of Colombia is acceptable, based on the NDPM that can be the 'fairest' indicator, in front of these two countries. It's behaivour is close to the Japanese model, this having to realize a confinement to its citizens. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}