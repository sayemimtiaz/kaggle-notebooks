{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About the data.\nThere are various features in the data regarding the candidate everything from the class 10th till the MBA. To predict if the candidate will get placed or not.\n\n1. sl_no : Serial Number.\n2. gender : Sex of the candidate.\n3. ssc_p : Class 10th percentage.\n4. ssc_b : Class 10th board of education.\n5. hsc_p : Class 12th percentage.\n6. hsc_b : Class 12th board of education.\n7. hsc_s : Field of study in Class 12th.\n8. degree_p : Degree percentage.\n9. degree_t : Undergrad field of education.\n10. workex : Years of work experience.\n11. etest_p : Employability test percentage.\n12. specialisation : Field of study during Post Graduation.\n13. mba_p : MBA percentage.\n14. status : placed/not placed."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.under_sampling import TomekLinks\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets get a sense of the numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Problem is to predict if the candidate will get placed or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop('salary', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis."},{"metadata":{},"cell_type":"markdown","source":"## Pairplot.\nSource to learn about pairplot: https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = sns.PairGrid(data= data, hue='status')\ngrid = grid.map_upper(plt.scatter)\ngrid = grid.map_diag(sns.kdeplot, shade=True)\ngrid = grid.map_lower(sns.kdeplot)\nplt.title('Distribution of the features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observation.\n1. All the numerical features are almost symmetrically distributed with respect to the mean.\n2. Placed candidates have higher class 10, class 12, and degree percentages."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feats = ['gender', 'ssc_b', 'hsc_b', 'hsc_s', 'degree_t', 'workex', 'specialisation']\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 15))\n\nsns.countplot(data.gender, hue=data.status, ax=axes[0][0])\nsns.countplot(data.ssc_b, hue=data.status, ax=axes[0][1])\nsns.countplot(data.hsc_b, hue=data.status, ax=axes[0][2])\nsns.countplot(data.hsc_s, hue=data.status, ax=axes[0][3])\nsns.countplot(data.degree_t, hue=data.status, ax=axes[1][0])\nsns.countplot(data.workex, hue=data.status, ax=axes[1][1])\nsns.countplot(data.specialisation, hue=data.status, ax=axes[1][2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusions from the above plots.\n\n1. Males have performed a lot better during the interview process than females.\n2. The board of study of class 10 and 12 actually donot matter.\n3. A company will need people who have good amount of knoweledge with respect to how to manage a company, manage the financial part of the company, etc. Another set of people who can work on the technology part. Hence Management and Technology become the two major parts on which the company relies on. We see that these two fields have better job opportunities than the rest of them.\n4. Definitely a person who has prior experience will have better chance of getting placed.\n5. Most frequently asked question, HR of Finance in MBA? Looking at the plot above its clear that Finance has better opportunities than HR. But HR is definitely a position where you earn a lot of respect."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_feats = ['ssc_p', 'hsc_p', 'degree_p', 'etest_p', 'mba_p']\n\nfig1, axes1 = plt.subplots(2, 3, figsize=(15, 10))\n\nsns.boxplot(data.status, data.ssc_p, ax=axes1[0][0])\nsns.boxplot(data.status, data.hsc_p, ax=axes1[0][1])\nsns.boxplot(data.status, data.degree_p, ax=axes1[0][2])\nsns.boxplot(data.status, data.etest_p, ax=axes1[1][0])\nsns.boxplot(data.status, data.mba_p, ax=axes1[1][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusions.\n1. There is signigicant difference between the distribution of the ssc_p, hsc_p, and degree_p of placed and not placed candidates. Done this mean marks matter? Not exactly, as the dataset is very small and yeah its true for this set of examples.\n2. Whereas etest_p and mba_p should have mattered more than the anything else. But that does not seem to be true here.\n\n#### Hence conclusion, marks matters but will never be the deciding factor. What matters is the knoweledge one has gained and the field in which he is working on."},{"metadata":{},"cell_type":"markdown","source":"# Exploring Categories.\n\nHandling categorical features seems to be the most important part of analysis and preprocessing."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.gender.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.ssc_b.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hsc_b.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hsc_s.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.degree_t.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.workex.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Less candidates have got work experience. Hence having a work experience will give and edge over other candidates."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.specialisation.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding Categorical Features."},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = LabelEncoder()\n\ndata['gender'] = encoder.fit_transform(data['gender'])\ndata['ssc_b'] = encoder.fit_transform(data['ssc_b'])\ndata['hsc_b'] = encoder.fit_transform(data['hsc_b'])\ndata['hsc_s'] = encoder.fit_transform(data['hsc_s'])\ndata['degree_t'] = encoder.fit_transform(data['degree_t'])\ndata['workex'] = encoder.fit_transform(data['workex'])\ndata['specialisation'] = encoder.fit_transform(data['specialisation'])\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encode target labels.\ndef encode(col):\n    if col[0] == 'Placed':\n        return 1\n    else:\n        return 0\n    \ndata['status'] = data[['status']].apply(encode, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Test Split."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data = train_test_split(data, test_size=0.2, stratify=data.status)\n\nprint('train size : ' + str(train_data.shape[0]))\nprint('test size : ' + str(test_data.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Imbalance."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_data.status)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.status.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Just 31% of the dataset constitutes the negative (not placed) samples. This makes the dataset slightly imbalanced.\n\nAs the size of the train data is vary small, downsampling would reduce the size of the dataset further. Hence I guess upsampling technique would work better here.\n\n## Data Imbalance Handling Techniques Resources.\n\n1. Basic Techniques: https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/\n2. Geometric Smote: https://towardsdatascience.com/handling-imbalanced-data-using-geometric-smote-770b49d5c7b5\n3. ADASYN: https://towardsdatascience.com/adasyn-adaptive-synthetic-sampling-method-for-imbalanced-data-602a3673ba16"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = pd.DataFrame(train_data.status, columns=['status'])\ntrain_data.drop('status', axis=1, inplace=True)\n\ntest_labels = pd.DataFrame(test_data.status, columns=['status'])\ntest_data.drop('status', axis=1, inplace=True)\n\nsampler = TomekLinks()\ntrain_res, labels_res = sampler.fit_resample(train_data, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_res.status.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### After sampling.\n1. Size of class 1 : 106\n2. Size of class 0 : 54\n\n12 examples from the majority class were dropped off."},{"metadata":{},"cell_type":"markdown","source":"# Correlation and Feature Importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = pd.concat([train_res, labels_res], axis=1).corr()\nplt.figure(figsize=(15, 15))\nsns.heatmap(corr, cmap='YlGnBu', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_tree_forest = ExtraTreesClassifier() \n  \nextra_tree_forest.fit(train_res, labels_res) \n\nfeature_importance = extra_tree_forest.feature_importances_ \n\nplt.figure(figsize=(15, 15))\nplt.bar(train_res.columns, feature_importance) \nplt.xlabel('Feature Labels') \nplt.ylabel('Feature Importances') \nplt.title('Comparison of different Feature Importances') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What can we infer from this?\n1. Features representing the percentage seem to have very high importance ni this problem.\n2. ssc_p (Class 10 percentage) is the most important feature.\n3. Board of education have very less importance as seen before.\n4. ssc_p, hsc_p, and degree_p are high positive correlation with each other. This says that most of the candidates were consistent in their performance from class 10 till the post graduation. Some candidates might have improved or deproved their performance.\n5. Usually serial number of the candidate does not have influence over if the candidate has to be selected or not. But the model could find some pattern in that feature. This might help the model accuracy.\n\nLets train now!!"},{"metadata":{},"cell_type":"markdown","source":"# Model Training Without Tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_metrics(predicts, true_labels):\n    print('Accuracy : ' + str(round(accuracy_score(predicts, true_labels), 2)))\n    print('Precision : ' + str(round(precision_score(predicts, true_labels), 2)))\n    print('Recall : ' + str(round(recall_score(predicts, true_labels), 2)))\n    print('f1score : ' + str(round(f1_score(predicts, true_labels), 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.reset_index(inplace=True, drop=True)\ntest_labels.reset_index(inplace=True, drop=True)\n\ntrain_res.reset_index(inplace=True, drop=True)\nlabels_res.reset_index(inplace=True, drop=True)\n\ntrain_res.drop('index', axis=1, inplace=True)\nlabels_res.drop('index', axis=1, inplace=True)\ntest_data.drop('index', axis=1, inplace=True)\ntest_labels.drop('index', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scale_data(train, test, num_cols):\n    scaler = MinMaxScaler(feature_range=(0, 3))\n\n    temp_data = train.copy()\n    temp_test = test.copy()\n\n    scaled_data = pd.DataFrame(scaler.fit_transform(temp_data[num_cols]), columns = num_cols)\n    temp_data.drop(num_cols, axis=1, inplace=True)\n    final_data = pd.concat([temp_data, scaled_data], axis=1)\n\n    scaled_test = pd.DataFrame(scaler.fit_transform(temp_test[num_cols]), columns = num_cols)\n    temp_test.drop(num_cols, axis=1, inplace=True)\n    final_test = pd.concat([temp_test, scaled_test], axis=1)\n    \n    return final_data, final_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic = LogisticRegression()\nlog_data, log_test = scale_data(train_res, test_data, num_feats + ['sl_no'])\nlogistic.fit(log_data, labels_res)\npreds = logistic.predict(log_test)\nprint_metrics(preds, test_labels)\n\nlog_matrix = confusion_matrix(preds, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc = DecisionTreeClassifier()\ndtc.fit(train_res, labels_res)\npreds = dtc.predict(test_data)\nprint_metrics(preds, test_labels)\n\ndtc_matrix = confusion_matrix(preds, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier()\nrfc.fit(train_res, labels_res)\npreds = rfc.predict(test_data)\nprint_metrics(preds, test_labels)\n\nrfc_matrix = confusion_matrix(preds, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes2 = plt.subplots(1, 3, figsize=(20, 5))\n\nsns.heatmap(log_matrix, cmap='YlGnBu', annot=True, ax=axes2[0])\nsns.heatmap(dtc_matrix, cmap='YlGnBu', annot=True, ax=axes2[1])\nsns.heatmap(rfc_matrix, cmap='YlGnBu', annot=True, ax=axes2[2])\n\naxes2[0].set_title(\"Logistic Regression\")\naxes2[1].set_title(\"Decision Tree\")\naxes2[2].set_title(\"Random Forest\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training with Parameter Tuning."},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_tune = LogisticRegression()\nlog_data, log_test = scale_data(train_res, test_data, num_feats + ['sl_no'])\n\nparams = {\n    'penalty' : ['l1', 'l2'],\n    'max_iter' : [80, 90, 100, 110, 120]\n}\n\nsearch = RandomizedSearchCV(logistic_tune, params, n_iter=20, cv=6, random_state=21)\n\nbest_model = search.fit(log_data, labels_res)\nbest_logistic = LogisticRegression(**best_model.best_estimator_.get_params())\nbest_logistic.fit(log_data, labels_res)\npreds = best_logistic.predict(log_test)\nprint_metrics(preds, test_labels)\n\nlog_matrix = confusion_matrix(preds, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc_tune = DecisionTreeClassifier()\n\nparams = {\n    'max_depth' : [6, 7, 8],\n    'max_features' : [7, 8, 9]    \n}\n\nsearch = RandomizedSearchCV(dtc_tune, params, n_iter=50, cv=8, random_state=21)\n\nbest_model = search.fit(train_res, labels_res)\nbest_dtc = DecisionTreeClassifier(**best_model.best_estimator_.get_params())\nbest_dtc.fit(train_res, labels_res)\npreds = best_dtc.predict(test_data)\nprint_metrics(preds, test_labels)\n\ndtc_matrix = confusion_matrix(preds, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_tune = RandomForestClassifier()\n\nparams = {\n    'n_estimators' : [160, 170, 180],\n    'max_depth' : [6, 7, 8],\n    'max_features' : [5, 6, 7],\n    'bootstrap' : [True],\n    'min_samples_leaf' : [2, 3]    \n}\n\nsearch = RandomizedSearchCV(rfc_tune, params, n_iter=40, cv=8, random_state=21)\n\nbest_model = search.fit(train_res, labels_res)\nbest_rfc = RandomForestClassifier(**best_model.best_estimator_.get_params())\nbest_rfc.fit(train_res, labels_res)\npreds = best_rfc.predict(test_data)\nprint_metrics(preds, test_labels)\n\nrfc_matrix = confusion_matrix(preds, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes3 = plt.subplots(1, 3, figsize=(20, 5))\n\nsns.heatmap(log_matrix, cmap='YlGnBu', annot=True, ax=axes3[0])\nsns.heatmap(dtc_matrix, cmap='YlGnBu', annot=True, ax=axes3[1])\nsns.heatmap(rfc_matrix, cmap='YlGnBu', annot=True, ax=axes3[2])\n\naxes3[0].set_title(\"Logistic Regression Tuned\")\naxes3[1].set_title(\"Decision Tree Tuned\")\naxes3[2].set_title(\"Random Forest Tuned\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\nThe dataset size was 215. Split the dataset into 80% train and 20% test set. After analysis the dataset was trained on three different models and tested on the same. The test accuracies were as shown below.\n\n### Logistic Regression\n1. Without tuning : 95%\n2. With tuning : 95%\n\n### Decision Tree\n1. Without tuning : 70%\n2. With tuning : 79%\n\n### Random Forest\n1. Without tuning : 86%\n2. With tuning : 86%\n\nLogistic Regression and Random Forest showed no improvment after tuning them as they were already in their best with default parameter values. But could see some significant improvement in the Decision Tree."},{"metadata":{},"cell_type":"markdown","source":"## Thank you for completing the notebook. Hope you like it. If you got to learn something from this notebook consider dropping a like. If an expert is reading this notebook to review it, do like as this would encourage me to learn more and improve myself.\n\n## Also do have a look at my another notebook on Red Wine Quality Prediction.\nhttps://www.kaggle.com/nayakroshan/red-wine-quality-prediction-accuracy-0-98"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}