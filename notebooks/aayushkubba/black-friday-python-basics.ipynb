{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Table of contents <a class=\"anchor\" id=\"0.1\"></a>\n\n\n1.  [Introduction to Pandas](#1)\n\n1.\t[Key features of Pandas](#2)\n\n1.\t[Advantages of Pandas](#3)\n\n1.\t[Importing Pandas](#4)\n\n1.\t[Data structures in Pandas](#5)\n\n1.\t[Pandas series](#6)\n\n1.\t[Pandas dataframe](#7)\n\n1.\t[Pandas panel](#8)\n\n1.\t[Data import with pandas](#9)\n\n1.\t[Exploratory data analysis](#10)\n\n1.\t[Handle missing values with pandas](#11)\n\n1.\t[Indexing and slicing in pandas](#12)\n\n1.\t[Indexing and reindexing in pandas](#13)\n\n1.\t[MultiIndex or advanced indexing](#14)\n\n1.\t[Sorting in pandas](#15)\n\n1.\t[Categorical data in pandas](#16)\n\n1.\t[Basic functionality in pandas](#17)\n\n1.\t[Descriptive statistics in pandas](#18)\n\n1.\t[Statistical functions in pandas](#19)\n\n1.\t[Window functions in pandas](#20)\n\n1.\t[Aggregations in pandas](#21)\n\n1.\t[Iteration in pandas](#22)\n\n1.\t[Function application in pandas](#23)\n\n1.\t[Pandas GroupBy operations](#24)\n\n1.\t[Pandas merging and joining](#25)\n\n1.\t[Pandas concatenation operation](#26)\n\n1.\t[Reshaping by melt and pivot](#27)\n\n1.\t[Reshaping by stacking and unstacking](#28)\n\n1.\t[Options and customization with pandas](#29)\n\n1.\t[Summary and conclusion](#30)\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Introduction to Pandas <a class=\"anchor\" id=\"1\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n\n- Today, Python is considered as the most popular programming language for doing data science work.  The reason behind this popularity is that Python provides great packages for doing data analysis and visualization work. \n\n\n\n- **Pandas** is one of those packages that makes analysing data much easier. Pandas is an open source library for data analysis in Python. It was developed by Wes McKinney in 2008. Over the years, it has become the standard library for data analysis using Python.\n\n\n- According to the Wikipedia page on Pandas,\n\n\n\n  - **\"Pandas offers data structures and operations for manipulating numerical tables and time series. It is free software released under the three-clause BSD license. The name is derived from the term 'panel data', an econometrics term for data sets that include observations over multiple time periods for the same individuals.\"**\n\n\n\n- In this project, I explore Pandas and various data analysis tools provided by Pandas.\n","metadata":{}},{"cell_type":"markdown","source":"# 2. Key features of Pandas <a class=\"anchor\" id=\"2\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nSome key features of Pandas are as follows:-\n\n\n1.\tIt provides tools for reading and writing data from a wide variety of sources such as CSV files, excel files, \n    databases such as SQL, JSON files.    \n    \n2.\tIt provides different data structures like series, dataframe and panel for data manipulation and indexing.\n\n3.\tIt can handle wide variety of data sets in different formats – time series, heterogeneous data, tabular and matrix data.\n\n4.\tIt can perform variety of operations on datasets. It includes subsetting, slicing, filtering, merging, joining, groupby, reordering and reshaping operations.\n    \n5.\tIt can deal with missing data by either deleting them or filling them with zeros or a suitable test statistic.\n\n6.\tIt can be used for parsing and conversion of data.\n\n7.\tIt provides data filtration techniques.\n\n8.\tIt provides time series functionality – date range generation, frequency conversion, moving window statistics, \n    data shifting and lagging.    \n \n9.\tIt integrates well with other Python libraries such as Scikit-learn, statsmodels and SciPy.\n\n10.\tIt delivers fast performance. Also, it can be speeded up even more by making use of Cython (C extensions to Python).\n","metadata":{}},{"cell_type":"markdown","source":"# 3. Advantages of Pandas <a class=\"anchor\" id=\"3\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nPandas is a core component of the Python data analysis toolkit. Pandas provides data structure and operations facilities, \nwhich is particularly useful for data analysis. There are various advantages of using Pandas for data analysis. \n\nThese advantages are as follows:-\n\n### **Data representation** \n\nIt represents data in a form that is very much suited for data analysis through its Dataframe and Series data structures.                              \n                           \n### **Data subsetting and filtering** \n\nIt provides for easy subsetting and filtering of data. It provides procedures that are suited for data analysis.                                    \n                                 \n### **Concise and clear code** \n\nIt provides functionality to write clear and concise code. It allows us to focus on the task at hand, rather than have to write tedious code.\n","metadata":{}},{"cell_type":"markdown","source":"# 4. Importing Pandas <a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)\n\n\nIn order to use Pandas in our work, we need to import the Pandas library first. We can import the Pandas library with the following command:-\n\n\n`import pandas`\n\n\nUsually, we import the Pandas library by appending the alias `as pd`.  It makes things easier because now instead of writing `pandas.command` we need to write `pd.command`. So, we will import pandas with the following command:-\n\n\n`import pandas as pd`\n\n\nAlso, I will import Numpy as well, because it is very useful library for scientific computing with Python. I will import Numpy with the following command:-\n\n\n`import numpy as np`\n","metadata":{}},{"cell_type":"code","source":"# import pandas and numpy\n\nimport pandas as pd\n\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Data structures in Pandas <a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)\n\n\nPandas provide easy to use data structures. \n\n\nThere are three main data structures in Pandas. They are:-\n\n\n-\tSeries\n\n-\tDataframe\n\n-\tPanel\n\n\nThese data structures are built on top of Numpy array, which means they are fast. I have described these data structures in the following sections.\n","metadata":{}},{"cell_type":"markdown","source":"# 6. Pandas Series <a class=\"anchor\" id=\"6\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n\nA Pandas Series is a one-dimensional array like structure with homogeneous data.  \n\n\nThe data can be of any type (integer, string, float, etc.). The axis labels are collectively called index. \n\n\nFor example, the following series is a collection of integers 10, 20, 30, 40, 50, 60, 70, 80, 90, 100.\n\n\n\n\n\n### Key Points of Pandas Series\n\n\n-\tHomogeneous data\n\n-\tSize of series immutable\n\n-\tValues of data mutable\n\n\n\n\n\n### Series Constructor\n\n\n\n\nA Pandas Series can be created using the following constructor −\n\n\n`pandas. Series (data, index, dtype, copy)`\n\n\nThe parameters of the constructor are as follows –\n\n\n-\t**data** - data takes various forms like ndarray, list, dictionary, constants, etc.\n\n\n-\t**index**- index values must be unique, hashable and have the same length as data. The default index is RangeIndex (0, 1, 2,\n               …, n) if no index is passed.\n     \n     \n-\t**dtype** - dtype is for data type. If none, data type will be inferred.\n\n\n-\t**copy** - Copy input data. Default value is False.\n","metadata":{}},{"cell_type":"markdown","source":"# 7. Pandas DataFrame <a class=\"anchor\" id=\"7\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n\nA Dataframe is a two-dimensional data structure. So, data is aligned in a tabular fashion in rows and columns. \nIts column types can be heterogeneous:  - that is, of varying types. It is similar to structured arrays in NumPy \nwith mutability added.\n\n\n\n### Properties of Dataframe are as follows:-\n\n\n-\tThe dataframe is conceptually analogous to a table or spreadsheet of data. \n\n\n-\tIts columns are of different types – float64, int, bool, and so on.\n\n\n-\tA Dataframe column is a Series structure.\n\n\n-\tIts size is mutable – columns can be inserted and deleted.\n\n\n-\tIt has labelled axes (rows and columns).\n\n\n-\tIt can be thought of as a dictionary of Series structures where both the rows and columns are indexed, \n    denoted as `index` in the case of rows and `columns` in the case of columns.\n    \n    \n-\tIt can perform arithmetic operations on rows and columns.\n\n\n\n### Dataframe Constructor\n\n\n\nDataframe is the most commonly used data structure in pandas. \n\n\nA pandas Dataframe can be created using the following constructor-\n\n\n`pandas.DataFrame(data, index, columns, dtype, copy)`\n\n\nThe constructor accepts many different types of arguments: \n\n\n\tDictionary of 1D ndarrays, lists, dictionaries, or Series structures \n    \n\t2D NumPy array\n    \n\tStructured or record ndarray\n    \n\tSeries structures\n    \n\tAnother DataFrame structure \n\n\n\nThe parameters description of the constructor is as follows –\n\n\n-**data** - data takes various forms like ndarray, series, map, lists, dict, constants and also another DataFrame.\n\n\n-**index**- Index or array-like \n\n\n            Index to use for resulting frame. Will default to RangeIndex if no indexing information part of \n            input data and no index provided\n            \n\n-**columns**- Index or array-like\n\n\n              Column labels to use for resulting frame. Will default to RangeIndex (0, 1, 2, …, n) if no column labels are  \n              provided.\n              \n              \n-**dtype** - data type of each column\n\n\n\n-**copy** - boolean, default False\n\n\n            Copy data from inputs. Only affects DataFrame / 2d ndarray input\n\n\n\n\n### Dataframe Creation\n\n\nA pandas Dataframe can be created using various inputs like −\n\n\n•\tLists\n\n•\tdict\n\n•\tSeries\n\n•\tNumpy ndarrays\n\n•\tAnother Dataframe\n\n","metadata":{}},{"cell_type":"markdown","source":"# 8. Pandas Panel <a class=\"anchor\" id=\"8\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n\nA panel is a 3D container of data. \n\n\nThe term Panel data is derived from **econometrics** and is partially responsible for the name pandas − pan(el)-da(ta)-s.\n\n\nThe names for the 3 axes are intended to give some semantic meaning to describing operations involving panel data. \n\n\nThey are −\n\n\n`items − axis 0`, each item corresponds to a DataFrame contained inside.\n\n\n`major_axis − axis 1`, it is the index (rows) of each of the DataFrames.\n\n\n`minor_axis − axis 2`, it is the columns of each of the DataFrames.","metadata":{}},{"cell_type":"markdown","source":"# 9. Data import with Pandas <a class=\"anchor\" id=\"9\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nPandas input output API provides several functions that can be used to import and export various file formats. \n\n\nBelow is the list of file formats and the corresponding functions to import these file formats.\n\n\n- Flat files - read_csv(), to_csv()\n\n- Excel files - read_excel(), ExcelWriter(), to_excel()\n\n- JSON files - read_json(), to_json()\n\n- HTML tables - read_html(), to_html()\n\n- SAS files - read_sas()\n\n- SQL files - read_sql(), read_sql_query(), read_sql_table(), to_sql()\n\n- STATA files - read_stata(), to_stata()\n\n- pickle object - read_pickle(), to_pickle()\n\n- HDF5 files - read_hdf(), to_hdf()","metadata":{}},{"cell_type":"markdown","source":"In this project, I work with the **BlackFriday dataset** which is a comma-separated values (CSV) file type. In a CSV file type, the data is stored as a comma-separated values where each row is separated by a new line, and each column by a comma (,). Also, in some sections, I create my own dataset to discuss the respective functionality.\n\n\nSo, I use the **read_csv()** function to import the file as follows:-","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = '/kaggle/input/black-friday/train.csv'\n\ndf = pd.read_csv(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Exploratory Data Analysis <a class=\"anchor\" id=\"10\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nThe next step is to conduct exploratory data analysis. ","metadata":{}},{"cell_type":"markdown","source":"### check the type of df\n\n\nI have imported the dataset. The next step is to check its type. We can check its type with the following command:-","metadata":{}},{"cell_type":"code","source":"type(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the `df` is the pandas dataframe.","metadata":{}},{"cell_type":"markdown","source":"### check shape of dataframe\n\n\nThe next step is to check the shape of the dataframe. We can check the shape of the dataframe as follows:-","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 550068 rows and 12 columns in the train dataset.","metadata":{}},{"cell_type":"markdown","source":"### view the first five rows of the dataframe\n\n\nWe can view the first 5 rows of the dataframe with **head()** method as follows:-","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### view concise summary of dataframe\n\nWe can view the concise summary of dataframe with **info()** method as follows:-","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Handle missing values with pandas <a class=\"anchor\" id=\"11\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nWe can check the total number of missing values in each column in the dataset with the following command:-","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are 166986 missing values in `Product_Category_2` and 373299 columns in `Product_Category_3` columns.","metadata":{}},{"cell_type":"markdown","source":"###  isna() and notna() functions to detect 'NA' values\n\n\nPandas provides `isna()` and `notna()` functions to detect 'NA' values. \n\nThese are also methods on Series and DataFrame objects.\n\nExamples of isna() and notna() commands.\n\n\n\ndetect ‘NA’ values in the dataframe\t\n\n`df.isna().sum()`\n\n\n\ndetect ‘NA’ values in a particular column in the dataframe\n\n\n`pd.isna(df[‘col_name’])`\n\n\n`df[‘col_name’].notna()`\n","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that all the missing values are encoded as `NA` values. If the missing values are encoded in different ways we should encode them first.","metadata":{}},{"cell_type":"markdown","source":"### Encode missing numerical values\n\n\nMissing values are encoded in different ways. They can appear as `NaN`, `NA`, `?`, `zeros`, `xx`, `-1` or a blank space `“ ”`. \nWe can use various pandas methods to deal with missing values. \n\nBut, pandas always recognize missing values as `NaN`.  So, it is essential that we should first convert all the `?`, `zeros`, `xx`, `-1` or `“ ”` to `NaN`. If the missing values isn’t identified as `NaN`, then we have to first convert or replace \nsuch `non NaN` entry with a `NaN`.\n\n\n\n### Convert '?' to ‘NaN’\n\n`df[df == '?'] = np.nan`\n","metadata":{}},{"cell_type":"markdown","source":"### Handle missing numerical values\n\nThere are several methods to handle missing values. Each method has its own advantages and disadvantages. The choice of the method is subjective and depends on the nature of data and the missing values. In this section, I have listed the most commonly used methods to deal with missing values. They are as follows:-\n\n\n- Drop missing values with dropna() method\n\n- Fill missing values with zeros\n\n- Fill missing values with a test statistic\n\n- Fill missing values backward or forward\n\n\n\nIn this section, I have fill the missing values with forward or backward filling.\n\n\nThe **pad or fill** option fill values forward, while **bfill or backfill** option fill values backward. \n\n\nThe following code helps us to achieve this task:-","metadata":{}},{"cell_type":"code","source":"df = df.fillna(method = 'pad')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, we should check whether missing values are removed or not.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the `Product_Category_2` and `Product_Category_3` have 1 missing value. We can use the **head()** to check this.","metadata":{}},{"cell_type":"code","source":"df[['Product_Category_2', 'Product_Category_3']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the first element of each column are NaN. So, in this case **pad** or **fill** option does not work. Here, we\nshould use **bfill** or **backfill** options as follows:-","metadata":{}},{"cell_type":"code","source":"df = df.fillna(method = 'backfill')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, we should check whether missing values are filled or not.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check with ASSERT statement\n\n\nFinally, we should check for missing values programmatically. If we drop or fill missing values, we expect no missing values. \nWe can write an assert statement to verify this. So, we can use an assert statement to programmatically check that no missing or unexpected '0' value is present. This gives confidence that our code is running properly.\n\n\nAssert statement will return nothing if the value being tested is true and will throw an AssertionError if the value is false.\n\n\nAsserts\n\n\n•\tassert 1 == 1   (return Nothing if the value is True)\n\n\n•\tassert 1 == 2   (return AssertionError if the value is False)\n","metadata":{}},{"cell_type":"code","source":"#assert that there are no missing values in the dataframe\n\nassert pd.notnull(df).all().all()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above command does not throw any AssertionError. So, it is confirmed that there are no missing values in the dataframe.","metadata":{}},{"cell_type":"markdown","source":"# 12. Indexing and slicing in pandas <a class=\"anchor\" id=\"12\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n","metadata":{}},{"cell_type":"markdown","source":"In this section, I will discuss how to slice and dice the data and get the subset of pandas dataframe.\n\n\nPandas provides three types of Multi-axes indexing. Those three types are mentioned in the following table:-\n\n\n- 1. **.loc** - Label based\n\n\n- 2. **.iloc** - Integer based\n\n\n- 3. **.ix**  - Both Label and Integer based\n\n\nStarting with pandas 0.20.0, the .ix indexer is deprecated, in favor of the more strict .iloc and .loc indexers. So, I will not discuss it here and limit the discussion to .loc and .iloc indexers.","metadata":{}},{"cell_type":"markdown","source":"### Label based indexing using .loc indexer\n\n\nPandas provide **.loc indexer** to have purely label based indexing. When slicing, the start bound is also included. \nIntegers are valid labels, but they refer to the label and not the position.\n\n\n.loc indexer has multiple access methods like −\n\n\n- A single scalar label\n\n- A list of labels\n\n- A slice object\n\n- A Boolean array\n\n\n**Syntax**-\n\n\n.loc takes two single/list/range operator separated by ','. \n\n\nThe first one indicates the row and the second one indicates columns.\n\n\nBelow are the examples of selecting data using .loc indexer:-","metadata":{}},{"cell_type":"code","source":"# make a copy of dataframe\ndf1 = df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select first row of dataframe\n\ndf1.loc[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#select first five rows for a specific column\n\ndf1.loc[:,'Purchase'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similar examples of selecting data using .loc indexer are as follows:-\n\n\nSelect all rows for multiple columns, say list[]\n\n`df1.loc[:,['Age','Occupation']]`\n\n\n\nSelect first five rows for multiple columns, say list[]\n\n`df1.loc[[0, 1, 2, 3, 4],['Age','Occupation']]`\n\n\n\nSelect range of rows for all columns\n\n`df1.loc[0:4]`\n\n\n\nThe above functionality can also be given by\n\n`df1.head()`","metadata":{}},{"cell_type":"markdown","source":"### Integer position based indexing using .iloc indexer\n\n\nPandas provides **.iloc indexer** for integer position based indexing.\n\n\n.iloc is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. \n.iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing.  Allowed inputs of .iloc indexer are:-\n\n\n- An integer e.g. 5.\n\n\n- A list or array of integers [4, 3, 0].\n\n\n- A slice object with ints 1:7.\n\n\n- A boolean array.","metadata":{}},{"cell_type":"markdown","source":"### Rows selection using .iloc indexer\n\n\nBelow are the examples of row selection using .iloc indexer\n\n\n#### select first row of dataframe\n\n\ndf1.iloc[0]\n\n\n\n#### select second row of dataframe\n\n\ndf1.iloc[1]\n\n\n\n#### select last row of dataframe\n\n\ndf1.iloc[-1]\n\n\n\n#### select second last row of dataframe\n\n\ndf1.iloc[-2]","metadata":{}},{"cell_type":"code","source":"#select first row of dataframe\n\ndf1.iloc[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#select last row of dataframe\n\ndf1.iloc[-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Columns selection using .iloc indexer\n\n\n#### select first column of dataframe\n\n`df1.iloc[:,0]`\n\n\n\n#### select second column of dataframe\n\n`df1.iloc[:,1]`\n\n\n\n#### select last column of dataframe\n\n`df1.iloc[:,-1]`\n\n\n\n#### select second last column of dataframe\n\n`df1.iloc[:,-2]`","metadata":{}},{"cell_type":"markdown","source":"### Multiple rows and columns selection using .iloc indexer\n\n\n\n#### select first five rows of dataframe\n\n`df1.iloc[0:5]`\n\n\n\n#### select first five columns of data frame with all rows\n\n`df1.loc[:, 0:5]`\n\n\n\n\n#### select 1st, 5th and 10th rows with 1st, 4th and 7th columns\n\n`df1.iloc[[0,4,9]], [0,3,6]]`\n\n\n\n\n#### select first 5 rows and 5th, 6th, 7th columns of data frame\n\n`df1.iloc[0:5, 5:8]`","metadata":{}},{"cell_type":"markdown","source":"### Indexing first occurrence of maximum or minimum values with idxmax() and idxmin()\n\n\nPandas provide two functions **idxmax()** and **idxmin()** that return index of first occurrence of maximum or minimum values over requested axis. NA/null values are excluded from the output.","metadata":{}},{"cell_type":"code","source":"# get index of first occurence of maximum Purchase value \n\ndf1['Purchase'].idxmax()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the row with the maximum Purchase value \n\ndf1.loc[df1['Purchase'].idxmax()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Indexing a single value with at() and iat()\n\n\nPandas provides **at()** and **iat()** functions to access a single value for a row and column pair by label or by integer position.","metadata":{}},{"cell_type":"code","source":"# get value at 1st row and Purchase column pair\n\ndf1.at[1, 'Purchase']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get value at 1st row and 11th column pair\n\ndf1.iat[1, 11]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Boolean indexing in pandas\n\n\n\n**Boolean indexing** is the use of boolean vectors to filter and select the data. The operators for boolean indexing are -\n\n\n- 1. | for or, \n\n\n- 2. & for and,\n\n\n- 3. ~ for not. \n\n\nThese must be grouped by using parentheses. Using a boolean vector to index a Series works exactly as in a NumPy ndarray.\n\n\nConditional selections with boolean arrays using **df.loc[selection]** is the most common method to use with Pandas DataFrames. With boolean indexing or logical selection, we can pass an array or Series of True/False values to the .loc indexer to select the rows where the Series has True values. Then, we will make selections based on the values of different columns in dataset.\n\n\nWe can use a boolean True/False series to select rows in a pandas dataframe where there are true values. Then, a second argument can be passed to .loc indexer to select other columns of the dataframe with the same label. The columns are referred to by name for the loc indexer and can be a single string, a list of columns, or a slice \":\" operation.","metadata":{}},{"cell_type":"code","source":"# make a copy of dataframe df\n\ndf2 = df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the purchase amount with a given user_id and product_id\n\ndf2.loc[((df2['User_ID'] == 1000001) & (df2['Product_ID'] == 'P00069042')), 'Purchase']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Indexing with isin() method\n\n\nThe **isin()** method of Series, returns a boolean vector. It is true wherever the Series elements exist in the passed list. This allows you to select rows where one or more columns have values we want to access. The same method is available for Index objects. It is useful for the cases when we don't know which of the sought labels are in fact present.\n\n\nDataFrame also has an **isin()** method. When calling isin, we pass a set of values as either an array or dict. If values is an array, isin returns a DataFrame of booleans that is the same shape as the original DataFrame, with True wherever the element is in the sequence of values.","metadata":{}},{"cell_type":"code","source":"values=[1000001,'P00069042','F',0-17,10,'A',2,0,3,6,14,8370]\n\ndf2_indexed=df2.isin(values)\n\n\ndf2_indexed.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can combine DataFrame's isin with the **any()** and **all()** methods to quickly select subsets of the data that meet a given criteria. We can select a row where each column meets its own criterion as follows:-","metadata":{}},{"cell_type":"code","source":"row_mask = df2.isin(values).any(1)\n\ndf[row_mask]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The where() method and masking\n\n\nWe can select values from a Series with a boolean vector and it returns a subset of the data. To guarantee that the output \nhas the same shape as the original data, we can use the where method in Series and DataFrame.\n\n\nWe can select values from a DataFrame with a boolean criterion. It also preserves input data shape.\n\n\nThe  below code is equivalent to \n\n\n`df2[df2==0]`\n\n\nIt replaces values with `NaN` where the condition is false. \n","metadata":{}},{"cell_type":"code","source":"df2_where=df2.where(df2 == 0)\n\n\n(df2_where).head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Indexing with query() method\n\n\nThere is a **query()** method in the DataFrame objects that allows selection using an expression. This method queries the columns of a DataFrame with a boolean expression.\n","metadata":{}},{"cell_type":"code","source":"df2.query('(Product_Category_1 > Product_Category_2) & (Product_Category_2 > Product_Category_3)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 13. Indexing and reindexing in pandas <a class=\"anchor\" id=\"13\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nReindexing changes the row labels and column labels of a DataFrame. To reindex means to conform the data to match \na given set of labels along a particular axis.\n\n\nMultiple operations can be accomplished through indexing like :−\n\n\n- Reorder the existing data to match a new set of labels.\n\n\n- Insert missing value (NA) markers in label locations where no data for the label existed.","metadata":{}},{"cell_type":"markdown","source":"### Create a new dataframe\n\n\nFirst of all, I will create a new dataframe as follows:- ","metadata":{}},{"cell_type":"code","source":"# let's create a new dataframe \n\nfood = pd.DataFrame({'Place':['Home', 'Home', 'Hotel', 'Hotel'],\n                   'Time': ['Lunch', 'Dinner', 'Lunch', 'Dinner'],\n                   'Food':['Soup', 'Rice', 'Soup', 'Chapati'],\n                   'Price($)':[10, 20, 30, 40]})\n\nfood","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set an index \n\n\nDataFrame has a **set_index()** method which takes a column name (for a regular Index) or a list of column names (for a MultiIndex). This method sets the dataframe index using existing columns.\n\nI will create a new, re-indexed DataFrame with **set_index()** method as follows:-","metadata":{}},{"cell_type":"code","source":"food_indexed1=food.set_index('Place')\n\nfood_indexed1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"food_indexed2=food.set_index(['Place', 'Time'])\n\nfood_indexed2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reset the index\n\n\nThere is a function called **reset_index()** which transfers the index values into the DataFrame’s columns and sets a simple integer index. This is the inverse operation of set_index().","metadata":{}},{"cell_type":"code","source":"food_indexed2.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 14. MultiIndex or advanced indexing <a class=\"anchor\" id=\"14\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nIn this section, I will explore indexing with a MultiIndex and other advanced indexing strategies.\n\n\n\n\n### Hierarchical indexing  or MultiIndex\n\n\n\nThe MultiIndex object is the hierarchical analogue of the standard index object which stores the axis labels in pandas objects. A MultiIndex is an array of tuples where each tuple is unique. A MultiIndex can be created from a list of arrays (using **MultiIndex.from_arrays()**), an array of tuples (using **MultiIndex.from_tuples()**), a crossed set of iterables (using **MultiIndex.from_product()**), or a DataFrame (using **MultiIndex.from_frame()**). The Index constructor will attempt to return a MultiIndex when it is passed a list of tuples.\n\n\nTo demonstrate the concept of hierarchical or multiple indexing, first I will create a hypothetical dataframe as follows:- ","metadata":{}},{"cell_type":"code","source":"sales=pd.DataFrame([['books','online', 200, 50],['books','retail', 250, 75], \n                    ['toys','online', 100, 20],['toys','retail', 140, 30],\n                    ['watches','online', 500, 100],['watches','retail', 600, 150],\n                    ['computers','online', 1000, 200],['computers','retail', 1200, 300],\n                    ['laptops','online', 1100, 400],['laptops','retail', 1400, 500],\n                    ['smartphones','online', 600, 200],['smartphones','retail', 800, 250]],\n                    columns=['Items', 'Mode', 'Price', 'Profit'])\n\n\nsales","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create the hierarchical index in pandas\n\n\nWe can create a hierarchical index in pandas using the **set_index()** function which is used for indexing. First the data is indexed on `Items` and then on `Mode` column as follows:-","metadata":{}},{"cell_type":"code","source":"sales1=sales.set_index(['Items', 'Mode'])\n\nsales1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" The resultant dataframe will be a hierarchical dataframe as shown above.","metadata":{}},{"cell_type":"markdown","source":"### View index in hierarchical index\n\n\nOne can view the details of index as shown below:-","metadata":{}},{"cell_type":"code","source":"# View index\n\nsales1.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Swap the column in hierarchical index\n\n\nNow, I will swap the \"Items\" and \"Mode\" columns in the above hierarchical dataframe as shown below:-","metadata":{}},{"cell_type":"code","source":"# Swap the column  in multiple index\n\nsales2=sales1.swaplevel('Mode', 'Items')\n\nsales2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 15. Sorting in pandas <a class=\"anchor\" id=\"15\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nPandas provides two kinds of sorting. They are:-\n\n\n- 1. Sorting by label\n\n- 2. Sorting by actual value\n\n\nThey are described below:-\n","metadata":{}},{"cell_type":"markdown","source":"### 1. Sorting by label\n\n\nWe can use the **sort_index()** method to sort the object by labels. DataFrame can be sorted by passing the axis arguments and the order of sorting. By default, sorting is done on row labels in ascending order.\n\n\nThe following examples illustrate the idea of sorting by label.","metadata":{}},{"cell_type":"code","source":"# sort the dataframe df2 by label\n\ndf2.sort_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Order of sorting\n\nBy passing the Boolean value to ascending parameter, the order of the sorting can be controlled. \n\n\n\n#### sort the dataframe df2 by label in reverse order\n\n`df2.sort_index(ascending=False)`\n\n\n\n### Sorting by columns\n\n\nBy passing the axis argument with a value 0 or 1, the sorting can be done on the row or column labels. \n\nThe default value of axis=0. In this case, sorting can be done by rows. \n\nIf we set axis=1, sorting is done by columns.\n\n\n#### sort the dataframe df2 by columns\n\n`df2.sort_index(axis=1)`\n","metadata":{}},{"cell_type":"markdown","source":"### 2. Sorting by values\n\n\nThe second method of sorting is sorting by values. Pandas provides **sort_values()** method to sort by values. It accepts a 'by' argument which will use the column name of the DataFrame with which the values are to be sorted.\n\n\nThe following example illustrates the idea:-\n","metadata":{}},{"cell_type":"code","source":"df2.sort_values(by=['Product_Category_1'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Sort by multiple columns\n\n\n`df2.sort_values(by=['Product_Category_1', 'Product_Category_2'])`\n\n\n\n\n#### Sort in descending order\n\n\n`df2.sort_values(by='Product_Category_1', ascending=False)`\n","metadata":{}},{"cell_type":"markdown","source":"# 16. Categorical data in pandas <a class=\"anchor\" id=\"16\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nWe can check the data types of variables in the dataset with the following command:-","metadata":{}},{"cell_type":"code","source":"df3 = df.copy()\n\ndf3.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that our dataset has 5 categorical variables. They are **Product_ID**, **Gender**, **Age**, **City_Category** and\n**Stay_In_Current_City_Years**. They have data types as **object**.\n\nNow, I will explore these categorical variables.","metadata":{}},{"cell_type":"markdown","source":"###  Description of categorical data\n\n\nThe **describe()** method on categorical data will produce similar output to a Series or DataFrame of type string.","metadata":{}},{"cell_type":"code","source":"df3['Gender'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `Gender` category has 537577 counts, 2 unique values and frequency of top value M is 405380.","metadata":{}},{"cell_type":"code","source":"df3['Age'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 7 unique categories in `Age` variable. The most frequent category is `26-35` with frequency count of 214690.","metadata":{}},{"cell_type":"code","source":"df3['City_Category'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 3 unique categories in  `City_Category` variable. The most frequent category is `B` with frequency count of 226493.","metadata":{}},{"cell_type":"markdown","source":"### Working with categorical data\n\n\nCategorical data has a categories and a ordered property, which list their possible values and whether the ordering matters or not. These properties are exposed as `s.cat.categories` and `s.cat.ordered`. \n\nIf we don't manually specify categories and ordering, they are inferred from the passed arguments.\n\n\n`s.cat.categories`\n\n`s.cat.ordered`\n\nwhere `s` is a series object.","metadata":{}},{"cell_type":"markdown","source":"### Unique values in categorical data\n\n\nWe can get the unique values in a series object by **unique()** method. It returns categories in the order of appearance, \nand it only includes values that are actually present.","metadata":{}},{"cell_type":"code","source":"df3['Gender'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3['Age'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Rename categories\n\n\nRenaming categories is done by assigning new values to the `Series.cat.categories` property or by using the `rename_categories()` method.\n\n\n\n\n### Append new categories\n\n\nAppending categories can be done by using the `add_categories()` method.\n\n\n\n\n### Remove categories\n\n\nRemoving categories can be done by using the `remove_categories()` method. Values which are removed are replaced by np.nan.\n\n\n\n\n### Setting categories\n\n\nIf we want to remove and add new categories in one step (which has some speed advantage), or simply set the categories to a predefined scale, we can use `set_categories()` method.\n\n\n\n\n### Reordering categories\n\n\nReordering the categories is possible via the `Categorical.reorder_categories()` and the `Categorical.set_categories()` methods. \n\n\n\n### Operations on categorical data \n\n\nThere are several operations like `Series.min()`, `Series.max()`, `Series.median()` and `Series.mode()` which are possible with categorical data. ","metadata":{}},{"cell_type":"markdown","source":"### Frequency counts of categorical data\n\n\nSeries methods like `Series.value_counts()` will return the frequency counts of the categories present in the series.","metadata":{}},{"cell_type":"code","source":"df3['Gender'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3['City_Category'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Series.value_counts()` will return the frequency counts of the categories in descending order. To get the categories in \nascending order we should set `ascending=True` as follows:-","metadata":{}},{"cell_type":"code","source":"df3['Gender'].value_counts(ascending=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3['City_Category'].value_counts(ascending=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 17. Basic functionality in pandas <a class=\"anchor\" id=\"17\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n\n### Series basic functionality\n\n\n\nThe following table lists the important attributes or methods in Series basic functionality.\n\n\n\n- **axes** - Returns a list of the row axis labels\n\n\n- **dtype** - Returns the dtype of the object.\n\n\n- **empty** - Returns True if series is empty.\n\n\n- **ndim** - Returns the number of dimensions of the underlying data, by definition 1.\n\n\n- **size** - Returns the number of elements in the underlying data.\n\n\n- **values** - Returns the Series as ndarray.\n\n\n- **head()** - Returns the first n rows.\n\n\n- **tail()** - Returns the last n rows.\n\n\n\n\n### Dataframe basic functionality\n\n\n\nThe following tables lists the important attributes or methods in Dataframe basic functionality.\n\n\n\n- **T** - Transposes rows and columns.\n\n\n- **axes** - Returns a list with the row axis labels and column axis labels as the only members.\n\n\n- **dtypes** - Returns the dtypes in this object.\n\n\n- **empty** -  True if NDFrame is entirely empty [no items]; if any of the axes are of length 0.\n\n\n- **ndim** -  Number of axes / array dimensions.\n\n\n- **shape** -  Returns a tuple representing the dimensionality of the Dataframe.\n\n\n- **size** - Number of elements in the NDFrame.\n\n\n- **values** - Numpy representation of NDFrame.\n\n\n\n- **head()** - Returns the first n rows.\n\n\n- **tail()** - Returns last n rows.","metadata":{}},{"cell_type":"markdown","source":"# 18. Descriptive statistics in pandas <a class=\"anchor\" id=\"18\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n\nThere exists a large number of methods for computing descriptive statistics and other related operations on Series, DataFrame, and Panel. Most of these are aggregations (hence producing a lower-dimensional result) like sum(), mean(), and quantile(), but some of them, like cumsum() and cumprod(), produce an object of the same size. Generally speaking, these methods take an axis argument, just like ndarray.{sum, std, …}, but the axis can be specified by name or integer.\n\n\n- Series: no axis argument needed.\n\n\n- DataFrame: “index” (axis=0, default), “columns” (axis=1).\n\n\n- Panel: “items” (axis=0), “major” (axis=1, default), “minor” (axis=2).\n\n\n\n### Functions and description\n\n\nThe following table list down the important functions under Descriptive Statistics in Python Pandas. \n\n\n\n- 1   **count()** -\tNumber of non-null observations\n\n\n- 2\t  **sum()**\t  - Sum of values\n\n\n- 3\t  **mean()**  -\tMean of values\n\n\n- 4\t **median()** -\tMedian of values\n\n\n- 5\t **mode()**  -\tMode of values\n\n\n- 6\t **std()**   -\tStandard deviation of the values\n\n\n- 7\t **min()**   -\tMinimum value\n\n\n- 8\t **max()**   -\tMaximum value\n\n\n- 9\t **abs()**   -\tAbsolute value\n\n\n- 10 **prod()**  -\tProduct of values\n\n\n- 11 **cumsum()** -\tCumulative sum\n\n\n- 12 **cumprod()** - Cumulative product\n\n\n\nThe dataframe is a heterogeneous data structure. So, the different column values have different data types. Generic operations don't work with all functions.\n\n\nFunctions like **sum()**, **cumsum()** work with both numeric and character (or) string data elements without any error. \nIn practice, character aggregations are never used generally. These functions do not throw any exception.\n\n\nFunctions like **abs()**, **cumprod()** throw exception when the dataframe contains character or string data because such operations cannot be performed.\n","metadata":{}},{"cell_type":"code","source":"df4=df.copy()\n\ndf4.max(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summarizing data\n\n\n\nThe **describe()** function computes the summary statistics of the numerical columns in the dataframe.\n\n\n\nThis function gives the mean, std and IQR values. It excludes the character columns and gives summary about numeric columns. \nIt includes the argument which is used to pass necessary information regarding what columns need to be considered for summarizing. It takes the list of values; by default, 'number'.\n\n\n- object − Summarizes string columns\n\n\n- number − Summarizes numeric columns\n\n\n- all − Summarizes all columns together","metadata":{}},{"cell_type":"code","source":"df4.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 19. Statistical functions in pandas <a class=\"anchor\" id=\"19\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n\nStatistical functions help us to understand and analyze the behavior of data. In this section, I will discuss few statistical functions, which we can apply on Pandas objects.\n\n\n\n### Percent_change\n\n\nSeries, datFrames and panel, all have the function **pct_change()**. This function compares every element with its prior element and computes the change percentage.\n\nBy default, the pct_change() operates on columns; if you want to apply the same row wise, then use axis=1() argument.\n\n\n\n### Covariance\n\n\nCovariance is applied on series data. The series object has a method **cov()** to compute covariance between series objects. NA values will be excluded automatically.\n\n\n**Series.cov()** can be used to compute covariance between series (excluding missing values).\n\n\nAnalogously, **dataFrame.cov()** to compute pairwise covariances among the series in the dataFrame, also excluding NA/null values.\n","metadata":{}},{"cell_type":"code","source":"df5=df.copy()\n\n\n# view the covariance\n\ndf5.cov()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation\n\n\n**Correlation** shows the linear relationship between any two array of values (series). There are multiple methods to compute the correlation. These methods are listed below:-\n\n\n\n**Method name**  \t  **Description**\n\n\n- pearson (default)\t-  Standard correlation coefficient\n\n\n- kendall           -  Kendall Tau correlation coefficient\n\n\n- spearman\t        -  Spearman rank correlation coefficient\n\n\n\nAll of these are currently computed using pairwise complete observations.\n\n\nAny non-numeric columns will be automatically excluded from the correlation calculation.","metadata":{}},{"cell_type":"code","source":"# view the correlation\n\ndf5.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Ranking\n\n\nData Ranking produces ranking for each element in the array of elements. In case of ties, assigns the mean rank. \n\n\nThe **rank()** method produces a data ranking with ties being assigned the mean of the ranks (by default) for the group.\n\n\nThe **rank()** is also a dataframe method and can rank either the rows (axis=0) or the columns (axis=1). NaN values are excluded from the ranking.\n\n\nIt optionally takes a parameter ascending which true by default. If it is set to false, data is ranked in descending order, with larger values assigned a smaller rank.\n\n\nThe **rank()** supports different tie-breaking methods, specified with the method parameter as follows:-\n\n\n- **average** - average rank of tied group\n\n\n- **min** - lowest rank in the group\n\n\n- **max** - highest rank in the group\n\n\n- **first** - ranks assigned in the order they appear in the array","metadata":{}},{"cell_type":"code","source":"# view the top 25 rows of ranked dataframe\n\ndf5.rank(1).head(25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Common statistical functions\n\n\nThere are a number of common statistical functions. These are listed below:-\n\n\n\n**Method**    -  **Description**\n\n\n- **count()** - Number of non-null observations\n\n\n- **sum()** -   Sum of values\n\n\n- **mean()** -  Mean of values\n\n\n- **median()** - Arithmetic median of values\n\n\n- **min()** -\tMinimum\n\n\n- **max()** - \tMaximum\n\n\n- **std()** -\tStandard deviation\n\n\n- **var()** -\tVariance\n\n\n- **skew()** -\tSkewness\n\n\n- **kurt()** -\tKurtosis\n\n\n- **quantile()** -\tQuantile\n\n\n- **apply()** -\tGeneric apply\n\n\n- **cov()** -\tCovariance\n\n\n- **corr()** -\tCorrelation\n\n\n\nThe **apply()** function takes an extra **func** argument and performs generic rolling computations. The **func** argument should be a single function that produces a single value from an ndarray input.","metadata":{}},{"cell_type":"markdown","source":"# 20. Window functions in pandas <a class=\"anchor\" id=\"20\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nFor working with numerical data, Pandas provide few variants like **rolling**, **expanding** and **exponentially moving weights** for window statistics. \n\n\nAmong these are **count**, **sum**, **mean**, **median**, **correlation**, **variance**, **covariance**, **standard deviation**, **skewness** and **kurtosis**.\n\n\nThe **rolling()** and **expanding()** functions can be used directly from DataFrameGroupBy objects.\n\n\nIn this section, we work with rolling, expanding and exponentially weighted data through the corresponding objects, **Rolling**, **Expanding** and **EWM**.","metadata":{}},{"cell_type":"markdown","source":"### rolling() function\n\n\nThis function can be applied on a series of data. Specify the **window=n** argument and apply the appropriate statistical function on top of it.\n\n\n`df6=df.copy()`\n\n\n`df6.rolling(window=3).mean()`\n\n\nSince the window size is 3, for first two elements there are nulls and from third the value will be the average of the n, n-1 and n-2 elements. We can also apply various functions.\n\n\n\n### expanding() function\n\n\nThis function can be applied on a series of data. We specify the **min_periods=n** argument and apply the appropriate statistical function on top of it.\n\n\n`df6.expanding(min_periods=3).mean()`\n\n\n\n\n### ewm() function\n\n\n**ewm** is applied on a series of data. We have to specify any of the com, span, halflife argument and apply the appropriate statistical function on top of it. It assigns the weights exponentially.\n\n\n`df6.ewm(com=0.5).mean()`\n\n\n\nWindow functions are used in finding the trends within the data graphically by smoothing the curve. If there is a lot of variation in the data, then we can apply window functions to smooth out the curve or the trend.\n","metadata":{}},{"cell_type":"markdown","source":"# 21. Aggregations in pandas <a class=\"anchor\" id=\"21\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nOnce the rolling, expanding and ewm objects are created, several methods are available to perform aggregations on data.\n\n\n### Apply aggregation on a whole dataframe\n\n\n`df6=df.copy`\n\n\n`df6.aggregate(np.sum)`\n\n\n\n### Apply aggregation on a single column of a dataframe\n","metadata":{}},{"cell_type":"code","source":"df6=df.copy()\n\n\ndf6['Purchase'].aggregate(np.sum)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply multiple functions on a single column of a dataframe","metadata":{}},{"cell_type":"code","source":"df6['Purchase'].aggregate([np.sum, np.mean])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply aggregation on multiple columns of a dataframe","metadata":{}},{"cell_type":"code","source":"df6[['Product_Category_1', 'Product_Category_2', 'Product_Category_3']].aggregate(np.mean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply multiple functions on multiple columns of a dataframe","metadata":{}},{"cell_type":"code","source":"df6[['Product_Category_1', 'Product_Category_2', 'Product_Category_3']].aggregate([np.sum, np.mean])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Apply different functions to different columns of a dataframe","metadata":{}},{"cell_type":"code","source":"df6.aggregate({'Product_Category_1' : np.sum ,'Product_Category_2' : np.mean})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 22. Iteration in pandas <a class=\"anchor\" id=\"22\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n\nThe behavior of basic iteration over Pandas objects depends on the type. When iterating over a Series, it is regarded as \narray-like, and basic iteration produces the values. Other data structures, like DataFrame and Panel, follow the **dict-like** convention of iterating over the **keys** of the objects.\n\n\n\nIterating a dataframe gives column names.\n\n\n\nTo iterate over the rows of the DataFrame, we can use the following functions −\n\n\n\n- **iteritems()** − to iterate over the (key,value) pairs\n\n\n- **iterrows()** − iterate over the rows as (index,series) pairs\n\n\n- **itertuples()** − iterate over the rows as namedtuples","metadata":{}},{"cell_type":"markdown","source":"# 23. Function application in pandas <a class=\"anchor\" id=\"23\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n\nThere are three important methods that enable us to apply our own or another library's functions to pandas objects. These methods differentiate on their scope of usage. These functions expect to operate on an entire dataframe, row- or column-wise\noperation, or element wise operation. These methods are described below:-\n\n\n- Table wise Function Application: **pipe()**\n\n\n\n- Row or Column Wise Function Application: **apply()**\n\n\n\n- Element wise Function Application: **applymap()**\n","metadata":{}},{"cell_type":"markdown","source":"### Table-wise Function Application:pipe()\n\n\nCustom operations can be performed by passing the function and the appropriate number of parameters as pipe arguments. Thus, operation is performed on the whole DataFrame.\n\n\nFor example, if we want to add a value 10 to all the elements in the DataFrame. Then, we can make use of **pipe()** function \nas follows:-\n\n\n`def addten(x1,x2):`\n\n\n    `return x1+x2`\n   \n\n`df7=df.copy()` \n\n\n`df7.pipe(addten,10)`","metadata":{}},{"cell_type":"markdown","source":"### Row or Column Wise Function Application: apply()\n\n\nArbitrary functions can be applied along the axes of a DataFrame or Panel using the **apply()** method. It takes an optional axis argument. By default, the operation performs column wise, taking each column as an array-like.\n\n\n`df7.apply(np.mean)`\n\n\nBy passing axis parameter, operations can be performed row wise.\n\n\n`df7.apply(np.mean,axis=1)`\n\n\n`df.apply(lambda x: x.max() - x.min())`","metadata":{}},{"cell_type":"markdown","source":"### Element Wise Function Application: applymap()\n\n\n\nThe methods **applymap()** on dataframe and analogously **map()** on series accept any Python function. It takes a single value and returns a single value.\n\n\n`df7.applymap(lambda x:x*100)`\n","metadata":{}},{"cell_type":"markdown","source":"# 24. Pandas GroupBy operations <a class=\"anchor\" id=\"24\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nA groupby operation involves one of the following operations on the original object. They are as follows :−\n\n\n- **Splitting** the Object\n\n\n- **Applying** a function\n\n\n- **Combining** the results\n\n\n\nThe split step is the most straightforward out of these. In many situations, we may wish to split the data set into groups \nand perform operations on those groups.\n\n\nIn the apply functionality, we can perform the following operations :−\n\n\n\n- **Aggregation** − compute a summary statistic (or statistics) for each group. Some examples are :- \n\n     - Compute group sums or means.                  \n                  \n     - Compute group sizes / counts.\n\n\n\n- **Transformation** − perform some group-specific computations and return a like-indexed object. Some examples are :-\n\n    - Standardize data (zscore) within a group.\n    \n    - Filling NAs within groups with a value derived from each group.\n\n\n\n- **Filtration** − discarding the data with some condition.  Some examples are :-\n\n    - Discard data that belongs to groups with only a few members.\n    \n    - Filter out data based on the group sum or mean.\n    \n    \n    \n- Some combination of the above: **GroupBy** will examine the results of the apply step and try to return a sensibly combined result if it doesn't fit into either of the above two categories.","metadata":{}},{"cell_type":"markdown","source":"### Split Data into Groups\n\n\nPandas object can be split into any of their objects. There are multiple ways to split an object as follows :-\n\n\n- obj.groupby('key')\n\n\n- obj.groupby(['key1','key2'])\n\n\n- obj.groupby(key,axis=1)\n\n\nThe following example illustrates the idea:-","metadata":{}},{"cell_type":"code","source":"df8=df.copy()\n\ndf8.groupby('Gender')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view groups of Gender column\n\ndf8.groupby('Gender').groups","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Group by with multiple columns\n\n\n`df8.groupby(['Gender', 'Age']).groups`\n\n\n\n\n### Iterate through groups\n\n\nWith the groupby object in hand, we can iterate through the object similar to itertools.obj.\n\n\n`df8_grouped = df8.groupby('Gender')`\n\n\n`for Age, Occupation in df8_grouped:`\n\n   `print Age`\n   \n   `print Occupation`\n   \n   \n   \n   \n   \n### Select a group with get_group() method\n\n\nUsing the **get_group()** method, we can select a single group.\n\n\n`df8_grouped = df8.groupby('City_Category')`\n\n\n`print(df8_grouped.get_group('A')`","metadata":{}},{"cell_type":"markdown","source":"### Aggregation functions with groupby\n\n\nAn aggregation function returns a single aggregated value for each group. Once the group by object is created, several aggregation operations can be performed on the grouped data as follows:-","metadata":{}},{"cell_type":"code","source":"# apply aggregation function sum with groupby\n\ndf8.groupby('Gender').sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alternative way to apply aggregation function sum\n\ndf8.groupby('Gender').agg(np.sum)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another way to see the size of each group is by applying the **size()** function as follows:-","metadata":{}},{"cell_type":"code","source":"# attribute access in python pandas\n\ndf8_grouped = df8.groupby('Gender')\n\nprint(df8_grouped.agg(np.size))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying multiple aggregation functions at once\n\n\nWith grouped Series, you can also pass a list or dict of functions to do aggregation with, and generate DataFrame as output as \nfollows:-","metadata":{}},{"cell_type":"code","source":"df8.groupby('Gender')['Purchase'].agg([np.sum, np.mean])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transformations\n\n\nTransformation on a group or a column returns an object that is indexed the same size of that is being grouped. \nThus, the transform should return a result that is the same size as that of a group chunk.","metadata":{}},{"cell_type":"code","source":"df9=df.copy()\n\n\nscore = lambda x: (x - x.mean()) / x.std()*10\n\n\nprint(df9.groupby('Gender')['Purchase'].transform(score).head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filtration\n\n\nFiltration filters the data on a defined criteria and returns the subset of data. The **filter()** function is used to filter the data.","metadata":{}},{"cell_type":"code","source":"df10=df.copy()\n\n\ndf10.groupby('Gender').filter(lambda x: len(x) > 4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 25. Pandas merging and joining <a class=\"anchor\" id=\"25\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nPandas has full-featured, high performance in-memory join operations that are very similar to relational databases like SQL. These methods perform significantly better than other open source implementations like base::merge.data.frame in R. The reason for this is careful algorithmic design and the internal layout of the data in DataFrame.\n\n\nPandas provides a single function, **merge**, as the entry point for all standard database join operations between DataFrame objects.\n\n\nThe syntax of the merge function is as follows:-\n\n\n\n`pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=True)`\n\n\n\nThe description of the parameters used is as follows−\n\n\n- **left** − A DataFrame object.\n\n\n- **right** − Another DataFrame object.\n\n\n- **on** − Columns (names) to join on. Must be found in both the left and right DataFrame objects.\n\n\n- **left_on** − Columns from the left DataFrame to use as keys. Can either be column names or arrays with length equal to the length of the DataFrame.\n\n\n- **right_on** − Columns from the right DataFrame to use as keys. Can either be column names or arrays with length equal to the length of the DataFrame.\n\n\n- **left_index** − If True, use the index (row labels) from the left DataFrame as its join key(s). In case of a DataFrame with a MultiIndex (hierarchical), the number of levels must match the number of join keys from the right DataFrame.\n\n\n- **right_index** − Same usage as left_index for the right DataFrame.\n\n\n- **how** − One of 'left', 'right', 'outer', 'inner'. Defaults to inner. \n\n\n- **sort** − Sort the result DataFrame by the join keys in lexicographical order. Defaults to True, setting to False will improve the performance substantially in many cases.\n\n\nNow, I will create two different DataFrames and perform the merging operations on them as follows:-","metadata":{}},{"cell_type":"code","source":"# let's create two dataframes\n\nbatsmen = pd.DataFrame({\n   'id':[1,2,3,4,5],\n   'Name': ['Rohit', 'Dhawan', 'Virat', 'Dhoni', 'Kedar'],\n   'subject_id':['sub1','sub2','sub4','sub6','sub5']})\n\nbowler = pd.DataFrame(\n   {'id':[1,2,3,4,5],\n   'Name': ['Kumar', 'Bumrah', 'Shami', 'Kuldeep', 'Chahal'],\n   'subject_id':['sub2','sub4','sub3','sub6','sub5']})\n\n\nprint(batsmen)\n\n\nprint(bowler)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge two dataframes on a key\n\npd.merge(batsmen, bowler, on='id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge two dataframes on multiple keys\n\npd.merge(batsmen, bowler, on=['id', 'subject_id'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merge using 'how' argument\n\n\n\nThe **how** argument to merge specifies how to determine which keys are to be included in the resulting table. If a key combination does not appear in either the left or the right tables, the values in the joined table will be **NA**.\n\n\nHere is a summary of the how options and their SQL equivalent names −\n\n\n\n- **Merge Method** -\t**SQL Equivalent**\t-  **Description**\n\n\n-  left            -     LEFT OUTER JOIN\t-   Use keys from left object\n\n\n- right\t           -     RIGHT OUTER JOIN\t-   Use keys from right object\n\n\n- outer\t           -     FULL OUTER JOIN\t-   Use union of keys\n\n\n- inner\t           -     INNER JOIN\t        -   Use intersection of keys","metadata":{}},{"cell_type":"code","source":"# left join\n\npd.merge(batsmen, bowler, on='subject_id', how='left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# right join\n\npd.merge(batsmen, bowler, on='subject_id', how='right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# outer join\n\npd.merge(batsmen, bowler, on='subject_id', how='outer')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inner join\n\npd.merge(batsmen, bowler, on='subject_id', how='inner')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 26. Pandas concatenation operation <a class=\"anchor\" id=\"26\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n\nPandas provides various facilities for easily combining together Series, DataFrame, and Panel objects.\n\n\nThe **concat()** function does all of the heavy lifting of performing concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes.\n\n\nThe syntax of the **concat()** function is as follows:-\n\n\n\n`pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)`\n\n\n\nThe description of the arguments is as follows:-\n\n\n\n- **objs** − This is a sequence or mapping of Series, DataFrame, or Panel objects.\n\n\n- **axis** − {0, 1, ...}, default 0. This is the axis to concatenate along.\n\n\n- **join** − {'inner', 'outer'}, default 'outer'. How to handle indexes on other axis(es). Outer for union and inner for intersection.\n\n\n- **ignore_index** − boolean, default False. If True, do not use the index values on the concatenation axis. The resulting axis will be labeled 0, ..., n - 1.\n\n\n- **join_axes** − This is the list of index objects. Specific indexes to use for the other (n-1) axes instead of performing inner/outer set logic.\n\n\n- **keys** : sequence, default None. Construct hierarchical index using the passed keys as the outermost level. If multiple levels passed, should contain tuples.\n\n\n- **levels** : list of sequences, default None. Specific levels (unique values) to use for constructing a MultiIndex. Otherwise they will be inferred from the keys.\n\n\n- **names** : list, default None. Names for the levels in the resulting hierarchical index.\n\n\n- **verify_integrity** : boolean, default False. Check whether the new concatenated axis contains duplicates. This can be very expensive relative to the actual data concatenation.\n\n\n- **copy** : boolean, default True. If False, do not copy data unnecessarily.\n\n\nNow, I will create two dataframes and do concatenation:-","metadata":{}},{"cell_type":"code","source":"# let's create two dataframes\n\nbatsmen = pd.DataFrame({\n   'id':[1,2,3,4,5],\n   'Name': ['Rohit', 'Dhawan', 'Virat', 'Dhoni', 'Kedar'],\n   'subject_id':['sub1','sub2','sub4','sub6','sub5']})\n\nbowler = pd.DataFrame(\n   {'id':[1,2,3,4,5],\n   'Name': ['Kumar', 'Bumrah', 'Shami', 'Kuldeep', 'Chahal'],\n   'subject_id':['sub2','sub4','sub3','sub6','sub5']})\n\n\nprint(batsmen)\n\n\nprint(bowler)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenate the dataframes\n\n\nteam=[batsmen, bowler]\n\npd.concat(team)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# associate keys with the dataframes\n\npd.concat(team, keys=['x', 'y'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the index of the resultant dataframe is duplicated. So each index is repeated.\n\nIf the resultant object has to follow its own indexing, we can set **ignore_index** option to True as follows:-","metadata":{}},{"cell_type":"code","source":"pd.concat(team, keys=['x', 'y'], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the index changes completely and the Keys are also overridden.","metadata":{}},{"cell_type":"markdown","source":"If two objects need to be added along axis=1, then the new columns will be appended as follows:-","metadata":{}},{"cell_type":"code","source":"pd.concat(team, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Concatenating using append\n\n\nA useful shortcut to concat are the append instance methods on Series and DataFrame. These methods actually predated concat. They concatenate along axis=0, namely the index as follows:−","metadata":{}},{"cell_type":"code","source":"batsmen.append(bowler)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 27. Reshaping by melt and pivot <a class=\"anchor\" id=\"27\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n\n### Melt creates wide-to-long format dataframe\n\n\n\nWhen we take a closer look at our original dataframe, we can see that our dataset is not in the tidy data format.\n\nThe columns `Product_Category_1`, `Product_Category_2` and `Product_Category_3` contain values of product_category rather than variables. We should reorganize our dataframe into tidy data format.\n\nThe **melt()** function is useful to convert a DataFrame from **wide-to-long** format where one or more columns are identifier variables, while all other columns are considered measured variables. The measured variables are then \"unpivoted\" to the row axis, leaving non-identifier columns, \"variable\" and \"value\". The names of those columns can be customized by supplying the var_name and value_name parameters.\n\nWe can convert our dataset into long data format using the **melt()** function as follows:-","metadata":{}},{"cell_type":"code","source":"df11=df.copy()\n\ndf11.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df12=(pd.melt(frame=df11, id_vars=['User_ID','Product_ID', 'Gender','Age','Occupation','City_Category',\n                             'Marital_Status','Purchase'],                          \n                    value_vars=['Product_Category_1','Product_Category_2','Product_Category_3'], \n                    var_name='Product_Category', value_name='Amount'))\n\ndf12.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pivot creates long-to-wide format dataframe\n\n\nI have melt three columns `Product_Category_1`, `Product_Category_2` and `Product_Category_3` into a single column named\n`Product_Category` with **melt()** function. So, I have converted the above dataframe from wide to long format.\n\n\nNow, I will convert the above column `Product_Category` from long to wide format with **pivot()** function. \n**pivot()** function takes 3 arguments with the following names - index, columns, and values. As a value for each of these parameters we need to specify a column name in the original table. Then the **pivot()** function will create a new table, \nwhose row and column indices are the unique values of the respective parameters. The cell values of the new table are taken \nfrom column given as the values parameter.\n\n\nThis is illustrated below:-","metadata":{}},{"cell_type":"code","source":"df13=df12[['Product_Category', 'Amount']]\n\ndf14=df13.pivot(index=None, columns='Product_Category', values='Amount')\n\ndf14.head(25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reshaping with pivot_table function\n\n\nBefore calling the pivot() function, we need to ensure that our dataset does not have rows with duplicate values for the specified columns. If there are duplicate entries for rows in the dataset, the pivot() function, will throw a value error.\n\nIn this case, the **pivot_table()** method comes to rescue. It works like pivot, but it aggregates the values from rows with duplicate entries for the specified columns. The syntax of the pivot_table() function is given below:-\n\n\n`df.pivot_table(values=None, index=None, columns=None, aggfunc='mean', fill_value=None, \nmargins=False, dropna=True, margins_name='All')`","metadata":{}},{"cell_type":"markdown","source":"# 28. Reshaping by stacking and unstacking <a class=\"anchor\" id=\"28\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nThere are two other methods called **stack()** and **unstack()** which closely resemble the **pivot()** method. These methods are designed to work together with multiindex objects. The functionality of these methods is described below:-\n\n\n\n### Stacking\n\n\nStacking means \"pivot\" a level of the (possibly hierarchical) column labels, returning a DataFrame with an index with a new inner-most level of row labels. So. stacking a dataframe means moving or pivoting the innermost column index to become the innermost row index. \n\n\nIt return a reshaped dataframe or series having a multi-level index with one or more new inner-most levels compared to the current dataframe. The new inner-most levels are created by pivoting the columns of the current dataframe.\n\n\n\n- if the columns have a single level, the output is a Series.\n\n\n- if the columns have multiple levels, the new index level(s) is (are) taken from the prescribed level(s) and the output is a DataFrame.\n\n\nIn this case, we look at a dataframe with single level hierarchical indices on both axes. Stacking takes the most-inner column index (height, weight), makes it the most inner row index and reshuffles the cell values accordingly. ","metadata":{}},{"cell_type":"code","source":"cols=pd.MultiIndex.from_tuples([('weight', 'kg'), ('weight', 'pounds')])\n\ndf15=pd.DataFrame([[75,165], [60, 132]],\n                 index=['husband', 'wife'],\n                 columns=cols)\n\ndf15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df16=df15.stack()\n\ndf16","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unstacking\n\n\nIt is the inverse operation of stacking. It means \"pivot\" a level of the (possibly hierarchical) row index to the column axis, producing a reshaped dataframe with a new inner-most level of column labels.\n\n\nI will convert the stacked dataframe df16 back to original form as follows:-","metadata":{}},{"cell_type":"code","source":"df16.unstack()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 29. Options and customization with pandas <a class=\"anchor\" id=\"29\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n\nPandas provide API to customize some aspects of its behavior. In most cases, we would like to adjust the display related options.\n\n\nThe API is composed of five relevant functions. They are as follows :−\n\n\n- 1. **get_option()**\n\n\n- 2. **set_option()**\n\n\n- 3. **reset_option()**\n\n\n- 4. **describe_option()**\n\n\n- 5. **option_context()**\n\n\nLet us now understand how the functions operate.","metadata":{}},{"cell_type":"markdown","source":"### 1. get_option(param)\n\n\n\n**get_option()** takes a single parameter and returns the value as given in the output below −","metadata":{}},{"cell_type":"code","source":"# display maximum rows\n\npd.get_option(\"display.max_rows\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display maximum columns\n\npd.get_option(\"display.max_columns\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. set_option(param,value)\n\n\n**set_option()** takes two arguments and sets the value to the parameter as shown below −","metadata":{}},{"cell_type":"code","source":"# set maximum rows\n\npd.set_option(\"display.max_rows\", 80)\n\npd.get_option(\"display.max_rows\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set maximum columns\n\npd.set_option(\"display.max_columns\", 30)\n\npd.get_option(\"display.max_columns\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. reset_option(param)\n\n\n**reset_option()** takes an argument and sets the value back to the default value.","metadata":{}},{"cell_type":"code","source":"# display maximum rows\n\npd.reset_option(\"display.max_rows\")\n\npd.get_option(\"display.max_rows\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display maximum columns\n\npd.reset_option(\"display.max_columns\")\n\npd.get_option(\"display.max_columns\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. describe_option(param)\n\n\n**describe_option()** prints the description of the argument.","metadata":{}},{"cell_type":"code","source":"# description of the display maximum rows parameter\n\npd.describe_option(\"display.max_rows\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. option_context()\n\n\n**option_context()** context manager is used to set the option in with statement temporarily. Option values are restored automatically when you exit with block.","metadata":{}},{"cell_type":"code","source":"# set the parameter value with option_context\n\nwith pd.option_context(\"display.max_rows\",10):\n   print(pd.get_option(\"display.max_rows\"))\n   print(pd.get_option(\"display.max_rows\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a difference between the first and the second print statements. The first statement prints the value set by **option_context()** which is temporary within the with context itself. After the with context, the second print statement prints the configured value.","metadata":{}},{"cell_type":"markdown","source":"# 30. Summary and conclusion <a class=\"anchor\" id=\"30\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n- In this kernel, I have explored pandas and important data analysis tools of pandas. \n\n- I have used the Black Friday dataset and explore various functionalities offered by pandas.\n\n- I have shed light on important functionalities of pandas like **aggregations in pandas**, **iteration in pandas**, **Pandas GroupBy operations**, **Pandas merging and joining**.\n\n- I have also discussed **Pandas concatenation operation**, **Reshaping by melt and pivot** and **Reshaping by stacking and unstacking**.\n\n- I have also discussed **basic functionality in Pandas**, **descriptive statistics in Pandas** and **statistical functions in Pandas**.\n\n- Lastly, I have discussed **options and customization options with Pandas**.\n\n","metadata":{}},{"cell_type":"markdown","source":"[Go to Top](#0)","metadata":{}}]}