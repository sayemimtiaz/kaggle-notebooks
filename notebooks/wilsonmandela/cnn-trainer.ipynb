{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Convolutional Neural Network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport h5py\nimport librosa\nimport itertools\nfrom copy import copy\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nimport warnings  \nwith warnings.catch_warnings():  \n    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Add\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import PReLU\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import GlobalMaxPooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG = '../input/gtzan-dataset-music-genre-classification/Data/images_original'\n#IMG = './dataset/'\nimg_dataset = []\ngenre_target = []\ngenres = {}\nclasses = []\ni = 0\nfor root, dirs, files in os.walk(IMG):\n    for name in files:\n        filename = os.path.join(root, name)\n        img_dataset.append(filename)\n        genre = filename.split('/')[-2]\n        genre_target.append(genre)\n        \n        if(genre not in genres):\n            classes.append(genre)\n            genres[genre] = i\n            i+=1\n\nimg = cv2.imread(img_dataset[0],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_borders(img,x1=35,x2=252,y1=54,y2=389):\n    cropped = img[x1:x2,y1:y2]\n    return cropped\n\ndef get_y():\n    '''Convierte los generos en un array de targets y'''\n    y = []\n    for genre in genre_target:\n        n = genres[genre]\n        y.append(n)\n    return np.array(y)\n\ndef get_x(shape=[999,217,335], flag=1):\n    x = np.empty(shape, np.uint8)\n    for i in range(len(img_dataset)):\n        img = cv2.imread(img_dataset[i],flag)\n        img = crop_borders(img)\n        x[i] = img\n    return np.array(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread(img_dataset[0])\nimg = crop_borders(img)\n\nimg.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = get_x(shape=[999,img.shape[0], img.shape[1], img.shape[2]]) #Imagenes en color, RGB -> 3 canales\ny = get_y()\n\nm = len(y)\nnum_labels = 10 #estilos de musica diferente\n\nprint(X.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n\ny = to_categorical(y)\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv_block(x, n_filters, pool_size=(2, 2)):\n    x = Conv2D(n_filters, (3, 3), strides=(1, 1), padding='same')(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D(pool_size=pool_size, strides=pool_size)(x)\n    x = Dropout(0.25)(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Definition\ndef create_model(input_shape, num_genres):\n    inpt = Input(shape=input_shape)\n    x = conv_block(inpt, 16)\n    x = conv_block(x, 32)\n    x = conv_block(x, 64)\n    x = conv_block(x, 128)\n    x = conv_block(x, 256)\n    \n    # Classifier with MLP (MultiLayerPerceptron)\n    x = Flatten()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n    x = Dropout(0.25)(x)\n    predictions = Dense(num_genres,activation='softmax',kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n    \n    model = Model(inputs=inpt, outputs=predictions)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model(X_train[0].shape, num_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=tf.keras.losses.categorical_crossentropy,\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduceLROnPlat = ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.95,\n    patience=3,\n    verbose=1,\n    mode='min',\n    min_delta=0.0001,\n    cooldown=2,\n    min_lr=1e-5\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit(\n    X_train,y_train,\n    validation_data=(X_test, y_test),\n    epochs=150,\n    verbose=1,\n    callbacks=[reduceLROnPlat])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\nprint(\"test_loss = {:.3f} and test_acc = {:.3f}\".format(test_loss, test_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\n\nplt.subplot(1,2,1)\nplt.plot(hist.history['accuracy'], label='train')\nplt.plot(hist.history['val_accuracy'], label='validation')\nplt.title('Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(hist.history['loss'], label='train')\nplt.plot(hist.history['val_loss'], label='validation')\nplt.title('Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.argmax(model.predict(X_test), axis = 1)\ny_orig = np.argmax(y_test, axis = 1)\ncm = confusion_matrix(preds, y_orig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#keys = OrderedDict(sorted(genres.items(), key=lambda t: t[1])).keys()\nplt.figure(figsize=(10,10))\nplot_confusion_matrix(cm, classes, normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the model\nmodel.save('custom_cnn_2d_color_final.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Prueba de canciones de fuera del dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"files = os.listdir('../input/test-songs-gtzan/images')\nsongs = [ '../input/test-songs-gtzan/images/'+img for img in files if img.split('.')[-1] == 'png']\nsongs = np.sort(songs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for song in songs:\n    if(song[-7:-4] != \"cut\" and song[-7:-4] != \"BnW\"): #quitamos imagenes no cortadas y en blanco y negro\n        img = cv2.imread(song, 1)\n        img = img.reshape(-1,217,335,3)\n        pred = np.argmax(model.predict(img), axis = 1)\n        print(song[33:-4],\"-->\",classes[pred[0]],'\\n')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}