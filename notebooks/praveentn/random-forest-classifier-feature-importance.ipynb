{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# Random Forest Classifier with Feature Importance\n\n\nHello friends,\n\n\nRandom Forest is a supervised machine learning algorithm which is based on ensemble learning. In this kernel, I build two Random Forest Classifier models to predict whether a person makes over 50K a year, one with 10 decision-trees and another one with 100 decision-trees. The expected accuracy increases with number of decision-trees in the model. I have demonstrated the **feature selection process** using the Random Forest model to find only the important features, rebuild the model using these features and see its effect on accuracy. I have used the **Income classification data set** for this project."},{"metadata":{},"cell_type":"markdown","source":"**I hope you find this kernel useful and your <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated.**"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n## Table of Contents\n\n\n1.\t[The problem statement](#1)\n1.\t[Import libraries](#2)\n1.\t[Import dataset](#3)\n1.\t[Exploratory data analysis](#4)\n1.  [Explore categorical variables](#5)\n1.  [Explore numerical variables](#6)\n1.  [Declare feature vector and target variable](#7)\n1.\t[Split data into separate training and test set](#8)\n1.\t[Feature Engineering](#9)\n1.  [Feature Scaling](#10)\n1.\t[Random Forest Classifier with default parameters](#11)\n1.\t[Random Forest Classifier with 100 Decision Tress](#12)\n1.\t[Find important features with Random Forest model](#13)\n1.\t[Visualize feature scores of the features](#14)\n1.\t[Build the Random Forest model on selected features](#15)\n1.\t[Confusion matrix](#16)\n1.\t[Classification report](#17)\n1.\t[Results and conclusion](#18)\n"},{"metadata":{},"cell_type":"markdown","source":"## 1. The problem statement <a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)\n\n\nIn this kernel, I try to make predictions where the prediction task is to determine whether a person makes over 50K a year. I implement Random Forest Classification with Python and Scikit-Learn. So, to answer the question, I build a Random Forest classifier to predict whether a person makes over 50K a year.\n\nI have used the **Income classification data set** for this project.\n"},{"metadata":{},"cell_type":"markdown","source":"## 2. Import libraries <a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsns.set(style=\"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Import dataset <a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = '/kaggle/input/income-classification/income_evaluation.csv'\n\ndf = pd.read_csv(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Exploratory data analysis <a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)\n\n\nNow, I will explore the data to gain insights about the data. "},{"metadata":{},"cell_type":"markdown","source":"### 4.1  View dimensions of dataset <a class=\"anchor\" id=\"4.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the shape\nprint('The shape of the dataset : ', df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 32561 instances and 15 attributes in the data set."},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Preview the dataset <a class=\"anchor\" id=\"4.2\"></a>"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Rename column names <a class=\"anchor\" id=\"4.3\"></a>\n\nWe can see that the dataset does not have proper column names. The column names contain underscore. We should give proper names to the columns. I will do it as follows:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship',\n             'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n\ndf.columns = col_names\n\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 View summary of dataset <a class=\"anchor\" id=\"4.4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Findings\n\n- We can see that the dataset contains 9 character variables and 6 numerical variables.\n\n- `income` is the target variable.\n\n- There are no missing values in the dataset. I will explore this later,"},{"metadata":{},"cell_type":"markdown","source":"### 4.5 Check the data types of columns <a class=\"anchor\" id=\"4.5\"></a>\n\n- The above `df.info()` command gives us the number of filled values along with the data types of columns.\n\n- If we simply want to check the data type of a particular column, we can use the following command."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.6 View statistical properties of dataset <a class=\"anchor\" id=\"4.6\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The above `df.describe()` command presents statistical properties in vertical form.\n\n- If we want to view the statistical properties in horizontal form, we should run the following command."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the above `df.describe().T` command presents statistical properties in horizontal form."},{"metadata":{},"cell_type":"markdown","source":"#### Important points to note\n\n\n- The above command `df.describe()` helps us to view the statistical properties of numerical variables. It excludes character variables.\n\n- If we want to view the statistical properties of character variables, we should run the following command -\n\n        `df.describe(include=['object'])`\n\n- If we want to view the statistical properties of all the variables, we should run the following command -\n\n        `df.describe(include='all')`"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.7 Check for missing values <a class=\"anchor\" id=\"4.7\"></a>\n\n\n- In Python missing data is represented by two values:\n\n   - **None** : None is a Python singleton object that is often used for missing data in Python code.\n\n   - **NaN** : NaN is an acronym for Not a Number. It is a special floating-point value recognized by all systems   that use the standard IEEE floating-point representation.\n\n- There are different methods in place on how to detect missing values.\n\n\n#### Pandas isnull() and notnull() functions \n\n- Pandas offers two functions to test for missing values - **isnull()** and **notnull()**. \n\n- These are simple functions that return a boolean value indicating whether the passed in argument value is in fact missing data.\n\n\nBelow, I will list some useful commands to deal with missing values.\n\n\n#### Useful commands to detect missing values \n\n- **df.isnull()**\n\nThe above command checks whether each cell in a dataframe contains missing values or not. If the cell contains missing value, it returns True otherwise it returns False.\n\n- **df.isnull().sum()**\n\nThe above command returns total number of missing values in each column in the dataframe.\n\n- **df.isnull().sum().sum()**\n\nIt returns total number of missing values in the dataframe.\n\n\n- **df.isnull().mean()**\n\nIt returns percentage of missing values in each column in the dataframe.\n\n\n- **df.isnull().any()**\n\nIt checks which column has null values and which has not. The columns which has null values returns TRUE and FALSE otherwise.\n\n- **df.isnull().any().any()**\n\nIt returns a boolean value indicating whether the dataframe has missing values or not. If dataframe contains missing values it returns TRUE and FALSE otherwise.\n\n- **df.isnull().values.any()**\n\nIt checks whether a particular column has missing values or not. If the column contains missing values, then it returns TRUE otherwise FALSE.\n\n- **df.isnull().values.sum()**\n\nIt returns the total number of missing values in the dataframe.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for missing values\n\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Interpretation\n\nWe can see that there are no missing values in the dataset."},{"metadata":{},"cell_type":"markdown","source":"### 4.8 Check with ASSERT statement <a class=\"anchor\" id=\"4.8\"></a>\n\n\n- We must confirm that our dataset has no missing values.\n\n- We can write an **Assert statement** to verify this.\n\n- We can use an assert statement to programmatically check that no missing, unexpected 0 or negative values are present.\n\n- This gives us confidence that our code is running properly.\n\n- **Assert statement** will return nothing if the value being tested is true and will throw an AssertionError if the value is false.\n\n- Asserts\n\n   - assert 1 == 1 (return Nothing if the value is True)\n\n   - assert 1 == 2 (return AssertionError if the value is False)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#assert that there are no missing values in the dataframe\n\nassert pd.notnull(df).all().all()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Interpretation\n\n- The above command does not throw any error. Hence, it is confirmed that there are no missing or negative values in the dataset.\n\n- All the values are greater than or equal to zero excluding character values."},{"metadata":{},"cell_type":"markdown","source":"### 4.9 Functional approach to EDA <a class=\"anchor\" id=\"4.9\"></a>\n\n- An alternative approach to EDA is to write a function that presents initial EDA of dataset.\n\n- We can write such a function as follows :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"def initial_eda(df):\n    if isinstance(df, pd.DataFrame):\n        total_na = df.isna().sum().sum()\n        print(\"Dimensions : %d rows, %d columns\" % (df.shape[0], df.shape[1]))\n        print(\"Total NA Values : %d \" % (total_na))\n        print(\"%38s %10s     %10s %10s\" % (\"Column Name\", \"Data Type\", \"#Distinct\", \"NA Values\"))\n        col_name = df.columns\n        dtyp = df.dtypes\n        uniq = df.nunique()\n        na_val = df.isna().sum()\n        for i in range(len(df.columns)):\n            print(\"%38s %10s   %10s %10s\" % (col_name[i], dtyp[i], uniq[i], na_val[i]))\n        \n    else:\n        print(\"Expect a DataFrame but got a %15s\" % (type(df)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"initial_eda(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Types of variables\n\n- In this section, I segregate the dataset into categorical and numerical variables. \n\n- There are a mixture of categorical and numerical variables in the dataset. \n\n- Categorical variables have data type object. Numerical variables have data type int64.\n\n- First of all, I will explore categorical variables."},{"metadata":{},"cell_type":"markdown","source":"## 5. Explore Categorical Variables <a class=\"anchor\" id=\"5\"></a>\n\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Find categorical variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = [var for var in df.columns if df[var].dtype=='O']\n\nprint('There are {} categorical variables\\n'.format(len(categorical)))\n\nprint('The categorical variables are :\\n\\n', categorical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 5.2 Preview categorical variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"df[categorical].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3 Summary of categorical variables \n\n- There are 9 categorical variables in the dataset.\n\n- The categorical variables are given by `workclass`, `education`, `marital_status`, `occupation`, `relationship`, `race`, `sex`, `native_country` and `income`.\n\n- `income` is the target variable."},{"metadata":{},"cell_type":"markdown","source":"### 5.4 Frequency distribution of categorical variables \n\nNow, we will check the frequency distribution of categorical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in categorical: \n    \n    print(df[var].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.5 Percentage of frequency distribution of values"},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in categorical:\n    \n     print(df[var].value_counts()/np.float(len(df)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comment\n\n- Now, we can see that there are several variables like `workclass`, `occupation` and `native_country` which contain missing values. \n\n- Generally, the missing values are coded as `NaN` and python will detect them with the usual command of df.isnull().sum().\n\n- But, in this case the missing values are coded as `?`. Python fail to detect these as missing values because it does not consider `?` as missing values. \n\n- So, I have to replace `?` with `NaN` so that Python can detect these missing values.\n\n- I will explore these variables and replace `?` with `NaN`."},{"metadata":{},"cell_type":"markdown","source":"### 5.6 Explore the variables "},{"metadata":{},"cell_type":"markdown","source":"#### Explore `income` target variable "},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for missing values\n\ndf['income'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are no missing values in the `income` target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# view number of unique values\n\ndf['income'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 2 unique values in the `income` variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the unique values\n\ndf['income'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two unique values are `<=50K` and `>50K`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the frequency distribution of values\n\ndf['income'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view percentage of frequency distribution of values\n\ndf['income'].value_counts()/len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize frequency distribution of income variable\n\nf,ax=plt.subplots(1,2,figsize=(18,8))\n\nax[0] = df['income'].value_counts().plot.pie(explode=[0,0],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Income Share')\n\n\n#f, ax = plt.subplots(figsize=(6, 8))\nax[1] = sns.countplot(x=\"income\", data=df, palette=\"Set1\")\nax[1].set_title(\"Frequency distribution of income variable\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can plot the bars horizontally as follows :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(y=\"income\", data=df, palette=\"Set1\")\nax.set_title(\"Frequency distribution of income variable\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualize `income` wrt `sex` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 8))\nax = sns.countplot(x=\"income\", hue=\"sex\", data=df, palette=\"Set1\")\nax.set_title(\"Frequency distribution of income variable wrt sex\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Interpretation\n\n\n- We can see that males make more money than females in both the income categories."},{"metadata":{},"cell_type":"markdown","source":"#### Visualize `income` wrt `race`"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 8))\nax = sns.countplot(x=\"income\", hue=\"race\", data=df, palette=\"Set1\")\nax.set_title(\"Frequency distribution of income variable wrt race\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Interpretation\n\n\n- We can see that whites make more money than non-whites in both the income categories."},{"metadata":{},"cell_type":"markdown","source":"#### Explore `workclass` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check number of unique labels \n\ndf.workclass.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the unique labels\n\ndf.workclass.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view frequency distribution of values\n\ndf.workclass.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 1836 values encoded as `?` in workclass variable. I will replace these `?` with `NaN`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace '?' values in workclass variable with `NaN`\n\ndf['workclass'].replace(' ?', np.NaN, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# again check the frequency distribution of values in workclass variable\n\ndf.workclass.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now, we can see that there are no values encoded as `?` in the workclass variable.\n\n- I will adopt similar approach with `occupation` and `native_country` column."},{"metadata":{},"cell_type":"markdown","source":"#### Visualize `workclass` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 6))\nax = df.workclass.value_counts().plot(kind=\"bar\", color=\"green\")\nax.set_title(\"Frequency distribution of workclass variable\")\nax.set_xticklabels(df.workclass.value_counts().index, rotation=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Interpretation\n\n\n- We can see that there are lot more private workers than other category of workers."},{"metadata":{},"cell_type":"markdown","source":"#### Visualize `workclass` variable wrt `income` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 8))\nax = sns.countplot(x=\"workclass\", hue=\"income\", data=df, palette=\"Set1\")\nax.set_title(\"Frequency distribution of workclass variable wrt income\")\nax.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Interpretation\n\n\n- We can see that workers make less than equal to 50k in most of the working categories.\n\n- But this trend is more appealing in Private `workclass` category."},{"metadata":{},"cell_type":"markdown","source":"#### Visualize `workclass` variable wrt `sex` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 8))\nax = sns.countplot(x=\"workclass\", hue=\"sex\", data=df, palette=\"Set1\")\nax.set_title(\"Frequency distribution of workclass variable wrt sex\")\nax.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Interpretation\n\n\n- We can see that there are more male workers than female workers in all the working category.\n\n- The trend is more appealing in Private sector."},{"metadata":{},"cell_type":"markdown","source":"#### Explore `occupation` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check number of unique labels\n\ndf.occupation.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view unique labels\n\ndf.occupation.unique()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view frequency distribution of values\n\ndf.occupation.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 1843 values encoded as `?` in occupation variable. I will replace these `?` with `NaN`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace '?' values in occupation variable with `NaN`\n\ndf['occupation'].replace(' ?', np.NaN, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# again check the frequency distribution of values\n\ndf.occupation.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize frequency distribution of `occupation` variable\n\nf, ax = plt.subplots(figsize=(12, 8))\nax = sns.countplot(x=\"occupation\", data=df, palette=\"Set1\")\nax.set_title(\"Frequency distribution of occupation variable\")\nax.set_xticklabels(df.occupation.value_counts().index, rotation=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Explore `native_country` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check number of unique labels\n\ndf.native_country.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view unique labels \n\ndf.native_country.unique()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check frequency distribution of values\n\ndf.native_country.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 583 values encoded as `?` in native_country variable. I will replace these `?` with `NaN`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace '?' values in native_country variable with `NaN`\n\ndf['native_country'].replace(' ?', np.NaN, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# again check the frequency distribution of values\n\ndf.native_country.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize frequency distribution of `native_country` variable\n\nf, ax = plt.subplots(figsize=(16, 12))\nax = sns.countplot(x=\"native_country\", data=df, palette=\"Set1\")\nax.set_title(\"Frequency distribution of native_country variable\")\nax.set_xticklabels(df.native_country.value_counts().index, rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that `United-States` dominate amongst the `native_country` variables."},{"metadata":{},"cell_type":"markdown","source":"### 5.7 Check missing values in categorical variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"df[categorical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can see that `workclass`, `occupation` and `native_country` variable contains missing values."},{"metadata":{},"cell_type":"markdown","source":"### 5.8 Number of labels: Cardinality \n\n- The number of labels within a categorical variable is known as **cardinality**. \n\n- A high number of labels within a variable is known as **high cardinality**. \n\n- High cardinality may pose some serious problems in the machine learning model. So, I will check for high cardinality."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for cardinality in categorical variables\n\nfor var in categorical:\n    \n    print(var, ' contains ', len(df[var].unique()), ' labels')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that native_country column contains relatively large number of labels as compared to other columns. I will check for cardinality after train-test split."},{"metadata":{},"cell_type":"markdown","source":"## 6. Explore Numerical Variables <a class=\"anchor\" id=\"6\"></a>\n\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### 6.1  Find numerical variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = [var for var in df.columns if df[var].dtype!='O']\n\nprint('There are {} numerical variables\\n'.format(len(numerical)))\n\nprint('The numerical variables are :\\n\\n', numerical)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2 Preview the numerical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[numerical].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.3 Summary of numerical variables\n\n- There are 6 numerical variables.\n\n- These are given by `age`, `fnlwgt`, `education_num`,`capital_gain`, `capital_loss` and `hours_per_week`.\n\n- All of the numerical variables are of discrete data type."},{"metadata":{},"cell_type":"markdown","source":"### 6.4 Check missing values in numerical variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"df[numerical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are no missing values in the numerical variables."},{"metadata":{},"cell_type":"markdown","source":"### 6.5 Explore numerical variables"},{"metadata":{},"cell_type":"markdown","source":"#### Explore `age` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['age'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### View the distribution of `age` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10,8))\nx = df['age']\nax = sns.distplot(x, bins=10, color='blue')\nax.set_title(\"Distribution of age variable\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that `age` is slightly positively skewed."},{"metadata":{},"cell_type":"markdown","source":"We can use Pandas series object to get an informative axis label as follows :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10,8))\nx = df['age']\nx = pd.Series(x, name=\"Age variable\")\nax = sns.distplot(x, bins=10, color='blue')\nax.set_title(\"Distribution of age variable\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can shade under the density curve and use a different color as follows:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10,8))\nx = df['age']\nx = pd.Series(x, name=\"Age variable\")\nax = sns.kdeplot(x, shade=True, color='red')\nax.set_title(\"Distribution of age variable\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Detect outliers in `age` variable with boxplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10,8))\nx = df['age']\nax = sns.boxplot(x)\nax.set_title(\"Visualize outliers in age variable\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are lots of outliers in `age` variable."},{"metadata":{},"cell_type":"markdown","source":"#### Explore relationship between `age` and `income` variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 8))\nax = sns.boxplot(x=\"income\", y=\"age\", data=df)\nax.set_title(\"Visualize income wrt age variable\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Interpretation\n\n- As expected, younger people make less money as compared to senior people."},{"metadata":{},"cell_type":"markdown","source":"#### Visualize `income` wrt `age` and `sex` variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 8))\nax = sns.boxplot(x=\"income\", y=\"age\", hue=\"sex\", data=df)\nax.set_title(\"Visualize income wrt age and sex variable\")\nax.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nax = sns.catplot(x=\"income\", y=\"age\", col=\"sex\", data=df, kind=\"box\", height=8, aspect=1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Interpretation\n\n- Senior people make more money than younger people."},{"metadata":{},"cell_type":"markdown","source":"#### Visualize relationship between `race` and `age`"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.boxplot(x ='race', y=\"age\", data = df)\nplt.title(\"Visualize age wrt race\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Interpretation\n\n- Whites are more older than other groups of people."},{"metadata":{},"cell_type":"markdown","source":"#### Find out the correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot correlation heatmap to find out correlations\n\ndf.corr().style.format(\"{:.4}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Interpretation\n\n- We can see that there is no strong correlation between variables."},{"metadata":{},"cell_type":"markdown","source":"#### Plot pairwise relationships in dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Interpretation\n\n- We can see that `age` and `fnlwgt` are positively skewed.\n\n- The variable `education_num` is negatively skewed while `hours_per_week` is normally distributed.\n\n- There exists weak positive correlation between `capital_gain` and `education_num` (correlation coefficient=0.1226). "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, hue=\"income\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df, hue=\"sex\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Declare feature vector and target variable <a class=\"anchor\" id=\"7\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['income'], axis=1)\n\ny = df['income']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Split data into separate training and test set <a class=\"anchor\" id=\"8\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the shape of X_train and X_test\n\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Feature Engineering  <a class=\"anchor\" id=\"9\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n- **Feature Engineering** is the process of transforming raw data into useful features that help us to understand our model better and increase its predictive power. \n\n- I will carry out feature engineering on different types of variables.\n\n- First, I will display the categorical and numerical variables in training set separately."},{"metadata":{},"cell_type":"markdown","source":"### 9.1 Display categorical variables in training set\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']\n\ncategorical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9.2 Display numerical variables in training set\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']\n\nnumerical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9.3 Engineering missing values in categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print percentage of missing values in the categorical variables in training set\n\nX_train[categorical].isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print categorical variables with missing data\n\nfor col in categorical:\n    if X_train[col].isnull().mean()>0:\n        print(col, (X_train[col].isnull().mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# impute missing categorical variables with most frequent value\n\nfor df2 in [X_train, X_test]:\n    df2['workclass'].fillna(X_train['workclass'].mode()[0], inplace=True)\n    df2['occupation'].fillna(X_train['occupation'].mode()[0], inplace=True)\n    df2['native_country'].fillna(X_train['native_country'].mode()[0], inplace=True)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing values in categorical variables in X_train\n\nX_train[categorical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing values in categorical variables in X_test\n\nX_test[categorical].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a final check, I will check for missing values in X_train and X_test."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing values in X_train\n\nX_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing values in X_test\n\nX_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are no missing values in X_train and X_test."},{"metadata":{},"cell_type":"markdown","source":"### 9.4 Encode categorical variables\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# preview categorical variables in X_train\n\nX_train[categorical].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import category encoders\n\nimport category_encoders as ce","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode categorical variables with one-hot encoding\n\nencoder = ce.OneHotEncoder(cols=['workclass', 'education', 'marital_status', 'occupation', 'relationship', \n                                 'race', 'sex', 'native_country'])\n\nX_train = encoder.fit_transform(X_train)\n\nX_test = encoder.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that from the initial 14 columns, we now have 105 columns in training set."},{"metadata":{},"cell_type":"markdown","source":"Similarly, I will take a look at the X_test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We now have training and testing set ready for model building. Before that, we should map all the feature variables onto the same scale. It is called **feature scaling**. We will do it as follows."},{"metadata":{},"cell_type":"markdown","source":"## 10. Feature Scaling <a class=\"anchor\" id=\"10\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = X_train.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.DataFrame(X_train, columns=[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = pd.DataFrame(X_test, columns=[cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have X_train dataset ready to be fed into the Random Forest classifier. We will do it as follows."},{"metadata":{},"cell_type":"markdown","source":"## 11. Random Forest Classifier model with default parameters <a class=\"anchor\" id=\"11\"></a>\n\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import Random Forest classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\n# instantiate the classifier \n\nrfc = RandomForestClassifier(random_state=0)\n\n\n\n# fit the model\n\nrfc.fit(X_train, y_train)\n\n\n\n# Predict the Test set results\n\ny_pred = rfc.predict(X_test)\n\n\n\n# Check accuracy score \n\nfrom sklearn.metrics import accuracy_score\n\nprint('Model accuracy score with 10 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, **y_test** are the true class labels and **y_pred** are the predicted class labels in the test-set."},{"metadata":{},"cell_type":"markdown","source":"Here, I have build the Random Forest Classifier model with default parameter of `n_estimators = 10`. So, I have used 10 decision-trees to build the model. Now, I will increase the number of decision-trees and see its effect on accuracy."},{"metadata":{},"cell_type":"markdown","source":"## 12. Random Forest Classifier model with 100 Decision Trees  <a class=\"anchor\" id=\"12\"></a>\n\n\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate the classifier with n_estimators = 100\n\nrfc_100 = RandomForestClassifier(n_estimators=100, random_state=0)\n\n\n\n# fit the model to the training set\n\nrfc_100.fit(X_train, y_train)\n\n\n\n# Predict on the test set results\n\ny_pred_100 = rfc_100.predict(X_test)\n\n\n\n# Check accuracy score \n\nprint('Model accuracy score with 100 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred_100)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model accuracy score with 10 decision-trees is 0.8446 but the same with 100 decision-trees is 0.8521. So, as expected accuracy increases with number of decision-trees in the model."},{"metadata":{},"cell_type":"markdown","source":"## 13. Find important features with Random Forest model <a class=\"anchor\" id=\"13\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nUntil now, I have used all the features given in the model. Now, I will select only the important features, build the model using these features and see its effect on accuracy. \n\n\nFirst, I will create the Random Forest model as follows:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the classifier with n_estimators = 100\n\nclf = RandomForestClassifier(n_estimators=100, random_state=0)\n\n\n\n# fit the model to the training set\n\nclf.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I will use the feature importance variable to see feature importance scores."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# view the feature scores\n\nfeature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n\nfeature_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the most important feature is `fnlwgt` and least important feature is `native_country_41`."},{"metadata":{},"cell_type":"markdown","source":"## 14. Visualize feature scores of the features <a class=\"anchor\" id=\"14\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nNow, I will visualize the feature scores with matplotlib and seaborn."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a seaborn bar plot\n\nf, ax = plt.subplots(figsize=(30, 24))\nax = sns.barplot(x=feature_scores, y=feature_scores.index, data=df)\nax.set_title(\"Visualize feature scores of the features\")\nax.set_yticklabels(feature_scores.index)\nax.set_xlabel(\"Feature importance score\")\nax.set_ylabel(\"Features\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Interpretation\n\n\n- The above plot confirms that the most important feature is `fnlwgt` and least important feature is `native_country_41`."},{"metadata":{},"cell_type":"markdown","source":"## 15. Build the Random Forest model on selected features <a class=\"anchor\" id=\"15\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nNow, I will drop the least important feature `native_country_41` from the model, rebuild the model and check its effect on accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the least important feature from X_train and X_test\n\nX_train = X_train.drop(['native_country_41'], axis=1)\n\nX_test = X_test.drop(['native_country_41'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I will build the random forest model again and check accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate the classifier with n_estimators = 100\n\nclf = RandomForestClassifier(n_estimators=100, random_state=0)\n\n\n\n# fit the model to the training set\n\nclf.fit(X_train, y_train)\n\n\n# Predict on the test set results\n\ny_pred = clf.predict(X_test)\n\n\n\n# Check accuracy score \n\nprint('Model accuracy score with native_country_41 variable removed : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Interpretation\n\n- I have removed the `native_country_41` variable from the model, rebuild it and checked its accuracy. \n\n- The accuracy of the model now comes out to be 0.8544. \n\n- The accuracy of the model with all the variables taken into account is 0.8521. \n\n- So, we can see that the model accuracy has been improved with `native_country_41` variable removed from the model."},{"metadata":{},"cell_type":"markdown","source":"Now, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\n\n\nBut, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making. \n\n\nWe have another tool called `Confusion matrix` that comes to our rescue."},{"metadata":{},"cell_type":"markdown","source":"## 16. Confusion matrix <a class=\"anchor\" id=\"16\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\nA confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n\n\nFour types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n\n\n**True Positives (TP)** – True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n\n\n**True Negatives (TN)** – True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n\n\n**False Positives (FP)** – False Positives occur when we predict an observation belongs to a    certain class but the observation actually does not belong to that class. This type of error is called **Type I error.**\n\n\n\n**False Negatives (FN)** – False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called **Type II error.**\n\n\n\nThese four outcomes are summarized in a confusion matrix given below.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\n\nprint('Confusion matrix\\n\\n', cm)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize confusion matrix with seaborn heatmap\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 17. Classification Report <a class=\"anchor\" id=\"17\"></a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n**Classification report** is another way to evaluate the classification model performance. It displays the  **precision**, **recall**, **f1** and **support** scores for the model. I have described these terms in later.\n\nWe can print a classification report as follows:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 18. Results and Conclusion <a class=\"anchor\" id=\"18\"></a>\n\n[Back to Table of Contents](#0.1)\n\n\n1.\tIn this project, I build a Random Forest Classifier to predict the income of a person. I build two models, one with 10 decision-trees and another one with 100 decision-trees. \n2.\tThe model accuracy score with `10 decision-trees is 0.8446` but the same with `100 decision-trees is 0.8521`. So, as expected accuracy increases with number of decision-trees in the model.\n3.\tI have used the Random Forest model to find only the important features, build the model using these features and see its effect on accuracy. \n4.\tI have removed the `native_country_41` variable from the model, rebuild it and checked its accuracy. The `accuracy of the model with native_country_41 variable removed is 0.8544`. So, we can see that the model accuracy has been improved with `native_country_41` variable removed from the model.\n5.\tConfusion matrix and classification report are another tool to visualize the model performance. They yield good performance.\n\n"},{"metadata":{},"cell_type":"markdown","source":"That is the end of this kernel. I hope you find it useful and enjoyable.\n\nYour feedback and comments are most welcome."},{"metadata":{},"cell_type":"markdown","source":"[Go to Top](#0)"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}