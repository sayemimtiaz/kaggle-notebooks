{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this tutorial cum analysis, my focus will be on 3 important concepts of statistics, viz. \n\n1. Probability Mass Function\n2. Cumulative Distribution Function\n3. Normal Probability Plot\n\nIf you haven't read my previous analysis, here is the link:\n[The Bihar Statistics - 1](http://www.kaggle.com/ritikpnayak/the-bihar-statistics-1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Let's Begin","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nfrom scipy import stats\nfrom collections import defaultdict\nstyle.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gross_enrollment = pd.read_csv('/kaggle/input/indian-school-education-statistics/gross-enrollment-ratio-2013-2016.csv', index_col = 'State_UT')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Got the Fix!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In my last analysis, I didn't change the names of some states that were recognized with different names in different years, the reason being; their names were changed in the subsequent year.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If you observe in the above cell, you can notice that I used the 'State_UT' column as index such that I can rename some entries of the column. I reset the index later on.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gross_enrollment.rename({'MADHYA PRADESH' : 'Madhya Pradesh', 'Pondicherry' : 'Puducherry', 'Uttaranchal' : 'Uttarakhand'}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gross_enrollment.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gross_enrollment.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gross_enrollment.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Object to Float conversion","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gross_enrollment.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_convert = ['Higher_Secondary_Boys', 'Higher_Secondary_Girls', 'Higher_Secondary_Total']\nnp_vals = ['NR', '@']\ngross_enrollment[cols_to_convert] = gross_enrollment[cols_to_convert].replace(np_vals,np.nan)\ngross_enrollment[cols_to_convert] = gross_enrollment[cols_to_convert].astype('float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gross_enrollment.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Histograms are a good way to observe distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\ngross_enrollment.Primary_Total.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\n\ngross_enrollment.Primary_Boys.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\n\ngross_enrollment.Primary_Girls.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Adding Columns and Probability Mass Function","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**What is a Probability mass function (Pmf)?**\n\nSimply put, pmfs are a representation of a distribution as a function that maps from values to probabilities. (from Think stats book by Alan B. Downey)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In our case, we do not have a handful of numbers as output instead we can take values belonging to a certain range for single or double digit numbers. Recall, in school, all the students getting marks within 91 - 100 were alotted an A1. Here, we are doing the same ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gross_enrollment['class'] = gross_enrollment.Primary_Total.apply(lambda x:1 if x<=70\n                                                                else 2 if x<=80\n                                                                else 3 if x<=90\n                                                                else 4 if x<=100\n                                                                else 5 if x<=110\n                                                                else 6 if x<=120\n                                                                else 7 if x<=130\n                                                                else 8 if x<=140\n                                                                else 9)   \n\ngross_enrollment['class_boys'] = gross_enrollment.Primary_Boys.apply(lambda x:1 if x<=70\n                                                                else 2 if x<=80\n                                                                else 3 if x<=90\n                                                                else 4 if x<=100\n                                                                else 5 if x<=110\n                                                                else 6 if x<=120\n                                                                else 7 if x<=130\n                                                                else 8 if x<=140\n                                                                else 9)      \n\ngross_enrollment['class_girls'] = gross_enrollment.Primary_Girls.apply(lambda x:1 if x<=70\n                                                                else 2 if x<=80\n                                                                else 3 if x<=90\n                                                                else 4 if x<=100\n                                                                else 5 if x<=110\n                                                                else 6 if x<=120\n                                                                else 7 if x<=130\n                                                                else 8 if x<=140\n                                                                else 9)                                                        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bihar = gross_enrollment[gross_enrollment.State_UT == 'Bihar']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have created 3 separate columns for **Primary Total**, **Primary Boys** and **Primary Girls**, we can move on to make our pmf functions separately for both **All India** and **Bihar**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_all = dict(gross_enrollment['class'].value_counts())\nhist_bihar = dict(bihar['class'].value_counts())\n\npmf_all = defaultdict()\npmf_bihar = defaultdict()\n\nn_all = sum(hist_all.values())\nn_bihar = sum(hist_bihar.values())\n\nfor index, freq in hist_all.items():\n    pmf_all[index] = freq/n_all\n\nfor index, freq in hist_bihar.items():\n    pmf_bihar[index] = freq/n_bihar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, **hist_all** and **hist_bihar** contain the data of the **class** column in the form of a dictionary in which the index is the range value and the corresponding values are the number of times those values appear in the class column. \n\nFor instance, the number 5 comes 41 times in hist_all which means that the **GER in the range 110 - 120** occupies as many as 42 columns in the dataset. Which means many a state have had **GER** in this range at some of time between **2013 - 2016**. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**pmf_all and pmf_bihar** represent the probability of choosing these unique range/values of **GER**. The probability is always between 0 and 1. So more the value of pmf, the more are the chances for those unique values being selected. \n\nFor instance, the pmf of 5 in pmf_all is 0.37 which is also the highest. This means that the probability of selecting a State/UT having a GER in the range of 110-12 at any point of time is 0.37 or 37%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pmf_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pmf_bihar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, in the same way we can create pmf for our newly created **'class_boys'** and **'class_girls'** columns as well","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_boys = dict(gross_enrollment['class_boys'].value_counts())\nhist_girls = dict(gross_enrollment['class_girls'].value_counts())\n\npmf_boys = defaultdict()\npmf_girls = defaultdict()\n\nn_boys = sum(hist_boys.values())\nn_girls = sum(hist_girls.values())\n\nfor index, freq in hist_boys.items():\n    pmf_boys[index] = freq/n_boys\n\nfor index, freq in hist_girls.items():\n    pmf_girls[index] = freq/n_girls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.a. Plotting PMFs","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now that we have the pmfs for **'class_boys'** and **'class_girls'**, we can plot a graph of the pmfs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"l = []\nfor i in pmf_boys.keys():\n    if i in pmf_girls.keys():\n        l.append(i)\n\ndiffs = []\nfor val in l:\n    p_boys = pmf_boys[val]\n    p_girls = pmf_girls[val]\n    diff = 100*(p_boys-p_girls)\n    diffs.append(diff)\n\nplt.figure(figsize=(20,8))\n\nplt.bar(l, diffs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph illustrates some significant observations:\n\n1. the probability of values 2, 3 and 4 of 'class_boys' outpace the probabilities of 'class_girls' which means that the GER in range 70-100 is more dense in 'pmf_boys'.\n3. the probability of values 5, 6 and 9 of 'class_girls' outpace the probabilities of 'class_boys' which means that the GER in range 100-120 and more than 140 is more dense in 'pmf_girls'.\n\nThis inference among a couple of other less significant observations is important for it portrays that the GER of girls in India in Primary schools is significantly higher than that of the boys. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Plotting a pmf for Bihar will not be wise because there are only 3 values in case of Bihar. Although it can still illustrate useful information, but for such a small dataset, it can be interpreted visually as well.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5. Cumulative Distribution Function","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**What is a Cumulative Distribution Function?**\n\nTo understand that, you first need to know what a percentile is. I won't dwell on the definition of percentile so I'm putting a link for reference; [What is a Percentile rank?](https://en.wikipedia.org/wiki/Percentile_rank).\n\nA Cumulative Distribution Function (cdf), straightforwardly, is a function that maps from a value to its percentile rank.\n\n**\"The CDF is a function of x, where x is any value that might appear in the\ndistribution. To evaluate CDF(x) for a particular value of x, we compute\nthe fraction of values in the distribution less than or equal to x.\" (from Think Stats book)** ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Defining our cdf function.\n\ndef cdf(sample, val):\n    count = 0.0\n    for value in sample:\n        if value <= val:\n            count += 1\n            \n    prob = count/len(sample)\n    return prob\n\nsample = [1,2,2,4,5,1,3]\n\nd = {}\nfor i in set(sample):\n    d[i] = cdf(sample, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The cdf of 5 is 1.0 because it is the greatest value in the sample.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Defining a cdf function that takes a dictionary as sample.\n# it will be beneficial because we are dealing with Series object that can be easily converted to dictionary dataframe.\n\ndef cdf_dict(sample, val):\n    count = 0.0\n    for value in sample:\n        if sample[value] <= sample[val]:\n            count += 1\n            \n    prob = count/len(sample)\n    return prob\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_boys = dict(gross_enrollment['class_boys'].value_counts())\nsample_girls = dict(gross_enrollment['class_girls'].value_counts())\n\ny_boys = {}\nfor i in set(gross_enrollment['class_boys']):\n    cdf = cdf_dict(sample_boys, i)\n    y_boys[i] = cdf\n\ny_girls = {}\nfor i in set(gross_enrollment['class_girls']):\n    cdf = cdf_dict(sample_girls, i)\n    y_girls[i] = cdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, figsize = (20,8))\nfig.suptitle('CDF Subplots')\naxs[0].bar(y_boys.keys(), y_boys.values(), color = 'blue')\naxs[1].bar(y_girls.keys(), y_girls.values())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I'm not infering the observations from the above cdf subplots because everything is quite obvious**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 6. Normal Probability Plot","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**\"For the exponential distribution, and a few others, there are simple transformations we can use to test whether an analytic distribution is a good model for a dataset\".**\n\n**\"For the normal distribution there is no such transformation, but there is an alternative called a normal probability plot\". (from Think Stats book)**\n\nFor more information about normal probability plot; click on the following link; [What is a Normal Probability Plot?](https://en.wikipedia.org/wiki/Normal_probability_plot)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I'll plot the normal probability plots using the scipy library. I imported stats from scipy. I'll use ***stats.probplot*** for this purpose.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's look at an example","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.probplot(x = gross_enrollment['Upper_Primary_Total'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## This is how the graph of probplot looks like\n\nstats.probplot(x = gross_enrollment['Upper_Primary_Total'], plot = plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(4, figsize = (20,12))\nfig.suptitle('Vertically stacked Normal Probability Plots')\n\naxs[0].plot(stats.probplot(x = gross_enrollment['Primary_Total'])[0][0], stats.probplot(x = gross_enrollment['Primary_Total'])[0][1])\naxs[1].plot(stats.probplot(x = gross_enrollment['Upper_Primary_Total'])[0][0], stats.probplot(x = gross_enrollment['Upper_Primary_Total'])[0][1])\naxs[2].plot(stats.probplot(x = gross_enrollment['Secondary_Total'])[0][0], stats.probplot(x = gross_enrollment['Secondary_Total'])[0][1])\naxs[3].plot(stats.probplot(x = gross_enrollment['Higher_Secondary_Total'])[0][0], stats.probplot(x = gross_enrollment['Higher_Secondary_Total'])[0][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**If the distribution of the sample is approximately normal, the result is a straight line with intercept mu and slope sigma. **\n\nNow you have 2 new terminologies to explore about, they are; ***mu and sigma*** . Click on the following link; [What is Normal Distribution and what are mu and sigma](https://en.wikipedia.org/wiki/Normal_distribution)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Based on the definiton in the previous cell, you can infer obvious interpretations.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Epilogue","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Dwelling on statistical model in a slight shift from my previous analysis. Statistics, in my opinion are the building blocks of data analysis, because our analysis itself begins with a brief discription of the data. This includes 'mean', 'median', 'standard deviation' among other things. That said, one need not have prior knowledge of herculean statistical models. It is with experience that one shall be able to discover and apply these statistical models. In this part of analysis, I wished to introduce the audience with some basic statistical models. Though they are not enough, they still are important, for they beget useful inferences about the data. It is also evident that I restricted my analysis to only a few columns/data of our dataframe. A lot more could have been done with only these three concepts, nonetheless, I leave it to the readers to apply these concepts further in the data. My aim is to answer a question on which my whole analysis is premised, i.e.; **What is the gross enrolment rate in Primary Schools of Bihar and is it faring well as compared to the corresponding statistics of all india?**\n\n**Kindly follow, like and comment to help me tweak my work.**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}