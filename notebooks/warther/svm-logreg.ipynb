{"cells":[{"source":"**INTRODUCTION**\n\nWe want to predict the developpement of diabete within a given population, using SVM and Logistic Regression.\n\nThis is my first project using Jupyter Notebook, so tips and advice are welcomed :)","cell_type":"markdown","metadata":{"_cell_guid":"e78d01d9-af4a-32a5-4d10-42c4a4febcb7","_uuid":"81847fa73af6b75de05b1e534c8ea7d8aef1e51c"}},{"source":"**IMPORT**\n\nWe will import all libraries and data as a main data frame called main_df.\nWe use the following libraries numpy, pandas and matplotlib; the specific libraries for machine learning will be imported later.\nLet's see first how our data look like.","cell_type":"markdown","metadata":{"_cell_guid":"0c6beba3-1f8f-098a-5652-8f332872b7f2","_uuid":"a7b532ce5deb0a75afebf819aec28a976b848c40"}},{"source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nway='../input'\n\nmain_df=pd.read_csv(way+'/diabetes.csv')\n\nprint(main_df.head())\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"09e98e90-060a-e551-c9da-d1b62885e175","collapsed":true,"_uuid":"e72f3f5b20a5bc5ec3ba524de814aa19beaea721"}},{"source":"The \"main\" must be split in train and test set, the X are explanatory variables and y is the predict. \nLet's use train_test_split method to this end.","cell_type":"markdown","metadata":{"_cell_guid":"c06ca953-122a-7431-fe00-c246ace3fa62","_uuid":"5835b2fb57cb6cf304d6ae56a3c90c901f48d0a0"}},{"source":"##### Split between X and y #####\n\nX_main=main_df.drop('Outcome',axis=1)\ny_main=main_df['Outcome']\n\n##### Split between train and main #####\n\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(X_main,y_main,\n                                               test_size=0.33,random_state=1)\n\nprint('train size is %i'%y_train.shape[0])\nprint('test size is %i'%y_test.shape[0])","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"98a05c99-d2c2-fab6-c108-4451cd5bc9da","collapsed":true,"_uuid":"3234f94a9770ed0a606e451a81da354236feaceb"}},{"source":"**DATA ANALYSE**\n\nWe want to check if there are missing or wrong values, and, in that case, to transform into -1 values.\nIf these values exist we need an imputer in our pipeline later on.","cell_type":"markdown","metadata":{"_cell_guid":"ca86b6e2-3b12-bdc6-8a63-7fb9e9c1b8af","_uuid":"15123203820f421775a62bac2845e562de4ccccb"}},{"source":"names=main_df.columns.values\n\n##### Repérer les valeurs abstraites #####\n         \nzeros=[]\nfor i in range(len(names)):\n    zeros.append(np.count_nonzero(main_df[names[i]]==0))\nCountZero=pd.DataFrame({'names':names,'zeros':zeros})\nprint(CountZero)\n\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"600b7733-363e-87f4-249c-1682e5f1582a","collapsed":true,"_uuid":"a6ddbce5223949428cf6af4ca0df7b1c01febc65"}},{"source":"Values zero are reported for glucose, blood pressure, skin thickness, BMI and insulin, these values are abnormal.\nWe must transfrom them into -1, before using the imputer(which will not be done for pregnancies).","cell_type":"markdown","metadata":{"_cell_guid":"19957cbb-52a8-444d-1fb2-ffce4d1b307e","_uuid":"9d682d010ef5a82720a6e753f67bf93821e06b1a"}},{"source":"##### Transform the O in -1, except for pregnancies #####\n\nfor j in range(1,len(names)-1):\n    for i in range(len(main_df)):\n        if main_df.iloc[i,j]==0:\n            main_df.iloc[i,j]=-1","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"72d24fc0-e747-ff66-0c2a-bd8b11c1bf91","collapsed":true,"_uuid":"e4d00acb4a267d13892a43b045c5b99b70c1e4c5"}},{"source":"We will plot different tools to see how features behave.","cell_type":"markdown","metadata":{"_cell_guid":"7e276fd7-114e-e840-e796-b639c657c186","_uuid":"8e1d18acbdda8ed27dc146e4ae450e8ef372c026"}},{"source":"##### Covariance matrix #####\n\ncorr=X_main.corr()\nplt.matshow(corr)\n\ny_label=X_main.columns.values\ny_pos=np.arange(len(y_label))\nx_label=y_label.copy()\nfor i in range(len(x_label)):\n    x_label[i]=y_label[i][:4]\nx_pos=np.arange(len(x_label))\nplt.xticks(x_pos,x_label)\nplt.yticks(y_pos,y_label)\nplt.colorbar()\nplt.title('Covariance Matrix')\n\nplt.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"4abeda6a-b8b7-e8f8-6fe8-90694f5c16a4","collapsed":true,"_uuid":"5378018660774eaee16b1a04915b7b0ee8e8ac85"}},{"source":"The features are not much correlated, so we don't need to use a matrix dimension reducer like PCA.","cell_type":"markdown","metadata":{"_cell_guid":"fd0afb49-07f3-606b-8707-504c5510335f","_uuid":"8df060983fb5fedc503f6b7d459897c20a192a11"}},{"source":"##### Histogrammes ###### \n\nplt.figure(2)\nplt.hist(main_df['Pregnancies'],color='g',bins=range(0,20),align='left')\nplt.title('Histogramme Pregnancies')\nplt.show()\n\nplt.figure(3)\nplt.hist(main_df['Age'],color='y',bins=range(20,90,2),align='left')\nplt.title('Histogramme Age')\nplt.show()\n\nplt.figure(4)\nplt.hist2d(main_df['Age'],main_df['Pregnancies'],bins=[range(21,80,2),range(0,20)])\nplt.xlabel('Age')\nplt.ylabel('Pregnancies')\nplt.title('Histogramme 2D Pregnancies/Age')\nplt.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"f44acdcb-6bc3-121d-3f19-71f388063fef","collapsed":true,"_uuid":"cc0bd0ff1caa961905a94a9fa77c9db0fcd31d1f"}},{"source":"We can see the relationship between 'Age' and 'Pregancies', the most correlated features.\n\n\n\nWith regard to these elements, I consider that my data are ready to be used in machine learning algorithms.\nLet's try SVM and Logistic Regression on them.","cell_type":"markdown","metadata":{"_cell_guid":"892a20d2-1125-3ef0-d6fd-c5dac494acbf","_uuid":"5ca72a82df33edc18801de27cb204be7ff4da53e"}},{"source":"**ALGORITHM**\n\nSVM and Logistic Regression will be tested in a pipeline with preprocessing jobs: an imputer, a polynomial transformation and a data scale.\n\nWe will put the two algorithm into a loop.\n\nThe results will be anlysed with a ROC curve, and learning curve will be used to search how we can improve our score.\n","cell_type":"markdown","metadata":{"_cell_guid":"650c8484-6ded-fd16-57af-8d21bfe84db6","_uuid":"7ad2964e50d91552960e83a20f8dd91f173e8916"}},{"source":"IMPORT LIBRARY","cell_type":"markdown","metadata":{"_cell_guid":"b400e6bb-8f29-48a1-93ca-5ea01f84d797","_uuid":"8f0915bdb51fbba7a91d026edeaaa97555617478"}},{"source":"from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import Imputer\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn import metrics\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"5ee7c503-e640-8f2a-d706-7be2514c3d9e","collapsed":true,"_uuid":"8b8c7d9597ad1ae4ed2a39181f88d63577e917ef"}},{"source":"DECLARATION AND INITIALIZATION","cell_type":"markdown","metadata":{"_cell_guid":"5d917524-1bf0-202e-1d2e-1c699ca46b86","_uuid":"b7c3d26cd53fa2b6d629789906f2108d3d69557b"}},{"source":"We will test polynomial transformation for degree 2.\nIn the imputer, we use strategy='mean', features don't have a large variance. \nIn other case with features with large high variance or outliners we have to use strategy='median' as an example.","cell_type":"markdown","metadata":{"_cell_guid":"962a0a27-68b1-acb9-2a76-9b99be3caa40","_uuid":"a7f33f884043edb2bfcbbaaa31f189625b78e31b"}},{"source":"## classifier\nsvm=SVC(probability=True)\nreglog=LogisticRegression()\n\n## preprocess\nscale_pipe=StandardScaler()\npoly=PolynomialFeatures(degree=2)\nimput=Imputer(missing_values=-1,strategy='mean')\n\n## grid parameters\nparam_svm=dict(clf__C=[0.001,0.1,1,10],clf__kernel=['rbf','linear','sigmoid'])\nparam_reglog=dict(clf__C=[0.001,0.1,1,10])\n\n## declaration\nclf_name=['SVM','RegLog']\nclf=[svm,reglog]\nparam_grid=[param_svm,param_reglog]\n\nauc_all=[]\nfpr_all=[]\ntpr_all=[]\n\ntrain_size_all=[]\ntrain_score_all=[]\ncv_score_all=[]","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"e18d0946-0001-86e8-b7bf-d1f9ed64c172","collapsed":true,"_uuid":"29dbc2a7adca9d45e08b3770300ce6fd030a6155"}},{"source":"LOOP","cell_type":"markdown","metadata":{"_cell_guid":"2e9b76cc-9dfc-1c56-5a30-31f830707516","_uuid":"1ab5e4b05edd83b6e7fec9534bfbedbbb764e595"}},{"source":"\nfor i in [0,1]:\n\n### pipeline ###\n\n    pipe=Pipeline([('imput',imput),('poly',poly),\n                   ('scale',scale_pipe),('clf',clf[i])])\n\n### grid ###\n\n    grid=GridSearchCV(pipe,param_grid=param_grid[i],cv=4,scoring='accuracy')\n    g=grid.fit(X_train,y_train)\n\n### prediction ###\n\n## resultats CV\n    result=grid.cv_results_\n\n## best parameters \n    bp=grid.best_params_\n    print('Best parameters for %s:'%clf_name[i])\n    print(bp)\n\n## best estimator \n    be=grid.best_estimator_\n\n\n### Results ### \n\n## prediction score\n    \n    predict=be.predict(X_test)\n\n## scores\n\n    report=metrics.classification_report(y_test,predict)\n\n    conf_mat=metrics.confusion_matrix(y_test,predict)\n\n    print('Reporting for %s:'%clf_name[i])\n    print(report)\n\n    print('Confusion matrix for %s:'%clf_name[i])\n    print(conf_mat)\n\n### learning curve ###\n\n    from sklearn.model_selection import learning_curve\n    from sklearn.model_selection import ShuffleSplit\n\n    cv=ShuffleSplit(n_splits=10,test_size=0.2,train_size=None,random_state=1)\n\n\n    train_size,train_score,cv_score=learning_curve(be,X_main,\n                                                 y_main,\n                                                 cv=cv,scoring='accuracy')\n    \n    train_size_all.append(train_size)\n    train_score_all.append(train_score)\n    cv_score_all.append(cv_score)\n\n### ROC curve ###\n\n    clf_proba=be.predict_proba(X_test)\n\n    fpr,tpr,thresolds=roc_curve(y_test,clf_proba[:,1])\n    \n    auc=roc_auc_score(y_test,clf_proba[:,1])\n    \n    fpr_all.append(fpr)\n    tpr_all.append(tpr)\n    auc_all.append(auc)\n\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"d3497c6c-bf8c-e29c-c180-459e88b58c3d","collapsed":true,"_uuid":"cdd8dd0ba078e93baa3dc3af573a42d850295c67"}},{"source":"The two algorithms seems providing similar results.\n\nLet's check them with the ROC curve, which will give us a score considering the threshold that we can choose. ","cell_type":"markdown","metadata":{"_cell_guid":"899b887a-73a0-d01a-09cb-889180e099ba","_uuid":"e11184c66a26a088a105571c0c088f788db7b75f"}},{"source":"**RESULTS VISUALISATION**\n\nROC Curves","cell_type":"markdown","metadata":{"_cell_guid":"53b6f8e7-3853-18f1-fde6-5abc60428a8d","_uuid":"abacd586e1f5c10a0cc2a8f0b3cbe8732dd5bdf4"}},{"source":"plt.plot(fpr_all[0],tpr_all[0],'g',label='AUC SVM rate=%0.4f'%auc_all[0])\nplt.plot(fpr_all[1],tpr_all[1],'b',label='AUC RegLog rate=%0.4f'%auc_all[1])\n\nplt.plot([0,1],[0,1],'k--')\nplt.title('ROC Curve')\nplt.xlabel('fpr')\nplt.ylabel('recall')\nplt.legend(loc='lower right')\n\nplt.show()\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a2ad6a25-91d5-0b80-9696-1d77704d26e4","collapsed":true,"_uuid":"e77ec9f05ef00a27e3d1c341bb9907d218ff8258"}},{"source":"Learning Curves","cell_type":"markdown","metadata":{"_cell_guid":"e5479419-ee9d-4369-65fe-07bbb9d4f33f","_uuid":"1a2651e33b7cf66bca853f4699c00f6aa449cfda"}},{"source":"## learning curve\n\n# train_score all have 3 dimensions: 1. algorithm: 2 values\n#                                   2. train size: 5 valeurs\n#                                   3. score: n values, n=n_split\ntrain_score_mean=np.mean(train_score_all,axis=2) \ncv_score_mean=np.mean(cv_score_all,axis=2)\n    \n\nplt.plot(train_size,train_score_mean[0],marker='+',color='g',label='train score')\nplt.plot(train_size,cv_score_mean[0],marker='+',color='b',label='CV score')\nplt.title('Learning Curve SVM')\nplt.xlabel('Nombre de données du train')\nplt.ylabel('Score')\nplt.legend(loc='lower right')\nplt.grid(True)\n\nplt.show()\n\nplt.plot(train_size,train_score_mean[1],marker='+',color='g',label='train score')\nplt.plot(train_size,cv_score_mean[1],marker='+',color='b',label='CV score')\nplt.title('Learning Curve RegLog')\nplt.xlabel('Nombre de données du train')\nplt.ylabel('Score')\nplt.legend(loc='lower right')\nplt.grid(True)\n\nplt.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"230eeed8-6cc0-5604-75e2-92e57cd402c8","collapsed":true,"_uuid":"03d90a48f4766dd955383fcd731f30a26eb11756"}},{"source":"**CONCLUSION**\n\nThe ROC curves show us that we have rather good results, for both algorithms, with an AUC rate above 0.85.\n\nWhen we look at the learning curve, we can see that there is variance in our algorithms because the gap \nbetween train score and cv score.\nWe need more data to correct this.\n\nDespite of this, with more data we could test a more complex algorithm to improve our score.\n\nFurthermore I think we can get more precision if we try to predict each type of diabetes (1, 2 or 3).\n\n\nThanks for reading :)","cell_type":"markdown","metadata":{"_cell_guid":"26643343-5074-26be-9357-db7b7e1a8822","_uuid":"0c92042d41f038fb28cceed2817319b02b9980a7"}}],"nbformat":4,"nbformat_minor":1,"metadata":{"_change_revision":0,"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3","file_extension":".py"},"_is_fork":false,"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}}}