{"cells":[{"metadata":{"_uuid":"3f6c2bfe6b2e26c92357e896a1511195d836956e"},"cell_type":"markdown","source":"<center>\n<img src=\"https://habrastorage.org/files/fd4/502/43d/fd450243dd604b81b9713213a247aa20.jpg\">\n    \n## [mlcourse.ai](https://mlcourse.ai) â€“ Open Machine Learning Course \nAuthor: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."},{"metadata":{"_uuid":"cb01ca96934e5c83a36a2308da9645b87a9c52a0"},"cell_type":"markdown","source":"## <center> Assignment 4. Sarcasm detection with logistic regression\n    \nWe'll be using the dataset from the [paper](https://arxiv.org/abs/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https://www.kaggle.com/danofer/sarcasm).\n\nSarcasm detection is easy. \n<img src=\"https://habrastorage.org/webt/1f/0d/ta/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" />"},{"metadata":{"trusted":true,"_uuid":"23a833b42b3c214b5191dfdc2482f2f901118247"},"cell_type":"code","source":"!ls ../input/sarcasm/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffa03aec57ab6150f9bec0fa56cd3a5791a3e6f4"},"cell_type":"code","source":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b23e4fc7a1973d60e0c6da8bd60f3d921542a856"},"cell_type":"code","source":"train_df = pd.read_csv('../input/sarcasm/train-balanced-sarcasm.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dc7b3787afa46c7eb0d0e33b0c41ab9821c4a27"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a7ed9557943806c6813ad59c3d5ebdb403ffd78"},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6472f52fb5ecb8bb2a6e3b292678a2042fcfe34c"},"cell_type":"markdown","source":"Some comments are missing, so we drop the corresponding rows."},{"metadata":{"trusted":true,"_uuid":"97b2d85627fcde52a506dbdd55d4d6e4c87d3f08"},"cell_type":"code","source":"train_df.dropna(subset=['comment'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d51637ee70dca7693737ad0da1dbb8c6ce9230b"},"cell_type":"markdown","source":"We notice that the dataset is indeed balanced"},{"metadata":{"trusted":true,"_uuid":"addd77c640423d30fd146c8d3a012d3c14481e11"},"cell_type":"code","source":"train_df['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['label'].hist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b836574e5093c5eb2e9063fefe1c8d198dcba79"},"cell_type":"markdown","source":"We split data into training and validation parts."},{"metadata":{"trusted":true,"_uuid":"c200add4e1dcbaa75164bbcc73b9c12ecb863c96"},"cell_type":"code","source":"train_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train_df['comment'], train_df['label'], random_state=17)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7f0f47b98e49a185cd5cffe19fcbe28409bf00c0"},"cell_type":"markdown","source":"## Tasks:\n1. Analyze the dataset, make some plots. This [Kernel](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc) might serve as an example\n2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n3. Plot the words/bigrams which a most predictive of sarcasm (you can use [eli5](https://github.com/TeamHG-Memex/eli5) for that)\n4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.\n\n## Links:\n  - Machine learning library [Scikit-learn](https://scikit-learn.org/stable/index.html) (a.k.a. sklearn)\n  - Kernels on [logistic regression](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-2-classification) and its applications to [text classification](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n  - [Kaggle Kernel](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n  - [ELI5](https://github.com/TeamHG-Memex/eli5) to explain model predictions"},{"metadata":{},"cell_type":"markdown","source":"Let's look at our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find most common words for two types of comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"sarcasm_texsts = train_df[train_df[\"label\"] == 1]\nnon_sarcasm_texsts = train_df[train_df[\"label\"] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Method for find top n words in cv vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For sarcasm comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"most_sarcasm_words = get_top_n_words(train_df[train_df[\"label\"] == 1][\"comment\"], 30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For non sarcasm comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"most_non_sarcasm_words = get_top_n_words(train_df[train_df[\"label\"] == 0][\"comment\"], 30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot words in sarcasm comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.DataFrame(most_sarcasm_words, columns=[\"word\", \"frequency\"])\nfig_dims = (18, 4)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.barplot(x=\"word\", y=\"frequency\", data=data, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And in non sarcasm comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.DataFrame(most_non_sarcasm_words, columns=[\"word\", \"frequency\"])\nfig_dims = (18, 4)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.barplot(x=\"word\", y=\"frequency\", data=data, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the words are almost the same, we can assume that they will have small weights in our model"},{"metadata":{},"cell_type":"markdown","source":"Use CountVectorizer to process all comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer()\ncv.fit(train_texts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Length of our vocabulary of all used in comments words"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(cv.vocabulary_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform all comments in sparse matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = cv.transform(train_texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cv.get_feature_names()[10000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[10000].nonzero()[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = cv.transform(valid_texts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's fit our model LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logit = LogisticRegression(solver='lbfgs', n_jobs=-1, random_state=7)\nlogit.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And check result on the test sample"},{"metadata":{"trusted":true},"cell_type":"code","source":"logit.score(X_test, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets make pipeline for our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_pipe_logit = make_pipeline(CountVectorizer(), LogisticRegression(solver='lbfgs', \n                                                                       n_jobs=1,\n                                                                       random_state=7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntext_pipe_logit.fit(train_texts, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_pipe_logit.score(valid_texts, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find optimal regularization parameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid_logit = {'logisticregression__C': np.logspace(-3, 3, 20)}\ngrid_logit = GridSearchCV(text_pipe_logit, \n                          param_grid_logit, \n                          return_train_score=True, \n                          cv=3, n_jobs=-1)\n\ngrid_logit.fit(train_texts, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_logit.best_params_, grid_logit.best_score_,sep=\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the final score"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_logit.score(valid_texts, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(grid_logit.param_grid[\"logisticregression__C\"], grid_logit.cv_results_[\"mean_test_score\"],\n        color=\"red\", label=\"test\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In our case, almost nothing depends on the regularization parameter"},{"metadata":{},"cell_type":"markdown","source":"Most important words"},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\neli5.show_weights(text_pipe_logit, top=20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}