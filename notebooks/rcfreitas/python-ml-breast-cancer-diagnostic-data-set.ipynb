{"nbformat_minor":0,"cells":[{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"9238e5cc-2caf-4093-ad4c-b1dea9dc9323","collapsed":false,"_uuid":"9f3982b97d5b5a01181484a6026cb4cd5382c73b"},"source":"# **A brief tutorial on using Python to make predictions - Breast Cancer Wisconsin (Diagnostic) Data Set**\n\n### de Freitas, R. C."},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"e393e6c8-10b8-4802-84b6-2fa6e28f33ac","collapsed":false,"_uuid":"bbdd1a31c45343af49f128855c5ea078e591d4e2"},"source":"# 1 - Introduction"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"1ffc61eb-0fef-490f-b416-4645ea99d0b2","collapsed":false,"_uuid":"17cffd56d7b820b2b561da3504bfb41f02363ff7"},"source":"The aim of this notebook is to me (and others) to understand the process of organizing and preparing the data, selecting the features, choosing and applying the machine learning tools, comparing, selecting and improving the best models. \n\nThe features from the data set describe characteristics of the cell nuclei and are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. As described in [UCI Machine Learning Repository][1], the attribute informations are:\n\n1. ID number\n2. Diagnosis (M = malignant, B = benign)\n\n3 - 32  Ten real-valued features are computed for each cell nucleus:\n\n* a) radius (mean of distances from center to points on the perimeter)\n* b) texture (standard deviation of gray-scale values)\n* c) perimeter\n* d) area\n* e) smoothness (local variation in radius lengths)\n* f) compactness (perimeter^2 / area - 1.0)\n* g) concavity (severity of concave portions of the contour)\n* h) concave points (number of concave portions of the contour)\n* i) symmetry\n* j) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\n\n  [1]: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"cb2d4788-9e68-4d4e-b386-d76496916a3c","collapsed":false,"_uuid":"2226bdc589a886908b00daf7d875ede601c30c15"},"source":"# 2 - Preparing the data "},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"1c514647-89b0-4f97-a581-3ffa6ea22532","collapsed":false,"_uuid":"a7f28633d7069174b4fb67ad048617cbc59a0239"},"source":"We will start loading some of the packages that will help us organize and visualize the data. Other packages will be loaded as necessary. "},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"2733a412-ebaa-4dbd-94c2-017d837feec9","collapsed":false,"_uuid":"ef659b72ec573edfbb44e2d8fb02495fa2e9122b"},"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"227e151e-dfaf-4347-98fd-8c285b36d63f","collapsed":false,"_uuid":"2750ab93363c544e9d7e808686fba784aad54eab"},"source":"With help of [Pandas][1] we will load the data set and print some basic informations.\n\n\n  [1]: http://pandas.pydata.org/"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"db98d754-c297-43c1-ba6f-7ecdfe5992d6","collapsed":false,"_uuid":"d73d7c66c9f49d54ce6476644c5dc6dce3736043"},"source":"data = pd.read_csv('../input/data.csv');\n\nprint(\"\\n \\t The data frame has {0[0]} rows and {0[1]} columns. \\n\".format(data.shape))\ndata.info()\n\ndata.head(3)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"13161f74-1692-4861-962c-cd35bb0405d5","collapsed":false,"_uuid":"abc3318cef2955e34e3e33ac19e6b487fd3b9553"},"source":"As can bee seen above, except for the diagnosis (that is M = malignant or B = benign ) all other features are of type `float64` and have 0 non-null numbers.\n\nDuring the data set loading a extra column was created. We will use the code below to delete this entire column. "},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"81371fe1-f427-4644-ab4f-fa2ea70000f0","collapsed":false,"_uuid":"84a8bfdbb7f53a118a8462b0b3cfc10f39dff5d4"},"source":"data.drop(data.columns[[-1, 0]], axis=1, inplace=True)\n\ndata.info()"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"781888a4-1f46-4cab-beb8-3c16a0428504","collapsed":false,"_uuid":"0733beb2b10b569a08116bd426835aa9bbffeb10"},"source":"Now we can count how many diagnosis are malignant (M) and how many are benign (B). This is done below."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"ff4293d0-ebb5-433a-afae-f6e639492eca","collapsed":false,"_uuid":"def4d3f0159e70913d39acdee8b4e4668c3c4215"},"source":"diagnosis_all = list(data.shape)[0]\ndiagnosis_categories = list(data['diagnosis'].value_counts())\n\nprint(\"\\n \\t The data has {} diagnosis, {} malignant and {} benign.\".format(diagnosis_all, \n                                                                                 diagnosis_categories[0], \n                                                                                 diagnosis_categories[1]))"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"298a4265-12fb-42c2-a011-64ac305df170","collapsed":false,"_uuid":"61f5cb3c1508840ad917bd4fddf416a4b4f90eff"},"source":"# **3 - Visualizing the data**"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"da48f9ab-62a3-4586-bd79-40c1f04a92b1","collapsed":false,"_uuid":"e8a9e58fa3ae674c5d0bb29d697c0bebd92de0f4"},"source":"In this section we will build visualizations of the data in order to decide how to proceed with the machine learning tools. To do that, we will need to use the [Seaborn][1] and the [Matplotlib][2] packages. \n\nWe are interested mainly in the mean values of the features, so we will separate those features in the list below in order to make some work easier and the code more readably.\n\n\n  [1]: https://seaborn.pydata.org/\n  [2]: https://matplotlib.org/"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"c01219a2-d591-43a7-9aaa-91076a3cb539","collapsed":false,"_uuid":"305fc39e7f947a20cd6bd42a0ea9e171d5dd841a"},"source":"features_mean= list(data.columns[1:11])"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"4210e133-4b8c-457e-b6c0-2dfd03a03ba1","collapsed":false,"_uuid":"07ed22d4f3dbe0b875de3a271ff6dbb783eb8d1a"},"source":"Below we will use Seaborn to create a heat map of the correlations between the features."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"76ed231c-4a07-457f-a8ad-49d0f8b7c0da","collapsed":false,"_uuid":"45d01e6c7b2f201a7ec6d202a12870c475eab1e3"},"source":"plt.figure(figsize=(10,10))\nsns.heatmap(data[features_mean].corr(), annot=True, square=True, cmap='coolwarm')\nplt.show()"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"ad57b9b6-ec7b-4052-b728-c60f10e6e3d1","collapsed":false,"_uuid":"c626c81e4dda473b8b7cf11963f24f26de18f65e"},"source":"It is also possible to create a scatter matrix with the features. The red dots correspond to malignant diagnosis and blue to benign. Look how in some cases reds and blues dots occupies different regions of the plots. "},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"c55d8882-5c6d-41f9-87c0-f4c9ce8347ee","collapsed":false,"_uuid":"cfe3ed3fe324c30ca268f15f56118b366fa76a35"},"source":"color_dic = {'M':'red', 'B':'blue'}\ncolors = data['diagnosis'].map(lambda x: color_dic.get(x))\n\nsm = pd.scatter_matrix(data[features_mean], c=colors, alpha=0.4, figsize=((15,15)));\n\nplt.show()"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"4d0118ec-4607-420d-a375-82153e94e9fc","collapsed":false,"_uuid":"eb700220920e491460293477e750ec9fa189c4e0"},"source":"We can also see how the malignant or benign tumors cells can have (or not) different values for the features plotting the distribution of each type of diagnosis for each of the mean features. "},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"c9a59fdc-398e-4fba-a078-1729f542df32","collapsed":false,"_uuid":"99c02cb5fc869ce7df959d4d4de57da27fb5966d"},"source":"bins = 12\nplt.figure(figsize=(15,15))\nfor i, feature in enumerate(features_mean):\n    rows = int(len(features_mean)/2)\n    \n    plt.subplot(rows, 2, i+1)\n    \n    sns.distplot(data[data['diagnosis']=='M'][feature], bins=bins, color='red', label='M');\n    sns.distplot(data[data['diagnosis']=='B'][feature], bins=bins, color='blue', label='B');\n    \n    plt.legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"3c4fe43d-2328-4b42-b60b-fdaf5107f8ca","collapsed":false,"_uuid":"da8571181a586a682e52009695c80681e8ebc311"},"source":"Still another form of doing this could be using box plots, which is done below. "},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"9c26b39d-de4e-4d1a-bd75-add0262e8c3a","collapsed":false,"_uuid":"e16b0119a5b11e0ce200001a2b9922b267f10f81"},"source":"plt.figure(figsize=(15,15))\nfor i, feature in enumerate(features_mean):\n    rows = int(len(features_mean)/2)\n    \n    plt.subplot(rows, 2, i+1)\n    \n    sns.boxplot(x='diagnosis', y=feature, data=data, palette=\"Set1\")\n\nplt.tight_layout()\nplt.show()"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"3dac97ef-711d-42be-8200-026158eabdea","collapsed":false,"_uuid":"71d8a186ca214663cfcc6b350d689433f7118dce"},"source":"As we saw above, some of the features can have, most of the times, values that will fall in some range depending on the diagnosis been malignant or benign. We will select those features to use in the next section."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"be305a38-96e9-4064-aa1d-60ba45850016","collapsed":false,"_uuid":"9f0c102024ec90cf9d910183a2524b73b994ab5e"},"source":"features_selection = ['radius_mean', 'perimeter_mean', 'area_mean', 'concavity_mean', 'concave points_mean']"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"5508a001-4fa4-44e1-8d02-b92d8794a6da","collapsed":false,"_uuid":"db2ccebdaf6ef2804481e3b909c49a4795c01e5b"},"source":"# **4 - Machine learning**"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"f9509aab-386c-4ee9-bd2e-11aa0af276af","collapsed":false,"_uuid":"93eda761aa6bdb8e51a4131895c9c77fbc89f6b2"},"source":" In this section we will test and analyze machine learning algorithms for classification in order to identify if the tumor is malignant or benign based on the cell features. For this we will use [Scikit-learn][1] package. The necessary tools will be loaded as needed.\n\nThe problem we are dealing with here is a classification problem. To choose the right estimator (algorithm) we used the [flowchart][2] found in the Scikit-learn web page. \n\n\n  [1]: http://scikit-learn.org/stable/\n  [2]: http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"45617c8d-1214-4edc-926c-325d5bb6aa41","collapsed":false,"_uuid":"e941e021658da3deaebf3af714e5473a777615cf"},"source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score\n\nimport time"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"a26cc053-0944-41aa-b59b-cdf41d7b7a67","collapsed":false,"_uuid":"ee179f198478d0215ab21e4688b17ea5a53817cd"},"source":"The algorithms will process only numerical values. For this reason, we will transform the categories M and B into values 1 and 0, respectively."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"bc16e9b5-0483-4bfd-a1d6-ff9997b14a92","collapsed":false,"_uuid":"91ec81a7ad834485c6db152bae4a3d701153a2f5"},"source":"diag_map = {'M':1, 'B':0}\ndata['diagnosis'] = data['diagnosis'].map(diag_map)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"ee2c3754-3482-4046-b158-6d7a46edfffc","collapsed":false,"_uuid":"955d6cb187e8d55d5659948a24dc7b4af3d84690"},"source":"## **4.1 - Using all mean values features**"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"76543569-9bbd-4e03-8980-06e436825d3e","collapsed":false,"_uuid":"f97a7f21ccb9744c574456a9facac0ff8d47a278"},"source":"Our aim is to construct a \"function\" y = f(X) such that the value of y (1 or 0) will be determined once we input the values X into f. The \"function\" f will be construct by the machine learning algorithm based on the ys and Xs that are already known. \n\nAfter training our machine learning algorithm we need to test its accuracy. In order to avoid [Overfitting][1] we will use the function `train_test_split` to split the data randomly (`random_state = 42`) into a train and a test set. The test set will correspond to 20% of the total data (`test_size = 0.2`).\n\n\n  [1]: https://en.wikipedia.org/wiki/Overfitting"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"8f383010-bc38-43d1-9345-b3e6057ba0df","collapsed":false,"_uuid":"20079aec7ba6589dc4daaa29f5d84a6ee5b53004"},"source":"X = data.loc[:,features_mean]\ny = data.loc[:, 'diagnosis']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\naccuracy_all = []\ncvs_all = []"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"1c60e55b-6dfe-4d66-a470-38584d789908","collapsed":false,"_uuid":"4c181e0f14d5bcec60fee59f74436b731bbc72dc"},"source":"Next we will use nine different classifiers, all with standard parameters. In all cases, the procedure will be the following:\n\n1. the classifier `clf` is initialized;\n2. the classifier `clf` is fitted with the train data set `X_train` and `y_train`;\n3. the predictions are found using `X_test`;\n4. the accuracy is estimated with help of [cross-validation][1];\n5. the [accuracy][2] of the predictions is measured.\n\nAt the end the results are presents in %, along with the total time needed to run all the process. \n\n\n  [1]: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score\n  [2]: http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"ff7da984-cc86-4985-8baf-24127c6e9ed2","collapsed":false,"_uuid":"80e19fa9adba1c04368876ce4cf34382670e9cb1"},"source":"### **4.1.1 - Stochastic Gradient Descent**"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"97a81fd2-89e5-4596-a634-37a887f5e1e1","collapsed":false,"_uuid":"fc3cd2e8192a3d40e2e0a818f1a8d50d7e88c77d"},"source":"The first classifier is the [Stochastic Gradient Descent][1].\n\n\n  [1]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"baef7a14-0d28-401c-a18e-b7ff29920a82","collapsed":false,"_uuid":"c3bde681b1cb32c2c5da910f28a87193cedd21e1"},"source":"from sklearn.linear_model import SGDClassifier\n\nstart = time.time()\n\nclf = SGDClassifier()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_all.append(accuracy_score(prediction, y_test))\ncvs_all.append(np.mean(scores))\n\nprint(\"SGD Classifier Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"d5fa47ea-a589-42b6-9308-a3b19c9dbc8c","collapsed":false,"_uuid":"199bfec2a49ec2a3843a89c73228d4f644138f74"},"source":"### 4.1.2 - **Support Vector Machines**"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"899d52ce-d6f5-42b6-b7b1-19c88fc33451","collapsed":false,"_uuid":"93ec9ddbd5b0f431ce230e4c096938dbe9b8de64"},"source":"Now we will use three different [Support Vector Machines][1] classifiers.\n\n\n  [1]: http://scikit-learn.org/stable/modules/svm.html"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"103ac754-777d-443b-9d35-9aac57a2c75f","collapsed":false,"_uuid":"f0f88025b60798eceadfb21b9400621fa5290c38"},"source":"from sklearn.svm import SVC, NuSVC, LinearSVC\n\nstart = time.time()\n\nclf = SVC()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_all.append(accuracy_score(prediction, y_test))\ncvs_all.append(np.mean(scores))\n\nprint(\"SVC Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\n\nstart = time.time()\n\nclf = NuSVC()\nclf.fit(X_train, y_train)\nprediciton = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_all.append(accuracy_score(prediction, y_test))\ncvs_all.append(np.mean(scores))\n\nprint(\"NuSVC Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\n\nstart = time.time()\n\nclf = LinearSVC()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_all.append(accuracy_score(prediction, y_test))\ncvs_all.append(np.mean(scores))\n\nprint(\"LinearSVC Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"d16c53b1-ba31-40d7-8739-6623d5bb194a","collapsed":false,"_uuid":"e6926f7a57df61226ed6c0f6dcbc6239d8d288bf"},"source":"### **4.1.3 - Nearest Neighbors**"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"45a60436-b20c-465f-a4a0-4b611dbab4d7","collapsed":false,"_uuid":"87d2f82ddaf4d864348378a6264e3871967ed5c4"},"source":"The nearest neighbors classifier finds predefined number of training samples closest in distance to the new point, and predict the label from these."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"06df89f5-2e67-4a62-94f7-9d519884b0ac","collapsed":false,"_uuid":"d324d4b6dbe4da6f32548bb38c8f722285fb84f2"},"source":"from sklearn.neighbors import KNeighborsClassifier\n\nstart = time.time()\n\nclf = KNeighborsClassifier()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_all.append(accuracy_score(prediction, y_test))\ncvs_all.append(np.mean(scores))\n\nprint(\"Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"4d038581-196a-4236-a257-1b200990fc45","collapsed":false,"_uuid":"306b2a52c88ca6008cc9860b8312b16f98a5cc97"},"source":"### 4.1.3 - **Naive Bayes**"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"689cbed0-7a13-4cff-a704-b22ee5f9124d","collapsed":false,"_uuid":"063e54f8daf2a5040434e3925ca37b08d38d2c2f"},"source":"The Naive Bayes algorithm applies Bayesâ€™ theorem with the assumption of independence between every pair of features."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"dfb4ab15-9438-4981-9076-0651c35f579e","collapsed":false,"_uuid":"ec2ea3c806ddcfb8528e35fa2d069046be8cc0c4"},"source":"from sklearn.naive_bayes import GaussianNB\n\nstart = time.time()\n\nclf = GaussianNB()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_all.append(accuracy_score(prediction, y_test))\ncvs_all.append(np.mean(scores))\n\nprint(\"Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"f3566453-9185-42dc-aea6-1f739ba87c3a","collapsed":false,"_uuid":"555cc87ec695ee206f9da085b57a9b62ddcee426"},"source":"###  **4.1.4 - Forest and tree methods**"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"9fd70093-c8a3-4dff-839e-492eaeb5962f","collapsed":false,"_uuid":"c57aeefaba5d3dbdab45201f3b81c6a25c91143f"},"source":""},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"284bc71c-e269-404e-b60a-04afdd2622e2","collapsed":false,"_uuid":"357060c908049e9921dedd72da67d1d270650a9a"},"source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nstart = time.time()\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_all.append(accuracy_score(prediction, y_test))\ncvs_all.append(np.mean(scores))\n\nprint(\"Random Forest Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\n\nstart = time.time()\n\nclf = ExtraTreesClassifier()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_all.append(accuracy_score(prediction, y_test))\ncvs_all.append(np.mean(scores))\n\nprint(\"Extra Trees Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\n\nstart = time.time()\n\nclf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_all.append(accuracy_score(prediction, y_test))\ncvs_all.append(np.mean(scores))\n\nprint(\"Dedicion Tree Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"c13cc627-f578-401d-8e6c-2efa150369ac","collapsed":false,"_uuid":"9b57b06e969d5412c8922611b656bab9501420cb"},"source":"## **4.2 - Using the selected features**"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"43d596ad-6b2f-4695-b3df-631e6a45fcd6","collapsed":false,"_uuid":"63078ea2ddd5c7e0b02b33481fefb6a738c54d89"},"source":"In this section we will apply the same classifiers for the data with the features that were previously selected based on the analysis of section 3. To remember, those features are: radius_mean, perimeter_mean, area_mean, concavity_mean, concave points_mean.\n\nIn the end we will compare the accuracy the cross validation score for the selected set and the complete set of features."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"10934f5c-50c4-4b80-b988-5402b8c8b098","collapsed":false,"_uuid":"98201ce6d238e1286943cd36d1bc78040302e405"},"source":"X = data.loc[:,features_selection]\ny = data.loc[:, 'diagnosis']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\naccuracy_selection = []\ncvs_selection = []"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"d0778a8e-ea04-494b-957a-33d016d669ef","collapsed":false,"_uuid":"e2fb689a1bc829ecf771ae6cee87aac3f18c8e59"},"source":"### **4.2.1 - Stochastic Gradient Descent**"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"392a396b-261e-4382-b74a-2e52c224686b","collapsed":false,"_uuid":"01aae0131141147c231a5b801b2cdeab3182c091"},"source":"from sklearn.linear_model import SGDClassifier\n\nstart = time.time()\n\nclf = SGDClassifier()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_selection.append(accuracy_score(prediction, y_test))\ncvs_selection.append(np.mean(scores))\n\nprint(\"SGD Classifier Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"4aef005f-875f-43df-a222-05b77474273d","collapsed":false,"_uuid":"92c6938c37a106f49225acf76c88bc7911206bf9"},"source":"### **4.2.2 - Support Vector Machines**"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"734e7d7e-1b58-4d4d-82d1-7d0cb03fe4c4","collapsed":false,"_uuid":"d9741cb03b24e6925288ba2fa6e7ddc1d3930e4c"},"source":"from sklearn.svm import SVC, NuSVC, LinearSVC\n\nstart = time.time()\n\nclf = SVC()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_selection.append(accuracy_score(prediction, y_test))\ncvs_selection.append(np.mean(scores))\n\nprint(\"SVC Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))\n\nstart = time.time()\n\nclf = NuSVC()\nclf.fit(X_train, y_train)\nprediciton = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_selection.append(accuracy_score(prediction, y_test))\ncvs_selection.append(np.mean(scores))\n\nprint(\"NuSVC Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))\n\nstart = time.time()\n\nclf = LinearSVC()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_selection.append(accuracy_score(prediction, y_test))\ncvs_selection.append(np.mean(scores))\n\nprint(\"LinearSVC Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"60f7408f-f21b-402c-af12-41f81e6bf11c","collapsed":false,"_uuid":"dd5164816b5bc387f40e9698cdf239b3c8f0f895"},"source":"### **4.2.3 - Nearest Neighbors**"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"5f1455fd-786a-425f-bdd7-f8ca2e2e818c","collapsed":false,"_uuid":"5756034c41bf297476e687aa0f41ac9ed4909f28"},"source":"from sklearn.neighbors import KNeighborsClassifier\n\nstart = time.time()\n\nclf = KNeighborsClassifier()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_selection.append(accuracy_score(prediction, y_test))\ncvs_selection.append(np.mean(scores))\n\nprint(\"Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"f5afaa4e-d5ca-423b-83a9-d863a27d11d2","collapsed":false,"_uuid":"2992a584d088c4371419501a8cd2bb6a0ae90f2f"},"source":"### **4.2.4 - Naive Bayes**"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"7d4cb0c9-dd2d-4cba-811d-d57654feb8c2","collapsed":false,"_uuid":"da7818fe40a1147a681ab653efb029af4c5b9e6f"},"source":"from sklearn.naive_bayes import GaussianNB\n\nstart = time.time()\n\nclf = GaussianNB()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_selection.append(accuracy_score(prediction, y_test))\ncvs_selection.append(np.mean(scores))\n\nprint(\"Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"9a355100-4a39-442a-b9d8-29357a74891d","collapsed":false,"_uuid":"865fd15e6792c94833440593a0a555636737d2e2"},"source":"### **4.2.5 - Forest and tree methods**"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"5a7c4c72-92b6-49d8-9af6-e419f89915a7","collapsed":false,"_uuid":"92a95afdcaa5529f16b32d1909c5f5d9c2ec6f37"},"source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nstart = time.time()\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_selection.append(accuracy_score(prediction, y_test))\ncvs_selection.append(np.mean(scores))\n\nprint(\"Random Forest Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))\n\nstart = time.time()\n\nclf = ExtraTreesClassifier()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_selection.append(accuracy_score(prediction, y_test))\ncvs_selection.append(np.mean(scores))\n\nprint(\"Extra Trees Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))\n\nstart = time.time()\n\nclf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_selection.append(accuracy_score(prediction, y_test))\ncvs_selection.append(np.mean(scores))\n\nprint(\"Dedicion Tree Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"29b7b1e4-010f-48ed-88d9-5e4e545eb5e3","collapsed":false,"_uuid":"9669bf2c85a562b276cdf1858b9c047ae9521df2"},"source":"diff_accuracy = list(np.array(accuracy_selection) - np.array(accuracy_all))\ndiff_cvs = list(np.array(cvs_selection) - np.array(cvs_all))\n\nd = {'accuracy_all':accuracy_all, 'accuracy_selection':accuracy_selection, 'diff_accuracy':diff_accuracy, \n     'cvs_all':cvs_all, 'cvs_selection':cvs_selection, 'diff_cvs':diff_cvs,}\n\nindex = ['SGD', 'SVC', 'NuSVC', 'LinearSVC', 'KNeighbors', 'GaussianNB', 'RandomForest', 'ExtraTrees', 'DecisionTree']\n\ndf = pd.DataFrame(d, index=index)"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"2182d12e-9c2e-45f1-aacc-ba474c71c8e6","collapsed":false,"_uuid":"68886f0d563d730e9ec294f79933ad6101ab7763"},"source":"df"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"df9d05b6-483e-4258-b6e8-ca1b68316a52","collapsed":false,"_uuid":"70762cbe26d4c97012164f0e6e7c02fa1892f0ef"},"source":"As can be seen in the table above, using only some of the mean features reduced, in most of the cases, both accuracy and cross-validation scores."},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"75756773-0cb7-490b-98ff-32afba203b1d","collapsed":false,"_uuid":"6a633982f450e16acdd6145d1660b411f2d479b0"},"source":"# **5 - Improving the best model**"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"7652e7ed-0d54-4521-902a-cdd1001268cb","collapsed":false,"_uuid":"f2add5f90c6b914d1f69aced681d64d0ebea46d0"},"source":"Not all parameters of a classifier is learned from the estimators. Those parameters are called hyper-parameters and are passed as arguments to the constructor of the classifier. Each estimator has a different set of hyper-parameters, which can be found in the corresponding documentation. \n\nWe can search for the best performance of the classifier sampling different hyper-parameter combinations. This will be done with an [exhaustive grid search][1], provided by the GridSearchCV function. \n\nThe grid search will be done only on the best models, which are Naive Bayes, Random Forest, Extra Trees and Decision Trees.\n\nAfter running the piece of codes below, it will be presented the accuracy, the cross-validation score and the best set of parameters.  \n\n\n  [1]: http://scikit-learn.org/stable/modules/grid_search.html#grid-search"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"37e0b381-1622-4e5e-aba5-d2d894136f4c","collapsed":false,"_uuid":"ad59fa6883a53a6119a479131040a4982c1dee95"},"source":"from sklearn.model_selection import GridSearchCV\n\nX = data.loc[:,features_mean]\ny = data.loc[:, 'diagnosis']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\naccuracy_all = []\ncsv_all = []"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"ed132b1c-1ce5-47ad-b816-355a6c62905f","collapsed":false,"_uuid":"e326c5b82aaa040917441b2371c0719de07f39a7"},"source":"## **5.1 - Naive Bayes**"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"3ab15e34-b37e-4a61-8f0e-0fe8df79fc03","collapsed":false,"_uuid":"c44b4f053f5ddb6d9170c6fed4a0fd91b3c2f4bf"},"source":"start = time.time()\n\nparameters = {'priors':[[0.01, 0.99],[0.1, 0.9], [0.2, 0.8], [0.25, 0.75], [0.3, 0.7],[0.35, 0.65], [0.4, 0.6]]}\n\nclf = GridSearchCV(GaussianNB(), parameters, scoring = 'average_precision', n_jobs=-1)\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_all.append(accuracy_score(prediction, y_test))\ncvs_all.append(np.mean(scores))\n\nprint(\"Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: {0:.5} seconds \\n\".format(end-start))\n\nprint(\"Best parameters: {0}\".format(clf.best_params_))"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"40678f13-9014-414d-bd5c-000d8922679b","collapsed":false,"_uuid":"2bd88727c65b734c8af719c36729bfd816653673"},"source":"## **5.2 - Forest and tree methods**"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"9b87b95e-9148-4a19-b6cc-407d44699b28","collapsed":false,"_uuid":"fbcd1122fda5337a12fd521ffd679da9f6098bec"},"source":"start = time.time()\n\nparameters = {'n_estimators':list(range(1,101)), 'criterion':['gini', 'entropy']}\n\nclf = GridSearchCV(RandomForestClassifier(), parameters, scoring = 'average_precision', n_jobs=-1)\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_all.append(accuracy_score(prediction, y_test))\ncvs_all.append(np.mean(scores))\n\nprint(\"Random Forest Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))\n\nprint(\"Best parameters: {0} \\n\".format(clf.best_params_))\n\nstart = time.time()\n\nclf = GridSearchCV(ExtraTreesClassifier(), parameters, scoring = 'average_precision', n_jobs=-1)\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_all.append(accuracy_score(prediction, y_test))\ncvs_all.append(np.mean(scores))\n\nprint(\"Extra Trees Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))\n\nprint(\"Best parameters: {0} \\n\".format(clf.best_params_))\n\nstart = time.time()\n\nparameters = {'criterion':['gini', 'entropy'], 'splitter':['best', 'random']}\n\nclf = GridSearchCV(DecisionTreeClassifier(), parameters, scoring = 'average_precision', n_jobs=-1)\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nscores = cross_val_score(clf, X, y, cv=5)\n\nend = time.time()\n\naccuracy_all.append(accuracy_score(prediction, y_test))\ncvs_all.append(np.mean(scores))\n\nprint(\"Dedicion Tree Accuracy: {0:.2%}\".format(accuracy_score(prediction, y_test)))\nprint(\"Cross validation score: {0:.2%} (+/- {1:.2%})\".format(np.mean(scores), np.std(scores)*2))\nprint(\"Execution time: %s seconds \\n\" % \"{0:.5}\".format(end-start))\n\nprint(\"Best parameters: {0} \\n\".format(clf.best_params_))"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_cell_guid":"1e673c9e-e8dc-4603-8a09-984ca29be08e","collapsed":false,"_uuid":"87a35d9ef79f49875d382b0554bc1631bc34f8b9"},"source":"As can be seen, in one case (Extra Trees) both accuracy and cross-validations score were improved,  but only by some few percents and with the cost of more computational resources and time. In other cases only the accuracy or the cross-validation score could be improved."}],"nbformat":4,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"version":"3.6.1","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","name":"python"}}}