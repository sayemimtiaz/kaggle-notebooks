{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"25% of customers fall in the age of 28, 50% are 36 and 75% are 49.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Histogram of numerical features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndf_plots=df.select_dtypes(exclude=\"object\")\n\nfig=make_subplots(rows=2, cols=2,subplot_titles=df_plots.columns)\n\nindex=0\n\nfor i in range(1,3):\n    for j in range(1,3):\n        data=df[df_plots.columns[index]]\n        trace=go.Histogram(x=data)\n        fig.append_trace(trace,i,j)\n        index+=1\n        \nfig.update_layout(height=900,width=1200,title_text=\"Numerical Attributes\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most distributions seem to follow a somewhat Gaussian distribution.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's write the above code as a function so we can use it later.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist_num(df):\n    df_plots=df.select_dtypes(exclude=\"object\")\n\n    fig=make_subplots(rows=1, cols=3,subplot_titles=df_plots.columns)\n\n    index=0\n\n    for i in range(1,2):\n        for j in range(1,4):\n            data=df[df_plots.columns[index]]\n            trace=go.Histogram(x=data)\n            fig.append_trace(trace,i,j)\n            index+=1\n        \n    fig.update_layout(height=300,width=900,title_text=\"Numerical Attributes\")\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outliers\n\nWe will check this with the skewness value. Explains the extent to which data is normally distributed. Value should lie between -1 to +1. Any major deviation from this indicates presence of extreme values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the skewness values lie in the desired range, no changes will be made to the dataset.\n\nSource: https://stats.stackexchange.com/questions/328109/k-means-does-it-make-sense-to-remove-the-outliers-after-clustering-the-datasets#:~:text=4%20Answers&text=K%2Dmeans%20can%20be%20quite,means%2C%20or%20you%20use%20DBSCAN.\n\nIf there were outliers, we would need to treat it as K Means is sensitive to outliers. We have 2 options:\n\n1. Remove outlier first and then apply your clustering algorithm (for this step itself you may use clustering algorithms!). Please note that k-means itself is not a Soft Clustering algorithm so it does not model the overlaps. For that you may use algorithms like Fuzzy C-Means. There you can define an overlap by clusters for which the memberships of a sample are closer than a threshold.\n\n2. Ignore the outlier removal and just use more robust variations of K-means, e.g. K-medoids or K-Medians, to reduce the effect of outliers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Dropping unnecessary features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will also drop the CustomerID column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['CustomerID','Gender'],axis=1,inplace=True)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Standardizing the dataset using Power Transformer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\n\npt=PowerTransformer()\n\n#PowerTransformer() takes the input of the form {array-like, sparse matrix, dataframe} of shape (n_samples, n_features)\ndf_transformed=pt.fit_transform(df.values.reshape(-1,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert array to dataframe to plot it\npd_df_transformed=pd.DataFrame(df_transformed,columns=df.columns)\n\n#plot the histogram to see change in distrbution\nplot_hist_num(pd_df_transformed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd_df_transformed.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features are now standardized and have a gaussian distribution.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Standardizing the dataset using Quantile Transformer\n\nSource: https://scikit-learn.org/stable/modules/preprocessing.html#non-linear-transformation\n\nTwo types of transformations are available: quantile transforms and power transforms.\n\nQuantileTransformer applies a non-linear transformation such that the probability density function of each feature will be mapped to a uniform distribution. As RobustScaler, QuantileTransformer is robust to outliers in the sense that adding or removing outliers in the training set will yield approximately the same transformation on held out data.\n\n<b> Effect of different transformations on different types of distributions </b>\n\n<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_map_data_to_normal_0011.png\">\n\nWe will also use Quantile transformer on our dataset to see if it performs better than Power Transformer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\n\nqt=QuantileTransformer(random_state=0)\n\n#PowerTransformer() takes the input of the form {array-like, sparse matrix, dataframe} of shape (n_samples, n_features)\ndf_quantile_transformed=qt.fit_transform(df.values.reshape(-1,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert array to dataframe to plot it\npd_df_quantile_transformed=pd.DataFrame(df_quantile_transformed,columns=df.columns)\n\n#plot the histogram to see change in distrbution\nplot_hist_num(pd_df_quantile_transformed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Power Transformer seems to have done a better job and hence we will stick with it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# T-SNE\n\nSource: https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html\n\nWe can use the t-distributed stochastic neighbor embedding (t-SNE) algorithm to pre-process the data before performing clustering. t-SNE is a nonlinear embedding algorithm that is particularly adept at preserving points within clusters. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfrom sklearn.manifold import TSNE\n\n# Project the data: this step will take several seconds\ntsne = TSNE(n_components=2, init='random', random_state=0)\n\n#Fit_transform() accpets input of the type array, shape (n_samples, n_features) \nsne_df_transformed = tsne.fit_transform(df_transformed)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n#plot the clusters obtained from t-SNE\nfig = go.Figure(data=go.Scatter(x=sne_df_transformed.T[0],\n                                y=sne_df_transformed.T[1],\n                                mode='markers')) \n\nfig.update_layout(title='t-SNE distribution of data')\nfig.show()\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MDS for visualization\n\nt-SNE is framed as a visualization tool rather than a pre-processing or analysis tool.\n\nSource: https://stats.stackexchange.com/questions/351474/does-it-make-sense-to-run-dbscan-on-the-output-from-t-sne\n\nT-SNE is a manifold technique and as such does not preserve distances; therefore it is not recommended to run distance-based (e.g. k-means) or density-based (e.g. DBSCAN) clustering algorithms on the output of T-SNE. If you want a dimensional reduction algorithm that does preserve distances, you can use PCA  instead of T-SNE.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import MDS\nmds = MDS(n_components = 2)\n\nmds_df_transformed = mds.fit_transform(df_transformed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the clusters obtained from K Means\nfig = go.Figure(data=go.Scatter(x=mds_df_transformed.T[0],\n                                y=mds_df_transformed.T[1],\n                                mode='markers')) \n\nfig.update_layout(title='MDS Transformed data for visualization')\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering\n\nWe will try different clustering techqniues and see which one does best using the Silhoutte score.\n\n## 1. K Means","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"silhouette_k_means=[]\n\nfor k in range(2,10):\n    k_test=KMeans(n_clusters=k)\n    cluster_labels=k_test.fit_predict(df_transformed)\n    silhouette_avg = silhouette_score(df_transformed, cluster_labels)\n    silhouette_k_means.append(silhouette_avg)\n    \npx.line(x=range(2,10),y=silhouette_k_means)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will choose the number of clusters as 6 based on the above graph","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nWe will keep a track of the silhouette score and the model using silhouette_score_compiled\nWe will keep a track of the DB score and the model using db_score_compiled\n'''\n\nsilhouette_score_compiled={}\ndb_score_compiled={}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plug in optimal number of clusters \n\nk_means=KMeans(n_clusters=6)\nkmeans_labels=k_means.fit_predict(df_transformed)\nsilhouette_score_compiled['K Means'] = silhouette_score(df_transformed, kmeans_labels)\ndb_score_compiled['K Means']=metrics.davies_bouldin_score(df_transformed,kmeans_labels)\nprint(silhouette_score_compiled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the clusters obtained from K Means\nfig = go.Figure(data=go.Scatter(x=mds_df_transformed.T[0],\n                                y=mds_df_transformed.T[1],\n                                mode='markers',\n                                marker_color=kmeans_labels,text=kmeans_labels)) \n\nfig.update_layout(title='K Means')\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Mean Shift","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import MeanShift\nfrom sklearn.cluster import estimate_bandwidth\n\nest_bandwidth = estimate_bandwidth(df_transformed,quantile=0.1,n_samples=10000)\nms = MeanShift(bandwidth= est_bandwidth)\nms_labels=ms.fit_predict(df_transformed)\nsilhouette_score_compiled['Mean Shift'] = silhouette_score(df_transformed, ms_labels)\ndb_score_compiled['Mean Shift']=metrics.davies_bouldin_score(df_transformed,ms_labels)\nprint(silhouette_score_compiled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the clusters obtained from Mean Shift\nfig = go.Figure(data=go.Scatter(x=mds_df_transformed.T[0],\n                                y=mds_df_transformed.T[1],\n                                mode='markers',\n                                marker_color=ms_labels,text=ms_labels)) \n\nfig.update_layout(title='Mean Shift')\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. DBSCAN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import DBSCAN \nfrom matplotlib import pyplot as plt\nfrom sklearn.neighbors import NearestNeighbors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Choosing optimal epsilon value\n\nSource: https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc#:~:text=In%20layman's%20terms%2C%20we%20find,and%20select%20that%20as%20epsilon.\n\nWe must provide a value for epsilon which defines the maximum distance between two points. The following paper, describes an approach for automatically determining the optimal value for Eps:\nhttps://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf\n\nIn layman’s terms, we find a suitable value for epsilon by calculating the distance to the nearest n points for each point, sorting and plotting the results. Then we look to see where the change is most pronounced (think of the angle between your arm and forearm) and select that as epsilon.\n\nWe can calculate the distance from each point to its closest neighbour using the NearestNeighbors. The point itself is included in n_neighbors. The kneighbors method returns two arrays, one which contains the distance to the closest n_neighbors points and the other which contains the index for each of those points.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neigh = NearestNeighbors(n_neighbors=2)\nnbrs = neigh.fit(df_transformed)\ndistances, indices = nbrs.kneighbors(df_transformed)\n\n#sort and plot the results\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimal value for epsilon will be found at the point of maximum curvature. We train our model, selecting 0.2 for eps and setting min_samples to 5","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Choosing optimal 'minPts'\n\nSource: https://stackoverflow.com/questions/12893492/choosing-eps-and-minpts-for-dbscan-r\n\nA low minPts means it will build more clusters from noise, so don't choose it too small.\n\nminPts is best set by a domain expert who understands the data well. Unfortunately many cases we don't know the domain knowledge, especially after data is normalized. One heuristic approach is use ln(n), where n is the total number of points to be clustered.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.log(len(df_transformed))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will select the optimal values using grid search method\nfrom sklearn import metrics\n\ndb_results=pd.DataFrame(columns=['Eps','Min_Samples','Number of Cluster','Silhouette Score'])\nfor i in range(1,12):\n    for j in range(1,12):\n        dbscan_cluster = DBSCAN(eps=i*0.2, min_samples=j)\n        clusters=dbscan_cluster.fit_predict(df_transformed)\n        if len(np.unique(clusters))>2:\n              db_results=db_results.append({'Eps':i*0.2,\n                                      'Min_Samples':j,\n                                      'Number of Cluster':len(np.unique(clusters)),\n                                      'Silhouette Score':metrics.silhouette_score(df_transformed,clusters),\n                                      'Davies Bouldin Score':metrics.davies_bouldin_score(df_transformed,clusters)}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"db_results.sort_values('Silhouette Score',ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#choosing min_samples as 6 and eps as 0.6\ndbscan = DBSCAN(eps=0.6,min_samples=6)\ndbscan_labels= dbscan.fit_predict(df_transformed)\nsilhouette_score_compiled['DBSCAN'] = silhouette_score(df_transformed, dbscan_labels)\ndb_score_compiled['DBSCAN']=metrics.davies_bouldin_score(df_transformed,dbscan_labels)\nprint(silhouette_score_compiled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the clusters obtained from DBSCAN\nfig = go.Figure(data=go.Scatter(x=mds_df_transformed.T[0],\n                                y=mds_df_transformed.T[1],\n                                mode='markers',\n                                marker_color=dbscan_labels,text=dbscan_labels)) \n\nfig.update_layout(title='DBSCAN')\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Gaussian Mixture Models (GMMs)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Optimal n_components","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also compute Davies Bouldin score. The metric is Davies Bouldin that is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. The minimum score is zero, with lower values indicating better clustering.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters=['full','tied','diag','spherical']\nn_clusters=np.arange(1,10)\nresults_=pd.DataFrame(columns=['Covariance Type','Number of Cluster','Silhouette Score','Davies Bouldin Score'])\nfor i in parameters:\n    for j in n_clusters:\n        gmm_cluster=GaussianMixture(n_components=j,covariance_type=i,random_state=123)\n        clusters=gmm_cluster.fit_predict(df_transformed)\n        if len(np.unique(clusters))>=2:\n            results_=results_.append({\"Covariance Type\":i,'Number of Cluster':j,\"Silhouette Score\":metrics.silhouette_score(df_transformed,clusters),\n                                    'Davies Bouldin Score':metrics.davies_bouldin_score(df_transformed,clusters)}\n                                   ,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_.sort_values('Silhouette Score',ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gmm_labels = GaussianMixture(n_components=7,covariance_type='tied').fit_predict(df_transformed)\nsilhouette_score_compiled['GMM'] = silhouette_score(df_transformed, gmm_labels)\ndb_score_compiled['GMM']=metrics.davies_bouldin_score(df_transformed,gmm_labels)\nprint(silhouette_score_compiled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the clusters obtained from GMM\nfig = go.Figure(data=go.Scatter(x=mds_df_transformed.T[0],\n                                y=mds_df_transformed.T[1],\n                                mode='markers',\n                                marker_color=gmm_labels,text=gmm_labels)) \n\nfig.update_layout(title='GMM')\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Agglomerative Hierarchical Clustering\n\nHierarchical clustering is a clustering technique that aims to create a tree like clustering hierarchy within the data. On this model, to determine the n_clusters, we can able to use a dendogram.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters=['ward', 'complete', 'average', 'single']\nn_clusters=np.arange(1,10)\nagh_cluster_results_=pd.DataFrame(columns=['Linkage Type','Number of Cluster','Silhouette Score','Davies Bouldin Score'])\nfor i in parameters:\n    for j in n_clusters:\n        agh_cluster=AgglomerativeClustering(n_clusters=j,linkage=i)\n        clusters=agh_cluster.fit_predict(df_transformed)\n        if len(np.unique(clusters))>=2:\n            agh_cluster_results_=agh_cluster_results_.append({\"Linkage Type\":i,'Number of Cluster':j,\"Silhouette Score\":metrics.silhouette_score(df_transformed,clusters),\n                                    'Davies Bouldin Score':metrics.davies_bouldin_score(df_transformed,clusters)}\n                                   ,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agh_cluster_results_.sort_values('Silhouette Score',ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agh_labels=AgglomerativeClustering(n_clusters=8,linkage='average').fit_predict(df_transformed)\nsilhouette_score_compiled['Agglomerative Hierarchical Clustering'] = silhouette_score(df_transformed, agh_labels)\ndb_score_compiled['Agglomerative Hierarchical Clustering']=metrics.davies_bouldin_score(df_transformed,agh_labels)\nprint(silhouette_score_compiled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the clusters obtained from Agglomerative Hierarchical Clustering\nfig = go.Figure(data=go.Scatter(x=mds_df_transformed.T[0],\n                                y=mds_df_transformed.T[1],\n                                mode='markers',\n                                marker_color=agh_labels,text=agh_labels)) \n\nfig.update_layout(title='Agglomerative Hierarchical Clustering')\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compare the results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_df = pd.DataFrame(list(silhouette_score_compiled.items()),columns = ['Algo','Silhouette Score']) \ndb_df = pd.DataFrame(list(db_score_compiled.items()),columns = ['Algo','Davies Bouldin Score']) \nfinal_results=pd.merge(ss_df,db_df,left_on=\"Algo\",right_on=\"Algo\")\nfinal_results.sort_values('Silhouette Score',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"K-Means has the best Silhouette and the second best Davies Bouldin score. For this reason, K-Means Algorithm is more suitable for customer segmentation. Thus we have 6 customer types. Let’s try to understand behaviours or labels of customers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Understanding the results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Final Clusters']=kmeans_labels\ndf.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Final Clusters'].value_counts().index.sort_values(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age=[]\nincome=[]\nspend=[]\ncluster_k=[]\nfor i in df['Final Clusters'].value_counts().index.sort_values(ascending=True):\n    df_test=df[df['Final Clusters']==i]\n    cluster_k.append(i)\n    age.append(round(df_test['Age'].mean(),0))\n    income.append(round(df_test['Annual Income (k$)'].mean(),0))\n    spend.append(round(df_test['Spending Score (1-100)'].mean(),0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d={'CLuster':cluster_k,'Age':age,'Income(k$)':income,'Spending score':spend}\ndf_cluster_result=pd.DataFrame(d)\ndf_cluster_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_plots=df_cluster_result[[\"Age\",\"Income(k$)\",\"Spending score\"]]\nfig=make_subplots(rows=1, cols=3,subplot_titles=df_plots.columns)\n\nindex=0\n\nfor i in range(1,2):\n    for j in range(1,4):\n        data=df_cluster_result[df_plots.columns[index]]\n        trace=go.Box(x=data)\n        fig.append_trace(trace,i,j)\n        index+=1\n        \nfig.update_layout(height=300,width=900,title_text=\"Boxplot of features of final Clusters\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Cluster 0: Young age, high income and highest spending score - Marketing campaigns towards these groups to keep encouraging them to spend\n- Cluster 1: Senior age, low income, low spending score\n- Cluster 2: Senior age, high income, low spending socre - More marketing campaigns targeted towards these people as income is high but spending is very low\n- CLuster 3: Young age, medium income, medium spending score\n- cluster 4: Senior age, medium income, medium spending score\n- Cluster 5: Young age, low income, high spending score - Marketing campaigns towards these groups to keep encouraging them to spend","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Scatter3d(\n    x= df['Age'],\n    y= df['Spending Score (1-100)'],\n    z= df['Annual Income (k$)'],\n    mode='markers',\n     marker=dict(\n        color = df['Final Clusters'], \n        size= 10,\n        line=dict(\n            color= df['Final Clusters'],\n            width= 12\n        ),\n        opacity=0.8\n     )\n)\ndata1 = [trace1]\n\nlayout = go.Layout(\n    title = 'Character vs Gender vs Alive or not',\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0  \n    ),\n    scene = dict(\n            xaxis = dict(title  = 'Age'),\n            yaxis = dict(title  = 'Spending Score'),\n            zaxis = dict(title  = 'Annual Income')\n        )\n)\n\nfig = go.Figure(data = data1, layout = layout)\nfig.show(\"notebook\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Source:\nhttps://github.com/muhammetbektas/Unsupervised-Learning/blob/master/Segmentation_of_Credit_Card_Users.ipynb","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}