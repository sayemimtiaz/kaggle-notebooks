{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.express as px # interactive plotting\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer # tf-idf is a common way of turning a text into a vector of term frequencies. Texts that are semantically close will be close in this (high-dimensional) space\nfrom sklearn.manifold import TSNE # t-SNE is a dimensionality reduction tool which is optimized to preserve distances, i.e. points that are close in the high-dimensional space should be close in the low-dimensional space and vice versa\nfrom sklearn.cluster import DBSCAN # density based clustering method. It groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (source: Wikipedia)\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator # for drawing word clouds and also setting stop words for vectorization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\ndf = pd.read_csv('../input/CORD-19-research-challenge/metadata.csv')\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the most part, academic papers can be basically summed up by the title and abstract, so we'll keep those two fields. We'll also keep the doi in case we need to find a paper (this seems to be the most universal identifier) and whether full-text is available. However, for the most part, only titles and abstracts will interest us."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df.loc[:,['title','abstract','doi','has_full_text']].dropna(subset=['title','abstract']).drop_duplicates('title')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of the titles and abstracts, it makes more sense to vectorize the abstracts as those are basically condensed versions of the whole paper. Some words are common headings of academic abstracts, e.g. Abstract, Background, Methods, so those should be considered stopwords, in addition to the usual stopwords."},{"metadata":{"trusted":true},"cell_type":"code","source":"ABSTRACT_STOPWORDS = set(['BACKGROUND','METHOD','METHODS','RESULT','RESULTS','CONCLUSION','CONCLUSIONS','SUPPLEMENTARY MATERIAL','AIM','CONTEXT','PARTICIPANTS','SUBJECTS','PATIENTS','ABSTRACT'])\nX = TfidfVectorizer(input='content', stop_words=list(STOPWORDS.union(ABSTRACT_STOPWORDS)), max_features=200).fit_transform(df_clean.loc[:, 'abstract']).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we've vectorized into a high dimension, we must do dimensionality reduction. Different techniques are appropriate for preserving different features of the data. In our case, we did the vectorization to look at semantic distance, ie which papers are \"closer together\" in some sense. If this is what we want to preserve, then t-SNE (as opposed to, say, PCA) is the way to go."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean['x_tsne'] = np.nan\ndf_clean['y_tsne'] = np.nan\ndf_clean.loc[:,['x_tsne','y_tsne']] = TSNE().fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot the low-dimensional vectorizations interactively, so that by hovering over a point, we see the title of the paper, as well as its index in the data frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(df_clean.reset_index(), x='x_tsne', y='y_tsne', hover_name='title', hover_data=['index'], width=1000, height=1000)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice there are some clusters and some outliers. When it comes to scientific literature, one paper is rarely the final word on anything, but rather a contribution to a wider conversation. Therefore, the more papers in a cluster, the more that cluster should be paid attention to. Therefore, let's not just assign cluster numbers, but have those cluster numbers be ordered with respect to the number of papers in the cluster."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean['cluster'] = DBSCAN(min_samples=1).fit(df_clean.loc[:,['x_tsne','y_tsne']]).labels_\ndf_clean.cluster = df_clean.cluster.replace(dict(np.array(list(enumerate(df_clean.cluster.value_counts().sort_values(ascending=False).index)))[:,::-1])) #so that cluster 0 is the biggest, 1 is the next biggest etc.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we plot interactively again, looking only at the largest clusters. We can hover over the points in a cluster to see their individual titles to confirm that they are indeed a semantic cluster."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(df_clean.loc[df_clean.cluster<200].reset_index(), x='x_tsne', y='y_tsne', hover_name='title', color='cluster', hover_data=['cluster','index'], width=1000, height=1000)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Any results you write to the current directory are saved as output.\ndf_clean.to_csv('df_clean.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now make a wordcloud for both titles and abstracts to find some common themes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"TITLE_STOPWORDS = set([])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_number = 187\ntitle_wordcloud = WordCloud(width=600, height=500, stopwords=STOPWORDS.union(TITLE_STOPWORDS)).generate(' '.join(df_clean.loc[df_clean.cluster==cluster_number, 'title']))\nabstract_wordcloud = WordCloud(width=600, height=500, stopwords=STOPWORDS.union(ABSTRACT_STOPWORDS)).generate(' '.join(df_clean.loc[df_clean.cluster==cluster_number, 'abstract']))\nfig, axes = plt.subplots(ncols=2, figsize=(30,10))\naxes[0].imshow(title_wordcloud, interpolation='bilinear')\naxes[0].set_title('Titles')\naxes[1].imshow(abstract_wordcloud, interpolation='bilinear')\naxes[1].set_title('Abstracts')\nfor i in range(2):\n    axes[i].set_xticks([])\n    axes[i].set_yticks([])\nfig.suptitle('Word cloud for %d articles in cluster %d' % ((df_clean.cluster==cluster_number).sum(), cluster_number))\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}