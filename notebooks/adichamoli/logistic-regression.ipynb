{"cells":[{"metadata":{},"cell_type":"markdown","source":"** Classification **\n\nClassification is a very important area of supervised machine learning. A large number of important machine learning problems fall within this area. There are many classification methods, and logistic regression is one of them.\n\n** What Is Classification? **\n\nSupervised machine learning algorithms define models that capture relationships among data. Classification is an area of supervised machine learning that tries to predict which class or category some entity belongs to, based on its features.\n\nFor example, you might analyze the employees of some company and try to establish a dependence on the features or variables, such as the level of education, number of years in a current position, age, salary, odds for being promoted, and so on. The set of data related to a single employee is one observation. The features or variables can take one of two forms:\n\nIndependent variables, also called inputs or predictors, donâ€™t depend on other features of interest (or at least you assume so for the purpose of the analysis).\nDependent variables, also called outputs or responses, depend on the independent variables.\n\n** When Do You Need Classification? **\n\nYou can apply classification in many fields of science and technology. For example, text classification algorithms are used to separate legitimate and spam emails, as well as positive and negative comments. You can check out Practical Text Classification With Python and Keras to get some insight into this topic. Other examples involve medical applications, biological classification, credit scoring, and more.\n\nImage recognition tasks are often represented as classification problems. For example, you might ask if an image is depicting a human face or not, or if itâ€™s a mouse or an elephant, or which digit from zero to nine it represents, and so on. To learn more about this, check out Traditional Face Detection With Python and Face Recognition with Python, in Under 25 Lines of Code.\n\n** Problem Formulation **\n\nWhen youâ€™re implementing the logistic regression of some dependent variable ð‘¦ on the set of independent variables ð± = (ð‘¥â‚, â€¦, ð‘¥áµ£), where ð‘Ÿ is the number of predictors ( or inputs), you start with the known values of the predictors ð±áµ¢ and the corresponding actual response (or output) ð‘¦áµ¢ for each observation ð‘– = 1, â€¦, ð‘›.\n\nYour goal is to find the logistic regression function ð‘(ð±) such that the predicted responses ð‘(ð±áµ¢) are as close as possible to the actual response ð‘¦áµ¢ for each observation ð‘– = 1, â€¦, ð‘›. Remember that the actual response can be only 0 or 1 in binary classification problems! This means that each ð‘(ð±áµ¢) should be close to either 0 or 1. Thatâ€™s why itâ€™s convenient to use the sigmoid function.\n\nOnce you have the logistic regression function ð‘(ð±), you can use it to predict the outputs for new and unseen inputs, assuming that the underlying mathematical dependence is unchanged.\n\n** Methodology **\n\nLogistic regression is a linear classifier, so youâ€™ll use a linear function ð‘“(ð±) = ð‘â‚€ + ð‘â‚ð‘¥â‚ + â‹¯ + ð‘áµ£ð‘¥áµ£, also called the logit. The variables ð‘â‚€, ð‘â‚, â€¦, ð‘áµ£ are the estimators of the regression coefficients, which are also called the predicted weights or just coefficients.\n\nThe logistic regression function ð‘(ð±) is the sigmoid function of ð‘“(ð±): ð‘(ð±) = 1 / (1 + exp(âˆ’ð‘“(ð±)). As such, itâ€™s often close to either 0 or 1. The function ð‘(ð±) is often interpreted as the predicted probability that the output for a given ð± is equal to 1. Therefore, 1 âˆ’ ð‘(ð‘¥) is the probability that the output is 0.\n\nLogistic regression determines the best predicted weights ð‘â‚€, ð‘â‚, â€¦, ð‘áµ£ such that the function ð‘(ð±) is as close as possible to all actual responses ð‘¦áµ¢, ð‘– = 1, â€¦, ð‘›, where ð‘› is the number of observations. The process of calculating the best weights using available observations is called model training or fitting.\n\nTo get the best weights, you usually maximize the log-likelihood function (LLF) for all observations ð‘– = 1, â€¦, ð‘›. This method is called the maximum likelihood estimation and is represented by the equation LLF = Î£áµ¢(ð‘¦áµ¢ log(ð‘(ð±áµ¢)) + (1 âˆ’ ð‘¦áµ¢) log(1 âˆ’ ð‘(ð±áµ¢))).\n\nWhen ð‘¦áµ¢ = 0, the LLF for the corresponding observation is equal to log(1 âˆ’ ð‘(ð±áµ¢)). If ð‘(ð±áµ¢) is close to ð‘¦áµ¢ = 0, then log(1 âˆ’ ð‘(ð±áµ¢)) is close to 0. This is the result you want. If ð‘(ð±áµ¢) is far from 0, then log(1 âˆ’ ð‘(ð±áµ¢)) drops significantly. You donâ€™t want that result because your goal is to obtain the maximum LLF. Similarly, when ð‘¦áµ¢ = 1, the LLF for that observation is ð‘¦áµ¢ log(ð‘(ð±áµ¢)). If ð‘(ð±áµ¢) is close to ð‘¦áµ¢ = 1, then log(ð‘(ð±áµ¢)) is close to 0. If ð‘(ð±áµ¢) is far from 1, then log(ð‘(ð±áµ¢)) is a large negative number.\n\nThere are several mathematical approaches that will calculate the best weights that correspond to the maximum LLF, but thatâ€™s beyond the scope of this tutorial. For now, you can leave these details to the logistic regression Python libraries youâ€™ll learn to use here!\n\nOnce you determine the best weights that define the function ð‘(ð±), you can get the predicted outputs ð‘(ð±áµ¢) for any given input ð±áµ¢. For each observation ð‘– = 1, â€¦, ð‘›, the predicted output is 1 if ð‘(ð±áµ¢) > 0.5 and 0 otherwise. The threshold doesnâ€™t have to be 0.5, but it usually is. You might define a lower or higher value if thatâ€™s more convenient for your situation.\n\nThereâ€™s one more important relationship between ð‘(ð±) and ð‘“(ð±), which is that log(ð‘(ð±) / (1 âˆ’ ð‘(ð±))) = ð‘“(ð±). This equality explains why ð‘“(ð±) is the logit. It implies that ð‘(ð±) = 0.5 when ð‘“(ð±) = 0 and that the predicted output is 1 if ð‘“(ð±) > 0 and 0 otherwise.\n\n** Classification Performance **\n\nBinary classification has four possible types of results:\n\n* True negatives: correctly predicted negatives (zeros)\n* True positives: correctly predicted positives (ones)\n* False negatives: incorrectly predicted negatives (zeros)\n* False positives: incorrectly predicted positives (ones)\n\nYou usually evaluate the performance of your classifier by comparing the actual and predicted outputsand counting the correct and incorrect predictions.\n\nThe most straightforward indicator of classification accuracy is the ratio of the number of correct predictions to the total number of predictions (or observations). Other indicators of binary classifiers include the following:\n\n* The positive predictive value is the ratio of the number of true positives to the sum of the numbers of true and false positives.\n* The negative predictive value is the ratio of the number of true negatives to the sum of the numbers of true and false negatives.\n* The sensitivity (also known as recall or true positive rate) is the ratio of the number of true positives to the number of actual positives.\n* The specificity (or true negative rate) is the ratio of the number of true negatives to the number of actual negatives.\n\nThe most suitable indicator depends on the problem of interest. In this tutorial, youâ€™ll use the most straightforward form of classification accuracy."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the Dataset\n\ndataset = pd.read_csv('../input/social-network-ads/Social_Network_Ads.csv')\n\nX = dataset.iloc[:, [2, 3]].values\ny = dataset.iloc[:, 4].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting Dataset into Training and Test set \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n# Feature Scaling\n\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Logistic Regression to Training Set\n\nclassifier = LogisticRegression(solver='lbfgs', random_state=0)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test Set Results\n\ny_pred = classifier.predict(X_test)\nprint(y_pred)\n\n# Making Confusion Matrix\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualising the Training set results\n\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualising the Test set results\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}