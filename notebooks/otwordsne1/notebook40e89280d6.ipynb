{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42730bc6-ab49-0b88-b57b-1d35c4e0e072"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n# from autocorrect import spell\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem.porter import *\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import RegexpTokenizer\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8df10bf-6520-68f4-2989-b2116d8cbb4c"},"outputs":[],"source":"df = pd.read_csv(\"../input/Combined_News_DJIA.csv\")\ndf.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d76af499-9593-acd5-823e-30383ba92797"},"outputs":[],"source":"def tokenize_text(text):\n    stop = set(stopwords.words('english'))\n    text = text.lower()\n    # tokenizer = get_tokenizer(\"en_US\")\n    tokenizer = RegexpTokenizer(r'[a-z]+')  # strips non-alphabetical characters\n    # cleaned_tokens = text_col.apply(lambda row: spell(tokenizer.tokenize(row)).lower)\n    cleaned_tokens = [spell(word) for word in tokenizer.tokenize(text) if word not in stop]\n    # tagged_tokens = nltk.pos_tag(cleaned_tokens)\n    return cleaned_tokens\n\ndef clean_and_tag(text, tokenizer):\n    stop = set(stopwords.words('english'))\n    text = str(text).lower()  # apply(lambda x: x.lower())\n    # tokenizer = get_tokenizer(\"en_US\")\n    cleaned_tokens = [word for word in tokenizer.tokenize(text) if word not in stop and len(word) > 1]\n    tagged_tokens = nltk.pos_tag(cleaned_tokens)\n    return tagged_tokens\n\ndef lemmatize_tokens(tokens_col, lemma):\n    tokens_col = tokens_col.apply(lambda row: [get_wordnet_pos(word) for word in row if get_wordnet_pos(word)[1] != ''])\n    lemmatized_words = tokens_col.apply(lambda row: [lemma.lemmatize(word[0],word[1]) for word in row])  # [lemma.lemmatize(x) for x in noun_tokens]\n    return lemmatized_words\n\n# Convert to wordnet part of speech from UPenn\ndef get_wordnet_pos(word_tag_tuple):\n    tag = word_tag_tuple[1]\n    if tag.startswith('J'):  # adjectives\n        tag = 'a'\n    elif tag.startswith('V'):  # verbs\n        tag = 'v'\n    elif tag.startswith('N'):  # nouns\n        tag = 'n'\n    elif tag.startswith('R'):  # adverbs\n        tag = 'r'\n    else:                      # default tag in nltk lemmatizer is noun\n        tag = ''\n    return (word_tag_tuple[0], tag)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"acfdef0f-98a0-d4e2-95ba-93d7903581ff"},"outputs":[],"source":"tokenizer = RegexpTokenizer(r'[a-z]+')  # strips non-alphabetical characters\nlemma = WordNetLemmatizer()\n\nend = 3\nheadline_cols = ['0']\nfor x in range(1, end):\n    headline = df['Top%s' %x] \n    tokenized_headline = headline.apply(lambda row: clean_and_tag(row, tokenizer))\n    print(tokenized_headline)\n    lemmatized_headline = lemmatize_tokens(tokenized_headline, lemma)\n    print(lemmatized_headline)\n    # temp = temp.concat(headline_col, axis=1)\n    headline_cols.append(lemmatized_headline)\ntemp = pd.concat(headline_cols[1:end-1])\ntemp.head()\ntemp.columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f161f535-8e89-d133-4a7f-75a9cb4b8437"},"outputs":[],"source":"\nfor x in range(1, end):"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}