{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Engineering for AI\n\n## Kaggle Prelims and Function Definition","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_values(df, colname, n_values=20, ascending=False):\n    counts = df\\\n        [colname]\\\n        .astype(str)\\\n        .replace(\"nan\", \"unknown\")\\\n        .value_counts()\\\n        .sort_values(ascending=ascending)\n    \n    plot = counts\\\n        .iloc[:n_values]\\\n        .plot\\\n        .bar(title=f\"{colname} ({counts.shape[0]} unique values)\")\n    \n    return plot\n\ndef hist_values(df, colname, bins=10):\n    if df[colname].dtype != \"object\":\n        plot = df.hist(column=colname, bins=bins)\n        return plot\n    \n\ndef plot_values(df, colname, n_values=20, top_n=True, bins=10):\n    if df[colname].dtype == \"float\":\n        return hist_values(df=df, colname=colname, bins=bins)\n    \n    else:\n        ascending = not top_n\n        return count_values(df=df, colname=colname, n_values=n_values, ascending=ascending)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Description\n\nFrom the kaggle dataset page:\n\n* maker - normalized all lowercase\n* model - normalized all lowercase\n* mileage - in KM\n* manufacture_year\n* engine_displacement - in ccm\n* engine_power - in kW\n* body_type - almost never present, but I scraped only personal cars, no motorcycles or utility vehicles\n* color_slug - also almost never present\n* stk_year - year of the last emission control\n* transmission - automatic or manual\n* door_count\n* seat_count\n* fuel_type - gasoline, diesel, cng, lpg, electric\n* date_created - when the ad was scraped\n* datelastseen - when the ad was last seen. Our policy was to remove all ads older than 60 days\n* price_eur - list price converted to EUR","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# IMPORT DATASET\n\nfilename = \"/kaggle/input/personal-cars-classifieds/all_anonymized_2015_11_2017_03.csv\"\nraw_df = pd.read_csv(filename)\nraw_shape = raw_df.shape\n\nprint(f\"Raw data has {raw_shape[0]} rows, and {raw_shape[1]} columns\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PASS IN DATA TYPES WHEN READING\n# - SPEED UP READ\n# - AVOID READ ERRORS\n\nraw_dtypes = {\n    \"maker\": str,\n    \"model\": str,\n    \"mileage\": float,\n    \"manufacture_year\": float, # np.NaN doesn't work with int\n    \"engine_displacement\": float,\n    \"engine_power\": float,\n    \"body_type\": str,\n    \"color_slug\": str,\n    \"stk_year\": str, # None's cannot be converted by pandas here\n    \"transmission\": str,\n    \"door_count\": str,\n    \"seat_count\": str,\n    \"fuel_type\": str,\n    \"date_created\": str,\n    \"date_last_seen\": str,\n    \"price_eur\": float}\n\nraw_df = pd.read_csv(filename, dtype=raw_dtypes)\nraw_shape = raw_df.shape\n\n\n# AND CONVERT TYPES TO NUMERIC WHERE REQUIRED\n# THIS WILL ALSO GET RID OF ANY UNEXPECTED TEXT IN THESE FIELDS (e.g. \"None\")\nto_num = [\"stk_year\", \"door_count\", \"seat_count\"]\nfor col in to_num:\n    raw_df[col] = pd.to_numeric(raw_df[col], errors=\"coerce\")\n    \nprint(f\"Raw data has {raw_shape[0]} rows, and {raw_shape[1]} columns\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SOME USEFUL METHODS TO START EXPLORING\n\nprint(raw_df.info(), \"\\n\\n\\n\")\nprint(raw_df.describe(), \"\\n\\n\\n\")\nprint(raw_df.isna().sum())\nraw_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EXPLORE EACH COLUMN\ncolname = raw_df.columns[0]\nplot_values(raw_df, colname, n_values=20, top_n=True, bins=8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A few issues:\n\n* Maker, transmission and fuel type are categorical variables\n* Many different car models\n* Some \"unexpected\" years of manufacture, door and seat counts\n* Body type is mostly \"other\" or missing\n* Color Slug and STK year also mostly missing\n* Date created and date last seen both in string (date) format\n\nPossible solutions:\n* Encode these variables\n* Drop car model*\n* Clean the values in these columns\n* Drop Body Type and Color Slug (unlikely to improve model)\n* Convert STK year into a boolean column (exists/not exists)\n* Convert these into dates, then take the difference","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# DEALING WITH NA's (CAN'T MODEL WITH THEM)\n\n# CREATE EXISTS COLUMNS\nraw_df[\"stk_bool\"] = raw_df[\"stk_year\"].notnull().astype(int)\n\n\n# IMPUTE SOME NAs - manufacture_year\n# - USE MAKE/MODEL TO ESTIMATE manufacture_year (IS THIS SENSIBLE)\nyear_of_scepticism = 1970\naverage_years = raw_df\\\n    .loc[raw_df[\"manufacture_year\"] >= year_of_scepticism]\\\n    .groupby([\"maker\", \"model\"])\\\n    [\"manufacture_year\"]\\\n    .mean()\\\n    .round()\\\n    .rename(\"avg_manufacture_year\")\\\n    .reset_index()\n\n\n# REPLACE SCEPTICAL VALUES AND NAs\nclean_df = raw_df.merge(average_years, how=\"left\", on=[\"maker\", \"model\"])\nclean_df.loc[raw_df[\"manufacture_year\"] < year_of_scepticism, \"manufacture_year\"] = np.nan\nclean_df[\"manufacture_year\"] = clean_df[\"manufacture_year\"].fillna(clean_df[\"avg_manufacture_year\"])\n\n\n# DROP COLUMNS AND CREATE NEW DF\ndrop_cols = [\"model\", \"body_type\", \"color_slug\", \"stk_year\", \"avg_manufacture_year\"]\nclean_df = clean_df.drop(drop_cols, axis=\"columns\")\n\n\n# DROP ALL ADVERTS WITH AT LEAST ONE NA\nclean_df = clean_df.dropna()\nclean_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make a simple regression model\n\nWe will use a standard linear regression model as the focus is on the features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FIT BENCHMARK MODEL (JUST NUMERIC FEATURES)\nfeatures_initial = [\"mileage\", \"manufacture_year\", \"engine_displacement\", \"engine_power\", \"door_count\", \"seat_count\", \"stk_bool\"]\ntarget = \"price_eur\"\n\nX_initial = clean_df[features_initial]\ny_initial = clean_df[target]\n\n\ndef fit_and_score(X, y):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X,\n        y,\n        test_size=0.3,\n        random_state=42)\n\n    lr = LinearRegression()\n    lr.fit(X_train, y_train)\n    score = lr.score(X_test, y_test)\n    rmse = mean_squared_error(y_test, lr.predict(X_test))\n    actual_predicted = pd.DataFrame({\"predicted\": lr.predict(X_test), \"actual\": y_test})\n    \n    return (score, actual_predicted)\n\n\n\nscore_initial = fit_and_score(X_initial, y_initial)\nprint(\"Initial model R Squared: %0.3f (%i observations)\" % (score_initial[0], X_initial.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Yikes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score_initial[1].plot.scatter(x=\"actual\", y=\"predicted\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PRICE\nhist_values(clean_df, target, bins=100)\n# hist_values(clean_df.loc[clean_df[target] < 100000], target, bins=100)\n# hist_values(clean_df.assign(log=np.log1p(clean_df[target])), \"log\", bins=100)\n# clean_df[clean_df[target] == 1295.34]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MODEL ON LOG(PRICE)\nX_logged = clean_df[features_initial]\ny_logged = np.log1p(clean_df[target])\n\n\nscore_logged = fit_and_score(X_logged, y_logged)\nprint(\"Initial model R Squared: %0.3f (%i observations)\" % (score_initial[0], X_initial.shape[0]))\nprint(\"Logged model R Squared: %0.3f (%i observations)\" % (score_logged[0], X_logged.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FILTER OUT UNUSUAL DATA\nfiltered_df = clean_df.copy()\nfiltered_df = filtered_df[filtered_df[target] != 1295.34]\n\nX_filtered = filtered_df[features_initial]\ny_filtered = np.log1p(filtered_df[target])\n\nscore_filtered = fit_and_score(X_filtered, y_filtered)\nprint(\"Initial model R Squared: %0.3f (%i observations)\" % (score_initial[0], X_initial.shape[0]))\nprint(\"Logged model R Squared: %0.3f (%i observations)\" % (score_logged[0], X_logged.shape[0]))\nprint(\"Filtered model R Squared: %0.3f (%i observations)\" % (score_filtered[0], X_filtered.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## That's more like it!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# count_values(filtered_df, \"maker\", ascending=False)\ncount_values(filtered_df, \"seat_count\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CREATE MANUFACTURER CATEGORIES\nluxurious_makers = [\n    'bentley', 'bmw', 'chevrolet', 'dodge',\n    'hummer', 'mercedes-benz', 'rolls-royce']\n\nsports_makers = [\n    'alfa-romeo', 'aston-martin', 'audi', 'jaguar',\n    'lamborghini', 'lotus', 'maserati', 'porsche', 'tesla']\n\nfiltered_df[\"maker_type\"] = np.select(\n    condlist=[\n        filtered_df[\"maker\"].isin(luxurious_makers),\n        filtered_df[\"maker\"].isin(sports_makers)],\n    choicelist=[\n        \"luxurious\",\n        \"sports\"],\n    default=\"normal\")\n\n# CREATE CAR SIZE CATEGORIES\nfiltered_df[\"seat_str\"] = np.select(\n    condlist=[\n        (filtered_df[\"seat_count\"] >= 0) & (filtered_df[\"seat_count\"] < 4),\n        (filtered_df[\"seat_count\"] >= 4) & (filtered_df[\"seat_count\"] < 6),\n        (filtered_df[\"seat_count\"] >= 6) & (filtered_df[\"seat_count\"] < 10),\n        (filtered_df[\"seat_count\"] >= 10)],\n    choicelist=[\n        \"small\",\n        \"medium\",\n        \"large\",\n        \"very large\"],\n    default=\"unknown\")\n\nfiltered_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CALCULATE TIME THE AD HAS BEEN POSTED FOR\nfiltered_df[\"date_created\"] = pd.to_datetime(filtered_df[\"date_created\"])\nfiltered_df[\"date_last_seen\"] = pd.to_datetime(filtered_df[\"date_last_seen\"])\nfiltered_df[\"post_duration\"] = (filtered_df[\"date_last_seen\"] - filtered_df[\"date_created\"]).dt.total_seconds() / 86400","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ENCODING CATEGORICAL VARIABLES\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nstr_cols = [\"transmission\", \"fuel_type\", \"seat_str\", \"maker_type\"]\nmodel_df = pd.DataFrame(index=filtered_df.index)\nfor col in str_cols:\n    ohe = OneHotEncoder()\n    ohe.fit(filtered_df[[col]])\n    col_df = pd.DataFrame(\n        ohe.transform(filtered_df[[col]]).toarray(), \n        columns=ohe.get_feature_names([col]), \n        index=filtered_df.index)\n    \n    model_df = pd.concat([model_df, col_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ADD NUMERIC COLUMNS TO ENCODED COLUMNS\nnum_cols = [\"manufacture_year\", \"mileage\", \"engine_displacement\", \"engine_power\", \"door_count\", \"post_duration\", \"stk_bool\", target]\nmodel_df[num_cols] = filtered_df[num_cols]\nmodel_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_engineered = model_df.drop(target, axis=\"columns\")\ny_engineered = np.log1p(model_df[target])\n\nscore_engineered = fit_and_score(X_engineered, y_engineered)\nprint(\"Initial model R Squared: %0.3f (%i observations)\" % (score_initial[0], X_initial.shape[0]))\nprint(\"Logged model R Squared: %0.3f (%i observations)\" % (score_logged[0], X_logged.shape[0]))\nprint(\"Filtered model R Squared: %0.3f (%i observations)\" % (score_filtered[0], X_filtered.shape[0]))\nprint(\"Engineered model R Squared: %0.3f (%i observations)\" % (score_engineered[0], X_engineered.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting there...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score_engineered[1].plot.scatter(x=\"actual\", y=\"predicted\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# STILL A FEW OUTLIERS\n# REMOVE ADVERTS THAT ARE OUTLIERS IN ANY COLUMN\nfrom scipy import stats\n\nfinal_df = model_df\\\n    .loc[:, lambda df: df.std() > 0.05]\\\n    .loc[lambda df: (np.abs(stats.zscore(df)) < 3).all(axis=1)]\n\nX_final = final_df.drop(target, axis=\"columns\")\ny_final = np.log1p(final_df[target])\n\nscore_final = fit_and_score(X_final, y_final)\nprint(\"Initial model R Squared: %0.3f (%i observations)\" % (score_initial[0], X_initial.shape[0]))\nprint(\"Logged model R Squared: %0.3f (%i observations)\" % (score_logged[0], X_logged.shape[0]))\nprint(\"Filtered model R Squared: %0.3f (%i observations)\" % (score_filtered[0], X_filtered.shape[0]))\nprint(\"Engineered model R Squared: %0.3f (%i observations)\" % (score_engineered[0], X_engineered.shape[0]))\nprint(\"Final model R Squared: %0.3f (%i observations)\" % (score_final[0], X_final.shape[0]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_final[1].plot.scatter(x=\"actual\", y=\"predicted\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# COMPARE THE PERFORMANCE BY USING\n# SAME DATA FOR EACH MODEL\nindex_final = final_df.index\n\nX_initial1 = clean_df.loc[index_final, features_initial]\ny_initial1 = clean_df.loc[index_final, target]\n\nX_logged1 = clean_df.loc[index_final, features_initial]\ny_logged1 = np.log1p(clean_df.loc[index_final, target])\n\nX_filtered1 = filtered_df.loc[index_final, features_initial]\ny_filtered1 = np.log1p(filtered_df.loc[index_final, target])\n\nX_engineered1 = model_df.loc[index_final].drop(target, axis=\"columns\")\ny_engineered1 = np.log1p(model_df.loc[index_final, target])\n\nscore_initial1 = fit_and_score(X_initial1, y_initial1)\nscore_logged1 = fit_and_score(X_logged1, y_logged1)\nscore_filtered1 = fit_and_score(X_filtered1, y_filtered1)\nscore_engineered1 = fit_and_score(X_engineered1, y_engineered1)\n\nprint(\"All models with the same %i rows:\\n\" % X_final.shape[0])\nprint(\"Initial model R Squared: %0.3f (old: %0.3f)\" % (score_initial1[0], score_initial[0]))\nprint(\"Logged model R Squared: %0.3f (old: %0.3f)\" % (score_logged1[0], score_logged[0]))\nprint(\"Filtered model R Squared: %0.3f (old: %0.3f)\" % (score_filtered1[0], score_filtered[0]))\nprint(\"Engineered model R Squared: %0.3f (old: %0.3f)\" % (score_engineered1[0], score_engineered[0]))\nprint(\"Final model R Squared: %0.3f\" % (score_final[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_engineered1[1].plot.scatter(x=\"actual\", y=\"predicted\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}