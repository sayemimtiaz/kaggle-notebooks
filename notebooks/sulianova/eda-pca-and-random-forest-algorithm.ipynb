{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nsns.set()\nfrom matplotlib import pyplot as plt\nfrom sklearn import decomposition\nimport warnings\nfrom sklearn.preprocessing import StandardScaler\n#Ignore filter warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\n#By default, Pandas displays 20 columns and 60 rows, I will increase it to 150 and 100\npd.set_option('display.max_columns', 150)\npd.set_option('display.max_rows', 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f138445838f4477c8a7c8565d53c51641ecf9d8"},"cell_type":"markdown","source":"<center>\n<img align=\"center\" src=\"http://www.jcytol.org/articles/2016/33/4/images/JCytol_2016_33_4_182_190449_f1.jpg\">\n<br>\n</center>\n    \nHello Kagglers! This work is part of my Capstone project in Data Analytics, Predictive Analytics and Big Data course at Ryerson University, Toronto. Please see the full Table of Content below:\n\n## Table of Content\n    \n1. [Feature explanation](#Feature)\n\n2. [Exploratory Data Analysis and Data Visualization](#EDA)\n3. [PCA](#PCA)\n2. [t-SNE](#t-SNE)\n3. [Data cleaning](#Data-cleaning)\n4. [Normalization](#Normalization)\n5. [Decision tree](#Decision-tree)\n6. [Logistic Regression](#Logistic-Regression)\n    \n    a) [with normalized data](#with-normalized-data)\n    \n    b) [with non-normalized data](#with-non-normalized-data)\n    \n7. [Random Forest](#Random-Forest)\n8. [kNN](#kNN)\n9. [Conclusion](#Conclusion)\n\nThe \"Diagnostic Wisconsin Breast Cancer Database\" is a publicly available data set from the UCI machine learning repository. The dataset gives information about tumor features, that are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. For each observation there are 10 features, which describe tumor size, density, texture, symmetry, and other characteristics of the cell nuclei present in the image. The mean, standard error and \"worst\" mean (mean of the three largest values) of these features were computed for each image, resulting in 30 features. The categorical target feature indicates the type of the tumor.\n\nThe area on the aspirate slides to be analyzed was visually selected for minimal nuclear overlap. The image for digital analysis was generated by a JVC TK-1070U color video camera mounted above an Olympus microscope and the image was projected into the camera with a 63 x objective and a 2.5 x ocular. The image was captured as a 512 x 480 resolution, 8 bit/pixel (Black and White) file. The aspirated material was expressed onto a silane-coated glass slide, which was placed under a similar slide. A typical image contains approximately from 10 to 40 nuclei. After computing 10 features for each nucleus, the mean, standart error and extreme value was computed, as it mentioned above. These features are modeled such that higher values are typically associated with malignancy. \n\nLet's load our dataset as a dataframe and explore what kind of features are given along with their datatypes.","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/data.csv\", index_col = 'id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68acc6808a403ae327314bda4bbc18b317e3dc4b"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70419b0f78d5a9659d5fb7376b8dda6afbdd39a0"},"cell_type":"markdown","source":"### Feature Explanation\n\nFrom the database description, features like radius, perimeter, area are perfect comprehensible to me. But what do texture, smoothness, compactness, concavity, symmetry and fractal dimensions mean exactly?\n\nTexture is a standard deviation of gray-scale values. Each pixel of an image is represented by the 8-bit integer, or *a byte*, from 0 to 255 providing the *amount* of light, where 0 is clear black and 255 is clear white. The darker the image is the lower is the mean of intensity level of a pixel, i.e. byte. \nSo, the SD of gray-scale values means how intense levels are spread for particular individual cells. The higher SD the more contrasting the image is.\n\nNext, smoothness is quantified by measuring the difference between length of radial line and the mean length of two radial lines surrounding it\n    \n<center>\n<img align=\"center\" src=\"https://www.researchgate.net/profile/Nick_Street/publication/268356328/figure/fig3/AS:648234070442001@1531562454255/Line-segments-used-to-compute-smoothness-The-diierence-in-length-of-the-radial.png\">\n<br> \n</center>\n    \nIf the number is small, then the contour is smooth in that region:\n\n$$ smoothness = \\frac{\\sum\\limits_{i}\\left|r_i - \\frac{r_{i-1}+r_{i+1}}{2}\\right|}{perimeter}$$ \n\nThe concavity is captured by drawing chords between two boundary points, which lie outside the nuclear. For the concavity_mean the mean value of these lengths is calculated.\n\n# <center>\n<img align=\"center\" src=\"https://www.researchgate.net/profile/Nick_Street/publication/268356328/figure/fig4/AS:648234070466567@1531562454273/Line-segments-used-to-compute-concavity-and-concave-points-The-bold-line-segment.png\">\n<br> \nIn order to measure symmetry, the major axis, or longest chord through the center, is found. We then measure the length difference between lines perpendicular to the major axis and the nuclear boundary in both directions.\n    \n   # <center>\n<img align=\"center\" src=\"https://www.researchgate.net/profile/Nick_Street/publication/268356328/figure/fig5/AS:648234070458368@1531562454289/Line-segments-used-to-compute-symmetry-The-lengths-of-perpendicular-segments-on-the.png\">\n<br> \n    \nFractal dimensions is approximated using the coastline approximation: the perimeter of the nucleaus is measured by a using increasingly larger rulers, and as the ruler size increases, the perimeter decreases. Plotting log transformation of the perimeter against log of the ruler size and measuring the downward slope gives us the fractal dimension. As with all the shape features, a higher value corresponds to a less regular contour and thus to a higher probability of malignancy.\n\n****","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### EDA\n\nLet's look at the general statistics. including mean, std, median, percentiles, and range.","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"519d5ce054b6328c485f826112dbe6286007d266"},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33ce3a2b29d21686683dfa6a19a6ca8f98beb58b"},"cell_type":"markdown","source":"![](http://)Let's have a look how the target class is distributed:","execution_count":null},{"metadata":{"trusted":true,"_uuid":"6d5a2f41f481f1dcfd5d49db6c783b3a293cf09f"},"cell_type":"code","source":"df['diagnosis'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"285f73566122ee872e5981c6d40dbf68f4e670be"},"cell_type":"markdown","source":"![](http://)As it can be seen, there is no missing values, except for the last column. All independent features are numerical and the target feature is converted to categorical.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"b304a61391771e695994aaf7071d8443db12cbd3"},"cell_type":"code","source":"#And convert it to categorical feature:\ndf['diagnosis'] = df['diagnosis'].astype('category')\n\n#Remove the last empty column\ndf.drop('Unnamed: 32',axis = 1 ,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64c507af99c8eafb26de50e0366e7ca16e8fd3c3"},"cell_type":"markdown","source":"It might be usiful to convert the target class denoting malignant as 0 and benign as 1, and examine correlation among variables:","execution_count":null},{"metadata":{"trusted":true,"_uuid":"cc7eb6807e0b70e515ac4e077ba762495c9a3deb"},"cell_type":"code","source":"df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B':0})\ncorr = df.corr()\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(21, 19))\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap=cmap, center=0,annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5});","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97e84c84a2171cd34f2062d76901a4f4f20b54d8"},"cell_type":"markdown","source":"There are strong positive linear relationships between malignancy and radius of nuclear, number of concave points, perimeter and area. That is not surprising, as these features were modeled in such way that higher values are typically associated with malignancy. To examine *multicollinearity* I will look at pairwise scatter plots of pairs of first 10 and last 10 variables (in the sake of simplicity and visualization), looking for near perfect relationships. ","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"means = [col for col in df.columns if col.endswith('_mean')]\nse = [col for col in df.columns if col.endswith('_se')]\nworst = [col for col in df.columns if col.endswith('_worst')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%config InlineBackend.figure_format = 'png'\nsns.pairplot(df[means]);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, let's see how means are distributed among target class:","execution_count":null},{"metadata":{"trusted":true,"_uuid":"7b98c7e698fdc70a5e96f663c8c0810c7d4865b8"},"cell_type":"code","source":"means = [col for col in df.columns if col.endswith('_mean')]\nse = [col for col in df.columns if col.endswith('_se')]\nworst = [col for col in df.columns if col.endswith('_worst')]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67a467497f6938fa370496989e2e1df4fcd2c0de"},"cell_type":"markdown","source":"I standartized variables, as their ranges are quite different and not representable on a small graph. Every `violinplot` includes markers indicating the median and the interquartile (middle 50%) range. In first nine features the median of malignant tumor is easily contrasted with the benign. Generally speaking, for the benign mass the median is lower for all features, which makes sense, because features were modeled such that higher values are typically associated with malignancy. \n\nFor example,`area_mean` is higher for cancerous mass on average.\n\nLet's look at the standart deviation of features:","execution_count":null},{"metadata":{"trusted":true,"_uuid":"518498dafa7295f9b24633143b7095c28c40e1ea"},"cell_type":"code","source":"df.groupby(['diagnosis'])['area_mean'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99f4692b9d3bb22d8bede17a16fc91e509061447"},"cell_type":"code","source":"def plot_violinplot(feat_list):\n    scaler = StandardScaler()\n    feat_scaled = pd.DataFrame(scaler.fit_transform(df[feat_list]),columns=feat_list, index = df.index)\n    data = pd.concat([df['diagnosis'],feat_scaled],axis=1)\n    df_melt = pd.melt(frame=data, value_vars=feat_list, id_vars=['diagnosis'])\n    fig, ax = plt.subplots(1, 1, figsize = (15, 15), dpi=300)\n    sns.violinplot(x=\"variable\",y=\"value\",hue = \"diagnosis\",data=df_melt,split = True, inner=\"quart\",palette='Set2').set_title('Distribution of features among malignant and benign tumours'.format(feat_list))\n    plt.xticks(rotation=45)\n    L=plt.legend()\n    L.get_texts()[0].set_text('Benign')\n    L.get_texts()[1].set_text('Malignant')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"531e8712b0fc9fe7480d64ba3e4eb345fef30456"},"cell_type":"code","source":"plot_violinplot(means)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"588b48a2fca2c2006d369329dbd9d889dc3dfe80"},"cell_type":"code","source":"plot_violinplot(se)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba2e89d385a5b8e294599440c8366e24e1bc6a46"},"cell_type":"markdown","source":"To interpret these results, the area, radius, perimeter of cancerous cells are widely distributed and dramatically vary from cell to cell in the cytology slide, as well as the number of concave points vary broadly for malignant nuclei.\n\nAnd, finally, the worst, i.e. mean of the three largest values:","execution_count":null},{"metadata":{"trusted":true,"_uuid":"1579fedab5d0d6e109204ea84e9a300f66fd9511"},"cell_type":"code","source":"plot_violinplot(worst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0910da9243da61372a0b94e1b5f7e67c26e57c13"},"cell_type":"code","source":"#Cross table break down by diagnosis\nnumerical = df.drop('diagnosis',axis=1).columns\ndf.groupby(['diagnosis'])[numerical].agg([np.mean, np.std, np.min, np.max])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1265fcad7b2da6ccf861d9209f0ecbd5f0d0a106"},"cell_type":"markdown","source":"\n### PCA\n\nToo many variables can cause such problems as too complex visualizations, efficiency decrease by including variables that have no effect or difficult data interpretation. Principal component analysis (PCA) is a mathematical procedure that transforms a number of (possibly) correlated variables into a smaller number of uncorrelated variables called principal components. The data has 30 dimensions, but I reduce it creating 2 principal components to see whether variables can be separated into clusters. Two target classes, where dark is benign and light is malignant, are almost linearly separable:\n","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')\n\ndf = pd.read_csv(\"../input/data.csv\", index_col = 'id')\ndf.drop('Unnamed: 32',axis = 1 ,inplace = True)\ndf['diagnosis'] = df['diagnosis'].map({'M': 1, 'B':0})\nX = df.drop('diagnosis',axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX_scaled = StandardScaler().fit_transform(X)\n\npca = decomposition.PCA(n_components=2)\nX_pca_scaled = pca.fit_transform(X_scaled)\n\nprint('Projecting %d-dimensional data to 2D' % X_scaled.shape[1])\n\nplt.figure(figsize=(12,10))\nplt.scatter(X_pca_scaled[:, 0], X_pca_scaled[:, 1], c=df['diagnosis'], alpha=0.7, s=40);\nplt.colorbar()\nplt.title('PCA projection')\nplt.style.use('seaborn-muted');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### t-SNE\n\nt-Distributed stochastic neighbor embedding (t-SNE) minimizes the divergence between two distributions: *a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding*.\n\nWith t-SNE, the picture looks better since PCA has a linear constraint, while t-SNE uses a non-linear approach in the background. However, even with such a small dataset, the t-SNE algorithm takes significantly more time to complete than PCA.\n\nAlthough PCA reduces attribute space from a larger number of variables to a smaller number of components, Breast Cancer Wisconsin Diagnostic Data Set has only 31 features, which is not very large number. Moreover, PCA makes independent variables less interpretable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Invoke the TSNE method\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=2000,random_state = 17)\n\ndf_tsne_scaled = tsne.fit_transform(X_scaled)\n\nplt.figure(figsize=(12,10))\nplt.scatter(df_tsne_scaled[:, 0], df_tsne_scaled[:, 1], c=df['diagnosis'], \n            alpha=0.7, s=40)\nplt.colorbar()\nplt.title('t-SNE projection');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In practice, we would choose the number of principal components such that we can explain 90% of the initial data dispersion (via the explained_variance_ratio). Here, that means retaining 6 principal components; therefore, we reduce the dimensionality from 30 features to 6.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pca = decomposition.PCA().fit(X_scaled)\n\nplt.figure(figsize=(10,7))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\nplt.xlabel('Number of components')\nplt.ylabel('Total explained variance')\nplt.xlim(0, 29)\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.axvline(6, c='b')\nplt.axhline(0.91, c='r')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it can be seen, that the first 6 components correspond to approximately 91% of the cumulative sum over all the variance. I am going to stick with PCA, since it provides similar results to t-SNE and takes less time to compute the components. And later, I will compare classification performance for the initial dataset and for pca components.\n\nData cleaning\nAs the dataset is not large, I am not going to remove any outliers in order to keep as much data as possible. But to avoid multicollinearity, I will remove some of the features to prevent overfitting. From the EDA we now know that radius, perimeter, and area are highly correlated, which makes sense. That is why it would be better to remove, say, perimeter, and area, as well as all features from \"worst\" samples, since worst (or largest) instances are also considered in the initial sample, which means and standart errors were computed for, therefore it leads to high correlation (>0.80), which is not surprising, too. For example, the correlation between radius_worst and radius_mean is 0.97, for texture_mean and texture_worst pair it equals to 0.91, and so on and so forth.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"perimeters = [x for x in df.columns if 'perimeter' in x]\nareas = [x for x in df.columns if 'area' in x]\ndf.drop(perimeters, axis = 1 ,inplace = True)\ndf.drop(areas, axis = 1 ,inplace = True)\nworst = [col for col in df.columns if col.endswith('_worst')]\ndf.drop(worst, axis = 1 ,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalization\n\n\nBefore scaling numerical features, let's check whether they follow normal distribution:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['diagnosis'], axis=1)\n(X+0.001).hist(figsize=(20, 15), color = 'c');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Almost all distributions are skewed to the right, i.e. rise very sharply in the beginning (i.e. for very small values near zero), peaks out early, then decreases sharply and leave the long tail. Each histogram is similar to lognormal distribution, a continuous distribution in which the logarithm of a variable has a normal distribution. For algorithms like linear regressions and kNN, numerical features have to be scaled in order to avoid over fitting and make more accurate predictions. And to be scaled numerical features must follow normal distribution. \n\nA log transformation, a popular method, is often used to transform skewed data to approximately normal and thus, to augment the reliability of the linear regression analyses. I will apply log function to make features normally","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Log transformation\nX = df.drop(['diagnosis'], axis=1)\nX_log = np.log(X+0.001)\nX_log.hist(figsize=(20, 15), color = 'c');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall, almost all features have bell-shaped distribution, despite concave points features, which could be affected by malignant instances, where the number of contour concavities increases dramatically. I will use the log-transformation in Logistic regression and kNN algorithms before scaling the data. Then numerical features will be scaled with StandartScaler() function in Python, such that the distribution has a mean value of 0 and a standard deviation of 1.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#Scaler should be trained on train set only to prevent information about future from leaking.\n\ny = df['diagnosis']\n\nX_log_train, X_log_holdout, y_train, y_holdout = train_test_split(X_log, y, test_size=0.3, random_state=17)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision tree\n\nDecision tree is one of the simplest algorithms, which can be used for classification and regression, where the relationship between features and outcome is nonlinear or where features interact with each other. Decision trees in general do not usually require scaling. With the help of GridSearchCV function in Python, which exhaustively searches model optimal parameters by cross-validated grid-search over a parameter grid, best parameters, such as the depth of the tree, split criteria, the minimum number of samples for a leaf node, can be identified:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.3, random_state=17)\n\ntree = DecisionTreeClassifier(random_state=17)\n\ntree_params = {'max_depth': range(1,5), 'max_features': range(3,6), 'criterion': ['gini','entropy']}\n\ntree_grid = GridSearchCV(tree, tree_params, cv=10, scoring='recall')\ntree_grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Best Parameters\ntree_grid.best_params_, tree_grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best split criteria here is the entropy, the depth of the tree equals to 2, as all instances are fully exhausted within two splits. The visualization has the following meaning: the higher the number of concave points in the cell nucleus and the greater the radius is, the higher the probability that the cell is cancerous. \n\nOne of the main disadvantages of using Decision tree is a prone to overfitting. We need to avoid overfitting by pruning, setting a minimum number of samples in each leaf, or defining a maximum depth for the tree. Furthermore, it gives low prediction accuracy for a dataset as compared to other machine learning algorithms.\n\nIn addition, the trees are very sensitive to the noise in input data; the whole model could change if the training set is slightly modified (e.g. remove a feature, add some objects). This impairs the interpretability of the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, recall_score, precision_score\n\ntree_pred = tree_grid.predict(X_holdout)\n\nprint (\"Accuracy Score : \",accuracy_score(y_holdout, tree_pred) )\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, tree_pred))\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, tree_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall equals to 0.9672, Precision Score is 0.8310, and Accuracy Score = 0.9181","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_holdout, tree_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\ntree_graph = export_graphviz(tree_grid.best_estimator_, class_names = ['benign', 'malignant'], feature_names = df.drop(['diagnosis'], axis=1).columns, filled=True, out_file='tree.dot')\n!dot -Tpng tree.dot -o tree.png ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(filename = 'tree.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = df.drop('diagnosis',axis=1).columns\n\ndf.groupby(['diagnosis'])[numerical].agg([np.mean, np.std, np.min, np.max])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression\nwith normalized data\n\n\nLogistic Regression is one of the most used Machine Learning algorithms for binary classification. It is a widely used technique because it is very efficient, does not require too many computational resources, highly interpretable, and easy to regularize. The implementation of logistic regression in Python can be accessed from class LogisticRegression in scikit-learn library. This implementation can fit binary logistic regression with default L2 or L1 regularization.\n\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Cs = np.logspace(-1, 8, 5)\n\nlr_pipe = Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(random_state=17,solver='liblinear'))])\n\nlr_params = {'lr__C': Cs}\n\nlr_pipe_grid = GridSearchCV(lr_pipe, lr_params, cv=10, scoring='recall')\nlr_pipe_grid.fit(X_log_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Best parameters\nlr_pipe_grid.best_params_, lr_pipe_grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"scores=[]\nfor C in Cs:\n    pipe = Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(C=C, random_state=17,solver='liblinear'))])\n    scores.append(cross_val_score(pipe,X_log_train, y_train,cv=10, scoring='recall').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"score_C_1 = lr_pipe_grid.best_score_\nsns.set()\nplt.figure(figsize=(10,8))\nplt.plot(Cs, scores, 'ro-')\nplt.xscale('log')\nplt.xlabel('C')\nplt.ylabel('Recall')\nplt.title('Regularization Parameter Tuning')\n# horizontal line -- model quality with default C value\nplt.axhline(y=score_C_1, linewidth=.5, color='b', linestyle='dashed') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Accuracy Score : \",accuracy_score(y_holdout, lr_pipe_grid.predict(X_log_holdout)) )\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, lr_pipe_grid.predict(X_log_holdout)))\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, lr_pipe_grid.predict(X_log_holdout)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy Score : 0.9532, Recall is 0.9016, and precision is 0.9649","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"By default, LogisticRegression() use L2 penalty, and I search for a best regularization parameter C, the inverse of regularization strength.  I fitted the logit model both on non-normalized and normalized data to compare the results and examine the performance of both approaches:\n\na)\tLogistic regression fits raw data and makes predictions based on non-scaled features. It resulted in almost zero coefficients for three features: radius_mean, texture_mean and texture_se. These features have larger ranges in comparison with other attributes, and logistic regression assigns very small coefficients to them to reduce their impact on a result. At the same time, concavity_se and concave_points_se have small ranges and concave_points_se feature varies approximately from 0 to 0.05, while radius_mean lies within 6.98 and 28.11 values. It means that small changes in concave_points_se could affect the result and change the target class from 0 to 1 or the other way around, while small changes in radius_mean could not make such big impact and hardly has any effect on the response variable. In this case, greater coefficients are assigned for concavity_se and concave_points_se attributes:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_best_pipe = lr_pipe_grid.best_estimator_.named_steps['lr']\n\n#Create Data frame of Regression coefficients\ncoef= pd.DataFrame(lr_best_pipe.coef_.ravel())\n#Merge Regression coefficients with feature names\ndf_columns = pd.DataFrame(df.drop(['diagnosis'], axis=1).columns)\ncoef_and_feat = pd.merge(coef,df_columns,left_index= True,right_index= True, how = \"left\")\ncoef_and_feat.columns = [\"coefficients\",\"features\"]\ncoef_and_feat = coef_and_feat.sort_values(by = \"coefficients\",ascending = False)\n\n#Set up the matplotlib figure\nplt.rcParams['figure.figsize'] = (10,8)\n# Let's draw top 10 important features \nsns.barplot(x = 'features', y = 'coefficients', data = coef_and_feat).set_title('Feature importance')\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"with non-normalized data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"C_scores = np.logspace(-1, 8, 5)\n\nlr = LogisticRegression(random_state=17,solver='liblinear')\n\nlr_params = {'C': C_scores}\n\nlr_grid = GridSearchCV(lr, lr_params, cv=10, scoring='recall')\nlr_grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_grid.best_params_, lr_grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Accuracy Score : \",accuracy_score(y_holdout, lr_grid.predict(X_log_holdout)) )\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, lr_grid.predict(X_log_holdout)))\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, lr_grid.predict(X_log_holdout)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After that GridSearch() implements a “fit” and a “score” method. When “fitting” LogisticRegression on a dataset all the possible values of regularization parameter are evaluated using 10-fold stratified cross-validation and the best value and array of scores are retained.\n\nOne of the simplest options to understand the influence of given parameters in a linear classification model, is to consider the magnitude of its coefficient times the standard deviation of the corresponding parameters in the data. As variables have been already scaled, I simply visualize the magnitude of coefficients in the logit model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_best= lr_grid.best_estimator_\n\n#Create Data frame of Regression coefficients\ncoef= pd.DataFrame(lr_best.coef_.ravel())\n#Merge Regression coefficients with feature names\ndf_columns = pd.DataFrame(df.drop(['diagnosis'], axis=1).columns)\ncoef_and_feat = pd.merge(coef,df_columns,left_index= True,right_index= True, how = \"left\")\ncoef_and_feat.columns = [\"coefficients\",\"features\"]\ncoef_and_feat = coef_and_feat.sort_values(by = \"coefficients\",ascending = False)\n\n#Set up the matplotlib figure\nplt.rcParams['figure.figsize'] = (10,8)\n# Let's draw top 10 important features \nsns.barplot(x = 'features', y = 'coefficients', data = coef_and_feat).set_title('Feature importance')\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest\n\n\nRandom forest is the construction of uncorrelated trees using CART, bagging, and the random subspace method. Decision trees are a good choice for the base classifier in bagging since they are quite sophisticated and can achieve zero classification error on any sample. The random subspace method reduces the correlation between the trees and thus prevents overfitting. With bagging, the base algorithms are trained on different random subsets of the original feature set.\n\nThere are many successful use cases where the random forest algorithm was used in highly unbalanced datasets. But the best hyperparameters are usually impossible to determine ahead of time. Hyperparameter tuning relies more on experimental results than theory, and thus the best method to determine the optimal settings is to try many different combinations to evaluate the performance of each model. Using Scikit-Learn’s GridSearchCV method, I define a grid of hyperparameter ranges, and randomly sample from the grid, performing Stratified K-Fold cross-validation with each combination of values.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n#Stratified split for the validation process\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=17)\n\n#initialize the set of parameters for exhaustive search and fit to find out the optimal parameters\nrfc_params = {'max_features': range(1,11), 'min_samples_leaf': range(1,3), 'max_depth': range(3,13), 'criterion':['gini','entropy']}\n\nrfc = RandomForestClassifier(n_estimators=100, random_state=17, n_jobs= -1)\n\ngcv = GridSearchCV(rfc, rfc_params, n_jobs=-1, cv=skf, scoring='recall')\n\ngcv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gcv.best_params_, gcv.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RandomForest classifier with the default parameters \nrfc = RandomForestClassifier(n_estimators=100, criterion ='gini', max_depth = 8, max_features = 6, min_samples_leaf = 1, random_state = 17, n_jobs=-1)\nforest_pred = gcv.predict(X_holdout)\n\nprint (\"Accuracy Score : \",accuracy_score(y_holdout, forest_pred) )\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, forest_pred))\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, forest_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = gcv.best_estimator_\nestimators_tree_98 = rfc.estimators_[98]\n\nestimators_tree_3 = rfc.estimators_[3]\n\nestimators_tree_47 = rfc.estimators_[47]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators_tree_3.n_features_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To interpret results of Random Forest classifier randomly selected trees could be visualized:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_graph_98 = export_graphviz(estimators_tree_98, class_names = ['benign', 'malignant'], feature_names = df.drop(['diagnosis'], axis=1).columns, filled=True, out_file='tree_98.dot')\n!dot -Tpng tree_98.dot -o tree_98.png \n\ntree_graph_3 = export_graphviz(estimators_tree_3, class_names = ['benign', 'malignant'], feature_names = df.drop(['diagnosis'], axis=1).columns, filled=True, out_file='tree_3.dot')\n!dot -Tpng tree_3.dot -o tree_3.png \n\ntree_graph_47 = export_graphviz(estimators_tree_47, class_names = ['benign', 'malignant'], feature_names = df.drop(['diagnosis'], axis=1).columns, filled=True, out_file='tree_47.dot')\n!dot -Tpng tree_47.dot -o tree_47.png ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename = 'tree_98.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nImage(filename = 'tree_3.png')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename = 'tree_47.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen, concavity_mean is the root node and has the highest information gain, which is why it is split first. The second nodes are texture_mean and concavity_se: the less contrasted the picture is, the more probability that the cell nucleus is benign. \n\nOne of the greatest advantages of using Random forest in a classification problem is that it is robust to outliers because of the random sampling method. Moreover, it handles both continuous and discrete variables equally well. In practice, an increase in the tree number almost always improves the composition and therefore rarely overfits. But in comparison with a single decision tree, Random Forest's output is more difficult to interpret.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"rf_pipe = Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier(n_estimators=100, random_state=17, n_jobs= -1))])\n\nrf_params = {'rf__max_features': range(3,10), 'rf__min_samples_leaf': range(1,3), 'rf__max_depth': range(5,12), 'rf__criterion':['gini','entropy']}\n\n\nrf_pipe_grid = GridSearchCV(rf_pipe, rf_params, cv=10, scoring='recall')\nrf_pipe_grid.fit(X_log_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"rf_pipe_grid.best_params_, rf_pipe_grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Accuracy Score on scaled data: \",accuracy_score(y_holdout, rf_pipe_grid.predict(X_log_holdout)) )\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, rf_pipe_grid.predict(X_log_holdout)))\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, rf_pipe_grid.predict(X_log_holdout)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### kNN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The nearest neighbors method is another quite popular classification method that is also sometimes used in classification problems. This, like decision trees, is one of the most comprehensible approaches to classification. The underlying intuition is that you look like your neighbors. More formally, the method follows the compactness hypothesis: if the distance between the examples is measured well enough, then similar examples are much more likely to belong to the same class. \n\nFeatures with a larger range of values can dominate the distance metric relative to features that have a smaller range, so feature scaling is important. For continuous data, kNN uses a distance metric like Euclidean or Minkowski distance. As all features are numerical, we do not need to change the default metric, which is 'minkowski'.\n\n","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\ndf = pd.read_csv(\"../input/data.csv\", index_col = 'id')\ndf.drop('Unnamed: 32',axis = 1 ,inplace = True)\ndf['diagnosis'] = df['diagnosis'].map({'M': 1, 'B':0})\nX = df.drop('diagnosis',axis = 1)\nperimeters = [x for x in df.columns if 'perimeter' in x]\nareas = [x for x in df.columns if 'area' in x]\ndf.drop(perimeters, axis = 1 ,inplace = True)\ndf.drop(areas, axis = 1 ,inplace = True)\nworst = [col for col in df.columns if col.endswith('_worst')]\ndf.drop(worst, axis = 1 ,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"y = df['diagnosis']\nX = df.drop(['diagnosis'], axis=1).values\nX_scaled = StandardScaler().fit_transform(X)\n\n#Define k-NN classifier and train on a scaled dataset\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_scaled, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To assign the class, when neighbors do not have the same class, KNeighborsClassifier() method in Python has 'weights' parameter:\n\n•    'uniform' takes a simple majority vote from the neighbors. Whichever class has the greatest number of votes becomes the class for the new data point.\n•    'distance' takes a similar vote except gives a heavier weight to those neighbors that are closer. For example, if the neighbor is 5 units away, then weight its vote 1/5. As the neighbor gets further away, the weight gets smaller.\n\nUsing GridSearchCV() it can be computed that 'uniform' metric performs better than 'distance' in this case and that the best number of neighbors equals to five. ","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"knn_params = {'n_neighbors': range(1, 11), 'weights':['uniform', 'distance']}\n\nX_scaled_train, X_scaled_holdout, y_train, y_holdout = train_test_split(X_scaled, y, test_size=0.3,\n                                                                        random_state=17)\n\n#knn_grid.best_estimator_.predict(X_scaled_train)\nknn_grid = GridSearchCV(knn, knn_params, cv=10, n_jobs=-1, scoring='recall')\n\nknn_grid.fit(X_scaled_train, y_train)\n\nknn_grid.best_params_, knn_grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = knn_grid.best_estimator_.predict(X_scaled_holdout)\n\nprint (\"Accuracy Score : \",accuracy_score(y_holdout, pred))\nprint (\"Recall Score (how much of malignant tumours were predicted correctly) : \",recall_score(y_holdout, pred) )\nprint (\"Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'): \",precision_score(y_holdout, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_holdout, pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's compare how kNN performs, if we select 3 and 5 closest neighbors. To compare how kNN performs for 3 and 5 closest neighbors, the colour plot could be drawn, where purple background represents areas predicted as malignant and pink represents areas predicted as benign:","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from matplotlib.colors import ListedColormap\n\nh = .02  # step size in the mesh\nweights ='uniform'\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n# we only take the first two features: radius_mean and concave points_mean. We could avoid this ugly\n# slicing by using a two-dim dataset\n\nfor n_neighbors in [3,5]:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X_scaled[:,[0,5]], y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X_scaled[:,0].min() - 1, X_scaled[:,0].max() + 1\n    y_min, y_max = X_scaled[:,5].min() - 1, X_scaled[:,5].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X_scaled[:, 0], X_scaled[:, 5], c=y, cmap=cmap_bold,\n                edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"2-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\n    plt.xlabel(\"radius\")\n    plt.ylabel(\"concave points\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Recall score could be also compared between train and test sets plotting for each number of nearest neighbors. As can be observed, the best number of neighbours for the training data is 5 where recall score is above 0.93:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    train_scores.append(cross_val_score(knn, X_scaled_train,y_train,cv=10, scoring='recall').mean())\n    test_scores.append(cross_val_score(knn, X_scaled_holdout,y_holdout,cv=10, scoring='recall').mean())\n    \nplt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_scores,marker='o',label='Test Score')\nplt.xlabel(\"Neighbours\")\nplt.ylabel(\"Recall\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a number of model evaluation techniques for the classification problem, I decided to choose three performance metrics: accuracy, recall and precision scores. The most important is recall score, as we are interested in how many of malignant tumours were predicted correctly. Scores are computed for the holdout part, which takes 30% of data, using 10-fold cross validation, and compared with actual values.\n\n\nDecision tree:\nAccuracy Score:  0.9181286549707602\nRecall Score (how much of malignant tumours were predicted correctly):  0.9672131147540983\nPrecision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'):  0.8309859154929577\n\n\nRandom Forest:\n* CV accuracy score: 94.89%\n* CV recall score: 92.74%\n* CV precision score: 93.05%\n\n\nLogistic regression:\n\n* Accuracy Score:  0.3567251461988304\n* Recall Score (how much of malignant tumours were predicted correctly):  1.0\n* Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'):  0.3567251461988304\n\nkNN:\n* Accuracy Score :  0.9181286549707602\n* Recall Score (how much of malignant tumours were predicted correctly) :  0.9180327868852459\n* Precision Score (how much of tumours, which were predicted as 'malignant', were actually 'malignant'):  0.8615384615384616\n\n\nRandom forest is considered as an advanced machine learning technique, especially if the dataset is imbalanced or has categorical features. But in this case, all independent variables are numerical and target class has ratio approximately 0.6:0.4. An interesting observation I found when increasing the variance in the explanatory and noise variables, logistic regression consistently performed with higher overall accuracy as compared to random forest. Kaitlin Kirasich and Trace Smith described in their review [9] the main differences between Random Forest and Logistic Regression in Binary Classification for Heterogeneous Datasets. \n\nLogistic regression deals well with scaled numerical features and when the data is linearly separable. However, if variables are not normalized, the accuracy score drops sharply from 95% to 35%.\n\nThe difficulty of model selection by evaluating the overall classification performance between random forest and logistic regression for datasets comprised of various underlying structures: increasing the variance in the explanatory and noise variables, increasing the number of noise variables, increasing the number of explanatory variables, increasing the number of observations.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n\nRandom forest shows better performance based on recall scores, which means more malignant tumours were predicted correctly, although logistic regression has a higher precision score.  One of the greatest advantages of using Random forest in a classification problem is that it is robust to outliers because of the random sampling method and it handles both continuous and discrete variables equally well. Moreover, random forest is insensitive to the scaling of features as well as any other monotonic transformations due to the random subspace selection. However, the trees are very sensitive to the noise in input data; the whole model could change if the training set is slightly modified (e.g. remove a feature, add some objects). Another great drawback of using decision trees is that we need to avoid overfitting by pruning, setting a minimum number of samples in each leaf, or defining a maximum depth for the tree. \n\nOther researchers have applied computer-based image analysis to various aspects of breast cytology interpretation. Wittekind and Schulte found that mean nuclear area, mean maximum nuclear diameter and mean nuclear perimeter differed significantly between benign and malignant breast cell obtained by FNA. Other studies either have used direct scanning of Feulgen stained material or have analyzed digitized images. \n\nMost of the issues involved in the preparation of the sample lie in the medical realm. Certain selection bias is introduced in the process when the physician decides what part of the sample should be extracted. While the bias is very difficult to quantify, it is possible that if the physician suspects the sample to be malignant, then the selected cells will reflect that suspicion. The bias could be reduced by selecting a number of different areas for digitization, or possibly eliminated altogether by automating the selection process.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}