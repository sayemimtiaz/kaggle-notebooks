{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 Dataset Filtering and Sentence Extraction\nThis notebook combines 'Paper_id', 'Abstract' and 'Text' from json files and meta information from metadata.csv.\n\nAfter retrieving the data, the documents are filtered on the following basis :\n\n* **Presence of Covid-19 synonyms** - A list of synonyms of COVID-19 is prepared manually by observing the data and then assign '1' to the documents in which these words are present in abstract/text otherwise assign '0'.\n\n* **Year of Publication** - '2020' in current notebook\n\n* **Language of paper** - 'English (en)' in current notebook\n\nSentences are also extracted from each filtered document using SciSpaCy.\n\nFiltered document along with metadata and extracted sentences are available as tab separated files."},{"metadata":{},"cell_type":"markdown","source":"Installing python packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install spacy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false,"_kg_hide-output":true},"cell_type":"code","source":"!pip install scispacy","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install langdetect","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nfrom tqdm.notebook import tqdm\nfrom datetime import datetime\nfrom langdetect import detect\nimport scispacy\nimport spacy\nimport re\nimport en_core_sci_lg\nfrom IPython.display import display, HTML\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_json = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for f in filter(lambda x: x.split('.')[-1]=='json', filenames):\n    #for filename in filenames:\n        all_json.append(os.path.join(dirname,f))\n        #print(os.path.join(dirname, f))\nprint('Total json files available : ',len(all_json))\n\nprint(\"reading metadata...\")\nmetadf = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv', dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nprint(\"total entries in metadata : \",len(metadf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            if 'abstract' in content:\n                for entry in content['abstract']:\n                    self.abstract.append(entry['text'])\n            else:\n                self.abstract.append('no abstract')\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return self.paper_id +':' + ' '+ self.abstract[:200]+\"...\"+ self.body_text[:200]+'...'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paper_id = []\nabstract = []\ntext = []\ntitle = []\nstudy_link = []\nsource = []\ndate = []\njournal = []\nauthors = []\nurl = []\ncord_id = []\npmc_id =[]\npubmed_id = []\nmicrosoft_id = []\nfull_text_file = []\n\nfor i in tqdm(range(len(all_json)), total=len(all_json)):\n    #print(all_json[i])\n    content = FileReader(all_json[i])\n    meta_data = metadf.loc[metadf['sha'] == content.paper_id]\n    if len(meta_data) != 0:\n        title.append(meta_data['title'].values[0])\n        paper_id.append(content.paper_id)\n        if content.abstract != 'no abstract':\n            abstract.append(content.abstract)\n        else:\n            abstract.append(meta_data['abstract'].values[0])\n        text.append(content.body_text)\n        link = meta_data['doi'].values[0]\n        if link != 'nan':\n            study_link.append(link)\n        else:\n            study_link.append('not available')\n        publish_time = meta_data['publish_time'].values[0]\n        if publish_time != 'nan':\n            date.append(publish_time)\n        else:\n            date.append('not available')\n        jrnl = meta_data['journal'].values[0]\n        if jrnl != 'nan':\n            journal.append(jrnl)\n        else:\n            journal.append('not available')\n        src = meta_data['source_x'].values[0]\n        if src != 'nan':\n            source.append(src)\n        else:\n            source.append('not available')\n        pmc = str(meta_data['pmcid'].values[0])\n        if pmc != 'nan':\n            pmc_id.append(pmc)\n        else:\n            pmc_id.append('not available')\n        micro = str(meta_data['Microsoft Academic Paper ID'].values[0])\n        if micro != 'nan':\n            microsoft_id.append(micro)\n        else:\n            microsoft_id.append('not available')\n        pub_id = str(meta_data['pubmed_id'].values[0])\n        if pub_id != 'nan':\n            pubmed_id.append(pub_id)\n        else:\n            pubmed_id.append('not available')\n        authors.append(str(meta_data['authors'].values[0]))\n        url.append(str(meta_data['url'].values[0]))\n        cord_id.append(str(meta_data['cord_uid'].values[0]))\n        full_text_file.append(str(meta_data['full_text_file'].values[0]))\n    else: \n        meta_data = metadf.loc[metadf['pmcid'] == content.paper_id]\n        if len(meta_data) != 0:\n            title.append(meta_data['title'].values[0])\n            paper_id.append(content.paper_id)\n            if content.abstract != 'no abstract':\n                abstract.append(content.abstract)\n            else:\n                abstract.append(meta_data['abstract'].values[0])\n            text.append(content.body_text)\n            link = meta_data['doi'].values[0]\n            if link != 'nan':\n                study_link.append(link)\n            else:\n                study_link.append('not available')\n            publish_time = meta_data['publish_time'].values[0]\n            if publish_time != 'nan':\n                date.append(publish_time)\n            else:\n                date.append('not available')\n            jrnl = meta_data['journal'].values[0]\n            if jrnl != 'nan':\n                journal.append(jrnl)\n            else:\n                journal.append('not available')\n            src = meta_data['source_x'].values[0]\n            if src != 'nan':\n                source.append(src)\n            else:\n                source.append('not available')\n            pmc = str(meta_data['pmcid'].values[0])\n            if pmc != 'nan':\n                pmc_id.append(pmc)\n            else:\n                pmc_id.append('not available')\n            micro = str(meta_data['Microsoft Academic Paper ID'].values[0])\n            if micro != 'nan':\n                microsoft_id.append(micro)\n            else:\n                microsoft_id.append('not available')\n            pub_id = str(meta_data['pubmed_id'].values[0])\n            if pub_id != 'nan':\n                pubmed_id.append(pub_id)\n            else:\n                pubmed_id.append('not available')\n            authors.append(str(meta_data['authors'].values[0]))\n            url.append(str(meta_data['url'].values[0]))\n            cord_id.append(str(meta_data['cord_uid'].values[0]))\n            full_text_file.append(str(meta_data['full_text_file'].values[0]))\n\ndata_df = pd.DataFrame({'Cord_uid':cord_id, 'Paper_id':paper_id, 'Study_link':study_link, 'Date':date, 'Journal':journal, 'source':source, 'title':title, 'abstract':abstract, 'text':text, 'authors':authors, 'Study_url':url, 'Pmcid':pmc_id, 'Pubmed_id':pubmed_id, 'Microsoft_paper_id':microsoft_id, 'Full_text_file':full_text_file})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper function to filter document containing covid-19 related terms \ndef filter_document(original_data_df, corona_words):\n    flag_ent = []\n    for l in range(len(original_data_df)):\n        full_text = original_data_df['text'].iloc[l]\n        c_list = []\n        for cword in corona_words:\n            if cword in str(full_text).lower():\n                c_list.append(1)\n            else:\n                c_list.append(0)    \n        boolean = any([y == 1 for y in c_list])\n        if boolean:\n            flag = 1\n        else:\n            flag = 0\n        full_abstract = original_data_df['abstract'].iloc[l]\n        c_list1 = []\n        for cword1 in corona_words:\n            if cword1 in str(full_abstract).lower():\n                c_list1.append(1)\n            else:\n                c_list1.append(0)    \n        boolean = any([z == 1 for z in c_list1])\n        if boolean:\n            flag1 = 1\n        else:\n            flag1 = 0\n        f_list = []\n        f_list.append(flag)\n        f_list.append(flag1)\n        boolean = any([x == 1 for x in f_list])\n        if boolean:\n            flag_ent.append(1)\n        else:\n            flag_ent.append(0)        \n    original_data_df['covid_flag'] = flag_ent\n    r = (original_data_df['covid_flag']==1)\n    filtered_df = original_data_df.loc[r,:].reset_index()\n    return filtered_df, original_data_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_year_of_publish(filtered_df):\n    paper_year = []\n    for i in range(len(filtered_df['Date'])):\n        try:\n            datetime1 = datetime.strptime(filtered_df['Date'][i], '%Y-%m-%d')\n            paper_year.append(datetime1.date().year)\n        except:\n            paper_year.append(int(filtered_df['Date'][i]))\n    return paper_year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper function to collect text and abstract of dataset\ndef getting_text_and_abstract(filtered_df, paper_year):\n    doc_id = []\n    doc_titles = []\n    doc_abstract = []\n    doc_text = []\n    doc_source = []\n    doc_study_link = []\n    doc_date = []\n    doc_journal = []\n    doc_abstractandtext = []\n    doc_authors = []\n    doc_pmcid = []\n    doc_pubmedid = []\n    doc_micro_id = []\n    doc_full_text = []\n    doc_cord_uid = []\n    doc_url = []\n#    count = 0\n    for j in range(len(paper_year)):\n        if paper_year[j] == 2020 :\n            item = str(filtered_df['abstract'].iloc[j])\n            item = item.replace('Abstract\\n\\n', '')\n            item = item.replace('\\n\\n','')\n            item1 = str(filtered_df['text'].iloc[j])\n            item1 = item1.replace('Introduction\\n\\n', '')\n            item1 = item1.replace('INTRODUCTION\\n\\n', '')\n            item1 = item1.replace('\\n\\n',' ')\n            item1 = item1.replace('\\n', ' ')\n            if item != '' and item1 != '':\n                if detect(item1) == 'en':\n                    combine_item = item + ' ' + item1\n                    doc_abstract.append(item)\n                    doc_id.append(filtered_df['Paper_id'].iloc[j])\n                    doc_text.append(item1)\n                    doc_titles.append(filtered_df['title'].iloc[j])\n                    doc_source.append(filtered_df['source'].iloc[j])\n                    doc_study_link.append(filtered_df['Study_link'].iloc[j])\n                    doc_date.append(filtered_df['Date'].iloc[j])\n                    doc_journal.append(filtered_df['Journal'].iloc[j])\n                    doc_abstractandtext.append(combine_item)\n                    doc_authors.append(filtered_df['authors'].iloc[j])\n                    doc_pmcid.append(filtered_df['Pmcid'].iloc[j])\n                    doc_pubmedid.append(filtered_df['Pubmed_id'].iloc[j])\n                    doc_micro_id.append(filtered_df['Microsoft_paper_id'].iloc[j])\n                    doc_full_text.append(filtered_df['Full_text_file'].iloc[j])\n                    doc_cord_uid.append(filtered_df['Cord_uid'].iloc[j])\n                    doc_url.append(filtered_df['Study_url'].iloc[j])\n#            count = count + 1\n    return doc_id, doc_abstract, doc_text, doc_titles,doc_source, doc_date, doc_study_link, doc_journal, doc_abstractandtext, doc_authors, doc_pmcid, doc_pubmedid, doc_micro_id, doc_full_text, doc_cord_uid, doc_url","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper function to extract sentences from full text (abstract + text)\ndef get_sentences_from_full_text(doc_cord_uid, doc_abstractandtext, doc_titles):\n    nlp = en_core_sci_lg.load()\n    clean_full_text_sentences = []\n    clean_doc_uid = []\n    clean_titles = []\n    clean_sentence_id = []\n    for k in range(len(doc_cord_uid)):\n        full_text = doc_abstractandtext[k]\n        doc = nlp(full_text)\n        sentences = list(doc.sents)\n        for n in range(len(sentences)):\n            sent = str(sentences[n])\n            sent = sent.encode('ascii','ignore')\n            sent = sent.decode('utf-8')\n            sent = re.sub(r\"\\[[0-9]*?\\]\", \"\",sent)\n            sent = sent.strip()\n            if not sent.startswith('https') and len(sent) > 15:\n                clean_full_text_sentences.append(sent)\n                clean_doc_uid.append(doc_cord_uid[k])\n                clean_titles.append(doc_titles[k])\n                clean_sentence_id.append(str(n))\n    return clean_full_text_sentences, clean_doc_uid, clean_titles, clean_sentence_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# COVID-19 synonyms\ncorona_words = ['covid','covid19','covid-19','2019-ncov','sars-cov-2', '2019 ncov','ncov 2019','2019ncov','2019 n cov','coronavirus 2019','wuhan coronavirus','novel coronavirus']\nclean_data_df = data_df.drop_duplicates(subset=['title'])\nnew_data_df = clean_data_df.reset_index()\nnew_data_df.to_csv(\"full_dataset_along_with_metadata.csv\", sep='\\t')\noriginal_data_df = clean_data_df.dropna()\nfiltered_df, original_data_df = filter_document(original_data_df, corona_words)\nfiltered_df = filtered_df.drop(['index'], axis=1)\npaper_year = get_year_of_publish(filtered_df)\ndoc_id, doc_abstract, doc_text, doc_titles,doc_source, doc_date, doc_study_link, doc_journal, doc_abstractandtext, doc_authors, doc_pmcid, doc_pubmedid, doc_micro_id, doc_full_text, doc_cord_uid, doc_url = getting_text_and_abstract(filtered_df, paper_year)\n# saving tested documents\ntested_df = pd.DataFrame({'Cord_uid':doc_cord_uid,'Paper_id':doc_id, 'abstract':doc_abstract, 'text':doc_text, 'title':doc_titles, 'source':doc_source, 'Date':doc_date, 'Study_link':doc_study_link, 'Journal':doc_journal, 'Abstract_and_text':doc_abstractandtext, 'authors':doc_authors, 'Pmcid':doc_pmcid, 'Pubmed_id':doc_pubmedid, 'Microsoft_paper_id':doc_micro_id, 'Full_text_file':doc_full_text, 'Study_url':doc_url})\n#Save this file for future reference\nprint('total documents to be filtered : ',len(tested_df))\nprint('displaying samples of filtered documents...')\ndisplay(HTML(tested_df[:3].to_html()))\ntested_df.to_csv(\"Filtered_covid_documents_with_metadata.csv\", sep='\\t')\nprint('Collecting sentences')\nclean_full_text_sentences, clean_doc_uid, clean_titles, clean_sentence_id = get_sentences_from_full_text(doc_cord_uid, doc_abstractandtext, doc_titles)\nsentence_df = pd.DataFrame({'Cord_uid':clean_doc_uid,'Sentence':clean_full_text_sentences, 'Titles':clean_titles, 'Sentence_id':clean_sentence_id})\nprint('total number of sentences : ',len(clean_full_text_sentences))\nsentence_df.to_csv(\"Extracted_sentences_from_filtered_covid_documents.csv\", sep='\\t')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}