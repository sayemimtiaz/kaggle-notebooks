{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CORD-19 - Identifying relevant studies using word2vec, Key-phrases and Named Entities\n\nIn this notebook we have built a pipeline to identify relevant papers for Task 2 - [What do we know about COVID-19 risk factors?](http://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=558). \n\nWe have first generated word embeddings using [word2vec](http://en.wikipedia.org/wiki/Word2vec) on the entire CORD-19 dataset. We are proposing the use of stacked word embeddings representation for question phrases and [tf-idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) weighed similarity computation algorithm for obtaining relevant text from the studies. \n\nFurther, we have used key-phrases and named entities driven scoring and ranking algorithm to rank the studies according to a given question. \n\nThe key pros of using word embeddings is to broaden the scope of matching and retrieval without explicitly specifying similar terms to a given query term. Also, in case of multi-word search weighted scoring improves accuracy by assigning appropriate weights to rare/common terms in the query. However, the success of the approach is dependent on the quality of the generated word vectors."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install spacy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install scispacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install langdetect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Python libraries\nimport pandas as pd\nimport scispacy\nimport spacy\nfrom gensim.models import Word2Vec\nfrom nltk import word_tokenize\nimport os\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport math\nimport json\nfrom collections import OrderedDict\nimport re\nimport en_core_sci_lg\nfrom tqdm.notebook import tqdm\nfrom datetime import datetime\nfrom gensim.models import KeyedVectors\nimport tarfile\nfrom IPython.display import display, HTML\nfrom langdetect import detect\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport csv\nimport nltk\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper function to read csv\ndef getKeywordLists(keywordFile, seperator):\n    df = pd.read_csv(keywordFile,sep=seperator)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Retrieving Data\nCORD-19 dataset is filtered to obtained the studies relevant to COVID-19. [COVID-19 Dataset Filtering and Sentence Extraction](http://www.kaggle.com/tushargoel68/covid-19-dataset-filtering-and-sentence-extraction) notebook provided mechanism to filter out the COVID-19 related documents from the collection using a set of expert recommended keywords. The notebook's output containing filtered documents along with metadata and extracted sentences is used as input in the current notebook. "},{"metadata":{},"cell_type":"markdown","source":"# Training of Word2vec model on COVID-19 dataset\nThe Word2vec model for COVID-19 dataset is trained using text from 'title', 'abstracts' and 'full-text' of 35193 unique documents obatained from the enitre collection. The set of 35193 unique documents are obatained by removing duplicates by title, studies with NaN rows in abstract or text. A vocabulary of 1731074 words with 100 dimensional vector representation corresponding to each word is obtained using this trained model.   \n\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# source code to train Word2Vec using cbow model \n#nlp = en_core_sci_lg.load()\n#new_lines = []\n#total_word = []\n#nlp.max_length = 3000000\ndef gettitlewords(data_df):\n    print(\"getting title words...\")\n    for j in tqdm(range(len(data_df['title'])), total=len(data_df['title'])):\n        item = str(data_df['title'][j])\n        item = item.encode('ascii', 'ignore')\n        item = item.decode('utf-8')\n        if item != 'nan' and len(item) > 3:\n            item = item.lower()\n            words = word_tokenize(item)\n            new_lines.append(words)\n            for word in words:\n                total_word.append(word)\n                \n    return total_word, new_lines\n\ndef getabstractwords(data_df):\n    print(\"getting abstract words...\")\n    for l in tqdm(range(len(data_df['abstract'])), total=len(data_df['abstract'])):\n        abstract = str(data_df['abstract'][l])\n        if abstract != 'nan':\n            doc = nlp(abstract)\n            ab_sentences = list(doc.sents)\n            for sent in ab_sentences:\n                sent = str(sent)\n                sent = sent.lower()\n                sent = sent.encode('ascii','ignore')\n                sent = sent.decode('utf-8')\n                if len(sent) > 3:\n                    words = word_tokenize(sent)\n                    new_lines.append(words)\n                    for word in words:\n                        total_word.append(word)\n                \n    return total_word, new_lines\n\ndef gettextwords(data_df):\n    print(\"getting text words...\")\n#    oov_word = []\n    for i in tqdm(range(len(data_df['text'])), total=len(data_df['text'])):\n        a = str(data_df['text'][i])\n        if a != 'nan':\n            doc = nlp(a)\n            sentences = list(doc.sents)\n            for sent in sentences:\n                sent = str(sent)\n                sent = sent.lower()\n                sent = sent.encode('ascii','ignore')\n                sent = sent.decode('utf-8')\n                if len(sent) > 3:\n                    words = word_tokenize(sent)\n                    new_lines.append(words)\n                    for word in words:\n                        total_word.append(word)\n    return total_word, new_lines\n               \n#data_df = getKeywordLists(\"/kaggle/output/covid19_dataset_new.csv\", seperator='\\t')\n#total_word, new_lines = gettitlewords(data_df)\n#total_word, new_lines = getabstractwords(data_df)\n#total_word, new_lines = gettextwords(data_df)\n\n#print('total new lines are : {}'.format(len(new_lines)))\n#print('total words are : {}'.format(len(total_word)))\n#model = Word2Vec(new_lines, min_count =1)\n#print(model)\n#model.save(\"word2vec_model_covid19.bin\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to resource limitations on Kaggle, the model is trained on the local system. The resultant model is uploaded as input to the current notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to extract .tar file\ndef extract(tar_file, path):\n    opened_tar = tarfile.open(tar_file)\n    if tarfile.is_tarfile(tar_file):\n        opened_tar.extractall(path)\n    else:\n        print(\"the tar you entered is not a tar file\")\n        \nextract(\"/kaggle/input/trained-word2vec-model/word2vec_model.tar.xz\", \"/kaggle/output/kaggle/working\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to check whether a given word is a number or not\ndef is_number(word):\n    try:\n        word = word.translate(str.maketrans('','',b))\n        float(word)\n    except ValueError:\n        return False\n    return True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to calculate L2 norm\ndef l2_norm(a):\n    return math.sqrt(np.dot(a,a))\n\n# function to Calculate cosine similarity\ndef cosine_similarity(a,b):\n    return np.dot(a,b) / (l2_norm(a)*l2_norm(b))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Custom Stopwords**\n\nApart from standard list of common english stopwords, a customized list of specific stopwords has been manually identified thorugh observation. This includes stopwords like ‘methods’ , ‘herein’ , ‘whereas’ etc which generally occurs in the documents. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# punctuations\nb = '!\"#$%&\\'()*+,/:;<=>?@[\\\\]^_`{|}~'\n\n# load nlp model\nnlp = en_core_sci_lg.load()\n\n# load word2vec model\nmodel = KeyedVectors.load_word2vec_format(\"/kaggle/output/kaggle/working/word2vec_model.txt\")\nprint('word2vec model loaded successfully')\nword2vec_vocabulary = list(model.wv.vocab)\n\n# load extended stopword list\nnew_file1 = open(\"/kaggle/input/extended-stopword-list/Extended_Stopwords.txt\")\ntfidf_stopwords = new_file1.readlines()\nfor i in range(len(tfidf_stopwords)):\n    tfidf_stopwords[i] = tfidf_stopwords[i].replace(\"\\n\", \"\")\n    \npunctuation_list = list(punctuation)\npunctuation_list.append('``')     \npunctuation_list.append(\"''\")     \npunctuation_list.append(\"'s\")\npunctuation_list.append(\"n't\")\nnew_stopwords = set(tfidf_stopwords + stopwords.words(\"english\") + punctuation_list)\nstop_words = set(stopwords.words(\"english\"))\nprint('stopwords list successfully loaded')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Computing weights for words of risk factor phrases\nTo retrieve relevant sentences for each risk factor phrase, significance of phrase words in a document are computed by using their [inverse document frequency(IDF)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency).  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper function to filter stopwords from list of words\ndef remove_stopwords(words):\n    for stopword in stop_words:\n        if stopword in words:\n            words = list(filter(lambda a: a != stopword, words))\n    return words\n\n# Helper function to create a list of list of clean words\ndef cleandocdata(description):\n    for i in range(len(description)):\n        #description[i] = description[i].encode('ascii','ignore')\n        #description[i] = description[i].decode('utf-8')\n        description[i] = word_tokenize(description[i])\n        description[i] = [word.lower() for word in description[i]]\n        description[i] = list(filter(lambda a: len(a) > 2, description[i]))\n    # Remove all Stop words\n    for j in range(len(description)):\n        description[j] = remove_stopwords(description[j])    \n    return description\n\n# function to collect clean words from list of phrases\ndef gettingphrasewords(phrases):\n    phrase_words = []\n    for phrase in phrases:\n        w1 = word_tokenize(phrase)\n        for w in w1:\n            w = w.lower()\n            if w not in new_stopwords and w not in phrase_words:\n                if w in word2vec_vocabulary:\n                    phrase_words.append(w)\n    return phrase_words\n\n# function to calculate inverse document frequency(idf) of words\ndef calculate_idf(no_documents, no_documents_in_which_word_occured):\n    if no_documents_in_which_word_occured != 0:\n        idf = math.log1p(no_documents/(1 + no_documents_in_which_word_occured))\n    else:\n        idf = 1\n    return idf\n\n# function to create dictionary where keys are the phrase words and values are the idf of phrase words\ndef getting_phrase_word_dict_with_idf_value(description, phrase_words):\n    idf_dict = dict()\n    for iword in phrase_words:\n        count = 0\n        for text3 in description:\n            if iword in text3:\n                count+=1\n        no_documents_in_which_word_occured = count\n        if iword not in idf_dict:\n            idf_dict[iword] = calculate_idf(len(description), no_documents_in_which_word_occured)    \n    return idf_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating risk factor phrase representation\nA stacked embedding representation is used for multi-word phrases. A phrase constituting 'w' words has a 'w+1' size stack representation where embedding from 1 to wth layers correspond to the words in the phrases and the (w+1)th embedding is obtained by taking tuple-wise average of the constituent word embeddings. These representations are then used to compute phrase-sentence similarity  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper function to get phrase embedding\ndef getphraseembedding(phrases):\n    phrase_embedding = dict()\n    for indicator in phrases:\n        list1 = []\n        ind_words = word_tokenize(indicator)\n        for word in ind_words:\n            word = word.lower()\n            if word in indicator.lower() and word not in new_stopwords:\n                if word in word2vec_vocabulary and not is_number(word):\n                    list1.append(model[word])\n\n        if indicator not in phrase_embedding:\n            phrase_embedding[indicator] = np.mean(list1, axis =0)\n        else:\n            phrase_embedding[indicator] = 0 \n    return phrase_embedding","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extract neighbouring information of a sentence\nFor all sentences in the documents, neighboring sentences are analyzed to identify the disease that the sentences is about.\n* Sentences about COVID-19 are given a higher preference\n* Lesser or no preference is given to sentences about other diseases- like SARS, MERS, H1N1, etc.\n* **keepList** has all the disease terms that are relevant and the **ignoreList** has disease names that are not relevant to our analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"ignoreList = [ 'SARS-CoV', 'MERS', 'H1N1', 'H5N1', 'H7N9', 'rhinovirus', 'RSV', 'respiratory syncytial virus', 'metapneumovirus', 'parainfluenza', 'SFTSV', 'OC43', 'SARS', 'MERS-CoV', 'SARS-COV', 'MERS-COV', 'SARS-nCoV', 'SARS-nCov', 'SARS-nCOV']\nkeepList = ['COVID-19', '2019-nCoV', 'SARS-CoV-2', 'Wuhan coronavirus', 'covid-19', 'covid19', 'covid -19', 'covid- 19', 'covid - 19', 'covid 19', 'SARS-CoV2', 'COVID-2019', 'COVID 2019', '2019n-CoV']\n\ndef calcNeighbour(sid, sentIdDict, max1):\n    ns = []\n    #dL = []\n    lhs = 0\n    rhs = max1\n    row = sentIdDict[sid]\n    paper = row.Cord_uid\n    sent_id1 =  int(row.Sentence_id)\n    if sid<5:\n        lhs = 0\n    else :\n        lhs = sid-5\n    if (sid + 5)>max1:\n        rhs = max1\n    else:\n        rhs = sid+5\n    for i in range(lhs, rhs+1):\n        row = sentIdDict[i]\n        paperN = row.Cord_uid\n        sent_id2 = int(row.Sentence_id)\n        if paper == paperN and abs(sent_id1-sent_id2) <= 5:\n            ns.append(row.Sentence)\n    containedDiseaseN = []\n    for  s in ns:\n        #print(sentence, disease)\n        for idl in ignoreList:\n            if idl in s and 'SARS-CoV-2'.lower() not in s.lower() and 'SARS-CoV2'.lower() not in s.lower() :\n                containedDiseaseN.append(idl)\n                #print(idl,'_____________',  s)\n        #print(containedIrrevDisease)\n        \n        for kdl in keepList:\n            if kdl.lower() in s.lower():\n                containedDiseaseN.append(kdl)\n                #print(kdl,'_____________', s)\n    #print(len(ns), sid, rhs, lhs)\n    containedDiseaseN = set(containedDiseaseN)\n    #print(containedDiseaseN)\n    if len(containedDiseaseN) == 0:\n        x = []\n        return x\n    containedDiseaseN = list(containedDiseaseN)\n    return containedDiseaseN\n\n\ndef extract_neighbouring_info(dfE):\n    sentIdDict = dict()\n    max1 = 0\n    for row in dfE.itertuples(): \n        sid = int(row.Index)\n        if sid > max1:\n            max1 = sid\n        sentIdDict[sid] = row\n\n    #print(len(sentIdDict), max1)\n    dictLen = len(sentIdDict)\n\n    dfE['Rel_Disease'] = ''\n    dfE['Irrel_Disease'] =  ''\n    dfE['Neighboring_Sentence_Disease'] = ''\n    for i in range(0,max1+1):\n        row = sentIdDict[i]\n        sentence = row.Sentence\n        containedIrrevDisease = []\n        for idl in ignoreList:\n            if idl in sentence and 'SARS-CoV-2' not in sentence and 'SARS-CoV2'.lower() not in sentence.lower() :\n                containedIrrevDisease.append(idl)\n        #print(containedIrrevDisease)\n        containedDisease = []\n        for kdl in keepList:\n            if kdl.lower() in sentence.lower():\n                containedDisease.append(kdl)\n        dfE.set_value(row.Index, 'Rel_Disease', containedDisease)\n        dfE.set_value(row.Index, 'Irrel_Disease', containedIrrevDisease)\n        containedDiseaseN = []\n        containedDiseaseN = calcNeighbour(i, sentIdDict, max1)\n        dfE.set_value(row.Index, 'Neighboring_Sentence_Disease', containedDiseaseN)\n    return dfE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating word similarity score dictionary\nA word similarity dictionary is created consisting tuples (phrase_word, sentence_word) as a key and their cosine similarity as a value to optimize phrase-to-sentence similarty computation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper function to generate list of unique words of sentences\ndef generate_list_non_repeating_words(non_dup_sent):\n    list_of_words=[]\n    for j in tqdm(range(len(non_dup_sent)), total=len(non_dup_sent)):\n        words = word_tokenize(non_dup_sent[j])\n        words = map(lambda x : x.lower(),words)\n        list_of_words.extend(words)\n    non_duplicate_list_of_words = list(set(list_of_words))       \n    non_duplicate_list_of_words = filter(lambda x : x not in new_stopwords,non_duplicate_list_of_words)\n    non_duplicate_list_of_words = filter(lambda x : x in word2vec_vocabulary,non_duplicate_list_of_words)\n    non_duplicate_list_of_words = list(filter(lambda x : not is_number(x),non_duplicate_list_of_words))    \n    return non_duplicate_list_of_words\n\n# function to create word similarity dictionary\ndef generating_word_similarity_dictionary(non_dup_sent, phrase_words, phrase_embedding):\n    similarity_dict = {}\n    words_list = generate_list_non_repeating_words(non_dup_sent)\n    for phrase in phrase_words:\n        for s_word in words_list:\n            if (phrase, s_word) not in similarity_dict.keys():\n                similarity_dict[(phrase, s_word)] = round(cosine_similarity(phrase_embedding[phrase],model[s_word]),5)\n    return similarity_dict\n\n#print('Reading extracted sentences from filtered covid documents')\n#tot_sent = list(sentence_df['Sentence'])\n#non_dup_sent = list(set(tot_sent))\n#print('Total sentences available after removing duplicates : ',len(non_dup_sent))\n#phrases = ['covid-19 risk factors', 'hypertension covid-19', 'diabetes covid-19', 'heart disease covid-19', 'smoking covid-19', 'pulmonary disease covid-19', 'cancer covid-19', 'risk factors for neonates and pregnant women', 'respiratory disease covid-19', 'co-infections risk covid-19', 'incubation period covid-19', 'reproductive number covid-19', 'serial interval covid-19']\n#phrase_single_words = gettingphrasewords(phrases)\n#phrase_words = phrase_single_words + phrases\n#print('total number of phrase_words avialable : ',len(phrase_words))\n#phrase_embedding = getphraseembedding(phrase_words)\n#print('phrase embedding generated successfully')\n#similarity_dict = generating_word_similarity_dictionary(non_dup_sent, phrase_words, phrase_embedding)\n#np.save('phrase_word_similarity_dictionary.npy', similarity_dict)\nprint('Reading word similarity score dictionary')\nword_similarity_dictionary = np.load('/kaggle/input/kernel74dfc29773/phrase_word_similarity_dictionary.npy',allow_pickle='TRUE').item()\nprint('total length of dictionary is : ',len(word_similarity_dictionary))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to append list of words of a sentence\ndef adding_list_of_words(clean_sentence_df):\n    non_dup_sent_words = []\n    non_dup_sents = list(clean_sentence_df['Sentence'])\n    for i in range(len(non_dup_sents)):\n        #print(i)\n        non_dup_sent_words.append(word_tokenize(non_dup_sents[i]))\n    #print(len(non_dup_sent_words))\n    clean_sentence_df['list_of_sent_words'] = non_dup_sent_words\n    \n    return clean_sentence_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Retrieving Relevant Sentences for Risk Factors\nEach sentence and each word in a phrase now has a stacked representation and a significance value associated to it. The relevance of a sentence with respect to a given phrase is computed as a weighted function of cosine similarity between the contained words.\nLet *n* denote the number of words in a sentence, *Sj* denote the jth word of the sentence *S* and *sim(Pw, Sj)* denote the cosine similarity between a phrase word *Pw* and a sentence word *Sj*. For each indicator word *Pw*, *maxSim* function captures the maximum similarity between the word and the words of a sentence and is calculated as:\n\n*maxSim(Pw)* = *max[sim(Pw,Sj)]*      j = 1 to n\n\nThe final similarity score between the phrase and a sentence is calculated by multiplying the idf weights of phrase words to the corresponding *maxSim* score. Higher the score, more relevant is the sentence."},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to get final similarity score between the phrase and a sentence\ndef phrasematching(indicator, idf_dict, clean_sentence_df1):\n    all_score = []\n    clean_keywords = []\n    oov_word = []\n    new_keywords = word_tokenize(indicator)\n    if len(new_keywords) > 1:\n        new_keywords.append(indicator)\n\n    for m in range(len(new_keywords)):\n        if new_keywords[m] not in new_stopwords:\n            clean_keywords.append(new_keywords[m])\n    #    match_phrase = []\n    keyword_embedding = getphraseembedding(clean_keywords)\n    non_dup_sents = list(clean_sentence_df1['Sentence'])\n    for j in tqdm(range(len(non_dup_sents)), total=len(non_dup_sents)):\n        sent = non_dup_sents[j]\n        sent_words = clean_sentence_df1['list_of_sent_words'][j]\n        sent_score = []\n        for phrase in clean_keywords:\n            list3 = []\n            for s_word in sent_words:\n                s_word = s_word.lower()\n                if (phrase, s_word) in word_similarity_dictionary:\n                    list3.append(word_similarity_dictionary[(phrase, s_word)])\n                else:\n                    oov_word.append(s_word)\n\n            if list3 != []:\n                sent_score.append(np.max(list3))\n            else:\n                sent_score.append(0)\n        if len(sent_score) > 1:\n            for k in range(len(sent_score)-1):\n                if sent_score[k] >= 0.50:\n                    sent_score[k] = idf_dict[clean_keywords[k]]*sent_score[k]\n\n            sentence_score = np.mean(sent_score)\n        else:\n            sentence_score = np.mean(sent_score)\n            \n        all_score.append(sentence_score)\n\n    clean_sentence_df1['sentence_score'] = all_score\n    clean_sentence_df1 = clean_sentence_df1.sort_values(by=['sentence_score'], ascending = [False])\n    clean_sentence_df1 = clean_sentence_df1[clean_sentence_df1.sentence_score > 0.7]\n    return clean_sentence_df1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To calculate importance/specificity of phrase words, we are calculating Inverse Document Frequency of phrase words on abstract of filtered COVID documents. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_tokenizer(text):\n            tokens = word_tokenize(text)\n            tokens = [t for t in tokens if t not in stopwords.words('english')]\n            return tokens\n\n#function to calculate idf matrix on abstract of documents\ndef tfidf(sentences):\n            tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenizer,\n                                            stop_words=stopwords.words('english'),\n                                            lowercase=True)\n            tfidf_matrix = tfidf_vectorizer.fit_transform(sentences).todense()\n            tfidf = tfidf_vectorizer.idf_\n            dic = dict(zip(tfidf_vectorizer.get_feature_names(), tfidf))\n            #print(dic)\n            return dic\n\ndef get_tfidf_dict(filtered_covid_document_df):\n    sentences=[]\n    count=0\n    #df1 = pd.read_csv(\"/home/hduser1/Desktop/COVID/Tushar/Kaggle_uploaded_files/tested_covid_documents.csv\", sep='\\t')\n    for i in range(len(filtered_covid_document_df['abstract'])):\n        sent = filtered_covid_document_df['abstract'][i]\n        count+=1\n        #print(count)\n        sent = sent.replace(\".\",\" \") \n        sentences.append(sent)    \n    dict_tfidf = tfidf(sentences)\n    dict_tfidf_new={}\n    for i in dict_tfidf:\n        dict_tfidf_new[i]=math.log10(dict_tfidf[i])\n    return dict_tfidf_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Similar words for Risk factors\nA dictionary of similar words corresponding to each risk factor is created using top most similar words obtained from trained Word2Vec. The list is manually analyzed to retain semantically similar words for each risk factor. This list is further utilized for re-scoring."},{"metadata":{"trusted":true},"cell_type":"code","source":"myDict={}\n\nmyDict[\"hypertension covid-19\"]=['hypertension', 'blood pressure', 'HTN', 'HBP']\nmyDict[\"diabetes covid-19\"]=['insulin','insulin-dependent','insulin resistance','glucose control','blood glucose level','metaformin', 'hemoglobin A1c','hyperglycemia','hypoglycemia', 'hyperglycemic', 'hypoglycemic', 'pre-diabetes', 'mellitus']\nmyDict[\"heart disease covid-19\"]=['cardiac', 'cardiovascular', 'ventricular', 'cardiopulmonary', 'valvular', 'systolic', 'coronary', 'cardiorespiratory']\nmyDict[\"smoking covid-19\"]=['smoker','smoke','smokers','cigarette']\nmyDict[\"pulmonary disease covid-19\"]=['lung','vascular','airway','coronary', 'alveolar', 'bronchial']\nmyDict[\"cancer covid-19\"]=['cancers','carcinoma','hcc','cancer23','tumour','gbm','adenocarcinoma','tumor','nsclc']\nmyDict['covid-19 risk factors']=['covid-19','covid19','covid -19', 'covid- 19','covid - 19','covid 19','covid-2019','covid 2019','sars-cov2','sars-cov-2','2019-ncov','2019n-cov','wuhan coronavirus','risks','hazard','determinants','factor','cofactors','co-factors']\nmyDict['respiratory disease covid-19']=['lower-respiratory', 'upper-respiratory', 'respira-tory', 'upperrespiratory','aerodigestive','respiratory']\nmyDict['co-infections risk covid-19']=['coinfections','co-detections', 'coinfection','co-infection','co-detection','codetection', 'codetections','co-pathogens', 'copathogens','uris']\nmyDict['risk factors for neonates and pregnant women']=['newborns','infants','foals', 'babies', 'children', 'nonpregnant','non-pregnant','postmenopausal', 'pregnancy','infertile', 'mothers']\n\n#print(len(myDict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Re-scoring Sentences\nBased on few factors (like suppressing sentences other than Covid, giving importance to indicator and its synonym terms, mapping NER of indicator to NERs present in sentence), we are boosting or suppressing the scores of sentences extracted for each indicator.\n\nNow as we have sentence wise scores for each indicator of Task-2, we apply following steps for rescoring extracted sentences:\n1. For each sentence, we have used its neighbouring sentences from document to check the type of virus (i.e. if sentence is about Covid-19 or any other virus.) \n2. If sentence is about a disease other than COVID-19 (like SARS, MERS, H1N1, MERS-COV, SARS-COV, CORONA, CORONAVIRUS etc), simply rescoring it to zero.\n3. We have used idf weights of indicator terms (eg: if indicator is \"pulmonary_disease_risk\", here idf of pulmonary, disease, and risk is used to decide the importance of indicator terms.)\n4. Mapping NER of indicator to NERs present in sentence which are obtained using [Extracting entities from COVID documents](https://www.kaggle.com/suyashsangwan/extracting-entities-from-covid-documents) . (eg: if indicator term is \"coinfections\", whose NER is 'disease', so if disease entities are present in a sentence, then weightage is given to that sentence so that informative sentences are selected.)\n\nSuppose a sentence *'S'* has initial score *'Si'* and *'n'* words (after removing stopwords), then \n   Final Score(*Sj*) = *Si* + idf(indicator_terms) + (number_of_related_entities_present_in_sentence)/*n*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to rescore the sentences based on other factors\ndef rescoring(df, dict_tfidf_new, indicator, synonym_list_for_indicator):\n    final_score=[]\n    bad_char = '''['\"()]''' \n    exclude = set(string.punctuation)\n    good_list=['covid-19','covid19','covid -19', 'covid- 19','covid - 19','covid 19','covid-2019','covid 2019','sars-cov2','sars-cov-2','2019-ncov','2019n-cov','wuhan coronavirus']\n    bad_list=['covid','mers-cov','sars-cov','corona','corona virus','coronavirus','sars','mers']\n    suppress_list=['figure','table','fig','objective','aim','he','she','?']\n    #synonym_list_for_indicator=['insulin','insulin-dependent','insulin resistance','glucose control','blood glucose level','metaformin', 'hemoglobin A1c','hyperglycemia','hypoglycemia', 'hyperglycemic', 'hypoglycemic', 'pre-diabetes', 'obesity', 'mellitus', 'hypercholesterolemia', 'hypertension','hyperlipidemia','t2dm','dyslipidemia']\n    for ind in tqdm(range(len(df)), total=len(df)):\n        score=0      \n        score1=0.5\n        score2=0.3\n        flag=0\n        sent=df['Sentence'][ind]\n        #print(sent)\n        sent=sent.lower()\n        sent_without_punct = ''.join(ch for ch in sent if ch not in exclude)     #### removing punctuations\n        sent_without_punct_tokenized=word_tokenize(sent_without_punct)      #### tokenizing words\n        sent_without_punct_tokenized_without_sw=[word for word in sent_without_punct_tokenized if not word in stopwords.words()] ## removing sws\n        len_sent=len(sent_without_punct_tokenized_without_sw)  ### computing length of processed sent\n        indicator = indicator.lower()\n        search = indicator.split(\"_\")\n        search2 = indicator.replace(\"_\",\" \") \n\n        dis_rel=df['Rel_Disease'][ind]\n        dis_rel = ''.join(ch for ch in dis_rel if ch not in bad_char)    ### joining all entities and removing punctuations\n        listt=dis_rel.split(\",\")   ### making a list of entities (kp,np,disease,gene,cell,chemical,etc.)\n        listt_dis_rel=[]     ### pre-processing list of entities\n        for i in listt:\n            i=i.lower()\n            i=i.strip()\n            if i!=\"\":\n                listt_dis_rel.append(i)\n        \n        dis_irr=df['Irrel_Disease'][ind]\n        dis_irr = ''.join(ch for ch in dis_irr if ch not in bad_char)    ### joining all entities and removing punctuations\n        listt=dis_irr.split(\",\")   ### making a list of entities (kp,np,disease,gene,cell,chemical,etc.)\n\n        listt_dis_irr=[]     ### pre-processing list of entities\n        for i in listt:\n            i=i.lower()\n            i=i.strip()\n            if i!=\"\":\n                listt_dis_irr.append(i)\n\n        dis_neigh=df['Neighboring_Sentence_Disease'][ind]\n        dis_neigh = ''.join(ch for ch in dis_neigh if ch not in bad_char)    ### joining all entities and removing punctuations\n        listt=dis_neigh.split(\",\")   ### making a list of entities (kp,np,disease,gene,cell,chemical,etc.)\n\n        listt_dis_neigh=[]     ### pre-processing list of entities\n        for i in listt:\n            i=i.lower()\n            i=i.strip()\n            if i!=\"\":\n                listt_dis_neigh.append(i)\n\n        if len(listt_dis_rel)>0:    #### covid related term in sent\n            flag=1\n\n        if flag==0:                 #### discard sent (irrelevant disease in sent)\n            if len(listt_dis_irr)>0:\n                flag=2\n\n        if flag==0:                #### here flag=0 depicts that none of upper given conditions hold\n            for i in good_list:       #### if covid related term in neighbouring sent\n                if i in listt_dis_neigh:\n                    flag=1\n                    break   \n\n        if flag!=1:\n            final_score.append(0)\n\n        else:   \n            for i in search:\n                if i in dict_tfidf_new:\n                    if i in sent:\n                        score=score+dict_tfidf_new[i]\n\n            for i in synonym_list_for_indicator:\n                i=i.lower()\n                i=i.strip()\n                if i in dict_tfidf_new:\n                    if i in sent:\n                        score=score+dict_tfidf_new[i]\n\n            if search2 in sent:\n                score=score+score1\n            else:\n                if search[0] in sent:\n                    score=score+score2\n\n            for i in suppress_list:    \n                if i.lower() in sent:\n                    score=score-score1\n\n            score=score+df['sentence_score'][ind]  \n            Text=df['sent_jnlpba_ent'][ind]+\",\"+df['sent_craft_ent'][ind]+\",\"+df['sent_bc5cdr_ent'][ind]+\",\"+df['sent_bionlp13cg_ent'][ind]+\",\"+df['sent_sci_ent'][ind]\n            Text = ''.join(ch for ch in Text if ch not in bad_char)    ### joining all entities and removing punctuations\n            listt=Text.split(\",\")   ### making a list of entities (kp,np,disease,gene,cell,chemical,etc.)\n            listtt=[]     ### pre-processing list of entities\n            for i in listt:\n                i=i.lower()\n                i=i.strip()\n                if i!=\"\":\n                    listtt.append(i)\n\n            listtt_sorted=sorted(listtt, key=len, reverse=True)    #### sorting list by length of elements\n            superset_list=[]    #### keeping only superset strings\n            for i in listtt_sorted:\n                flag=0\n                for j in superset_list:\n                    if set(i)<=set(j):\n                        flag=1\n                        break\n                if flag==0:\n                    superset_list.append(i)\n            score=score+float(len(superset_list)/len_sent)\n            final_score.append(score)\n            #print(ind)\n    df['New_Scores']=final_score\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculating document score\nUsing the scores obtained from our Rescoring Algorithm, we are retrieving the topmost scored documents to each indicator of Task-2.\nDocument level score is calculated by adding constituent sentence scores. The score are then used to rank document."},{"metadata":{"trusted":true},"cell_type":"code","source":"def finding_relevant_document(clean_sentence_df1, filtered_covid_document_df1):\n    dict_doc_scores={}\n    for i in range(0,len(clean_sentence_df1)):\n        doc_id=clean_sentence_df1['Cord_uid'].loc[i]\n        if doc_id in dict_doc_scores:\n            if clean_sentence_df1['New_Scores'].loc[i]>0:\n                dict_doc_scores[doc_id]=dict_doc_scores[doc_id]+clean_sentence_df1['New_Scores'].loc[i]\n        else:\n            if clean_sentence_df1['New_Scores'].loc[i]>0:\n                dict_doc_scores[doc_id]=clean_sentence_df1['New_Scores'].loc[i]\n\n\n    #filtered_covid_docs_df = pd.read_csv(\"Filtered_Covid_Documents.csv\", sep='\\t')\n    doc_score = []\n    for i in range(0,len(filtered_covid_document_df1)):\n        doc_id = filtered_covid_document_df1['Cord_uid'].loc[i]\n        if doc_id in dict_doc_scores:\n            doc_score.append(dict_doc_scores[doc_id])\n        else:\n            doc_score.append(0)\n\n    filtered_covid_document_df1['Doc_score'] = doc_score\n    return filtered_covid_document_df1\n#filtered_covid_docs.to_csv(\"Filtered_Covid_Documents_with_Scores.csv\", sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Saving output in required format\nHere a mapping between risk factors and result file is created."},{"metadata":{"trusted":true},"cell_type":"code","source":"tags = dict()\ntags['smoke'] =  'smoking_status.csv'\ntags['pulmonary'] = 'pulmonary.csv'\ntags['respiratory'] = 'respiratory_disease.csv'\ntags['diabetes'] = 'diabetes.csv'\ntags['asthma'] = 'asthma.csv'\ntags['comorbidity'] = 'comorbidities.csv'\ntags['pregnant'] = 'neonatal_pregnancy.csv'\ntags['hypertension'] = 'hypertension.csv'\ntags['cerebral'] = 'cerebral.csv'\ntags['cancer'] = 'cancer.csv'\ntags['obesity'] = 'obesity.csv'\ntags['heart'] = 'heart_disease.csv'\ntags['alcohol'] = 'drinking.csv'\ntags['tuberculosis'] = 'tuberculosis.csv'\ntags['kidney'] = 'chronic_kidney_disease.csv'\ntags['risk_factor'] = 'covid_risk_factor.csv'\ntags['coinfection'] = 'coinfections.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To obtain the OR, CI, P values, design and sampling method for all the documents, the output of [obtaining-design-and-sampling-info-from-document](https://www.kaggle.com/akshararai10/cord19-obtaining-design-and-sampling-information) notebook is taken."},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to load extracted_DocData.csv\ndef loadFromCsv():\n    sevDict = dict()\n    fatalDict = dict()\n    sampleDict = dict()\n    sampleMethodDict = dict()\n    designDict = dict()\n    nameDict = dict()\n    dfP1 = pd.read_csv(dataFile, sep='\\t' )\n    dfP1 = dfP1.astype(str)\n    for row in dfP1.itertuples(): \n        paper = row.Cord_uid\n        name = row.Titles\n        nameDict[paper] =  name\n        string = row.Severe\n        string = string.strip('[')\n        string = string.strip(']')\n        string = string.split('\\'')\n        sevlis = []\n        for item in string:\n            if item!=',':\n                sevlis.append(item)\n        sevDict[paper] = sevlis\n        \n        string = row.Fatal\n        string = string.strip('[')\n        string = string.strip(']')\n        string = string.split('\\'')\n        fatallis = []\n        for item in string:\n            if item!=',':\n                fatallis.append(item)\n        fatalDict[paper] = fatallis\n\n        if row.Design ==  'nan':\n            designDict[paper] = ''\n        else:\n            designDict[paper] = row.Design\n            \n        if row.Sample ==  'nan':\n            sampleDict[paper] = ''\n        else:\n            sampleDict[paper] = row.Sample\n            \n        if dfP1.at[row.Index, 'Sampling Method'] ==  'nan':\n            sampleMethodDict[paper] = ''\n        else:\n            sampleMethodDict[paper] = dfP1.at[row.Index, 'Sampling Method']\n            \n        #print(paper, sampleMethodDict[paper])\n            \n    return sevDict, fatalDict, nameDict, designDict, sampleDict, sampleMethodDict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results obtained, after scoring documents on the basis of their relevance, is mapped with the severe/fatality values, design and sapling method to create the final output, saved in a csv file with the risks name."},{"metadata":{"trusted":true},"cell_type":"code","source":"#get required values from dict\ndef getData(paper, indicator):\n    severe = ''\n    fatal = ''\n    design = ''\n    sample = ''\n    sampleMethod = ''\n    design = designDict[paper]\n    sample = sampleDict[paper]\n    sampleMethod  = sampleMethodDict[paper]\n    \n    val = sevDict[paper]\n    #print('in getData', val)\n    for v in val:\n        #print(v)\n        if indicator in v:\n            if severe == '':\n                severe = v.split(' : ')[1]\n            else:\n                res = any(ele in v for ele in ['OR', 'CI', 'HR'])\n                if  not res:\n                    if not 'p' in v.lower():\n                        severe = v.split(' : ')[1]\n    val = fatalDict[paper]\n    #print('in getData', val)\n    for v in val:\n        #print(v)\n        if indicator in v:\n            if fatal == '':\n                fatal = v.split(' : ')[1]\n                \n            else:\n                res = any(ele in v for ele in ['OR', 'CI', 'HR'])\n                #print(res, v)\n                if  not res:\n                    if not 'p' in v.lower():\n                        fatal = v.split(' : ')[1]\n    \n    return severe, fatal, design, sample, sampleMethod\n\ndef formatData(indicator, sentDf):\n    #sentDf = pd.read_csv(sentFie, sep='\\t' )\n    sentDf = sentDf.sort_values('Doc_score',ascending=False)\n    takenDict = dict()\n    \n    \n    #indicator = 'hypertension'\n    dfObj = pd.DataFrame(columns=fields)\n    for row in sentDf.itertuples(): \n        if float(row.Doc_score) <= 0:\n            continue\n        #pid = row.Cord_uid\n        pid = row.Cord_uid\n        #pid = pid.split('.(')[0]\n        #pid = pid + '.'\n        if pid not  in takenDict.keys() and pid in nameDict.keys():\n            #print('inside')\n            severe, fatal, design, sample, sampleMethod = getData(pid, indicator)\n            takenDict[pid] = 1\n            #snippet = row.snippet\n            journal = row.Journal\n            date = row.Date\n            url = row.Study_link\n            study = row.title\n            sevSig =''\n            fatalSig = ''\n            searchVal = ['P=', 'P<', 'P>', 'P =', 'P <', 'P >', 'p=', 'p<', 'p>', 'p =', 'p <', 'p >', 'p-value']\n            study = study + '.('+row.source+')'\n            sevExtracted = ''\n            if severe!= '' :\n                sevExtracted = 'Extracted'\n                \n                sevSig = 'Significant'\n                for sv in searchVal:\n                    match = re.search(sv+r'[0-9. ]+', severe)\n                    if match:\n                        #print(\"match \", match.group())\n                        severeM = match.group()\n                        match2 = re.search(r'[0-9.]+', severeM)\n                        if match2:\n                            num = float(match2.group())\n                            if num>=0.05 and ('>' in match.group()):\n                                #print(\"Not Significant\")\n                                sevSig =  'Not Significant'\n                                \n            fatalExtracted = ''\n            if fatal != '':\n                fatalExtracted = 'Extracted'\n                fatalSig = 'Significant'\n                for sv in searchVal:\n                    match = re.search(sv+r'[0-9. ]+', severe)\n                    if match:\n                        #print(\"match \", match.group())\n                        severeM = match.group()\n                        match2 = re.search(r'[0-9.]+', severeM)\n                        if match2:\n                            num = float(match2.group())\n                            if num>=0.05 and ('>' in match.group()):\n                                #print(\"Not Significant\")\n                                fatalSig =  'Not Significant'\n                \n            \n                \n            #dfObj = dfObj.append({ 'Date':date , 'Study': study , 'Study Link': url, 'Journal': journal , 'Severe': severe, 'Severe Significant':sevSig, 'Severe Adjusted':'' , 'Severe Calculated':sevExtracted , 'Fatality': fatal , 'Fatality Significant':fatalSig , 'Fatality Adjusted':'' , 'Fatality Calculated':fatalExtracted , 'Multivariate adjustment':'', 'Design': design, 'Sample': sample, 'Study Population': sampleMethod, 'Snippet': snippet},ignore_index=True )\n            dfObj = dfObj.append({ 'Date':date , 'Study': study , 'Study Link': url, 'Journal': journal , 'Severe': severe, 'Severe Significant':sevSig, 'Severe Adjusted':'' , 'Severe Calculated':sevExtracted , 'Fatality': fatal , 'Fatality Significant':fatalSig , 'Fatality Adjusted':'' , 'Fatality Calculated':fatalExtracted , 'Multivariate adjustment':'', 'Design': design, 'Sample': sample, 'Study Population': sampleMethod}, ignore_index=True )\n                    \n    display(HTML(dfObj[:20].to_html()))\n    dfObj.to_csv(tags[indicator])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"document_path = \"/kaggle/input/covid-19-dataset-filtering-and-sentence-extraction/Filtered_covid_documents_with_metadata.csv\"\nprint('Data from filtered document is collected')\nfiltered_covid_document_df = getKeywordLists(document_path, seperator='\\t')\nfiltered_covid_document_df = filtered_covid_document_df.drop(['Unnamed: 0'], axis=1)\ndict_tfidf_new = get_tfidf_dict(filtered_covid_document_df)\nprint('Total number of documents having COVID-19 related terms and published in 2020 are : ',len(filtered_covid_document_df))\ndoc_abstractandtext = list(filtered_covid_document_df['Abstract_and_text'])\ndoc_abstractandtext = cleandocdata(doc_abstractandtext)\nsentence_path = \"/kaggle/input/extracting-entities-from-covid-documents/Extracted_entities_from_extracted_sentences_from_filtered_covid_documents.csv\"\nsentence_df = getKeywordLists(sentence_path, seperator='\\t')\nsentence_df = sentence_df.drop(['Unnamed: 0.1'], axis=1)\nsentence_df = extract_neighbouring_info(sentence_df)\nclean_sentence_df = sentence_df.drop_duplicates(subset=['Sentence'])\nclean_sentence_df = clean_sentence_df.reset_index()\nclean_sentence_df = clean_sentence_df.drop(['index', 'Unnamed: 0'], axis=1)\nprint('appending list of words of sentence in dataframe ')\nclean_sentence_df = adding_list_of_words(clean_sentence_df)\nprint('loading design and sample data ')\ndataFile = '/kaggle/input/cord19-obtaining-design-and-sampling-information/extracted_DocData.csv'\nsevDict, fatalDict, nameDict, designDict, sampleDict, sampleMethodDict = loadFromCsv()\nprint('design and sample data is loaded')\nfields = ['Date', 'Study', 'Study Link', 'Journal', 'Severe', 'Severe Significant', 'Severe Adjusted', 'Severe Calculated', 'Fatality', 'Fatality Significant', 'Fatality Adjusted', 'Fatality Calculated', 'Multivariate adjustment', 'Design', 'Sample', 'Study Population']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What do we know about COVID-19 risk factors?"},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = ['covid-19 risk factors']\nindicator1 = 'risk_factor'\nphrase_words = gettingphrasewords(phrases)\nidf_dict = getting_phrase_word_dict_with_idf_value(doc_abstractandtext, phrase_words)\nclean_sentence_df1 = phrasematching(phrases[0], idf_dict, clean_sentence_df)\nclean_sentence_df1= clean_sentence_df1.reset_index()\nclean_sentence_df1= clean_sentence_df1.drop(['index'], axis=1)\nfile_name = '_'.join(phrases[0].split())\nclean_sentence_df1 = rescoring(clean_sentence_df1, dict_tfidf_new, file_name, myDict[phrases[0]])\nfiltered_covid_document_df1 = finding_relevant_document(clean_sentence_df1, filtered_covid_document_df)\nformatData(indicator1, filtered_covid_document_df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cancer"},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = ['cancer covid-19']\nindicator1 = 'cancer'\nphrase_words = gettingphrasewords(phrases)\nidf_dict = getting_phrase_word_dict_with_idf_value(doc_abstractandtext, phrase_words)\nclean_sentence_df1 = phrasematching(phrases[0], idf_dict, clean_sentence_df)\nclean_sentence_df1= clean_sentence_df1.reset_index()\nclean_sentence_df1= clean_sentence_df1.drop(['index'], axis=1)\nfile_name = '_'.join(phrases[0].split())\nclean_sentence_df1 = rescoring(clean_sentence_df1, dict_tfidf_new, file_name, myDict[phrases[0]])\nfiltered_covid_document_df1 = finding_relevant_document(clean_sentence_df1, filtered_covid_document_df)\nformatData(indicator1, filtered_covid_document_df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hypertension"},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = ['hypertension covid-19']\nindicator1 = 'hypertension'\nphrase_words = gettingphrasewords(phrases)\nidf_dict = getting_phrase_word_dict_with_idf_value(doc_abstractandtext, phrase_words)\nclean_sentence_df1 = phrasematching(phrases[0], idf_dict, clean_sentence_df)\nclean_sentence_df1= clean_sentence_df1.reset_index()\nclean_sentence_df1= clean_sentence_df1.drop(['index'], axis=1)\nfile_name = '_'.join(phrases[0].split())\nclean_sentence_df1 = rescoring(clean_sentence_df1, dict_tfidf_new, indicator1, myDict[phrases[0]])\nfiltered_covid_document_df1 = finding_relevant_document(clean_sentence_df1, filtered_covid_document_df)\nformatData(indicator1, filtered_covid_document_df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heart disease"},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = ['heart disease covid-19']\nindicator1 = 'heart'\nphrase_words = gettingphrasewords(phrases)\nidf_dict = getting_phrase_word_dict_with_idf_value(doc_abstractandtext, phrase_words)\nclean_sentence_df1 = phrasematching(phrases[0], idf_dict, clean_sentence_df)\nclean_sentence_df1= clean_sentence_df1.reset_index()\nclean_sentence_df1= clean_sentence_df1.drop(['index'], axis=1)\nfile_name = '_'.join(phrases[0].split())\nclean_sentence_df1 = rescoring(clean_sentence_df1, dict_tfidf_new, file_name, myDict[phrases[0]])\nfiltered_covid_document_df1 = finding_relevant_document(clean_sentence_df1, filtered_covid_document_df)\nformatData(indicator1, filtered_covid_document_df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Smoking"},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = ['smoking covid-19']\nindicator1 = 'smoke'\nphrase_words = gettingphrasewords(phrases)\nidf_dict = getting_phrase_word_dict_with_idf_value(doc_abstractandtext, phrase_words)\nclean_sentence_df1 = phrasematching(phrases[0], idf_dict, clean_sentence_df)\nclean_sentence_df1= clean_sentence_df1.reset_index()\nclean_sentence_df1= clean_sentence_df1.drop(['index'], axis=1)\nfile_name = '_'.join(phrases[0].split())\nclean_sentence_df1 = rescoring(clean_sentence_df1, dict_tfidf_new, file_name, myDict[phrases[0]])\nfiltered_covid_document_df1 = finding_relevant_document(clean_sentence_df1, filtered_covid_document_df)\nformatData(indicator1, filtered_covid_document_df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Diabetes"},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = ['diabetes covid-19']\nindicator1 = 'diabetes'\nphrase_words = gettingphrasewords(phrases)\nidf_dict = getting_phrase_word_dict_with_idf_value(doc_abstractandtext, phrase_words)\nclean_sentence_df1 = phrasematching(phrases[0], idf_dict, clean_sentence_df)\nclean_sentence_df1= clean_sentence_df1.reset_index()\nclean_sentence_df1= clean_sentence_df1.drop(['index'], axis=1)\nfile_name = '_'.join(phrases[0].split())\nclean_sentence_df1 = rescoring(clean_sentence_df1, dict_tfidf_new, indicator1, myDict[phrases[0]])\nfiltered_covid_document_df1 = finding_relevant_document(clean_sentence_df1, filtered_covid_document_df)\nformatData(indicator1, filtered_covid_document_df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pulmonary Disease"},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = ['pulmonary disease covid-19']\nindicator1 = 'pulmonary'\nphrase_words = gettingphrasewords(phrases)\nidf_dict = getting_phrase_word_dict_with_idf_value(doc_abstractandtext, phrase_words)\nclean_sentence_df1 = phrasematching(phrases[0], idf_dict, clean_sentence_df)\nclean_sentence_df1= clean_sentence_df1.reset_index()\nclean_sentence_df1= clean_sentence_df1.drop(['index'], axis=1)\nfile_name = '_'.join(phrases[0].split())\nclean_sentence_df1 = rescoring(clean_sentence_df1, dict_tfidf_new, indicator1, myDict[phrases[0]])\nfiltered_covid_document_df1 = finding_relevant_document(clean_sentence_df1, filtered_covid_document_df)\nformatData(indicator1, filtered_covid_document_df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neonates and Pregnant Women"},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = ['risk factors for neonates and pregnant women']\nindicator1 = 'pregnant'\nphrase_words = gettingphrasewords(phrases)\nidf_dict = getting_phrase_word_dict_with_idf_value(doc_abstractandtext, phrase_words)\nclean_sentence_df1 = phrasematching(phrases[0], idf_dict, clean_sentence_df)\nclean_sentence_df1= clean_sentence_df1.reset_index()\nclean_sentence_df1= clean_sentence_df1.drop(['index'], axis=1)\nfile_name = '_'.join(phrases[0].split())\nclean_sentence_df1 = rescoring(clean_sentence_df1, dict_tfidf_new, file_name, myDict[phrases[0]])\nfiltered_covid_document_df1 = finding_relevant_document(clean_sentence_df1, filtered_covid_document_df)\nformatData(indicator1, filtered_covid_document_df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Respiratory disease"},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = ['respiratory disease covid-19']\nindicator1 = 'respiratory'\nphrase_words = gettingphrasewords(phrases)\nidf_dict = getting_phrase_word_dict_with_idf_value(doc_abstractandtext, phrase_words)\nclean_sentence_df1 = phrasematching(phrases[0], idf_dict, clean_sentence_df)\nclean_sentence_df1= clean_sentence_df1.reset_index()\nclean_sentence_df1= clean_sentence_df1.drop(['index'], axis=1)\nfile_name = '_'.join(phrases[0].split())\nclean_sentence_df1 = rescoring(clean_sentence_df1, dict_tfidf_new, indicator1, myDict[phrases[0]])\nfiltered_covid_document_df1 = finding_relevant_document(clean_sentence_df1, filtered_covid_document_df)\nformatData(indicator1, filtered_covid_document_df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Co-infections"},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = ['co-infections risk covid-19']\nindicator1 = 'coinfection'\nphrase_words = gettingphrasewords(phrases)\nidf_dict = getting_phrase_word_dict_with_idf_value(doc_abstractandtext, phrase_words)\nclean_sentence_df1 = phrasematching(phrases[0], idf_dict, clean_sentence_df)\nclean_sentence_df1= clean_sentence_df1.reset_index()\nclean_sentence_df1= clean_sentence_df1.drop(['index'], axis=1)\nfile_name = '_'.join(phrases[0].split())\nclean_sentence_df1 = rescoring(clean_sentence_df1, dict_tfidf_new, indicator1, myDict[phrases[0]])\nfiltered_covid_document_df1 = finding_relevant_document(clean_sentence_df1, filtered_covid_document_df)\nformatData(indicator1, filtered_covid_document_df1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}